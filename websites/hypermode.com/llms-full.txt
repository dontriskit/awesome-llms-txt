# Design
Source: https://docs.hypermode.com/badger/design

Architected for fast key-value storage in Go

We wrote Badger with these design goals in mind:

* Write a key-value database in pure Go
* Use latest research to build the fastest KV database for data sets spanning
  terabytes
* Optimize for modern storage devices

Badger’s design is based on a paper titled
[WiscKey: Separating Keys from Values in SSD-conscious Storage](https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf).

## References

The following blog posts are a great starting point for learning more about
Badger and the underlying design principles:

* [Introducing Badger: A fast key-value store written natively in Go](https://hypermode.com/blog/badger/)
* [Make Badger crash resilient with ALICE](https://hypermode.com/blog/alice/)
* [Badger vs LMDB vs BoltDB: Benchmarking key-value databases in Go](https://hypermode.com/blog/badger-lmdb-boltdb/)
* [Concurrent ACID Transactions in Badger](https://hypermode.com/blog/badger-txn/)

## Comparisons

| Feature                       | Badger                                 | RocksDB                      | BoltDB  |
| ----------------------------- | -------------------------------------- | ---------------------------- | ------- |
| Design                        | LSM tree with value log                | LSM tree only                | B+ tree |
| High Read throughput          | Yes                                    | No                           | Yes     |
| High Write throughput         | Yes                                    | Yes                          | No      |
| Designed for SSDs             | Yes (with latest research<sup>1</sup>) | Not specifically<sup>2</sup> | No      |
| Embeddable                    | Yes                                    | Yes                          | Yes     |
| Sorted KV access              | Yes                                    | Yes                          | Yes     |
| Pure Go (no Cgo)              | Yes                                    | No                           | Yes     |
| Transactions                  | Yes                                    | Yes                          | Yes     |
| ACID-compliant                | Yes, concurrent with SSI<sup>3</sup>   | No                           | Yes     |
| Snapshots                     | Yes                                    | Yes                          | Yes     |
| TTL support                   | Yes                                    | Yes                          | No      |
| 3D access (key-value-version) | Yes<sup>4</sup>                        | No                           | No      |

<sup>1</sup> The WiscKey paper (on which Badger is based) saw big wins with
separating values from keys, significantly reducing the write amplification
compared to a typical LSM tree.

<sup>2</sup> RocksDB is an SSD-optimized version of LevelDB, which was designed
specifically for rotating disks. As such RocksDB's design isn't aimed at SSDs.

<sup>3</sup> SSI: Serializable Snapshot Isolation. For more details, see the
blog post [Concurrent ACID Transactions in
Badger](https://hypermode.com/blog/badger-txn/)

<sup>4</sup> Badger provides direct access to value versions via its Iterator
API. Users can also specify how many versions to keep per key via Options.

## Benchmarks

We've run comprehensive benchmarks against RocksDB, BoltDB, and LMDB. The
benchmarking code with detailed logs are in the
[badger-bench](https://github.com/dgraph-io/badger-bench) repo.


# Badger
Source: https://docs.hypermode.com/badger/overview

Welcome to the Badger docs!

## What is Badger? {/* vale Google.Contractions = NO */}

BadgerDB is an embeddable, persistent, and fast key-value (KV) database written
in pure Go. It is the underlying database for [Dgraph](/dgraph/overview), a
fast, distributed graph database. It is meant to be an efficient alternative to
non-Go-based key-value stores like RocksDB.

## Changelog

We keep the
[repo Changelog](https://github.com/hypermodeinc/badger/blob/main/CHANGELOG.md)
up to date with each release.


# Quickstart
Source: https://docs.hypermode.com/badger/quickstart

Everything you need to get started with Badger

## Prerequisites

* [Go](https://go.dev/doc/install) - v1.23 or higher
* Text editor - we recommend [VS Code](https://code.visualstudio.com/)
* Terminal - access Badger through a command-line interface (CLI)

## Installing

To start using Badger, run the following command to retrieve the library.

```sh
go get github.com/dgraph-io/badger/v4
```

Then, install the Badger command line utility into your `$GOBIN` path.

```sh
go install github.com/dgraph-io/badger/v4/badger@latest
```

## Opening a database

The top-level object in Badger is a `DB`. It represents multiple files on disk
in specific directories, which contain the data for a single database.

To open your database, use the `badger.Open()` function, with the appropriate
options. The `Dir` and `ValueDir` options are mandatory and you must specify
them in your client. To simplify, you can set both options to the same value.

<Note>
  Badger obtains a lock on the directories. Multiple processes can't open the
  same database at the same time.
</Note>

```go
package main

import (
  "log"

  badger "github.com/dgraph-io/badger/v4"
)

func main() {
  // Open the Badger database located in the /tmp/badger directory.
  // It is created if it doesn't exist.
  db, err := badger.Open(badger.DefaultOptions("/tmp/badger"))
  if err != nil {
    log.Fatal(err)
  }

  defer db.Close()

  // your code here
}
```

### In-memory/diskless mode

By default, Badger ensures all data persists to disk. It also supports a pure
in-memory mode. When Badger is running in this mode, all data remains in memory
only. Reads and writes are much faster, but Badger loses all stored data in the
case of a crash or close. To open Badger in in-memory mode, set the `InMemory`
option.

```go
opt := badger.DefaultOptions("").WithInMemory(true)
```

### Encryption mode

If you enable encryption in Badger, you also need to set the index cache size.

<Tip>
  The cache improves the performance. Otherwise, reads can be very slow with
  encryption enabled.
</Tip>

For example, to set a `100 Mb` cache:

```go
opts.IndexCache = 100 << 20 // 100 mb or some other size based on the amount of data
```

## Transactions

### Read-only transactions

To start a read-only transaction, you can use the `DB.View()` method:

```go
err := db.View(func(txn *badger.Txn) error {
  // your code here

  return nil
})
```

You can't perform any writes or deletes within this transaction. Badger ensures
that you get a consistent view of the database within this closure. Any writes
that happen elsewhere after the transaction has started aren't seen by calls
made within the closure.

### Read-write transactions

To start a read-write transaction, you can use the `DB.Update()` method:

```go
err := db.Update(func(txn *badger.Txn) error {
  // Your code here…
  return nil
})
```

Badger allows all database operations inside a read-write transaction.

Always check the returned error value. If you return an error within your
closure it's passed through.

An `ErrConflict` error is reported in case of a conflict. Depending on the state
of your app, you have the option to retry the operation if you receive this
error.

An `ErrTxnTooBig` is reported in case the number of pending writes/deletes in
the transaction exceeds a certain limit. In that case, it's best to commit the
transaction and start a new transaction immediately. Here is an example (we
aren't checking for errors in some places for simplicity):

```go
updates := make(map[string]string)
txn := db.NewTransaction(true)
for k,v := range updates {
  if err := txn.Set([]byte(k),[]byte(v)); err == badger.ErrTxnTooBig {
    _ = txn.Commit()
    txn = db.NewTransaction(true)
    _ = txn.Set([]byte(k),[]byte(v))
  }
}
_ = txn.Commit()
```

### Managing transactions manually

The `DB.View()` and `DB.Update()` methods are wrappers around the
`DB.NewTransaction()` and `Txn.Commit()` methods (or `Txn.Discard()` in case of
read-only transactions). These helper methods start the transaction, execute a
function, and then safely discard your transaction if an error is returned. This
is the recommended way to use Badger transactions.

However, sometimes you may want to manually create and commit your transactions.
You can use the `DB.NewTransaction()` function directly, which takes in a
boolean argument to specify whether a read-write transaction is required. For
read-write transactions, it's necessary to call `Txn.Commit()` to ensure the
transaction is committed. For read-only transactions, calling `Txn.Discard()` is
sufficient. `Txn.Commit()` also calls `Txn.Discard()` internally to cleanup the
transaction, so just calling `Txn.Commit()` is sufficient for read-write
transaction. However, if your code doesn’t call `Txn.Commit()` for some reason
(for e.g it returns prematurely with an error), then please make sure you call
`Txn.Discard()` in a `defer` block. Refer to the code below.

```go
// Start a writable transaction.
txn := db.NewTransaction(true)
defer txn.Discard()

// Use the transaction...
err := txn.Set([]byte("answer"), []byte("42"))
if err != nil {
    return err
}

// Commit the transaction and check for error.
if err := txn.Commit(); err != nil {
    return err
}
```

The first argument to `DB.NewTransaction()` is a boolean stating if the
transaction should be writable.

Badger allows an optional callback to the `Txn.Commit()` method. Normally, the
callback can be set to `nil`, and the method returns after all the writes have
succeeded. However, if this callback is provided, the `Txn.Commit()` method
returns as soon as it has checked for any conflicts. The actual writing to the
disk happens asynchronously, and the callback is invoked once the writing has
finished, or an error has occurred. This can improve the throughput of the app
in some cases. But it also means that a transaction isn't durable until the
callback has been invoked with a `nil` error value.

## Using key/value pairs

To save a key/value pair, use the `Txn.Set()` method:

```go
err := db.Update(func(txn *badger.Txn) error {
  err := txn.Set([]byte("answer"), []byte("42"))
  return err
})
```

Key/Value pair can also be saved by first creating `Entry`, then setting this
`Entry` using `Txn.SetEntry()`. `Entry` also exposes methods to set properties
on it.

```go
err := db.Update(func(txn *badger.Txn) error {
  e := badger.NewEntry([]byte("answer"), []byte("42"))
  err := txn.SetEntry(e)
  return err
})
```

This sets the value of the `"answer"` key to `"42"`. To retrieve this value, we
can use the `Txn.Get()` method:

```go
err := db.View(func(txn *badger.Txn) error {
  item, err := txn.Get([]byte("answer"))
  handle(err)

  var valNot, valCopy []byte
  err := item.Value(func(val []byte) error {
    // This func with val would only be called if item.Value encounters no error.

    // Accessing val here is valid.
    fmt.Printf("The answer is: %s\n", val)

    // Copying or parsing val is valid.
    valCopy = append([]byte{}, val...)

    // Assigning val slice to another variable is NOT OK.
    valNot = val // Do not do this.
    return nil
  })
  handle(err)

  // DO NOT access val here. It is the most common cause of bugs.
  fmt.Printf("NEVER do this. %s\n", valNot)

  // You must copy it to use it outside item.Value(...).
  fmt.Printf("The answer is: %s\n", valCopy)

  // Alternatively, you could also use item.ValueCopy().
  valCopy, err = item.ValueCopy(nil)
  handle(err)
  fmt.Printf("The answer is: %s\n", valCopy)

  return nil
})
```

`Txn.Get()` returns `ErrKeyNotFound` if the value isn't found.

Please note that values returned from `Get()` are only valid while the
transaction is open. If you need to use a value outside of the transaction then
you must use `copy()` to copy it to another byte slice.

Use the `Txn.Delete()` method to delete a key.

## Monotonically increasing integers

To get unique monotonically increasing integers with strong durability, you can
use the `DB.GetSequence` method. This method returns a `Sequence` object, which
is thread-safe and can be used concurrently via various goroutines.

Badger would lease a range of integers to hand out from memory, with the
bandwidth provided to `DB.GetSequence`. The frequency at which disk writes are
done is determined by this lease bandwidth and the frequency of `Next`
invocations. Setting a bandwidth too low would do more disk writes, setting it
too high would result in wasted integers if Badger is closed or crashes. To
avoid wasted integers, call `Release` before closing Badger.

```go
seq, err := db.GetSequence(key, 1000)
defer seq.Release()
for {
  num, err := seq.Next()
}
```

## Merge operations

Badger provides support for ordered merge operations. You can define a func of
type `MergeFunc` which takes in an existing value, and a value to be *merged*
with it. It returns a new value which is the result of the merge operation. All
values are specified in byte arrays. For example, this is a merge function
(`add`) which appends a `[]byte` value to an existing `[]byte` value.

```go
// Merge function to append one byte slice to another
func add(originalValue, newValue []byte) []byte {
  return append(originalValue, newValue...)
}
```

This function can then be passed to the `DB.GetMergeOperator()` method, along
with a key, and a duration value. The duration specifies how often the merge
function is run on values that have been added using the `MergeOperator.Add()`
method.

`MergeOperator.Get()` method can be used to retrieve the cumulative value of the
key associated with the merge operation.

```go
key := []byte("merge")

m := db.GetMergeOperator(key, add, 200*time.Millisecond)
defer m.Stop()

m.Add([]byte("A"))
m.Add([]byte("B"))
m.Add([]byte("C"))

res, _ := m.Get() // res should have value ABC encoded
```

Example: merge operator which increments a counter

```go
func uint64ToBytes(i uint64) []byte {
  var buf [8]byte
  binary.BigEndian.PutUint64(buf[:], i)
  return buf[:]
}

func bytesToUint64(b []byte) uint64 {
  return binary.BigEndian.Uint64(b)
}

// Merge function to add two uint64 numbers
func add(existing, new []byte) []byte {
  return uint64ToBytes(bytesToUint64(existing) + bytesToUint64(new))
}
```

It can be used as

```go
key := []byte("merge")

m := db.GetMergeOperator(key, add, 200*time.Millisecond)
defer m.Stop()

m.Add(uint64ToBytes(1))
m.Add(uint64ToBytes(2))
m.Add(uint64ToBytes(3))

res, _ := m.Get() // res should have value 6 encoded
```

## Setting time to live and user metadata on keys

Badger allows setting an optional Time to Live (TTL) value on keys. Once the TTL
has elapsed, the key is no longer retrievable and is eligible for garbage
collection. A TTL can be set as a `time.Duration` value using the
`Entry.WithTTL()` and `Txn.SetEntry()` API methods.

```go
err := db.Update(func(txn *badger.Txn) error {
  e := badger.NewEntry([]byte("answer"), []byte("42")).WithTTL(time.Hour)
  err := txn.SetEntry(e)
  return err
})
```

An optional user metadata value can be set on each key. A user metadata value is
represented by a single byte. It can be used to set certain bits along with the
key to aid in interpreting or decoding the key-value pair. User metadata can be
set using `Entry.WithMeta()` and `Txn.SetEntry()` API methods.

```go
err := db.Update(func(txn *badger.Txn) error {
  e := badger.NewEntry([]byte("answer"), []byte("42")).WithMeta(byte(1))
  err := txn.SetEntry(e)
  return err
})
```

`Entry` APIs can be used to add the user metadata and TTL for same key. This
`Entry` then can be set using `Txn.SetEntry()`.

```go
err := db.Update(func(txn *badger.Txn) error {
  e := badger.NewEntry([]byte("answer"), []byte("42")).WithMeta(byte(1)).WithTTL(time.Hour)
  err := txn.SetEntry(e)
  return err
})
```

## Iterating over keys

To iterate over keys, we can use an `Iterator`, which can be obtained using the
`Txn.NewIterator()` method. Iteration happens in byte-wise lexicographical
sorting order.

```go
err := db.View(func(txn *badger.Txn) error {
  opts := badger.DefaultIteratorOptions
  opts.PrefetchSize = 10
  it := txn.NewIterator(opts)
  defer it.Close()
  for it.Rewind(); it.Valid(); it.Next() {
    item := it.Item()
    k := item.Key()
    err := item.Value(func(v []byte) error {
      fmt.Printf("key=%s, value=%s\n", k, v)
      return nil
    })
    if err != nil {
      return err
    }
  }
  return nil
})
```

The iterator allows you to move to a specific point in the list of keys and move
forward or backward through the keys one at a time.

By default, Badger prefetches the values of the next 100 items. You can adjust
that with the `IteratorOptions.PrefetchSize` field. However, setting it to a
value higher than `GOMAXPROCS` (which we recommend to be 128 or higher)
shouldn’t give any additional benefits. You can also turn off the fetching of
values altogether. See section below on key-only iteration.

### Prefix scans

To iterate over a key prefix, you can combine `Seek()` and `ValidForPrefix()`:

```go
db.View(func(txn *badger.Txn) error {
  it := txn.NewIterator(badger.DefaultIteratorOptions)
  defer it.Close()
  prefix := []byte("1234")
  for it.Seek(prefix); it.ValidForPrefix(prefix); it.Next() {
    item := it.Item()
    k := item.Key()
    err := item.Value(func(v []byte) error {
      fmt.Printf("key=%s, value=%s\n", k, v)
      return nil
    })
    if err != nil {
      return err
    }
  }
  return nil
})
```

### Possible pagination implementation using Prefix scans

Considering that iteration happens in **byte-wise lexicographical sorting**
order, it's possible to create a sorting-sensitive key. For example, a simple
blog post key might look like:`feed:userUuid:timestamp:postUuid`. Here, the
`timestamp` part of the key is treated as an attribute, and items are stored in
the corresponding order:

| Order Ascending | Key                                                           |
| :-------------: | :------------------------------------------------------------ |
|        1        | feed:tQpnEDVRoCxTFQDvyQEzdo:1733127889:tQpnEDVRoCxTFQDvyQEzdo |
|        2        | feed:tQpnEDVRoCxTFQDvyQEzdo:1733127533:1Mryrou1xoekEaxzrFiHwL |
|        3        | feed:tQpnEDVRoCxTFQDvyQEzdo:1733127486:pprRrNL2WP4yfVXsSNBSx6 |

It is important to properly configure keys for lexicographical sorting to avoid
incorrect ordering.

A **prefix scan** through the preceding keys can be achieved using the prefix
`feed:tQpnEDVRoCxTFQDvyQEzdo`. All matching keys are returned, sorted by
`timestamp`.\
Sorting can be done in ascending or descending order based on `timestamp` or
`reversed timestamp` as needed:

```go
reversedTimestamp := math.MaxInt64-time.Now().Unix()
```

This makes it possible to implement simple pagination by using a limit for the
number of keys and a cursor (the last key from the previous iteration) to
identify where to resume.

```go
// startCursor may look like 'feed:tQpnEDVRoCxTFQDvyQEzdo:1733127486'.
// A prefix scan with this cursor locates the specific key where
// the previous iteration stopped.
err = db.badger.View(func(txn *badger.Txn) error {
        it := txn.NewIterator(opts)
        defer it.Close()

        // Prefix example 'feed:tQpnEDVRoCxTFQDvyQEzdo'
        // if no cursor provided prefix scan starts from the beginning
        p := prefix
        if startCursor != nil {
             p = startCursor
        }
        iterNum := 0 // Tracks the number of iterations to enforce the limit.
        for it.Seek(p); it.ValidForPrefix(p); it.Next() {
            // The method it.ValidForPrefix ensures that iteration continues
            // as long as keys match the prefix.
            // For example, if p = 'feed:tQpnEDVRoCxTFQDvyQEzdo:1733127486',
            // it matches keys like
            // 'feed:tQpnEDVRoCxTFQDvyQEzdo:1733127889:pprRrNL2WP4yfVXsSNBSx6'.

            // Once the starting point for iteration is found, revert the prefix
            // back to 'feed:tQpnEDVRoCxTFQDvyQEzdo' to continue iterating sequentially.
            // Otherwise, iteration would stop after a single prefix-key match.
            p = prefix

            item := it.Item()
            key := string(item.Key())

            if iterNum > limit { // Limit reached.
                nextCursor = key // Save the next cursor for future iterations.
                return nil
            }
            iterNum++ // Increment iteration count.

            err := item.Value(func(v []byte) error {
                fmt.Printf("key=%s, value=%s\n", k, v)
                return nil
            })
            if err != nil {
                return err
            }
        }
        // If the number of iterations is less than the limit,
        // it means there are no more items for the prefix.
        if iterNum < limit {
            nextCursor = ""
        }
        return nil
    })
return nextCursor, err
```

### Key-only iteration

Badger supports a unique mode of iteration called *key-only* iteration. It is
several order of magnitudes faster than regular iteration, because it involves
access to the Log-structured merge (LSM)-tree only, which is usually resident
entirely in RAM. To enable key-only iteration, you need to set the
`IteratorOptions.PrefetchValues` field to `false`. This can also be used to do
sparse reads for selected keys during an iteration, by calling `item.Value()`
only when required.

```go
err := db.View(func(txn *badger.Txn) error {
  opts := badger.DefaultIteratorOptions
  opts.PrefetchValues = false
  it := txn.NewIterator(opts)
  defer it.Close()
  for it.Rewind(); it.Valid(); it.Next() {
    item := it.Item()
    k := item.Key()
    fmt.Printf("key=%s\n", k)
  }
  return nil
})
```

## Stream

Badger provides a Stream framework, which concurrently iterates over all or a
portion of the DB, converting data into custom key-values, and streams it out
serially to be sent over network, written to disk, or even written back to
Badger. This is a lot faster way to iterate over Badger than using a single
Iterator. Stream supports Badger in both managed and normal mode.

Stream uses the natural boundaries created by SSTables within the Log-structure
merge (LSM)-tree, to quickly generate key ranges. Each goroutine then picks a
range and runs an iterator to iterate over it. Each iterator iterates over all
versions of values and is created from the same transaction, thus working over a
snapshot of the DB. Every time a new key is encountered, it calls
`ChooseKey(item)`, followed by `KeyToList(key, itr)`. This allows a user to
select or reject that key, and if selected, convert the value versions into
custom key-values. The goroutine batches up 4 MB worth of key-values, before
sending it over to a channel. Another goroutine further batches up data from
this channel using *smart batching* algorithm and calls `Send` serially.

This framework is designed for high throughput key-value iteration, spreading
the work of iteration across many goroutines. `DB.Backup` uses this framework to
provide full and incremental backups quickly. Dgraph is a heavy user of this
framework. In fact, this framework was developed and used within Dgraph, before
getting ported over to Badger.

```go
stream := db.NewStream()
// db.NewStreamAt(readTs) for managed mode.

// -- Optional settings
stream.NumGo = 16                     // Set number of goroutines to use for iteration.
stream.Prefix = []byte("some-prefix") // Leave nil for iteration over the whole DB.
stream.LogPrefix = "Badger.Streaming" // For identifying stream logs. Outputs to Logger.

// ChooseKey is called concurrently for every key. If left nil, assumes true by default.
stream.ChooseKey = func(item *badger.Item) bool {
  return bytes.HasSuffix(item.Key(), []byte("er"))
}

// KeyToList is called concurrently for chosen keys. This can be used to convert
// Badger data into custom key-values. If nil, uses stream.ToList, a default
// implementation, which picks all valid key-values.
stream.KeyToList = nil

// -- End of optional settings.

// Send is called serially, while Stream.Orchestrate is running.
stream.Send = func(list *pb.KVList) error {
  return proto.MarshalText(w, list) // Write to w.
}

// Run the stream
if err := stream.Orchestrate(context.Background()); err != nil {
  return err
}
// Done.
```

## Garbage collection

Badger values need to be garbage collected, because of two reasons:

* Badger keeps values separately from the Log-structure merge (LSM)-tree. This
  means that the compaction operations that clean up the LSM tree do not touch
  the values at all. Values need to be cleaned up separately.

* Concurrent read/write transactions could leave behind multiple values for a
  single key, because they're stored with different versions. These could
  accumulate, and take up unneeded space beyond the time these older versions
  are needed.

Badger relies on the client to perform garbage collection at a time of their
choosing. It provides the following method, which can be invoked at an
appropriate time:

* `DB.RunValueLogGC()`: This method is designed to do garbage collection while
  Badger is online. Along with randomly picking a file, it uses statistics
  generated by the LSM tree compactions to pick files that are likely to lead to
  maximum space reclamation. It is recommended to be called during periods of
  low activity in your system, or periodically. One call would only result in
  removal of at max one log file. As an optimization, you could also immediately
  re-run it whenever it returns nil error (indicating a successful value log
  GC), as shown below.

  ```go
  ticker := time.NewTicker(5 * time.Minute)
  defer ticker.Stop()
  for range ticker.C {
  again:
    err := db.RunValueLogGC(0.7)
    if err == nil {
      goto again
    }
  }
  ```

* `DB.PurgeOlderVersions()`: This method is **DEPRECATED** since v1.5.0. Now,
  Badger's LSM tree automatically discards older/invalid versions of keys.

<Note>
  The `RunValueLogGC` method would not garbage collect the latest value log.
</Note>

## Database backup

There are two public API methods `DB.Backup()` and `DB.Load()` which can be used
to do online backups and restores. Badger v0.9 provides a CLI tool `badger`,
which can do offline backup/restore. Make sure you have `$GOPATH/bin` in your
PATH to use this tool.

The command below creates a version-agnostic backup of the database, to a file
`badger.bak` in the current working directory

```sh
badger backup --dir <path/to/badgerdb>
```

To restore `badger.bak` in the current working directory to a new database:

```sh
badger restore --dir <path/to/badgerdb>
```

See `badger --help` for more details.

If you have a Badger database that was created using v0.8 (or below), you can
use the `badger_backup` tool provided in v0.8.1, and then restore it using the
preceding command to upgrade your database to work with the latest version.

```sh
badger_backup --dir <path/to/badgerdb> --backup-file badger.bak
```

We recommend all users to use the `Backup` and `Restore` APIs and tools.
However, Badger is also rsync-friendly because all files are immutable, barring
the latest value log which is append-only. So, rsync can be used as rudimentary
way to perform a backup. In the following script, we repeat rsync to ensure that
the LSM tree remains consistent with the MANIFEST file while doing a full
backup.

```sh
#!/bin/bash
set -o history
set -o histexpand
# Makes a complete copy of a Badger database directory.
# Repeat rsync if the MANIFEST and SSTables are updated.
rsync -avz --delete db/ dst
while !! | grep -q "(MANIFEST\|\.sst)$"; do :; done
```

## Memory usage

Badger's memory usage can be managed by tweaking several options available in
the `Options` struct that's passed in when opening the database using `DB.Open`.

* Number of memtables (`Options.NumMemtables`)
  * If you modify `Options.NumMemtables`, also adjust
    `Options.NumLevelZeroTables` and `Options.NumLevelZeroTablesStall`
    accordingly.
* Number of concurrent compactions (`Options.NumCompactors`)
* Size of table (`Options.BaseTableSize`)
* Size of value log file (`Options.ValueLogFileSize`)

If you want to decrease the memory usage of Badger instance, tweak these options
(ideally one at a time) until you achieve the desired memory usage.


# Troubleshooting
Source: https://docs.hypermode.com/badger/troubleshooting

Common issues and solutions with Badger

## Writes are getting stuck

**Update: with the new `Value(func(v []byte))` API, this deadlock can no longer
happen.**

The following is true for users on Badger v1.x.

This can happen if a long running iteration with `Prefetch` is set to false, but
an `Item::Value` call is made internally in the loop. That causes Badger to
acquire read locks over the value log files to avoid value log GC removing the
file from underneath. As a side effect, this also blocks a new value log GC file
from being created, when the value log file boundary is hit.

Please see GitHub issues
[#293](https://github.com/hypermodeinc/badger/issues/293) and
[#315](https://github.com/hypermodeinc/badger/issues/315).

There are multiple workarounds during iteration:

1. Use `Item::ValueCopy` instead of `Item::Value` when retrieving value.
2. Set `Prefetch` to true. Badger would then copy over the value and release the
   file lock immediately.
3. When `Prefetch` is false, don't call `Item::Value` and do a pure key-only
   iteration. This might be useful if you just want to delete a lot of keys.
4. Do the writes in a separate transaction after the reads.

## Writes are really slow

Are you creating a new transaction for every single key update, and waiting for
it to `Commit` fully before creating a new one? This leads to very low
throughput.

We've created `WriteBatch` API which provides a way to batch up many updates
into a single transaction and `Commit` that transaction using callbacks to avoid
blocking. This amortizes the cost of a transaction really well, and provides the
most efficient way to do bulk writes.

```go
wb := db.NewWriteBatch()
defer wb.Cancel()

for i := 0; i < N; i++ {
  err := wb.Set(key(i), value(i), 0) // Will create txns as needed.
  handle(err)
}
handle(wb.Flush()) // Wait for all txns to finish.
```

Note that `WriteBatch` API doesn't allow any reads. For read-modify-write
workloads, you should be using the `Transaction` API.

## I don't see any disk writes

If you're using Badger with `SyncWrites=false`, then your writes might not be
written to value log and won't get synced to disk immediately. Writes to LSM
tree are done in-memory first, before they get compacted to disk. The compaction
would only happen once `BaseTableSize` has been reached. So, if you're doing a
few writes and then checking, you might not see anything on disk. Once you
`Close` the database, you'll see these writes on disk.

## Reverse iteration doesn't produce the right results

Just like forward iteration goes to the first key which is equal or greater than
the SEEK key, reverse iteration goes to the first key which is equal or lesser
than the SEEK key. Therefore, SEEK key would not be part of the results. You can
typically add a `0xff` byte as a suffix to the SEEK key to include it in the
results. See the following issues:
[#436](https://github.com/hypermodeinc/badger/issues/436) and
[#347](https://github.com/hypermodeinc/badger/issues/347).

## Which instances should I use for Badger?

We recommend using instances which provide local SSD storage, without any limit
on the maximum IOPS. In AWS, these are storage optimized instances like i3. They
provide local SSDs which clock 100K IOPS over 4KB blocks easily.

## I'm getting a closed channel error

```sh
panic: close of closed channel
panic: send on closed channel
```

If you're seeing panics like this, it is because you're operating on a closed
DB. This can happen, if you call `Close()` before sending a write, or multiple
times. You should ensure that you only call `Close()` once, and all your
read/write operations finish before closing.

## Are there any Go specific settings that I should use?

We *highly* recommend setting a high number for `GOMAXPROCS`, which allows Go to
observe the full IOPS throughput provided by modern SSDs. In Dgraph, we have set
it to 128. For more details,
[see this thread](https://groups.google.com/d/topic/golang-nuts/jPb_h3TvlKE/discussion).

## Are there any Linux specific settings that I should use?

We recommend setting `max file descriptors` to a high number depending upon the
expected size of your data. On Linux and Mac, you can check the file descriptor
limit with `ulimit -n -H` for the hard limit and `ulimit -n -S` for the soft
limit. A soft limit of `65535` is a good lower bound. You can adjust the limit
as needed.

## I see "manifest has unsupported version: X (we support Y)" error

This error means you have a badger directory which was created by an older
version of badger and you're trying to open in a newer version of badger. The
underlying data format can change across badger versions and users have to
migrate their data directory. Badger data can be migrated from version X of
badger to version Y of badger by following the steps listed below. Assume you
were on badger v1.6.0 and you wish to migrate to v2.0.0 version.

1. Install Badger version v1.6.0

   * `cd $GOPATH/src/github.com/dgraph-io/badger`
   * `git checkout v1.6.0`
   * `cd badger && go install`

     This should install the old Badger binary in your `$GOBIN`.

2. Create Backup
   * `badger backup --dir path/to/badger/directory -f badger.backup`

3. Install Badger version v2.0.0

   * `cd $GOPATH/src/github.com/dgraph-io/badger`
   * `git checkout v2.0.0`
   * `cd badger && go install`

     This should install the new Badger binary in your `$GOBIN`.

4. Restore data from backup

   * `badger restore --dir path/to/new/badger/directory -f badger.backup`

     This creates a new directory on `path/to/new/badger/directory` and adds
     data in the new format to it.

NOTE - The preceding steps shouldn't cause any data loss but please ensure the
new data is valid before deleting the old Badger directory.

## Why do I need gcc to build badger? Does badger need Cgo?

Badger doesn't directly use Cgo but it relies on [https://github.com/DataDog/zstd](https://github.com/DataDog/zstd)
library for zstd compression and the library requires
[`gcc/cgo`](https://pkg.go.dev/cmd/cgo). You can build Badger without Cgo by
running `CGO_ENABLED=0 go build`. This builds Badger without the support for
ZSTD compression algorithm.

As of Badger versions
[v2.2007.4](https://github.com/hypermodeinc/badger/releases/tag/v2.2007.4) and
[v3.2103.1](https://github.com/hypermodeinc/badger/releases/tag/v3.2103.1) the
DataDog ZSTD library was replaced by pure Golang version and Cgo is no longer
required. The new library is
[backwards compatible in nearly all cases](https://discuss.hypermode.com/t/use-pure-go-zstd-implementation/8670/10):

<Note>
  Yes they're compatible both ways. The only exception is 0 bytes of input which
  gives 0 bytes output with the Go zstd. But you already have the
  zstd.WithZeroFrames(true) which wraps 0 bytes in a header so it can be fed to
  DD zstd. This is only relevant when downgrading.
</Note>


# Community and Support
Source: https://docs.hypermode.com/community-and-support

Get help with Hypermode and connect with other developers

Whether you're just getting started or you're a seasoned developer, we're here
to help you every step of the way. We’re excited to have you as part of the
Hypermode community and look forward to seeing what you build!

## Community

[Discord](https://discord.hypermode.com) is our main forum where you can ask
questions, share your knowledge, and connect with other developers.

## Getting help

If you encounter a bug or have a feature request, you can open an issue on the
relevant GitHub repository:

* [Modus](https://github.com/hypermodeinc/modus/issues)
* [Dgraph](https://github.com/hypermodeinc/dgraph/issues)
* [Badger](https://github.com/hypermodeinc/badger/issues)
* [Hyp CLI](https://github.com/hypermodeinc/hyp-cli/issues)

All paid Hypermode packages include commercial support. Customers can reach out
via the Hypermode Console or through email at
[help@hypermode.com](mailto:help@hypermode.com).

## Stay connected

Stay up-to-date with the latest news, updates, and announcements from Hypermode:

* **[X](https://x.com/hypermodeinc)**: follow us on X for the latest news and
  updates
* **[Blog](https://hypermode.com/blog)**: explore our blog for in-depth
  articles, tutorials, and case studies


# Configure Environment
Source: https://docs.hypermode.com/configure-environment

Define environment parameters for your app

On your first deployment, and when you add new connections thereafter, you may
need to configure your environment for your app to be ready for traffic.

## Connection secrets

If you included parameters for Connection Secrets in your
[app manifest](/modus/app-manifest), you'll need to add the parameter values in
the Hypermode Console.

From your project home, navigate to **Settings** → **Connections**. Add the
values for your defined connection authentication parameters.

## Model hosting

Where available, Hypermode defaults to shared model instances. Refer to the
[Hosted Models](/hosted-models) documentation for instructions on setting up
hosted models.

## Scaling your runtime resources

We're working to make runtime scaling self-service. In the meantime, reach out
at [help@hypermode.com](mailto:help@hypermode.com) for assistance with this
request.


# Create Project
Source: https://docs.hypermode.com/create-project

Initialize your Modus app with Hypermode

A Hypermode project represents your Modus app and associated models. You can
create a project through the Hypermode Console or Hyp CLI.

## Create with Hypermode Console

<Note>
  Hypermode relies on Git as a source for project deployment. Through the
  project creation flow, you'll connect Hypermode to your GitHub account.
</Note>

From your organization home, click **New Project**. Set a name for the project
and click **Create**.

Once you have created a project, Hypermode prompts you to finish setting up your
project using the CLI.

First, install the Modus CLI and initialize your app. For more information on
creating your first Modus app, visit the [Modus quickstart](modus/quickstart).

Next, initialize the app with Hypermode through the [Hyp CLI](/hyp-cli) and link
your GitHub repo with your Modus app to Hypermode using:

```sh
hyp link
```

This command adds a default GitHub Actions workflow to build your Modus app and
a Hypermode GitHub app for auto-deployment. Once initialized, each commit to the
target branch triggers a deployment.

You can also connect to an existing GitHub repository.

The last step is triggering your first deployment. If you linked your project
through the Hyp CLI, make sure to push your changes first to trigger a
deployment.


# Deploy Project
Source: https://docs.hypermode.com/deploy

A git-based flow for simple deployment

Hypermode features a native GitHub integration for the deployment of Hypermode
projects. The deployment includes your Modus app as well as any models defined
in the [app manifest](/modus/app-manifest).

<Note>
  Preview environments for live validation of pull requests are in development.
</Note>

## Link your project to GitHub

After you push your Modus app to GitHub, you can link your Hypermode project to
the repo through the Hyp CLI.

```sh
hyp link
```

## Build

When you link your project with Hypermode, the Hyp CLI adds a GitHub Actions
workflow to your repo that builds your Modus app to Hypermode on commit.

## Deploy

On successful build of your project, Hypermode automatically deploys your
project changes. For [hosted models](/hosted-models), Hypermode creates a
connection for your app to a shared or dedicated model instance.

You can view the deployment status from the Deployments tab within the Hypermode
Console.


# Initial Import (Bulk Loader)
Source: https://docs.hypermode.com/dgraph/admin/bulk-loader



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph Bulk Loader serves a similar purpose to the Dgraph Live Loader, but can
only be used to load data into a new cluster. It can't be run on an existing
Dgraph cluster. Dgraph Bulk Loader is **considerably faster** than the Dgraph
Live Loader and is the recommended way to perform the initial import of large
datasets into Dgraph.

Only one or more Dgraph Zeros should be running for bulk loading. Dgraph Alphas
are started later.

You can [read the technical details](https://hypermode.com/blog/bulkloader/)
about the Bulk Loader on the blog.

<Warning>
  Don't use the Bulk Loader once the Dgraph cluster is up and running. Use it to
  import your existing data to a new cluster.
</Warning>

<Tip>
  It is crucial to tune the Bulk Loader's flags to get good performance. See the
  next section for details.
</Tip>

## Settings

<Note>
  Bulk Loader only accepts data in the [RDF
  N-Quad/Triple](https://www.w3.org/TR/n-quads/) or JSON formats. Data can be
  raw or compressed with gzip.
</Note>

```sh
$ dgraph bulk --help # To see the available flags.

# Read RDFs or JSON from the passed file.
$ dgraph bulk -f <path-to-gzipped-RDF-or-JSON-file> ...

# Read multiple RDFs or JSON from the passed path.
$ dgraph bulk -f <./path-to-gzipped-RDF-or-JSON-files> ...

# Read multiple files strictly by name.
$ dgraph bulk -f <file1.rdf, file2.rdf> ...
```

* **Reduce shards**: Before running the bulk load, you need to decide how many
  Alpha groups are running when the cluster starts. The number of Alpha groups
  is the same number of reduce shards you set with the `--reduce_shards` flag.
  For example, if your cluster has 3 Alpha with 3 replicas per group, then there
  is 1 group and `--reduce_shards` should be set to 1. If your cluster has 6
  Alphas with 3 replicas per group, then there are 2 groups and
  `--reduce_shards` should be set to 2.

* **Map shards**: The `--map_shards` option must be set to at least what's set
  for `--reduce_shards`. A higher number helps the Bulk Loader evenly distribute
  predicates between the reduce shards.

For example:

```sh
dgraph bulk -f goldendata.rdf.gz -s goldendata.schema --map_shards=4 --reduce_shards=2 --http localhost:8000 --zero=localhost:5080
```

```sh
{
  "DataFiles": "goldendata.rdf.gz",
  "DataFormat": "",
  "SchemaFile": "goldendata.schema",
  "DgraphsDir": "out",
  "TmpDir": "tmp",
  "NumGoroutines": 4,
  "MapBufSize": 67108864,
  "ExpandEdges": true,
  "SkipMapPhase": false,
  "CleanupTmp": true,
  "NumShufflers": 1,
  "Version": false,
  "StoreXids": false,
  "ZeroAddr": "localhost:5080",
  "HttpAddr": "localhost:8000",
  "IgnoreErrors": false,
  "MapShards": 4,
  "ReduceShards": 2
}
The bulk loader needs to open many files at once. This number depends on the size of the data set loaded,
the map file output size, and the level of indexing. 100,000 is adequate for most data set sizes.
See `man ulimit` for details of how to change the limit.

Current max open files limit: 1024
MAP 01s rdf_count:176.0 rdf_speed:174.4/sec edge_count:564.0 edge_speed:558.8/sec
MAP 02s rdf_count:399.0 rdf_speed:198.5/sec edge_count:1.291k edge_speed:642.4/sec
MAP 03s rdf_count:666.0 rdf_speed:221.3/sec edge_count:2.164k edge_speed:718.9/sec
MAP 04s rdf_count:952.0 rdf_speed:237.4/sec edge_count:3.014k edge_speed:751.5/sec
MAP 05s rdf_count:1.327k rdf_speed:264.8/sec edge_count:4.243k edge_speed:846.7/sec
MAP 06s rdf_count:1.774k rdf_speed:295.1/sec edge_count:5.720k edge_speed:951.5/sec
MAP 07s rdf_count:2.375k rdf_speed:338.7/sec edge_count:7.607k edge_speed:1.085k/sec
MAP 08s rdf_count:3.697k rdf_speed:461.4/sec edge_count:11.89k edge_speed:1.484k/sec
MAP 09s rdf_count:71.98k rdf_speed:7.987k/sec edge_count:225.4k edge_speed:25.01k/sec
MAP 10s rdf_count:354.8k rdf_speed:35.44k/sec edge_count:1.132M edge_speed:113.1k/sec
MAP 11s rdf_count:610.5k rdf_speed:55.39k/sec edge_count:1.985M edge_speed:180.1k/sec
MAP 12s rdf_count:883.9k rdf_speed:73.52k/sec edge_count:2.907M edge_speed:241.8k/sec
MAP 13s rdf_count:1.108M rdf_speed:85.10k/sec edge_count:3.653M edge_speed:280.5k/sec
MAP 14s rdf_count:1.121M rdf_speed:79.93k/sec edge_count:3.695M edge_speed:263.5k/sec
MAP 15s rdf_count:1.121M rdf_speed:74.61k/sec edge_count:3.695M edge_speed:246.0k/sec
REDUCE 16s [1.69%] edge_count:62.61k edge_speed:62.61k/sec plist_count:29.98k plist_speed:29.98k/sec
REDUCE 17s [18.43%] edge_count:681.2k edge_speed:651.7k/sec plist_count:328.1k plist_speed:313.9k/sec
REDUCE 18s [33.28%] edge_count:1.230M edge_speed:601.1k/sec plist_count:678.9k plist_speed:331.8k/sec
REDUCE 19s [45.70%] edge_count:1.689M edge_speed:554.4k/sec plist_count:905.9k plist_speed:297.4k/sec
REDUCE 20s [60.94%] edge_count:2.252M edge_speed:556.5k/sec plist_count:1.278M plist_speed:315.9k/sec
REDUCE 21s [93.21%] edge_count:3.444M edge_speed:681.5k/sec plist_count:1.555M plist_speed:307.7k/sec
REDUCE 22s [100.00%] edge_count:3.695M edge_speed:610.4k/sec plist_count:1.778M plist_speed:293.8k/sec
REDUCE 22s [100.00%] edge_count:3.695M edge_speed:584.4k/sec plist_count:1.778M plist_speed:281.3k/sec
Total: 22s
```

The output is generated in the `out` directory by default. Here's the bulk load
output from the preceding example:

```sh
tree ./out
```

```txt
./out
├── 0
│   └── p
│       ├── 000000.vlog
│       ├── 000002.sst
│       └── MANIFEST
└── 1
    └── p
        ├── 000000.vlog
        ├── 000002.sst
        └── MANIFEST

4 directories, 6 files
```

Because `--reduce_shards` was set to `2`, two sets of `p` directories are
generated:

* the `./out/0` folder
* the `./out/1` folder

Once the output is created, the files must be copied to all the servers that run
Dgraph Alphas:

* Each replica of the first group (`Alpha1`, `Alpha2`, `Alpha3`) should have a
  copy of `./out/0/p`
* Each replica of the second group (`Alpha4`, `Alpha5`, `Alpha6`) should have a
  copy of `./out/1/p`, and so on.

<Note>
  Each Dgraph Alpha must have a copy of the group's `p` directory output.
</Note>

![Bulk Loader diagram](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/bulk-loader.png)

### Other Bulk Loader options

You can further configure Bulk Loader using the following options:

* `--schema`, `-s`: set the location of the schema file.

* `--graphql_schema`, `-g` (optional): set the location of the GraphQL schema
  file.

* `--badger` superflag's `compression` option: Configure the compression of data
  on disk. By default, the Snappy compression format is used, but you can also
  use Zstandard compression. Or, you can choose no compression to minimize CPU
  usage. To learn more, see
  [Data Compression on Disk](/dgraph/self-managed/data-compression).

* `--new_uids`: (default: false): Assign new UIDs instead of using the existing
  UIDs in data files. This is useful to avoid overriding the data in a DB
  already in operation.

* `-f`, `--files`: Location of `*.rdf(.gz)` or `*.json(.gz)` files to load. It
  can load multiple files in a given path. If the path is a directory, then all
  files ending in `.rdf`, `.rdf.gz`, `.json`, and `.json.gz` are loaded.

* `--format` (optional): Specify file format (`rdf` or `json`) instead of
  getting it from filenames. This is useful if you need to define a strict
  format manually.

* `--store_xids`: Generate a xid edge for each node. It stores the XIDs (The
  identifier / Blank-nodes) in an attribute named `xid` in the entity itself.

* `--xidmap` (default: `disabled`. Need a path): Store xid to uid mapping to a
  directory. Dgraph saves all identifiers used in the load for later use in
  other data import operations. The mapping is saved in the path you provide and
  you must indicate that same path in the next load. It is recommended to use
  this flag if you have full control over your identifiers (Blank-nodes).
  Because the identifier is mapped to a specific UID.

* `--vault` superflag (and its options): specify the Vault server address, role
  id, secret id, and field that contains the encryption key required to decrypt
  the encrypted export.

## Load from S3

To bulk load from Amazon S3, you must have either [IAM](#iam-setup) or the
following AWS credentials set via environment variables:

| Environment Variable                        | Description                                                         |
| ------------------------------------------- | ------------------------------------------------------------------- |
| `AWS_ACCESS_KEY_ID` or `AWS_ACCESS_KEY`     | AWS access key with permissions to write to the destination bucket. |
| `AWS_SECRET_ACCESS_KEY` or `AWS_SECRET_KEY` | AWS access key with permissions to write to the destination bucket. |

### IAM setup

In AWS, you can accomplish this by doing the following:

1. Create an
   [IAM Role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create.html)
   with an IAM Policy that grants access to the S3 bucket.
2. Depending on whether you want to grant access to an EC2 instance, or to a pod
   running on [EKS](https://aws.amazon.com/eks/), you can do one of these
   options:
   * [Instance Profile](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html)
     can pass the IAM Role to an EC2 Instance
   * [IAM Roles for Amazon EC2](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html)
     to attach the IAM Role to a running EC2 Instance
   * [IAM roles for service accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html)
     to associate the IAM Role to a
     [Kubernetes Service Account](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/).

Once your setup is ready, you can execute the bulk load from S3:

```sh
dgraph bulk -f s3:///bucket-name/directory-with-rdf -s s3:///bucket-name/directory-with-rdf/schema.txt
```

## Load from MinIO

To bulk load from MinIO, you must have the following MinIO credentials set via
environment variables:

| Environment Variable | Description                                                           |
| -------------------- | --------------------------------------------------------------------- |
| `MINIO_ACCESS_KEY`   | MinIO access key with permissions to write to the destination bucket. |
| `MINIO_SECRET_KEY`   | MinIO secret key with permissions to write to the destination bucket. |

Once your setup is ready, you can execute the bulk load from MinIO:

```sh
dgraph bulk -f minio://minio-server:port/bucket-name/directory-with-rdf -s minio://minio-server:port/bucket-name/directory-with-rdf/schema.txt
```

## How to properly bulk load

Starting from Dgraph v20.03.7, depending on your dataset size, you can follow
one of the following ways to use Bulk Loader and initialize your new cluster.

*The following procedure is particularly relevant for Clusters that have
`--replicas` flag greater than 1*

### For small datasets

In case your dataset is small (a few gigabytes) it would be convenient to start
by initializing just one Alpha node and then let the snapshot be streamed among
the other Alpha replicas. You can follow these steps:

1. Run Bulk Loader only on one server

2. Once the `p` directory has been created by the Bulk Loader, then start
   **only** the first Alpha replica

3. Wait for 1 minute to ensure that a snapshot has been taken by the first Alpha
   node replica. You can confirm that a snapshot has been taken by looking for
   the following message":

   ```txt
   I1227 13:12:24.202196   14691 draft.go:571] Creating snapshot at index: 30. ReadTs: 4.
   ```

4. After confirming that the snapshot has been taken, you can start the other
   Alpha node replicas (number of Alpha nodes must be equal to the `--replicas`
   flag value set in the Zero nodes). Now the Alpha node (the one started in
   step 2) logs similar messages:

   ```txt
   I1227 13:18:16.154674   16779 snapshot.go:246] Streaming done. Sent 1093470 entries. Waiting for ACK...
   I1227 13:18:17.126494   16779 snapshot.go:251] Received ACK with done: true
   I1227 13:18:17.126514   16779 snapshot.go:292] Stream snapshot: OK
   ```

   These messages indicate that all replica nodes are now using the same
   snapshot. Thus, all your data is correctly in sync across the cluster. Also,
   the other Alpha nodes print (in their logs) something similar to:

   ```txt
   I1227 13:18:17.126621    1720 draft.go:567] Skipping snapshot at 28, because found one at 28
   ```

### For bigger datasets

When your dataset is pretty big (larger than 10 GB) it is faster that you just
copy the generated `p` directory (by the Bulk Loader) among all the Alphas
nodes. You can follow these steps:

1. Run Bulk Loader only on one server
2. Copy (or use `rsync`) the `p` directory to the other servers (the servers you
   are using to start the other Alpha nodes)
3. Now, start all Alpha nodes at the same time

If the process went well **all** Alpha nodes take a snapshot after 1 minute. You
should see something similar to this in the Alpha logs:

```txt
I1227 13:27:53.959671   29781 draft.go:571] Creating snapshot at index: 34. ReadTs: 6.
```

Note that `snapshot at index` value must be the same within the same Alpha group
and `ReadTs` must be the same value within and among all the Alpha groups.

## Enterprise features

### Multi-tenancy

By default, Bulk Loader preserves the namespace in the data and schema files. If
there's no namespace information available, it loads the data into the default
namespace.

Using the `--force-namespace` flag, you can load all the data into a specific
namespace. In that case, the namespace information from the data and schema
files are ignored.

For example, to force the bulk data loading into namespace `123`:

```sh
dgraph bulk -s /tmp/data/1million.schema -f /tmp/data/1million.rdf.gz --force-namespace 123
```

### Encryption at rest

Even before the Dgraph cluster starts, we can load data using Bulk Loader with
the encryption feature turned on. Later we can point the generated `p` directory
to a new Alpha server.

Here's an example to run Bulk Loader with a key used to write encrypted data:

```sh
dgraph bulk --encryption key-file=./enc_key_file -f data.json.gz -s data.schema --map_shards=1 --reduce_shards=1 --http localhost:8000 --zero=localhost:5080
```

Alternatively, starting with v20.07.0, the `vault_*` options can be used to
decrypt the encrypted export.

### Encrypting imports

The Bulk Loader’s `--encryption key-file=value` option was previously used to
encrypt the output `p` directory. This same option is also used to decrypt the
encrypted export data and schema files.

Another option, `--encrypted`, indicates whether the input `rdf`/`json` data and
schema files are encrypted or not. With this switch, we support the use case of
migrating data from unencrypted exports to encrypted import.

So, with the preceding two options there are four cases:

1. `--encrypted=true` and no `encryption key-file=value`.

   Error: if the input is encrypted, a key file must be provided.

2. `--encrypted=true` and `encryption key-file=path-to-key`.

   Input is encrypted and output `p` dir is encrypted as well.

3. `--encrypted=false` and no `encryption key-file=value`.

   Input isn't encrypted and the output `p` dir is also not encrypted.

4. `--encrypted=false` and `encryption key-file=path-to-key`.

   Input isn't encrypted but the output is encrypted. (This is the migration use
   case mentioned previously).

Alternatively, starting with v20.07.0, the `vault_*` options can be used instead
of the `--encryption key-file=value` option to achieve the same effect except
that the keys are sitting in a Vault server.

You can also use Bulk Loader, to turn off encryption. This generates a new
unencrypted `p` that's used by the Alpha process. In this, case you need to pass
`--encryption key-file`, `--encrypted` and `--encrypted_out` flags.

```sh
# Encryption Key from the file path
dgraph bulk --files "<path-to-gzipped-RDF-or-JSON-file>" --schema "<path-to-schema>" --zero "<dgraph-zero-address:grpc_port>" \
  --encrypted="true" --encrypted_out="false" \
  --encryption key-file="<path-to-enc_key_file>"

# Encryption Key from HashiCorp Vault
dgraph bulk --files "<path-to-gzipped-RDF-or-JSON-file>" --schema "<path-to-schema>" --zero "<dgraph-zero-address:grpc_port>" \
  --encrypted="true" --encrypted_out="false" \
  --vault addr="http://localhost:8200";enc-field="enc_key";enc-format="raw";path="secret/data/dgraph/alpha";role-id-file="./role_id";secret-id-file="./secret_id"

```

In this case, we're also passing the flag `--encrypted=true` as the exported
data has been taken from an encrypted Dgraph cluster and we're also specifying
the flag `--encrypted_out=false` to specify that we want the `p` directory (that
is generated by the Bulk Loader process) to be unencrypted.

## Tuning & monitoring

### Performance tuning

<Tip>
  We highly recommend [turning off swap
  space](https://askubuntu.com/questions/214805/how-do-i-disable-swap) when
  running Bulk Loader. It is better to fix the parameters to decrease memory
  usage, than to have swapping grind the loader down to a halt.
</Tip>

Flags can be used to control the behavior and performance characteristics of the
Bulk Loader. You can see the full list by running `dgraph bulk --help`. In
particular, **you should tune the flags so that Bulk Loader doesn't use more
memory than is available as RAM**. If it starts swapping, it becomes incredibly
slow.

**In the map phase**, tweaking the following flags can reduce memory usage:

* The `--num_go_routines` flag controls the number of worker threads. Lowering
  reduces memory consumption.

* The `--mapoutput_mb` flag controls the size of the map output files. Lowering
  reduces memory consumption.

For bigger datasets and machines with many cores, gzip decoding can be a
bottleneck during the map phase. Performance improvements can be obtained by
first splitting the RDFs up into many `.rdf.gz` files (e.g. 256MB each). This
has a negligible impact on memory usage.

**The reduce phase** is less memory heavy than the map phase, although can still
use a lot. Some flags may be increased to improve performance, *but only if you
have large amounts of RAM*:

* The `--reduce_shards` flag controls the number of resultant Dgraph Alpha
  instances. Increasing this increases memory consumption, but in exchange
  allows for higher CPU utilization.

* The `--map_shards` flag controls the number of separate map output shards.
  Increasing this increases memory consumption but balances the resultant Dgraph
  Alpha instances more evenly.


# Debugging
Source: https://docs.hypermode.com/dgraph/admin/debug



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Each Dgraph data node exposes profile over `/debug/pprof` endpoint and metrics
over `/debug/vars` endpoint. Each Dgraph data node has it's own profiling and
metrics information. Below is a list of debugging information exposed by Dgraph
and the corresponding commands to retrieve them.

## Metrics Information

If you are collecting these metrics from outside the Dgraph instance you need to
pass `--expose_trace=true` flag, otherwise there metrics can be collected by
connecting to the instance over localhost.

```sh
curl http://<IP>:<HTTP_PORT>/debug/vars
```

Metrics can also be retrieved in the Prometheus format at
`/debug/prometheus_metrics`. See the [Metrics](./metrics) section for the full
list of metrics.

## Profiling Information

Profiling information is available via the `go tool pprof` profiling tool built
into Go. The
["Profiling Go programs"](https://blog.golang.org/profiling-go-programs) Go blog
post should help you get started with using pprof. Each Dgraph Zero and Dgraph
Alpha exposes a debug endpoint at `/debug/pprof/<profile>` via the HTTP port.

```sh
go tool pprof http://<IP>:<HTTP_PORT>/debug/pprof/heap
Fetching profile from ...
Saved Profile in ...
```

The output of the command would show the location where the profile is stored.

In the interactive pprof shell, you can use commands like `top` to get a listing
of the top functions in the profile, `web` to get a visual graph of the profile
opened in a web browser, or `list` to display a code listing with profiling
information overlaid.

### CPU profile

```sh
go tool pprof http://<IP>:<HTTP_PORT>/debug/pprof/profile
```

### Memory profile

```sh
go tool pprof http://<IP>:<HTTP_PORT>/debug/pprof/heap
```

### Block profile

Dgraph by default doesn't collect the block profile. Dgraph must be started with
`--profile_mode=block` and `--block_rate=<N>` with N > 1.

```sh
go tool pprof http://<IP>:<HTTP_PORT>/debug/pprof/block
```

### Goroutine stack

The HTTP page `/debug/pprof/` is available at the HTTP port of a Dgraph Zero or
Dgraph Alpha. From this page a link to the "full goroutine stack dump" is
available (for example, on a Dgraph Alpha this page would be at
`http://localhost:8080/debug/pprof/goroutine?debug=2`). Looking at the full
goroutine stack can be useful to understand goroutine usage at that moment.

## Profiling Information with `debuginfo`

Instead of sending a request to the server for each CPU, memory, and `goroutine`
profile, you can use the `debuginfo` command to collect all of these profiles,
along with several metrics.

You can run the command like this:

```sh
dgraph debuginfo -a <alpha_address:port> -z <zero_address:port> -d <path_to_dir_to_store_profiles>
```

Your output should look like:

```log
I0311 14:13:53.243667   32654 run.go:118] using directory /tmp/dgraph-debuginfo037351492 for debug info dump.
I0311 14:13:53.243864   32654 debugging.go:68] fetching information over HTTP from http://localhost:8080/debug/pprof/heap
I0311 14:13:53.243872   32654 debugging.go:70] please wait... (30s)
I0311 14:13:53.245338   32654 debugging.go:58] saving heap metric in /tmp/dgraph-debuginfo037351492/alpha_heap.gz
I0311 14:13:53.245349   32654 debugging.go:68] fetching information over HTTP from http://localhost:8080/debug/pprof/profile?seconds=30
I0311 14:13:53.245357   32654 debugging.go:70] please wait... (30s)
I0311 14:14:23.250079   32654 debugging.go:58] saving cpu metric in /tmp/dgraph-debuginfo037351492/alpha_cpu.gz
I0311 14:14:23.250148   32654 debugging.go:68] fetching information over HTTP from http://localhost:8080/state
I0311 14:14:23.250173   32654 debugging.go:70] please wait... (30s)
I0311 14:14:23.255467   32654 debugging.go:58] saving state metric in /tmp/dgraph-debuginfo037351492/alpha_state.gz
I0311 14:14:23.255507   32654 debugging.go:68] fetching information over HTTP from http://localhost:8080/health
I0311 14:14:23.255528   32654 debugging.go:70] please wait... (30s)
I0311 14:14:23.257453   32654 debugging.go:58] saving health metric in /tmp/dgraph-debuginfo037351492/alpha_health.gz
I0311 14:14:23.257507   32654 debugging.go:68] fetching information over HTTP from http://localhost:8080/jemalloc
I0311 14:14:23.257548   32654 debugging.go:70] please wait... (30s)
I0311 14:14:23.259009   32654 debugging.go:58] saving jemalloc metric in /tmp/dgraph-debuginfo037351492/alpha_jemalloc.gz
I0311 14:14:23.259055   32654 debugging.go:68] fetching information over HTTP from http://localhost:8080/debug/pprof/trace?seconds=30
I0311 14:14:23.259091   32654 debugging.go:70] please wait... (30s)
I0311 14:14:53.266092   32654 debugging.go:58] saving trace metric in /tmp/dgraph-debuginfo037351492/alpha_trace.gz
I0311 14:14:53.266152   32654 debugging.go:68] fetching information over HTTP from http://localhost:8080/metrics
I0311 14:14:53.266181   32654 debugging.go:70] please wait... (30s)
I0311 14:14:53.276357   32654 debugging.go:58] saving metrics metric in /tmp/dgraph-debuginfo037351492/alpha_metrics.gz
I0311 14:14:53.276414   32654 debugging.go:68] fetching information over HTTP from http://localhost:8080/debug/vars
I0311 14:14:53.276439   32654 debugging.go:70] please wait... (30s)
I0311 14:14:53.278295   32654 debugging.go:58] saving vars metric in /tmp/dgraph-debuginfo037351492/alpha_vars.gz
I0311 14:14:53.278340   32654 debugging.go:68] fetching information over HTTP from http://localhost:8080/debug/pprof/trace?seconds=30
I0311 14:14:53.278366   32654 debugging.go:70] please wait... (30s)
I0311 14:15:23.286770   32654 debugging.go:58] saving trace metric in /tmp/dgraph-debuginfo037351492/alpha_trace.gz
I0311 14:15:23.286830   32654 debugging.go:68] fetching information over HTTP from http://localhost:8080/debug/pprof/goroutine?debug=2
I0311 14:15:23.286886   32654 debugging.go:70] please wait... (30s)
I0311 14:15:23.291120   32654 debugging.go:58] saving goroutine metric in /tmp/dgraph-debuginfo037351492/alpha_goroutine.gz
I0311 14:15:23.291164   32654 debugging.go:68] fetching information over HTTP from http://localhost:8080/debug/pprof/block
I0311 14:15:23.291192   32654 debugging.go:70] please wait... (30s)
I0311 14:15:23.304562   32654 debugging.go:58] saving block metric in /tmp/dgraph-debuginfo037351492/alpha_block.gz
I0311 14:15:23.304664   32654 debugging.go:68] fetching information over HTTP from http://localhost:8080/debug/pprof/mutex
I0311 14:15:23.304706   32654 debugging.go:70] please wait... (30s)
I0311 14:15:23.309171   32654 debugging.go:58] saving mutex metric in /tmp/dgraph-debuginfo037351492/alpha_mutex.gz
I0311 14:15:23.309228   32654 debugging.go:68] fetching information over HTTP from http://localhost:8080/debug/pprof/threadcreate
I0311 14:15:23.309256   32654 debugging.go:70] please wait... (30s)
I0311 14:15:23.313026   32654 debugging.go:58] saving threadcreate metric in /tmp/dgraph-debuginfo037351492/alpha_threadcreate.gz
I0311 14:15:23.385359   32654 run.go:150] Debuginfo archive successful: dgraph-debuginfo037351492.tar.gz
```

When the command finishes, `debuginfo` returns the tarball's file name. If no
destination has been specified, the file is created in the same directory from
where you ran the `debuginfo` command.

The following files contain the metrics collected by the `debuginfo` command:

```sh
dgraph-debuginfo639541060
├── alpha_block.gz
├── alpha_goroutine.gz
├── alpha_health.gz
├── alpha_heap.gz
├── alpha_jemalloc.gz
├── alpha_mutex.gz
├── alpha_profile.gz
├── alpha_state.gz
├── alpha_threadcreate.gz
├── alpha_trace.gz
├── zero_block.gz
├── zero_goroutine.gz
├── zero_health.gz
├── zero_heap.gz
├── zero_jemalloc.gz
├── zero_mutex.gz
├── zero_profile.gz
├── zero_state.gz
├── zero_threadcreate.gz
└── zero_trace.gz
```

### Command parameters

```sh
  -a, --alpha string       Address of running dgraph alpha. (default "localhost:8080")
  -x, --archive            Whether to archive the generated report (default true)
  -d, --directory string   Directory to write the debug info into.
  -h, --help               help for debuginfo
  -m, --metrics strings    List of metrics & profiles to dump in the report. (default [heap,cpu,state,health,jemalloc,trace,metrics,vars,trace,goroutine,block,mutex,threadcreate])
  -s, --seconds uint32     Duration for time-based metric collection. (default 30)
  -z, --zero string        Address of running dgraph zero.
```

#### The metrics flag (`-m`)

By default, `debuginfo` collects:

* `heap`
* `cpu`
* `state`
* `health`
* `jemalloc`
* `trace`
* `metrics`
* `vars`
* `trace`
* `goroutine`
* `block`
* `mutex`
* `threadcreate`

If needed, you can collect some of them (not necessarily all). For example, this
command collects only `jemalloc` and `health` profiles:

```sh
dgraph debuginfo -m jemalloc,health
```

### Profiles details

* `cpu profile`: CPU profile determines where a program spends its time while
  actively consuming CPU cycles (as opposed to while sleeping or waiting for
  I/O).

* `heap`: Heap profile reports memory allocation samples; used to monitor
  current and historical memory usage, and to check for memory leaks.

* `threadcreate`: Thread creation profile reports the sections of the program
  that lead the creation of new OS threads.

* `goroutine`: Goroutine profile reports the stack traces of all current
  goroutines.

* `block`: Block profile shows where goroutines block waiting on synchronization
  primitives (including timer channels).

* `mutex`: Mutex profile reports the lock contentions. When you think your CPU
  isn't fully utilized due to a mutex contention, use this profile.

* `trace`: this capture a wide range of runtime events. Execution tracer is a
  tool to detect latency and utilization problems. You can examine how well the
  CPU is utilized, and when networking or syscalls are a cause of preemption for
  the goroutines. Tracer is useful to identify poorly parallelized execution,
  understand some of the core runtime events, and how your goroutines execute.

## Using the `debug` tool

<Note>
  To debug a running Dgraph cluster, first copy the postings ("p") directory to
  another location. If the Dgraph cluster isn't running, then you can use the same
  postings directory with the debug tool. If the “p” directory has been encrypted,
  then the debug tool needs to use the `--keyfile <path-to-keyfile>` option. This
  file must contain the same key that was used to encrypt the “p” directory.
</Note>

The `dgraph debug` tool can be used to inspect Dgraph's posting list structure.
You can use the debug tool to inspect the data, schema, and indices of your
Dgraph cluster.

Some scenarios where the debug tool is useful:

* Verify that mutations committed to Dgraph have been persisted to disk.
* Verify that indices are created.
* Inspect the history of a posting list.
* Parse a badger key into meaningful struct

## Example

Debug the p directory.

```sh
dgraph debug --postings ./p
```

Debug the p directory, not opening in read-only mode. This is typically
necessary when the database wasn't closed properly.

```sh
dgraph debug --postings ./p --readonly=false
```

Debug the p directory, only outputting the keys for the predicate `0-name`. Note
that 0 is the namespace and name is the predicate.

```sh
dgraph debug --postings ./p --readonly=false --pred=0-name
```

Debug the p directory, looking up a particular key:

```sh
dgraph debug --postings ./p --lookup 01000000000000000000046e616d65
```

Debug the p directory, inspecting the history of a particular key:

```sh
dgraph debug --postings ./p --lookup 01000000000000000000046e616d65 --history
```

Debug an encrypted p directory with the key in a local file at the path
./key\_file:

```sh
dgraph debug --postings ./p --encryption=key-file=./key_file
```

<Note>
  The key file contains the key used to decrypt/encrypt the db. This key should be
  kept secret. As a best practice,

  * Don't store the key file on the disk permanently. Back it up in a safe place
    and delete it after using it with the debug tool.

  * If the this isn't possible, make sure correct privileges are set on the key
    file. Only the user who owns the dgraph process should be able to read or
    write the key file: `chmod 600`
</Note>

## Debug tool output

Let's go over an example with a Dgraph cluster with the following schema with a
term index, full-text index, and two separately committed mutations:

```sh
$ curl localhost:8080/alter -d '
  name: string @index(term) .
  url: string .
  description: string @index(fulltext) .
'
```

```sh
$ curl -H "Content-Type: application/rdf" "localhost:8080/mutate?commitNow=true" -d '{
  set {
    _:dgraph <name> "Dgraph" .
    _:dgraph <dgraph.type> "Software" .
    _:dgraph <url> "https://github.com/hypermodeinc/dgraph" .
    _:dgraph <description> "Fast, Transactional, Distributed Graph Database." .
  }
}'
```

```sh
$ curl -H "Content-Type: application/rdf" "localhost:8080/mutate?commitNow=true" -d '{
  set {
    _:badger <name> "Badger" .
    _:badger <dgraph.type> "Software" .
    _:badger <url> "https://github.com/hypermodeinc/badger" .
    _:badger <description> "Embeddable, persistent and fast key-value (KV) database written in pure Go." .
  }
}'
```

After stopping Dgraph, you can run the debug tool to inspect the postings
directory:

<Note>
  The debug output can be very large. Typically you would redirect the debug
  tool to a file first for easier analysis.
</Note>

```sh
dgraph debug --postings ./p
```

```text
Opening DB: ./p

prefix =
{d} ns: 0x0  attr: url uid: 1  ts: 5 item: [79, b0100] sz: 79 dcnt: 1 key: 000000000000000000000375726c000000000000000001
{d} ns: 0x0  attr: url uid: 2  ts: 8 item: [108, b1000] sz: 108 dcnt: 0 isz: 187 icount: 2 key: 000000000000000000000375726c000000000000000002
{d} ns: 0x0  attr: name uid: 1  ts: 5 item: [51, b0100] sz: 51 dcnt: 1 key: 00000000000000000000046e616d65000000000000000001
{d} ns: 0x0  attr: name uid: 2  ts: 8 item: [80, b1000] sz: 80 dcnt: 0 isz: 131 icount: 2 key: 00000000000000000000046e616d65000000000000000002
{i} ns: 0x0  attr: name term: [1] [badger]  ts: 8 item: [41, b1000] sz: 41 dcnt: 0 isz: 79 icount: 2 key: 00000000000000000000046e616d650201626164676572
{i} ns: 0x0  attr: name term: [1] [dgraph]  ts: 5 item: [38, b0100] sz: 38 dcnt: 1 key: 00000000000000000000046e616d650201646772617068
{d} ns: 0x0  attr: description uid: 1  ts: 5 item: [100, b0100] sz: 100 dcnt: 1 key: 000000000000000000000b6465736372697074696f6e000000000000000001
{d} ns: 0x0  attr: description uid: 2  ts: 8 item: [156, b1000] sz: 156 dcnt: 0 isz: 283 icount: 2 key: 000000000000000000000b6465736372697074696f6e000000000000000002
{i} ns: 0x0  attr: description term: [8] [databas]  ts: 8 item: [49, b1000] sz: 49 dcnt: 0 isz: 141 icount: 3 key: 000000000000000000000b6465736372697074696f6e020864617461626173
{i} ns: 0x0  attr: description term: [8] [distribut]  ts: 5 item: [48, b0100] sz: 48 dcnt: 1 key: 000000000000000000000b6465736372697074696f6e0208646973747269627574
{i} ns: 0x0  attr: description term: [8] [embedd]  ts: 8 item: [48, b1000] sz: 48 dcnt: 0 isz: 93 icount: 2 key: 000000000000000000000b6465736372697074696f6e0208656d62656464
{i} ns: 0x0  attr: description term: [8] [fast]  ts: 8 item: [46, b1000] sz: 46 dcnt: 0 isz: 132 icount: 3 key: 000000000000000000000b6465736372697074696f6e020866617374
{i} ns: 0x0  attr: description term: [8] [go]  ts: 8 item: [44, b1000] sz: 44 dcnt: 0 isz: 85 icount: 2 key: 000000000000000000000b6465736372697074696f6e0208676f
{i} ns: 0x0  attr: description term: [8] [graph]  ts: 5 item: [44, b0100] sz: 44 dcnt: 1 key: 000000000000000000000b6465736372697074696f6e02086772617068
{i} ns: 0x0  attr: description term: [8] [kei]  ts: 8 item: [45, b1000] sz: 45 dcnt: 0 isz: 87 icount: 2 key: 000000000000000000000b6465736372697074696f6e02086b6569
{i} ns: 0x0  attr: description term: [8] [kv]  ts: 8 item: [44, b1000] sz: 44 dcnt: 0 isz: 85 icount: 2 key: 000000000000000000000b6465736372697074696f6e02086b76
{i} ns: 0x0  attr: description term: [8] [persist]  ts: 8 item: [49, b1000] sz: 49 dcnt: 0 isz: 95 icount: 2 key: 000000000000000000000b6465736372697074696f6e020870657273697374
{i} ns: 0x0  attr: description term: [8] [pure]  ts: 8 item: [46, b1000] sz: 46 dcnt: 0 isz: 89 icount: 2 key: 000000000000000000000b6465736372697074696f6e020870757265
{i} ns: 0x0  attr: description term: [8] [transact]  ts: 5 item: [47, b0100] sz: 47 dcnt: 1 key: 000000000000000000000b6465736372697074696f6e02087472616e73616374
{i} ns: 0x0  attr: description term: [8] [valu]  ts: 8 item: [46, b1000] sz: 46 dcnt: 0 isz: 89 icount: 2 key: 000000000000000000000b6465736372697074696f6e020876616c75
{i} ns: 0x0  attr: description term: [8] [written]  ts: 8 item: [49, b1000] sz: 49 dcnt: 0 isz: 95 icount: 2 key: 000000000000000000000b6465736372697074696f6e02087772697474656e
{d} ns: 0x0  attr: dgraph.type uid: 1  ts: 5 item: [60, b0100] sz: 60 dcnt: 1 key: 000000000000000000000b6467726170682e74797065000000000000000001
{d} ns: 0x0  attr: dgraph.type uid: 2  ts: 8 item: [88, b1000] sz: 88 dcnt: 0 isz: 148 icount: 2 key: 000000000000000000000b6467726170682e74797065000000000000000002
{i} ns: 0x0  attr: dgraph.type term: [2] [Software]  ts: 8 item: [50, b1000] sz: 50 dcnt: 0 isz: 144 icount: 3 key: 000000000000000000000b6467726170682e747970650202536f667477617265
{s} ns: 0x0  attr: url ts: 3 item: [23, b0001] sz: 23 dcnt: 0 isz: 23 icount: 1 key: 010000000000000000000375726c
{s} ns: 0x0  attr: name ts: 3 item: [33, b0001] sz: 33 dcnt: 0 isz: 33 icount: 1 key: 01000000000000000000046e616d65
{s} ns: 0x0  attr: description ts: 3 item: [51, b0001] sz: 51 dcnt: 0 isz: 51 icount: 1 key: 010000000000000000000b6465736372697074696f6e
{s} ns: 0x0  attr: dgraph.type ts: 1 item: [50, b0001] sz: 50 dcnt: 0 isz: 50 icount: 1 key: 010000000000000000000b6467726170682e74797065
{s} ns: 0x0  attr: dgraph.drop.op ts: 1 item: [45, b0001] sz: 45 dcnt: 0 isz: 45 icount: 1 key: 010000000000000000000e6467726170682e64726f702e6f70
{s} ns: 0x0  attr: dgraph.graphql.xid ts: 1 item: [64, b0001] sz: 64 dcnt: 0 isz: 64 icount: 1 key: 01000000000000000000126467726170682e6772617068716c2e786964
{s} ns: 0x0  attr: dgraph.graphql.schema ts: 1 item: [59, b0001] sz: 59 dcnt: 0 isz: 59 icount: 1 key: 01000000000000000000156467726170682e6772617068716c2e736368656d61
{s} ns: 0x0  attr: dgraph.graphql.p_query ts: 1 item: [71, b0001] sz: 71 dcnt: 0 isz: 71 icount: 1 key: 01000000000000000000166467726170682e6772617068716c2e705f7175657279
 ns: 0x0  attr: dgraph.graphql ts: 1 item: [98, b0001] sz: 98 dcnt: 0 isz: 98 icount: 1 key: 020000000000000000000e6467726170682e6772617068716c
 ns: 0x0  attr: dgraph.graphql.persisted_query ts: 1 item: [105, b0001] sz: 105 dcnt: 0 isz: 105 icount: 1 key: 020000000000000000001e6467726170682e6772617068716c2e7065727369737465645f7175657279

Found 34 keys
```

Each line in the debug output contains a prefix indicating the type of the key:

* `{d}`: data key
* `{i}`: index key
* `{c}`: count key
* `{r}`: reverse key
* `{s}`: schema key

In the preceding debug output, we see data keys, index keys, and schema keys.

Each index key has a corresponding index type. For example, in
`attr: name term: [1] [dgraph]` the `[1]` shows that this is the term index
([0x1][tok_term]). In `attr: description term: [8] [fast]`, the `[8]` shows that
this is the full-text index ([0x8][tok_fulltext]). These IDs match the index IDs
in [tok.go][tok].

[tok_term]: https://github.com/hypermodeinc/dgraph/blob/ce82aaafba3d9e57cf5ea1aeb9b637193441e1e2/tok/tok.go#L39

[tok_fulltext]: https://github.com/hypermodeinc/dgraph/blob/ce82aaafba3d9e57cf5ea1aeb9b637193441e1e2/tok/tok.go#L48

[tok]: https://github.com/hypermodeinc/dgraph/blob/ce82aaafba3d9e57cf5ea1aeb9b637193441e1e2/tok/tok.go#L37-L53

## Key lookup

Every key can be inspected further with the `--lookup` flag for the specific
key.

```sh
dgraph debug --postings ./p --lookup 000000000000000000000b6465736372697074696f6e020866617374
```

```text
Opening DB: ./p

Key: 000000000000000000000b6465736372697074696f6e020866617374 Length: 2 Is multi-part list? false Uid: 1 Op: 0
 Uid: 2 Op: 0
```

For data keys, a lookup shows its type and value. Below, we see that the key for
`attr: url uid: 1` is a string value.

```sh
dgraph debug --postings ./p --lookup 000000000000000000000375726c000000000000000001
```

```text
Opening DB: ./p

Key: 000000000000000000000375726c000000000000000001 Length: 1 Is multi-part list? false Uid: 18446744073709551615 Op: 1  Type: STRING.  String Value: "https://github.com/hypermodeinc/dgraph
```

For index keys, a lookup shows the UIDs that are part of this index. Below, we
see that the `fast` index for the `<description>` predicate has UIDs 0x1 and
0x2.

```sh
dgraph debug --postings ./p --lookup 000000000000000000000b6465736372697074696f6e020866617374
```

```text
Opening DB: ./p
Key: 000000000000000000000b6465736372697074696f6e020866617374 Length: 2 Is multi-part list? false Uid: 1 Op: 0
 Uid: 2 Op: 0
```

## Key history

You can also look up the history of values for a key using the `--history`
option.

```sh
dgraph debug --postings ./p --lookup 000000000000000000000b6465736372697074696f6e020866617374 --history
```

```text
Opening DB: ./p

==> key: 000000000000000000000b6465736372697074696f6e020866617374. PK: UID: 0, Attr: 0-description, IsIndex: true, Term: 0
ts: 8 {item}{discard}{complete}
 Num uids = 2. Size = 16
 Uid = 1
 Uid = 2

ts: 7 {item}{delta}
 Uid: 2 Op: 1

ts: 5 {item}{delta}
 Uid: 1 Op: 1
```

Above, we see that UID 0x1 was committed to this index at ts 5, and UID 0x2 was
committed to this index at ts 7.

The debug output also shows UserMeta information:

* `{complete}`: Complete posting list
* `{uid}`: UID posting list
* `{delta}`: Delta posting list
* `{empty}`: Empty posting list
* `{item}`: Item posting list
* `{deleted}`: Delete marker

## Parse key

You can parse a key into its constituent components using `--parse_key`. This
doesn't require a p directory.

```sh
dgraph debug --parse_key 000000000000000000000b6467726170682e74797065000000000000000001
```

```text
{d} Key: UID: 1, Attr: 0-dgraph.type, Data key
```


# Drop Data
Source: https://docs.hypermode.com/dgraph/admin/drop-data



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

It is possible to drop all data from your Dgraph backend, and start fresh while
retaining the same endpoint.

Be careful, as this operation isn't reversible, and all data is lost. It is
highly recommended that you [export](./export) your data before you drop your
data.

### Dropping data programmatically

You can drop data by invoking the `dropData` mutation on `/admin/slash`
endpoint.

As an example, if your GraphQL endpoint is `https://<your-backend>/graphql`,
then the admin endpoint for schema is at `https://<your-backend>/admin/slash`.

This endpoint requires authentication.

Here is curl example.

```sh
curl 'https://<your-backend>/admin/slash' \
  -H 'X-Auth-Token: <your-token>' \
  -H 'Content-Type: application/graphql' \
  --data-binary 'mutation { dropData(allData: true) { response { code message } } }'
```

If you would like to drop the schema along with the data, then you can set the
`allDataAndSchema` flag.

```sh
curl 'https://<your-backend>/admin/slash' \
  -H 'X-Auth-Token: <your-token>' \
  -H 'Content-Type: application/graphql' \
  --data-binary 'mutation { dropData(allDataAndSchema: true) { response { code message } } }'
```

## Self-managed

### Drop data and schema

The `/alter` endpoint is used to drop data.

To drop all data and schema:

```sh
curl -X POST localhost:8080/alter -d '{"drop_all": true}'
```

To drop all data only (keep schema):

```sh
curl -X POST localhost:8080/alter -d '{"drop_op": "DATA"}'
```

The `/alter` endpoint can also be used to drop a specific property or all nodes
of a specific type.

To drop property `name`:

```sh
curl -X POST localhost:8080/alter -d '{"drop_attr": "name"}'
```

To drop the type `Film`:

```sh
curl -X POST localhost:8080/alter -d '{"drop_op": "TYPE", "drop_value": "Film"}'
```


# Export Data
Source: https://docs.hypermode.com/dgraph/admin/export



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## Export

As an `Administrator` you can export data from Dgraph to an object store, NFS,
or a file path.

When you export data, typically three files are generated:

* `g01.gql_schema.gz`: The GraphQL schema file. This file can be imported using
  the Schema APIs
* `g01.json.gz` or `g01.rdf.gz`: the data from your instance in JSON format or
  RDF format. By default, Dgraph exports data in RDF format.
* `g01.schema.gz`: This file is the internal Dgraph schema.

## Export data using the GraphQL admin endpoint

You can export the entire data by executing a GraphQL mutation on the `/admin`
endpoint of any Alpha node.

**Before you begin**:

* Ensure that there is sufficient space on disk to store the export. Each Dgraph
  Alpha leader for a group writes output as a gzipped file to the export
  directory specified through the `--export` flag (defaults to an **export**
  directory). If any of the groups fail because of insufficient space on the
  disk, the entire export process is considered failed and an error is returned.

* Make a note of the export directories of the Alpha server nodes. For more
  information about configuring the Dgraph Alpha server, see
  [Configuration](/dgraph/self-managed/config).

This mutation triggers the export from each of the Alpha leader for a group.
Depending on the Dgraph configuration several files are exported. It is
recommended that you copy the files from the Alpha server nodes to a safe place
when the export is complete.

```graphql
mutation {
  export(input: {}) {
    response {
      message
      code
    }
  }
}
```

The export data of the group:

* in the Alpha instance is stored in the Alpha.
* in every other group is stored in the Alpha leader of that group.

You need to retrieve the right export files from the Alpha instances in the
cluster. Dgraph does not copy all files to the Alpha that initiated the export.

When the export is complete a response similar to this appears:

```json
{
  "data": {
    "export": {
      "response": {
        "message": "Export completed.",
        "code": "Success"
      }
    }
  },
  "extensions": {
    "tracing": {
      "version": 1,
      "startTime": "2022-12-14T07:39:51.061712416Z",
      "endTime": "2022-12-14T07:39:51.129431494Z",
      "duration": 67719080
    }
  }
}
```

## Export data format

By default, Dgraph exports data in RDF format. Replace `<FORMAT>`with `json` or
`rdf` in this GraphQL mutation:

```graphql
mutation {
  export(input: { format: "<FORMAT>" }) {
    response {
      message
      code
    }
  }
}
```

## Export to NFS or a file path

You can override the default folder path by adding the `destination` input field
to the directory where you want to export data. Replace `<PATH>` in this GraphQL
mutation with the absolute path of the directory to export data.

```graphql
mutation {
  export(input: { format: "<FORMAT>", destination: "<PATH>" }) {
    response {
      message
      code
    }
  }
}
```

## Export to an object store

You can export to an AWS S3, Azure Blob Storage or Google Cloud Storage.

### Example mutation to export to AWS S3

```graphql
mutation {
  export(
    input: {
      destination: "s3://s3.<region>.amazonaws.com/<bucket-name>"
      accessKey: "<aws-access-key-id>"
      secretKey: "<aws-secret-access-key>"
    }
  ) {
    response {
      message
      code
    }
  }
}
```

<Note> The Dgraph URL used for S3 is different than the AWS CLI
tools with the `aws s3` command, which uses a shortened format:
`s3://<bucket-name>`. </Note>

### Example mutation to export to MinIO

```graphql
mutation {
  export(
    input: {
      destination: "minio://<address>:9000/<bucket-name>"
      accessKey: "<minio-access-key>"
      secretKey: "<minio-secret-key>"
    }
  ) {
    response {
      message
      code
    }
  }
}
```

## Export to a MinIO gateway

You can use MinIO as a gateway to other object stores, such as
[Azure Blob Storage](https://azure.microsoft.com/services/storage/blobs/) or
[Google Cloud Storage](https://cloud.google.com/storage).

### Azure Blob Storage

You can use
[Azure Blob Storage](https://azure.microsoft.com/services/storage/blobs/)
through the
[MinIO Azure Gateway](https://docs.min.io/docs/minio-gateway-for-azure.html).

**Before you begin**:

* Configure a
  [storage account](https://docs.microsoft.com/azure/storage/common/storage-account-overview)
  and a Blob
  [container](https://docs.microsoft.com/azure/storage/blobs/storage-blobs-introduction#containers)
  to organize the blobs.
* Make a note the name of the blob container. It is the `<bucket-name>` when
  specifying the `destination` in the GraphQL mutation.
* [Retrieve storage accounts keys](https://docs.microsoft.com/azure/storage/common/storage-account-keys-manage)
  to configure MinIO. Because,
  [MinIO Azure Gateway](https://docs.min.io/docs/minio-gateway-for-azure.html)
  uses `MINIO_ACCESS_KEY` and `MINIO_SECRET_KEY` to correspond to Azure Storage
  Account `AccountName` and `AccountKey`.

You can access Azure Blob Storage locally using one of these methods:

* Using
  [MinIO Azure Gateway](https://docs.min.io/docs/minio-gateway-for-azure.html)
  with the MinIO Binary

  ```sh
  export MINIO_ACCESS_KEY="<AccountName>"
  export MINIO_SECRET_KEY="<AccountKey>"
  minio gateway azure
  ```

* Using
  [MinIO Azure Gateway](https://docs.min.io/docs/minio-gateway-for-azure.html)
  with Docker

  ```sh
  docker run --detach --rm --name gateway \
   --publish 9000:9000 \
   --env MINIO_ACCESS_KEY="<AccountName>" \
   --env MINIO_SECRET_KEY="<AccountKey>" \
   minio/minio gateway azure
  ```

* Using
  [MinIO Azure Gateway](https://docs.min.io/docs/minio-gateway-for-azure.html)
  with the
  [MinIO Helm chart](https://github.com/minio/minio/tree/master/helm/minio) for
  Kubernetes:

  ```sh
  helm repo add minio https://helm.min.io/
  helm install my-gateway minio/minio \
    --set accessKey="<AccountName>",secretKey="<AccountKey>" \
    --set azuregateway.enabled=true
  ```

  You can use the [MinIO GraphQL mutation](#example-mutation-to-export-to-minio)
  with MinIO configured as a gateway.

### Google Cloud Storage

You can use [Google Cloud Storage](https://cloud.google.com/storage) through the
[MinIO GCS Gateway](https://docs.min.io/docs/minio-gateway-for-gcs.html).

**Before you begin**:

* Create
  [storage buckets](https://cloud.google.com/storage/docs/creating-buckets)
* Create a Service Account key for GCS and get a credentials file

When you have a `credentials.json`, you can access GCS locally using one of
these methods:

* Using [MinIO GCS Gateway](https://docs.min.io/docs/minio-gateway-for-gcs.html)
  with the MinIO Binary

  ```sh
  export GOOGLE_APPLICATION_CREDENTIALS="/path/to/credentials.json"
  export MINIO_ACCESS_KEY="<minio-access-key>"
  export MINIO_SECRET_KEY="<minio-secret-key>"
  minio gateway gcs "<project-id>"
  ```

* Using [MinIO GCS Gateway](https://docs.min.io/docs/minio-gateway-for-gcs.html)
  with Docker

  ```sh
  docker run --detach --rm --name gateway \
    --publish 9000:9000  \
    --volume "</path/to/credentials.json>":/credentials.json \
    --env GOOGLE_APPLICATION_CREDENTIALS=/credentials.json \
    --env MINIO_ACCESS_KEY="<minio-access-key>" \
    --env MINIO_SECRET_KEY="<minio-secret-key>" \
    minio/minio gateway gcs "<project-id>"
  ```

* Using [MinIO GCS Gateway](https://docs.min.io/docs/minio-gateway-for-gcs.html)
  with the
  [MinIO Helm chart](https://github.com/minio/minio/tree/master/helm/minio) for
  Kubernetes:

  ```sh
  ## create MinIO Helm config
  cat <<-EOF > myvalues.yaml
  accessKey: <minio-access-key>
  secretKey: <minio-secret-key>

  gcsgateway:
    enabled: true
    projectId: <project-id>
    gcsKeyJson: |
  $(IFS='\n'; while read -r LINE; do printf '    %s\n' "$LINE"; done < "</path/to/credentials.json>")
  EOF

  ## deploy MinIO GCS Gateway
  helm repo add minio https://helm.min.io/
  helm install my-gateway minio/minio \
    --values myvalues.yaml
  ```

  You can use the [MinIO GraphQL mutation](#example-mutation-to-export-to-minio)
  with MinIO configured as a gateway.

## Disable HTTPS for exports to S3 and MinIO

By default, Dgraph assumes the destination bucket is using HTTPS. If that's not
the case, the export fails. To export to a bucket using HTTP (insecure), set the
query parameter `secure=false` with the destination endpoint in the
`destination` field:

```graphql
mutation {
  export(
    input: {
      destination: "minio://<address>:9000/<bucket-name>?secure=false"
      accessKey: "<minio-access-key>"
      secretKey: "<minio-secret-key>"
    }
  ) {
    response {
      message
      code
    }
  }
}
```

## Use anonymous credentials

When exporting to S3 or MinIO where credentials aren't required, can set
`anonymous` to true.

```graphql
mutation {
  export(
    input: {
      destination: "s3://s3.<region>.amazonaws.com/<bucket-name>"
      anonymous: true
    }
  ) {
    response {
      message
      code
    }
  }
}
```

## Encrypt exports

Export is available wherever an Alpha is running. To encrypt an export, the
Alpha must be configured with the `--encryption key-file=value`.

<Note>
  The `--encryption key-file` used for [Encryption at
  Rest](/dgraph/enterprise/encryption-at-rest) and is also used for encrypted
  exports.
</Note>

## Use `curl` to trigger an export

This is an example of how you can use `curl` to trigger an export.

1. Create GraphQL file for the desired mutation:

   ```sh
   cat <<-EOF > export.graphql
   mutation {
     export(input: {
       destination: "s3://s3.<region>.amazonaws.com/<bucket-name>"
       accessKey: "<aws-access-key-id>"
       secretKey: "<aws-secret-access-key>"
     }) {
       response {
         message
         code
       }
     }
   }
   EOF
   ```

2. Trigger an export with `curl`

   ```sh
   curl http://localhost:8080/admin --silent --request POST \
     --header "Content-Type: application/graphql" \
     --upload-file export.graphql
   ```


# Import Data
Source: https://docs.hypermode.com/dgraph/admin/import



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

As an `Administrator` you can initialize a new Dgraph cluster by doing an
[Initial import](/dgraph/admin/bulk-loader) and you can import data into a
running instance by performing a [Live import](/dgraph/admin/live-loader).

Initial import is **considerably faster** than the live import but can only be
used to load data into a new cluster (without prior data) and is executed before
starting the Alpha nodes.

<Tip>
  Contact us if you need to do an initial import to a Dgraph backend on
  Hypermode.
</Tip>

<Note>
  Both options accept [RDF N-Quad/Triple data](https://www.w3.org/TR/n-quads/)
  or JSON format.
</Note>

To load CSV-formatted data or SQL data into Dgraph, first convert the dataset
into one of the accepted formats
([RDF N-Quad/Triple](https://www.w3.org/TR/n-quads/) or JSON) and then load the
resulting dataset into Dgraph.

After you convert the `.csv` or `.sql` files to
[RDF N-Quad/Triple](https://www.w3.org/TR/n-quads/) or JSON, you can use
[Dgraph Live Loader](/dgraph/admin/live-loader) or
[Dgraph Bulk Loader](/dgraph/admin/bulk-loader) to import your data.


# Import CSV Data
Source: https://docs.hypermode.com/dgraph/admin/import-csv



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## Convert CSV to JSON

There are many tools available to convert CSV to JSON. You can import large data
sets to Dgraph using [Dgraph Live Loader](/dgraph/admin/live-loader) or
[Dgraph Bulk Loader](/dgraph/admin/bulk-loader). In these examples, the
`csv2json` tool is used, and the data is imported using the **Mutate** tab in
Ratel.

### Before you begin

* Install [`csv2json`](https://www.npmjs.com/package/csv2json) conversion tool.
* Install `jq` a lightweight and flexible command-line JSON processor.
* Connect the Dgraph instance to Ratel for queries, mutations and
  visualizations.

#### Example 1

1. Create a `names.csv` file with these details:

   ```csv
   Name,URL
   Dgraph,https://github.com/hypermodeinc/dgraph
   Badger,https://github.com/hypermodeinc/badger
   ```

2. Change to the directory that contains the `names.csv` file and convert it to
   `names.json`:

   ```sh
   csv2json names.csv --out names.json
   ```

3. To prettify a JSON file, use the `jq '.'` command:

   ```sh
   cat names.json | jq '.'
   ```

   The output is similar to:

   ```sh
   [
     {
       "Name": "Dgraph",
       "URL": "https://github.com/hypermodeinc/dgraph"
     },
     {
       "Name": "Badger",
       "URL": "https://github.com/hypermodeinc/badger"
     }
   ]
   ```

   This JSON file follows the [JSON Mutation Format](/dgraph/dql/json), it can
   be loaded into Dgraph using [Dgraph Live Loader](./live-loader) ,
   [Dgraph Bulk Loader](./bulk-loader) or the programmatic clients.

4. To load the data to Ratel and HTTP clients. The JSON data has to be stored
   within the `"set"`
   [key](/dgraph/dql/json#json-syntax-using-raw-http-or-ratel-ui"). You can use
   `jq` to transform the JSON into the correct format:

   ```sh
   cat names.json | jq '{ set: . }'
   ```

   An output similar to this appears:

   ```json
   {
     "set": [
       {
         "Name": "Dgraph",
         "URL": "https://github.com/hypermodeinc/dgraph"
       },
       {
         "Name": "Badger",
         "URL": "https://github.com/hypermodeinc/badger"
       }
     ]
   }
   ```

5. Paste the output in the **Mutate** tab of **Console** in Ratel.

6. Click **Run** to import data.

7. To view the imported data paste the following in the **Query** tab and click
   **Run**:

   ```dql
   {
    names(func: has(URL)) {
    Name
    }
   }
   ```

#### Example 2

1. Create a `connects.csv` file that's connecting nodes together. The `connects`
   field should be of the `uid` type.

   ```csv
   uid,connects
   _:a,_:b
   _:a,_:c
   _:c,_:d
   _:d,_:a
   ```

2. To get the correct JSON format, you can convert the CSV into JSON and use
   `jq` to transform it in the correct format where the `connects` edge is a
   node `uid`. This JSON file can be loaded into Dgraph using the programmatic
   clients.

   ```sh
   csv2json connects.csv | jq '[ .[] | { uid: .uid, connects: { uid: .connects } } ]'
   ```

   The output is similar to:

   ```json
   [
     {
       "uid": "_:a",
       "connects": {
         "uid": "_:b"
       }
     },
     {
       "uid": "_:a",
       "connects": {
         "uid": "_:c"
       }
     },
     {
       "uid": "_:c",
       "connects": {
         "uid": "_:d"
       }
     },
     {
       "uid": "_:d",
       "connects": {
         "uid": "_:a"
       }
     }
   ]
   ```

3. To get an output of the mutation format accepted in Ratel UI and HTTP
   clients:

   ```sh
   csv2json connects.csv | jq '{ set: [ .[] | {uid: .uid, connects: { uid: .connects } } ] }'
   ```

   The output is similar to:

   ```json
   {
     "set": [
       {
         "uid": "_:a",
         "connects": {
           "uid": "_:b"
         }
       },
       {
         "uid": "_:a",
         "connects": {
           "uid": "_:c"
         }
       },
       {
         "uid": "_:c",
         "connects": {
           "uid": "_:d"
         }
       },
       {
         "uid": "_:d",
         "connects": {
           "uid": "_:a"
         }
       }
     ]
   }
   ```

   <Note>
     To reuse existing integer IDs from a CSV file as UIDs in Dgraph, use Dgraph
     Zero's [assign endpoint](/dgraph/self-managed/dgraph-zero) before loading
     data to allocate a range of UIDs that can be safely assigned.
   </Note>

4. Paste the output in the **Mutate** tab of **Console** in Ratel, and click
   **Run** to import data.


# Import MySQL Data
Source: https://docs.hypermode.com/dgraph/admin/import-mysql



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

You can use the Dgraph migration tool to convert a MySQL database tables into a
schema and RDF file, and then load the resulting dataset into Dgraph.

## Deriving a Dgraph schema from SQL

Before converting the data, the migration tool needs to derive the schema of
each predicate. Dgraph follows two simple rules for converting the schema:

1. For plain attributes, there is usually a one-to-one mapping between a SQL
   data type and the Dgraph datatype. For instance, a `Body` column in the
   `Posts` table is of type `text`, and hence, the predicate `posts.Body` is of
   type `string`: `posts.Body: string .`
2. The predicates representing inter-object relationships, like
   `posts.OwnerUserId.`, simply have the type `[uid]`, meaning following the
   predicate leads us to a set of other objects.

### Using the migration tool

You can run the Dgraph migrate tool using this command:

```sh
dgraph migrate [flags]
```

1. Create a `config.properties` file that has the following settings and values
   shouldn't be in quotes:

   ```txt
   user = <SQL_DB_USERNAME>
   password = <SQL_DB_PASSWORD>
   db = <SQL_DB>
   ```

2. Export the SQL database into `schema.txt` and `sql.rdf` file:

   ```sh
   dgraph migrate --config config.properties --output_schema schema.txt --output_data sql.rdf
   ```

   An output similar to this appears:

   ```txt
   Dumping table xyz
   Dumping table constraints xyz
   ...
   ```

<Note>
  If you are connecting to a remote DB hosted on AWS, Google Cloud, and others,
  you need to pass the flags `--host`, and `--port`. For description of the
  various flags in the migration tool, see [command line
  options](dgraph/cli/command-reference#dgraph-migrate).
</Note>

After the migration is complete, two new files are available:

* an RDF file `sql.rdf` containing all the N-Quad entries
* a schema file `schema.txt`.

### Importing the data

The two files can then be imported into Dgraph using the
[Dgraph Live Loader](/dgraph/admin/live-loader) or
[Bulk Loader](/dgraph/admin/bulk-loader). Sometimes you might want to customize
your schema. For example, you might add an index to a predicate, or change an
inter-object predicate (edge) from unidirectional to bidirectional by adding the
`@reverse` directive. If you would like such customizations, you should do it by
editing the schema file generated by the migration tool before feeding the files
to the Live Loader or Bulk Loader.

To import the data into Dgraph using the Live Loader to Dgraph Zero and Alpha
servers running on the default ports use:

```sh
dgraph live -z localhost:5080 -a localhost:9080 --files sql.rdf --format=rdf --schema schema.txt
```


# Increment Tool
Source: https://docs.hypermode.com/dgraph/admin/increment-tool



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The `dgraph increment` tool increments a counter value via transactions. The
increment tool can be used as a health check that an Alpha is able to service
transactions for both queries and mutations.

## Example

Increment the default predicate (`counter.val`) once. If the predicate doesn't
yet exist, then it is created starting at counter 0.

```sh
dgraph increment
```

Increment the counter predicate against the Alpha running at address `--alpha`
(default: `localhost:9080`):

```sh
dgraph increment --alpha=192.168.1.10:9080
```

Increment the counter predicate specified by `--pred` (default: `counter.val`):

```sh
dgraph increment --pred=counter.val.healthcheck
```

Run a read-only query for the counter predicate and doesn't run a mutation to
increment it:

```sh
dgraph increment --ro
```

Run a best-effort query for the counter predicate and doesn't run a mutation to
increment it:

```sh
dgraph increment --be
```

Run the increment tool 1000 times every 1 second:

```sh
dgraph increment --num=1000 --wait=1s
```

## Increment tool output

```sh
 Run increment a few times
$ dgraph increment
0410 10:31:16.379 Counter VAL: 1   [ Ts: 1 ]
$ dgraph increment
0410 10:34:53.017 Counter VAL: 2   [ Ts: 3 ]
$ dgraph increment
0410 10:34:53.648 Counter VAL: 3   [ Ts: 5 ]

 Run read-only queries to read the counter a few times
$ dgraph increment --ro
0410 10:34:57.35  Counter VAL: 3   [ Ts: 7 ]
$ dgraph increment --ro
0410 10:34:57.886 Counter VAL: 3   [ Ts: 7 ]
$ dgraph increment --ro
0410 10:34:58.129 Counter VAL: 3   [ Ts: 7 ]

 Run best-effort query to read the counter a few times
$ dgraph increment --be
0410 10:34:59.867 Counter VAL: 3   [ Ts: 7 ]
$ dgraph increment --be
0410 10:35:01.322 Counter VAL: 3   [ Ts: 7 ]
$ dgraph increment --be
0410 10:35:02.674 Counter VAL: 3   [ Ts: 7 ]

 Run a read-only query to read the counter 5 times
$ dgraph increment --ro --num=5
0410 10:35:18.812 Counter VAL: 3   [ Ts: 7 ]
0410 10:35:18.813 Counter VAL: 3   [ Ts: 7 ]
0410 10:35:18.815 Counter VAL: 3   [ Ts: 7 ]
0410 10:35:18.817 Counter VAL: 3   [ Ts: 7 ]
0410 10:35:18.818 Counter VAL: 3   [ Ts: 7 ]

 Increment the counter 5 times
$ dgraph increment --num=5
0410 10:35:24.028 Counter VAL: 4   [ Ts: 8 ]
0410 10:35:24.061 Counter VAL: 5   [ Ts: 10 ]
0410 10:35:24.104 Counter VAL: 6   [ Ts: 12 ]
0410 10:35:24.145 Counter VAL: 7   [ Ts: 14 ]
0410 10:35:24.178 Counter VAL: 8   [ Ts: 16 ]

 Increment the counter 5 times, once every second.
$ dgraph increment --num=5 --wait=1s
0410 10:35:26.95  Counter VAL: 9   [ Ts: 18 ]
0410 10:35:27.975 Counter VAL: 10   [ Ts: 20 ]
0410 10:35:28.999 Counter VAL: 11   [ Ts: 22 ]
0410 10:35:30.028 Counter VAL: 12   [ Ts: 24 ]
0410 10:35:31.054 Counter VAL: 13   [ Ts: 26 ]

 If the Alpha is too busy or unhealthy, the tool will timeout and retry.
$ dgraph increment
0410 10:36:50.857 While trying to process counter: Query error: rpc error: code = DeadlineExceeded desc = context deadline exceeded. Retrying...
```


# Live Import (Live Loader)
Source: https://docs.hypermode.com/dgraph/admin/live-loader



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

You can import data on a running Dgraph instance (which may have prior data)
using Dgraph CLI command
[dgraph live](/dgraph/cli/command-reference#dgraph-live) referred to as **Live
Loader**. Live Loader sends mutations to a Dgraph cluster and has options to
handle unique IDs assignment and to update existing data.

<Note>
  Live Loader accepts [RDF N-Quad/Triple data](https://www.w3.org/TR/n-quads/)
  or JSON in plain or gzipped format. Refers to [data migration](./import) to
  see how to convert other data formats.
</Note>

## Before you begin

Verify that you have a local folder `<local-path-to-data>` containing

* at least one **data file** in RDF or JSON in plain or gzip format with the
  data to import
* an optional **schema file**.

Those files have been generated by an [export](./export) or by a
[data migration](./import) tool.

## Batch upserts

You can use Live Loader to update existing data, either to modify existing
predicates are to add new predicates to existing nodes.

To do so, use the `-U, --upsertPredicate` flag or the `-x, --xidmap` flag.

### upsertPredicate flag

Use the `-U, --upsertPredicate` flag to specify the predicate name in your data
that serve as unique identifier.

For example:

```sh
dgraph live --files <directory-with-data-files> --schema <path-to-schema-file> --upsertPredicate xid
```

The upsert predicate used must be present the Dgraph instance or in the schema
file and must be indexed.

For each node, Live Loader uses the node name provided in the data file as the
upsert predicate value. For example if your data file contains

```txt
<_:my.org/customer/1>       <firstName>  "John"     .
```

The previous command creates or updates the node with predicate `xid` equal to
`my.org/customer/1` and sets the predicate `firstName` with the value `John`.

### xidmap flag

```sh
dgraph live --files <directory-with-data-files> --schema <path-to-schema-file> --xidmap <local-directory>
```

Live Loader uses `-x, --xidmap` directory to lookup the `uid` value for each
node name used in the data file or to store the mapping between the node names
and the generated `uid` for every new node.

## Import data on Dgraph self-hosted

Run the Live Loader using the `-a, --alpha` flag as follows

<Tabs>
  <Tab title="Docker">
    ```sh
    docker run -it --rm -v <local-path-to-data>:/tmp dgraph/dgraph:latest \
      dgraph live --alpha <Dgraph Alpha gRPC endpoint> -f /tmp/<data-file> -s /tmp/<schema-file>
    ```

    Load multiple data files by using

    ```sh
    docker run -it --rm -v <local-path-to-data>:/tmp dgraph/dgraph:latest \
      dgraph live --alpha <Dgraph Alpha gRPC endpoint> -f /tmp -s /tmp/<schema-file>
    ```

    `--alpha` default value is `localhost:9080`. You can specify a comma separated
    list of alphas addresses in the same cluster to distribute the load.

    When the path provided with `-f, --files` option is a directory, then all files
    ending in `.rdf`, `.rdf.gz`, `.json`, and `.json.gz` are loaded. Be sure that
    your schema file has another extension (.txt or .schema for example).
  </Tab>

  <Tab title="Local">
    ```sh
      dgraph live --alpha <grpc-endpoints> -f <local-path-to-data>/<data-file> -s <local-path-to-data>/<schema-file>
    ```

    `--alpha` default value is `localhost:9080`. You can specify a comma separated
    list of alphas addresses in the same cluster to distribute the load.
  </Tab>
</Tabs>

### Load from S3

To live load from
[Amazon S3 (Simple Storage Service)](https://aws.amazon.com/s3/), you must have
either permissions to access the S3 bucket from the system performing live load
(see [IAM setup](#iam-setup) below) or explicitly add the following AWS
credentials set via environment variables:

| Environment Variable                        | Description                                                         |
| ------------------------------------------- | ------------------------------------------------------------------- |
| `AWS_ACCESS_KEY_ID` or `AWS_ACCESS_KEY`     | AWS access key with permissions to write to the destination bucket. |
| `AWS_SECRET_ACCESS_KEY` or `AWS_SECRET_KEY` | AWS access key with permissions to write to the destination bucket. |

#### IAM setup

In AWS, you can accomplish this by doing the following:

1. Create an
   [IAM Role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create.html)
   with an IAM Policy that grants access to the S3 bucket.
2. Depending on whether you want to grant access to an EC2 instance, or to a pod
   running on [EKS](https://aws.amazon.com/eks/), you can do one of these
   options:

   * [Instance Profile](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html)
     can pass the IAM Role to an EC2 Instance
   * [IAM Roles for Amazon EC2](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html)
     to attach the IAM Role to a running EC2 Instance
   * [IAM roles for service accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html)
     to associate the IAM Role to a
     [Kubernetes Service Account](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/).

   Once your setup is ready, you can execute the live load from S3. As examples:

```sh
## short form of S3 URL
dgraph live \
  --files s3:///<bucket-name>/<directory-with-data-files> \
  --schema s3:///<bucket-name>/<directory-with-data-files>/schema.txt

## long form of S3 URL
dgraph live \
  --files s3://s3.<region>.amazonaws.com/<bucket>/<directory-with-data-files> \
  --schema s3://s3.<region>.amazonaws.com/<bucket>/<directory-with-data-files>/schema.txt
```

<Note>
  The short form of the S3 URL requires S3 URL is prefixed with `s3:///`
  (noticed the triple-slash `///`). The long form for S3 buckets requires a
  double slash (`s3://`).
</Note>

### Load from MinIO

To live load from MinIO, you must have the following MinIO credentials set via
environment variables:

| Environment Variable | Description                                                           |
| -------------------- | --------------------------------------------------------------------- |
| `MINIO_ACCESS_KEY`   | MinIO access key with permissions to write to the destination bucket. |
| `MINIO_SECRET_KEY`   | MinIO secret key with permissions to write to the destination bucket. |

Once your setup is ready, you can execute the bulk load from MinIO:

```sh
dgraph live \
  --files minio://minio-server:port/<bucket-name>/<directory-with-data-files> \
  --schema minio://minio-server:port/<bucket-name>/<directory-with-data-files>/schema.txt
```

## Enterprise features

### Multi-tenancy

Since [multi-tenancy](/dgraph/enterprise/multitenancy) requires ACL, when using
the Live Loader you must provide the login credentials using the `--creds` flag.
By default, Live Loader loads the data into the user's namespace.

[Guardians of the Galaxy](/dgraph/enterprise/multitenancy#guardians-of-the-galaxy)
can load the data into multiple namespaces. Using `--force-namespace`, a
*Guardian* can load the data into the namespace specified in the data and schema
files.

<Note>
  The Live Loader requires that the `namespace` from the data and schema files
  exist before loading the data.
</Note>

For example, to preserve the namespace while loading data first you need to
create the namespace(s) and then run the Live Loader command:

```sh
dgraph live \
  --schema /tmp/data/1million.schema \
  --files /tmp/data/1million.rdf.gz --creds="user=groot;password=password;namespace=0" \
  --force-namespace -1
```

A *Guardian of the Galaxy* can also load data into a specific namespace. For
example, to force the data loading into namespace `123`:

```sh
dgraph live \
  --schema /tmp/data/1million.schema \
  --files /tmp/data/1million.rdf.gz \
  --creds="user=groot;password=password;namespace=0" \
  --force-namespace 123
```

<Note>
  The Live Loader requires that the `namespace` from the data and schema files
  exist before loading the data.
</Note>

### Encrypted imports

A new flag `--encryption key-file=value` is added to the Live Loader. This
option is required to decrypt the encrypted export data and schema files. Once
the export files are decrypted, the Live Loader streams the data to a live Alpha
instance. Alternatively, starting with v20.07.0, the `vault_*` options can be
used to decrypt the encrypted export and schema files.

<Note>
  If the live Alpha instance has encryption turned on, the `p` directory is
  encrypted. Otherwise, the `p` directory is unencrypted.
</Note>

For example, to load an encrypted RDF/JSON file and schema via Live Loader:

```sh
dgraph live \
 --files <path-containerizing-encrypted-data-files> \
 --schema <path-to-encrypted-schema> \
 --encryption key-file=<path-to-keyfile-to-decrypt-files>
```

You can import your encrypted data into a new Dgraph Alpha node without
encryption enabled.

```sh
# Encryption Key from the file path
dgraph live --files "<path-to-gzipped-RDF-or-JSON-file>" --schema "<path-to-schema>"  \
  --alpha "<dgraph-alpha-address:grpc_port>" --zero "<dgraph-zero-address:grpc_port>" \
  --encryption key-file="<path-to-enc_key_file>"

# Encryption Key from HashiCorp Vault
dgraph live --files "<path-to-gzipped-RDF-or-JSON-file>" --schema "<path-to-schema>"  \
  --alpha "<dgraph-alpha-address:grpc_port>" --zero "<dgraph-zero-address:grpc_port>" \
  --vault addr="http://localhost:8200";enc-field="enc_key";enc-format="raw";path="secret/data/dgraph/alpha";role-id-file="./role_id";secret-id-file="./secret_id"

```

## Other Live Loader options

`--new_uids` (default: `false`): assign new UIDs instead of using the existing
UIDs in data files. This is useful to avoid overriding the data in a DB already
in operation.

`--format`: specify file format (`rdf` or `json`) instead of getting it from
filenames. This is useful if you need to define a strict format manually.

`-b, --batch` (default: `1000`): number of N-Quads to send as part of a
mutation.

`-c, --conc` (default: `10`): number of concurrent requests to make to Dgraph.
Don't confuse with `-C`.

`-C, --use_compression` (default: `false`): enable compression for connections
to and from the Alpha server.

`--vault` [superflag's](/dgraph/cli/command-reference) options specify the Vault
server address, role id, secret id, and field that contains the encryption key
required to decrypt the encrypted export.


# Logs
Source: https://docs.hypermode.com/dgraph/admin/logs

Dgraph logs requests for queries and mutations, and also provides audit logging capabilities with a Dgraph Enterprise license

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph logs requests for queries and mutations, and also provides audit logging
capabilities with a Dgraph [enterprise license](/dgraph/enterprise/license).

Dgraph's log format comes from the glog library and is
[formatted](https://github.com/golang/glog/blob/23def4e6c14b4da8ac2ed8007337bc5eb5007998/glog.go#L523-L533)
as follows:

```sh
Lmmdd hh:mm:ss.uuuuuu threadid file:line] msg...
```

The fields are defined as follows:

| Field             | Definition                                                            |
| ----------------- | --------------------------------------------------------------------- |
| `L`               | A single character, representing the log level (such as 'I' for INFO) |
| `mm`              | Month (zero padded; ie May is '05')                                   |
| `dd`              | Day (zero padded)                                                     |
| `hh:mm:ss.uuuuuu` | Time in hours, minutes and fractional seconds                         |
| `threadid`        | Space-padded thread ID as returned by GetTID()                        |
| `file`            | Filename                                                              |
| `line`            | Line number                                                           |
| `msg`             | User-supplied message                                                 |

## Log verbosity

To increase log verbosity, set the flag `-v=3` (or `-v=2`) which enables verbose
logging for everything. You can set this flag on both Zero and Alpha nodes.

<Note>Changing log verbosity requires a restart of the node.</Note>

## Request logging

Request logging, sometimes called *query logging*, lets you log queries and
mutations. You can dynamically turn request logging on or off. To toggle request
logging on, send the following GraphQL mutation to the `/admin` endpoint of an
Alpha node (for example `localhost:8080/admin`):

```graphql
mutation {
  config(input: { logDQLRequest: true }) {
    response {
      code
      message
    }
  }
}
```

<Note>This input flag was named `logRequest` in versions prior to v23.</Note>

The response should look like the following:

```json
{
  "data": {
    "config": {
      "response": {
        "code": "Success",
        "message": "Config updated successfully"
      }
    }
  },
  "extensions": {
    "tracing": {
      "version": 1,
      "startTime": "2020-12-07T14:53:28.240420495Z",
      "endTime": "2020-12-07T14:53:28.240569604Z",
      "duration": 149114
    }
  }
}
```

The Alpha node prints the following `INFO` message to confirm that the mutation
has been applied:

```sh
I1207 14:53:28.240516   20143 config.go:39] Got config update through GraphQL admin API
```

When enabling request logging this prints the requests that Dgraph Alpha
receives from Ratel or other clients. In this case, the Alpha log prints
something similar to:

```sh
I1201 13:06:26.686466   10905 server.go:908] Got a query: query:"{\n  query(func: allofterms(name@en, \"Marc Caro\"))
{\n  uid\n  name@en\n  director.film\n  }\n}"
```

As you can see, we got the query that Alpha received. To read it in the original
DQL format just replace every `\n` with a new line, any `\t` with a tab
character and `\"` with `"`:

```graphql
{
  query(func: allofterms(name@en, "Marc Caro")) {
  uid
  name@en
  director.film
  }
}
```

Similarly, you can turn off request logging by setting `logRequest` to `false`
in the `/admin` mutation.

```graphql
mutation {
  config(input: { logRequest: false }) {
    response {
      code
      message
    }
  }
}
```

## Audit logging (enterprise feature)

With a Dgraph enterprise license, you can enable audit logging so that all
requests are tracked and available for use in security audits. To learn more,
see [Audit Logging](/dgraph/enterprise/audit-logs).


# Metrics
Source: https://docs.hypermode.com/dgraph/admin/metrics

Dgraph database helps administrators by providing metrics on Dgraph instance activity, disk activity, server node health, memory, and Raft leadership

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph database provides metrics on Dgraph instance activity, disk activity,
server node health, memory, and Raft leadership. It also provides built-in
metrics provided by Go. Dgraph metrics follow the
[metric and label conventions for the Prometheus](https://prometheus.io/docs/practices/naming/)
monitoring and alerting toolkit.

## Activity metrics

Activity metrics let you track the mutations, queries, and proposals of a Dgraph
instance.

| Metric                                             | Description                                             |
| -------------------------------------------------- | ------------------------------------------------------- |
| `go_goroutines`                                    | Total number of goroutines currently running in Dgraph. |
| `dgraph_active_mutations_total`                    | Total number of mutations currently running.            |
| `dgraph_pending_proposals_total`                   | Total pending Raft proposals.                           |
| `dgraph_pending_queries_total`                     | Total number of queries in progress.                    |
| `dgraph_num_queries_total{method="Server.Mutate"}` | Total number of mutations run in Dgraph.                |
| `dgraph_num_queries_total{method="Server.Query"}`  | Total number of queries run in Dgraph.                  |

## Disk metrics

Disk metrics let you track the disk activity of the Dgraph process. Dgraph does
not interact directly with the filesystem. Instead it relies on
[Badger](https://github.com/hypermodeinc/badger) to read from and write to disk.

| Metric                              | Description                                                |
| ----------------------------------- | ---------------------------------------------------------- |
| `badger_read_num_vlog`              | Total count of reads by badger in vlog                     |
| `badger_write_num_vlog`             | Total count of writes by Badger in vlog                    |
| `badger_read_bytes_vlog`            | Total bytes read by Badger                                 |
| `badger_write_bytes_vlog`           | Total bytes written by Badger                              |
| `badger_read_bytes_lsm`             | Total bytes read by Badger                                 |
| `badger_write_bytes_l0`             | Total bytes written by Badger                              |
| `badger_write_bytes_compaction`     | Total bytes written by Badger                              |
| `badger_get_num_lsm`                | Total count of LSM gets                                    |
| `badger_get_num_memtable`           | Total count of LSM gets from memtable                      |
| `badger_hit_num_lsm_bloom_filter`   | Total count of LSM bloom hits                              |
| `badger_get_num_user`               | Total count of calls to Badger's `get`                     |
| `badger_put_num_user`               | Total count of calls to Badger's `put`                     |
| `badger_write_bytes_user`           | Total bytes written by user                                |
| `badger_get_with_result_num_user`   | Total count of calls to Badger's `get` that returned value |
| `badger_iterator_num_user`          | Total count of iterators made in badger                    |
| `badger_size_bytes_lsm`             | Size of the LSM in bytes                                   |
| `badger_size_bytes_vlog`            | Size of the value log in bytes                             |
| `badger_write_pending_num_memtable` | Total count of pending writes                              |
| `badger_compaction_current_num_lsm` | Number of tables being actively compacted                  |

In versions prior to v23.1, the disk metrics were:

| Metric                        | Description                                  |
| ----------------------------- | -------------------------------------------- |
| `badger_disk_reads_total`     | Total count of disk reads in Badger          |
| `badger_disk_writes_total`    | Total count of disk writes in Badger         |
| `badger_gets_total`           | Total count of calls to Badger's `get`       |
| `badger_memtable_gets_total`  | Total count of memtable accesses to Badger's |
| `get`. `badger_puts_total`    | Total count of calls to Badger's `put`       |
| `badger_read_bytes`           | Total bytes read from Badger                 |
| `badger_lsm_bloom_hits_total` | Total number of LSM tree bloom hits          |
| `badger_written_bytes`        | Total bytes written to Badger                |
| `badger_lsm_size_bytes`       | Total size in bytes of the LSM tree          |
| `badger_vlog_size_bytes`      | Total size in bytes of the value log         |

## Go metrics

Go's built-in metrics may also be useful to measure memory usage and garbage
collection time.

| Metric                         | Description                                                                                 |
| ------------------------------ | ------------------------------------------------------------------------------------------- |
| `go_memstats_gc_cpu_fraction`  | The fraction of this program's available CPU time used by the GC since the program started. |
| `go_memstats_heap_idle_bytes`  | Number of heap bytes waiting to be used.                                                    |
| `go_memstats_heap_inuse_bytes` | Number of heap bytes that are in use.                                                       |

## Health metrics

Health metrics let you check the health of a server node.

<Note>Health metrics are only available for Dgraph Alpha server nodes.</Note>

| Metric                       | Description                                                                                                                                                                                                                             |
| ---------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `dgraph_alpha_health_status` | Value is 1 when the Alpha node is ready to accept requests; otherwise 0.                                                                                                                                                                |
| `dgraph_max_assigned_ts`     | Latest max assigned timestamp–all Alpha nodes within the same Alpha group should show the same timestamp if they're in sync                                                                                                             |
| `dgraph_txn_aborts_total`    | Shows the total number of server-initiated transaction aborts that have occurred on the Alpha node.                                                                                                                                     |
| `dgraph_txn_commits_total`   | Shows the total number of successful commits that have occurred on the Alpha node.                                                                                                                                                      |
| `dgraph_txn_discards_total`  | Shows the total number of client-initiated transaction discards that have occurred on the Alpha node. This is incremented when the client calls for a transaction discard, such as using the Dgraph Go client's `txn.Discard` function. |

## Memory metrics

Memory metrics let you track the memory usage of the Dgraph process. The `idle`
and `inuse` metrics give you a better sense of the active memory usage of the
Dgraph process. The process memory metric shows the memory usage as measured by
the operating system.

By looking at all three metrics you can see how much memory a Dgraph process is
holding from the operating system and how much is actively in use.

| Metric                      | Description                                                                                |
| --------------------------- | ------------------------------------------------------------------------------------------ |
| `dgraph_memory_idle_bytes`  | Estimated amount of memory held idle that could be reclaimed by the OS                     |
| `dgraph_memory_inuse_bytes` | Total memory usage in bytes (sum of heap usage and stack usage)                            |
| `dgraph_memory_proc_bytes`  | Total memory usage in bytes of the Dgraph process–equivalent to resident set size on Linux |

## Raft leadership metrics

Raft leadership metrics let you track changes in Raft leadership for Dgraph
Alpha and Dgraph Zero nodes in your Cluster. These metrics include a group label
along with the node name, so that you can determine which metrics apply to which
Raft groups.

| Metric                             | Description                                                       |
| ---------------------------------- | ----------------------------------------------------------------- |
| `dgraph_raft_has_leader`           | Value is 1 when the node has a leader; otherwise 0.               |
| `dgraph_raft_is_leader`            | Value is 1 when the node is the leader of its group; otherwise 0. |
| `dgraph_raft_leader_changes_total` | The total number of leader changes seen by this node.             |


# Retrieve Schema
Source: https://docs.hypermode.com/dgraph/admin/schema



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

You can retrieve the Dgraph schema containing the list of predicates types and
node types by:

* issuing a query on /query endpoint using the
  [HTTP Client](/dgraph/http#query-current-dql-schema)
* issuing a query using any [DQL client library](/dgraph/sdks/overview)
* using [Ratel UI](/dgraph/ratel/schema)

When using a query, the request body is

```dql
schema {}
```

<Note>
  Unlike regular queries, the schema query isn't surrounded by curly braces.
  Also, schema queries and regular queries can't be combined.
</Note>

You can query for particular schema fields in the query body.

```dql
schema {
  type
  index
  reverse
  tokenizer
  list
  count
  upsert
  lang
}
```

You can also query for particular predicates:

```dql
schema(pred: [name, friend]) {
  type
  index
  reverse
  tokenizer
  list
  count
  upsert
  lang
}
```

<Note>
  If Access Control Lists (ACL) is enabled, then the schema query returns only
  the predicates for which the logged-in ACL user has read access.
</Note>

Types can also be queried. Below are some example queries.

```dql
schema(type: Movie) {}
schema(type: [Person, Animal]) {}
```

Note that type queries don't contain anything between the curly braces. The
output is the entire definition of the requested types.


# Sentry Integration
Source: https://docs.hypermode.com/dgraph/admin/sentry



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Sentry is a powerful service that allows apps to send arbitrary events,
messages, exceptions, and bread-crumbs (logs) to your sentry account. In
simplest terms, it's a dial-home service but also has a rich feature set
including event filtering, data scrubbing, several SDKs, and custom and release
tagging, as well as integration with third party tools such as Slack, GitHub.

Although Sentry reporting is on by default, starting from v20.03.1 and v20.07.0,
there is a configuration flag `enable-sentry` which can be used to completely
turn off Sentry events reporting.

## Basic Integration

### Panics (runtime and manual)

* As of now, at Dgraph, we use Sentry reporting for capturing panics only. For
  manual panics anywhere in the code, `sentry.CaptureException()` API is called.

* For runtime panics, Sentry doesn't have a native method. After further
  research, we chose the approach of a wrapper process to capture these panics.
  The basic idea for this is that whenever a dgraph instance is started, a
  second monitoring process is started whose only job is to monitor the `stderr`
  for panics of the monitored process. When a panic is seen, it's reported back
  to sentry via the CaptureException API.

### Reporting

Each event is tagged with the release version, environment, timestamp, tags, and
the panic stack trace as explained below.

### Release

This is the release version string of the Dgraph instance.

### Environments

We've defined 4 environments:

**dev-oss / dev-enterprise**: these are events seen on non-released / local
developer builds.

**prod-oss/prod-enterprise**: these are events on released version. Events in
this category are also sent on a slack channel private to Dgraph

**Tags:**

Tags are key-value pairs that provide additional context for an event. We've
defined the following tags:

`dgraph`: this tag can have values `zero` or `alpha` depending on which
sub-command saw the panic/exception.

## Data handling

We strive to handle your data with care in a variety of ways when sending events
to Sentry

1. **Event Selection:** only panic events are sent to Sentry from Dgraph.
2. **Data in Transit:** events sent from the SDK to the Sentry server are
   encrypted on the wire with industry-standard TLS protocol with 256 bit AES
   Cipher.
3. **Data at rest:** events on the Sentry server are also encrypted with 256 bit
   AES cipher. Sentry is hosted on Google Cloud and as such physical access is
   tightly controlled. Logical access is only available to sentry approved
   officials.
4. **Data Retention:** Sentry stores events only for 90 days after which they
   are removed permanently.
5. **Data Scrubbing**: the Data Scrubber option (default: on) in Sentry’s
   settings ensures personally identifiable information doesn’t get sent to or
   stored on Sentry’s servers, automatically removing any values that look like
   they contain sensitive information for values that contain various strings.
   The strings we currently monitor and scrub are:

* `password`
* `secret`
* `passwd`
* `api_key`
* `apikey`
* `access_token`
* `auth_token`
* `credentials`
* `mysql_pwd`
* `stripetoken`
* `card[number]`
* `ip addresses`


# Traces
Source: https://docs.hypermode.com/dgraph/admin/traces



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph is integrated with [OpenCensus](https://opencensus.io/zpages/) to collect
distributed traces from the Dgraph cluster.

Trace data is always collected within Dgraph. You can adjust the trace sampling
rate for Dgraph queries using the `--trace`
[superflag's](/dgraph/cli/command-reference) `ratio` option when running Dgraph
Alpha nodes. By default, `--trace ratio` is set to 0.01 to trace 1% of queries.

## Examining traces with zPages

The most basic way to view traces is with the integrated trace pages.

OpenCensus's [zPages](https://opencensus.io/zpages/) are accessible via the Zero
or Alpha HTTP port at `/z/tracez`.

## Examining traces with Jaeger

Jaeger collects distributed traces and provides a UI to view and query traces
across different services. This provides the necessary observability to
understand what's happening in the system.

Dgraph can be configured to send traces directly to a Jaeger collector with the
`trace` superflag's `jaeger` option. For example, if the Jaeger collector is
running on `http://localhost:14268`, then pass this option to the Dgraph Zero
and Dgraph Alpha instances as `--trace jaeger=http://localhost:14268`.

See
[Jaeger's Getting Started docs](https://www.jaegertracing.io/docs/getting-started/)
to get up and running with Jaeger.

### Setting up multiple Dgraph clusters with Jaeger

Jaeger allows you to examine traces from multiple Dgraph clusters. To do this,
use the `--collector.tags` on a Jaeger collector to set custom trace tags. For
example, run one collector with `--collector.tags env=qa` and then another
collector with `--collector.tags env=dev`. In Dgraph, set the `--trace jaeger`
option in the Dgraph QA cluster to the first collector and set this option in
the Dgraph development cluster to the second collector. You can run multiple
Jaeger collector components for the same single Jaeger backend. This is still a
single Jaeger installation but with different collectors customizing the tags
per environment.

Once you have this configured, you can filter by tags in the Jaeger UI. Filter
traces by tags matching `env=dev`:

![Jaeger UI](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/jaeger-ui.png)

Every trace has your custom tags set under the “Process” section of each span:

![Jaeger Query](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/jaeger-server-query.png)

Filter traces by tags matching `env=qa`:

![Jaeger JSON](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/jaeger-json.png)

![Jaeger Query Result](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/jaeger-server-query-2.png)

To learn more about Jaeger, see
[Jaeger's Deployment Guide](https://www.jaegertracing.io/docs/deployment/).


# Update Types
Source: https://docs.hypermode.com/dgraph/admin/update-types



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

You modify Dgraph types (node types and predicates types) by

* issuing a request to the `/alter` endpoint using the
  [HTTP Client](/dgraph/http#alter-the-dql-schema)
* using an `alter` operation of any [DQL client library](/dgraph/sdks/overview).
* using [Ratel UI](/dgraph/ratel/schema)

### Notes about predicate type change

If data is already stored, existing values aren't checked to conform to the
updated predicate type.

On query, Dgraph tries to convert existing values to the new predicate type and
ignores any that fail conversion.

If data exists and new indexes are specified, any old index not in the updated
schema is dropped. New indexes are created.

## Indexes in background

Indexes may take long time to compute depending upon the size of the data.

Indexes can be computed in the background and thus indexing may still be running
after an Alter operation returns.

To run index computation in the background set the flag `runInBackground` to
`true` .

```sh
curl localhost:8080/alter?runInBackground=true -XPOST -d $'
    name: string @index(fulltext, term) .
    age: int @index(int) @upsert .
    friend: [uid] @count @reverse .
' | python -m json.tool | less
```

```go
op := &api.Operation{}
op.Schema = `
  name: string @index(fulltext, term) .
  age: int @index(int) @upsert .
  friend: [uid] @count @reverse .
`
op.RunInBackground = true
err = dg.Alter(context.Background(), op)
```

### Notes

If executed before the indexing finishes, queries that require the new indices
fail with an error notifying that a given predicate isn't indexed or doesn't
have reverse edges.

In a multi-node cluster, it's possible that the alphas finish computing indexes
at different times. Alphas may return different schema in such a case until all
the indexes are done computing on all the Alphas.

You can check the background indexing status using the
[Health](/dgraph/self-managed/dgraph-alpha#querying-health) query on the
`/admin` endpoint.

An alter operation fails if one is already in progress with an error
`schema is already being modified. Please retry`.

Dgraph reports the indexes in the schema only when the indexes are done
computing.

## Deleting a node type

Type definitions can be deleted using the Alter endpoint.

Below is an example deleting the type `Person` using the Go client:

```go
err := c.Alter(context.Background(), &api.Operation{
                DropOp: api.Operation_TYPE,
                DropValue: "Person"})
```


# Changelog
Source: https://docs.hypermode.com/dgraph/changelog



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The latest Dgraph release is the v24 series.

Dgraph releases starting v22.0.0 following semantic versioning
[See the post here](https://discuss.hypermode.com/t/dgraph-v22-0-0-rc1-20221003-release-candidate/).

To learn about the latest releases and other important announcements, watch the
[Announce][] category on Discuss.

[Announce]: https://discuss.hypermode.com/c/announce

## Release series

| Release               | First Release Date | End of Life    |
| --------------------- | ------------------ | -------------- |
| [v24.1][]             | March 2025         | September 2026 |
| [v24.0][]             | June 2024          | December 2025  |
| [v23.1][]             | October 2023       | April 2025     |
| [v23.0][]             | May 2023           | November 2024  |
| [v22.0][]             | October 2022       | April 2024     |
| v21.12 (discontinued) | December 2021      | December 2022  |
| [v21.03][]            | March 2021         | June 2023      |
| [v20.11][]            | December 2020      | December 2021  |
| [v20.07][]            | July 2020          | July 2021      |
| [v20.03][]            | March 2020         | March 2021     |
| [v1.2][]              | January 2020       | January 2021   |
| [v1.1][]              | January 2020       | January 2021   |
| [v1.0][]              | December 2017      | March 2020     |

[v24.1]: https://hypermode.com/blog/dgraph-v241-knowledge-graphs-faster

[v24.0]: https://discuss.hypermode.com/t/dgraph-release-v24-0-0-is-now-available/19346

[v23.1]: https://discuss.hypermode.com/t/dgraph-23-1-0-is-generally-available-on-dgraph-cloud-dockerhub-and-github/18980

[v23.0]: https://discuss.hypermode.com/t/dgraph-release-v23-0-0-is-now-generally-available/18634

[v22.0]: https://discuss.hypermode.com/t/dgraph-release-v22-0-2-is-now-generally-available/18117

[v21.03]: https://discuss.hypermode.com/t/release-notes-v21-03-0-resilient-rocket/13587

[v20.11]: https://discuss.hypermode.com/t/release-notes-v20-11-0-tenacious-tchalla/11942

[v20.07]: https://discuss.hypermode.com/t/dgraph-v20-07-3-release/12107

[v20.03]: https://discuss.hypermode.com/t/dgraph-v20-03-7-release/12077

[v1.2]: https://discuss.hypermode.com/t/dgraph-v1-2-8-release/11183

[v1.1]: https://discuss.hypermode.com/t/dgraph-v1-1-1-release/5664

[v1.0]: https://discuss.hypermode.com/t/dgraph-v1-0-18-release/5663


# Command Reference
Source: https://docs.hypermode.com/dgraph/cli/command-reference



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

You can use the Dgraph command-line interface (CLI) to deploy and manage Dgraph.
You use it in self-managed deployment scenarios; such as running Dgraph on
on-premises servers hosted on your physical infrastructure, or running Dgraph in
the cloud on your AWS, Google Cloud, or Azure infrastructure.

Dgraph has a root command used throughout its CLI: `dgraph`. The `dgraph`
command is supported by multiple subcommands (such as `alpha` or `update`), some
of which are also supported by their own subcommands. For example, the
`dgraph acl` command requires you to specify one of its subcommands: `add`,
`del`, `info` or `mod`. As with other CLIs, you provide command options using
flags like `--help` or `--telemetry`.

<Tip>
  The term command is used instead of subcommand throughout this document,
  except when clarifying relationships in the CLI command hierarchy. The term
  command is also used for combinations of commands and their subcommands, such
  as `dgraph alpha debug`.
</Tip>

## Dgraph CLI superflags in release v21.03

Some flags are deprecated and replaced in release v21.03. In previous Dgraph
releases, multiple related flags are often used in a command, causing some
commands to be very long. Starting in release v21.03, Dgraph uses *superflags*
for some flags used by the most complex commands: `alpha`, `backup`, `bulk`,
`debug`, `live` and `zero`. Superflags are compound flags: they contain one or
more options that let you define multiple settings in a semicolon-delimited
list. Semicolons are required between superflag options, but a semicolon after
the last superflag option is optional.

The general syntax for superflags is as follows:
`--<super-flag-name> option-a=value; option-b=value`

<Note> You should encapsulate the options for a superflag in
double-quotes (`"`) if any of those option values include spaces. You can
encapsulate options in double-quotes to improve readability. You can also use
the following syntax for superflags:
`--<super-flag-name> "option-a=value; option-b=value"`. </Note>

Release v21.03 includes the following superflags:

* `--acl`
* `--badger`
* `--cache`
* `--encryption`
* `--graphql`
* `--limit`
* `--raft`
* `--security`
* `--telemetry`
* `--tls`
* `--trace`
* `--vault`

The following table maps Dgraph CLI flags from release v20.11 and earlier that
have been replaced by superflags (and their options) in release v21.03. Any
flags not shown here are unchanged in release v21.03.

### ACL superflag

|            Old flag | Old type      | New superflag and options | New type                                                                         | Applies to |                                   Notes                                  |
| ------------------: | :------------ | ------------------------: | :------------------------------------------------------------------------------- | :--------: | :----------------------------------------------------------------------: |
|                     |               |               **`--acl`** |                                                                                  |            | [Access Control List](/dgraph/enterprise/access-control-lists) superflag |
| `--acl_secret_file` | string        |             `secret-file` | string                                                                           |   `alpha`  |     File that stores the HMAC secret that is used for signing the JWT    |
|  `--acl_access_ttl` | time.Duration |              `access-ttl` | [string](https://github.com/hypermodeinc/ristretto/blob/main/z/flags.go#L80-L98) |   `alpha`  |                          TTL for the access JWT                          |
| `--acl_refresh_ttl` | time.Duration |             `refresh-ttl` | [string](https://github.com/hypermodeinc/ristretto/blob/main/z/flags.go#L80-L98) |   `alpha`  |                        The TTL for the refresh JWT                       |

### Badger superflag

|               Old flag | Old type | New superflag and options | New type |         Applies to        |                     Notes                     |
| ---------------------: | :------- | ------------------------: | :------- | :-----------------------: | :-------------------------------------------: |
|                        |          |            **`--badger`** |          |                           |      [Badger](/badger/overview) superflag     |
| `--badger.compression` | string   |             `compression` | string   | `alpha`, `bulk`, `backup` | Specifies the compression level and algorithm |
|                        |          |           `numgoroutines` | int      | `alpha`, `bulk`, `backup` |      Number of Go routines used by Dgraph     |

<Note>
  The `--badger` superflag allows you to set many advanced [Badger
  options](https://pkg.go.dev/github.com/dgraph-io/badger/v3#Options),
  including: `dir`, `valuedir`, `syncwrites`, `numversionstokeep`, `readonly`,
  `inmemory`, `metricsenabled`, `memtablesize`, `basetablesize`,
  `baselevelsize`, `levelsizemultiplier`, `tablesizemultiplier`, `maxlevels`,
  `vlogpercentile`, `valuethreshold`, `nummemtables`, `blocksize`,
  `bloomfalsepositive`, `blockcachesize`, `indexcachesize`,
  `numlevelzerotables`, `numlevelzerotablesstall`, `valuelogfilesize`,
  `valuelogmaxentries`, `numcompactors`, `compactl0onclose`, `lmaxcompaction`,
  `zstdcompressionlevel`, `verifyvaluechecksum`,
  `encryptionkeyrotationduration`, `bypasslockguard`,
  `checksumverificationmode`, `detectconflicts`, `namespaceoffset`.
</Note>

### Cache superflag

|           Old flag | Old type | New superflag and options | New type | Applies to |                         Notes                        |
| -----------------: | :------- | ------------------------: | :------- | :--------: | :--------------------------------------------------: |
|                    |          |             **`--cache`** |          |            |                    Cache superflag                   |
|         `cache_mb` | string   |                 `size-mb` | string   |   `alpha`  | Total size of cache (in MB) per shard in the reducer |
| `cache_percentage` | string   |              `percentage` | string   |   `alpha`  |   Cache percentages for block cache and index cache  |

### Encryption superflag

|                Old flag | Old type | New superflag and options | New type |                                Applies to                               |                  Notes                 |
| ----------------------: | :------- | ------------------------: | :------- | :---------------------------------------------------------------------: | :------------------------------------: |
|                         |          |        **`--encryption`** |          |                                                                         |          Encryption superflag          |
| `--encryption_key_file` | string   |                `key-file` | string   | `alpha`, `bulk`, `live`, `restore`, `debug`, `decrypt`, `export_backup` | The file that stores the symmetric key |

### GraphQL superflag

|                  Old flag | Old type      | New superflag and options | New type                                                                         | Applies to |                                      Notes                                     |
| ------------------------: | :------------ | ------------------------: | :------------------------------------------------------------------------------- | :--------: | :----------------------------------------------------------------------------: |
|                           |               |           **`--graphql`** |                                                                                  |            |                                GraphQL superflag                               |
| `--graphql_introspection` | bool          |           `introspection` | bool                                                                             |   `alpha`  |                      Enables GraphQL schema introspection                      |
|         `--graphql_debug` | bool          |                   `debug` | bool                                                                             |   `alpha`  |                          Enables debug mode in GraphQL                         |
|    `--graphql_extensions` | bool          |              `extensions` | bool                                                                             |   `alpha`  |                   Enables extensions in GraphQL response body                  |
| `--graphql_poll_interval` | time.Duration |           `poll-interval` | [string](https://github.com/hypermodeinc/ristretto/blob/main/z/flags.go#L80-L98) |   `alpha`  |                 The polling interval for GraphQL subscriptions                 |
|    `--graphql_lambda_url` | string        |              `lambda-url` | string                                                                           |   `alpha`  | The URL of a lambda server that implements custom GraphQL JavaScript resolvers |

### Limit superflag

|                  Old flag | Old type | New superflag and options | New type | Applies to |                                                        Notes                                                       |
| ------------------------: | :------- | ------------------------: | :------- | :--------: | :----------------------------------------------------------------------------------------------------------------: |
|                           |          |             **`--limit`** |          |            |                                      Limit-setting superflag for Dgraph Alpha                                      |
|      `--abort_older_than` | string   |         `txn-abort-after` | string   |   `alpha`  |                               Abort any pending transactions older than this duration                              |
|    `--disable_admin_http` | string   |      `disable-admin-http` | string   |   `zero`   |                                      Turn on/off the administrative endpoints                                      |
|           `--max_retries` | int      |             `max-retries` | int      |   `alpha`  |                                              Maximum number of retries                                             |
|             `--mutations` | string   |               `mutations` | string   |   `alpha`  |                                   Mutation mode: `allow`, `disallow`, or `strict`                                  |
|      `--query_edge_limit` | uint64   |              `query-edge` | uint64   |   `alpha`  |                               Maximum number of edges that can be returned in a query                              |
|  `--normalize_node_limit` | int      |          `normalize-node` | int      |   `alpha`  |              Maximum number of nodes that can be returned in a query that uses the normalize directive             |
| `--mutations_nquad_limit` | int      |         `mutations-nquad` | int      |   `alpha`  |                         Maximum number of nquads that can be inserted in a mutation request                        |
|   `--max-pending-queries` | int      |     `max-pending-queries` | int      |   `alpha`  | Maximum number of concurrently processing requests allowed before requests are rejected with 429 Too Many Requests |

### Raft superflag

|              Old flag | Old type |      New superflag and options | New type |    Applies to   |                                                    Notes                                                   |
| --------------------: | :------- | -----------------------------: | :------- | :-------------: | :--------------------------------------------------------------------------------------------------------: |
|                       |          |                   **`--raft`** |          |                 |                                   [Raft](/dgraph/concepts/raft) superflag                                  |
| `--pending_proposals` | int      |            `pending-proposals` | int      |     `alpha`     |                   Maximum number of pending mutation proposals; useful for rate limiting                   |
|               `--idx` | int      |                          `idx` | int      | `alpha`, `zero` |                 Provides an optional Raft ID that an Alpha node can use to join Raft groups                |
|             `--group` | int      |                        `group` | int      |     `alpha`     | Provides an optional Raft group ID that an Alpha node can use to request group membership from a Zero node |
|                       |          |                 (new)`learner` | bool     | `alpha`, `zero` |                        Make this Alpha a learner node (used for read-only replicas)                        |
|                       |          | (new)`snapshot-after-duration` | int      |     `alpha`     |                                Frequency at which Raft snapshots are created                               |
|    `--snapshot-after` | int      |       `snapshot-after-entries` | int      |     `alpha`     |                    Create a new Raft snapshot after the specified number of Raft entries                   |

### Security superflag

|          Old flag | Old type | New superflag and options | New type |     Applies to     |                                              Notes                                              |
| ----------------: | :------- | ------------------------: | :------- | :----------------: | :---------------------------------------------------------------------------------------------: |
|                   |          |          **`--security`** |          |                    |                                        Security superflag                                       |
|    `--auth_token` | string   |                   `token` | string   |       `alpha`      |                                       Authentication token                                      |
|     `--whitelist` | string   |               `whitelist` | string   |       `alpha`      | A comma separated list of IP addresses, IP ranges, CIDR blocks, or hostnames for administration |
|                   |          |         **`--telemetry`** |          |                    |                                       Telemetry superflag                                       |
|     `--telemetry` | bool     |                 `reports` | bool     | `alpha` and `zero` |                             Sends anonymous telemetry data to Dgraph                            |
| `--enable_sentry` | bool     |                  `sentry` | bool     | `alpha` and `zero` |                              Enable sending crash events to Sentry                              |

### TLS superflag

|                      Old flag | Old type | New superflag and options | New type |                 Applies to                |                                        Notes                                       |
| ----------------------------: | :------- | ------------------------: | :------- | :---------------------------------------: | :--------------------------------------------------------------------------------: |
|                               |          |               **`--tls`** |          |                                           |               [TLS](/dgraph/self-managed/tls-configuration) superflag              |
|                `--tls_cacert` | string   |                 `ca-cert` | string   | `alpha`, `zero`, `bulk`, `backup`, `live` |                 The CA cert file used to verify server certificates                |
|         `--tls_use_system_ca` | bool     |           `use-system-ca` | bool     | `alpha`, `zero`, `bulk`, `backup`, `live` |                        Include System CA with Dgraph Root CA                       |
|           `--tls_server_name` | string   |             `server-name` | string   | `alpha`, `zero`, `bulk`, `backup`, `live` |             Server name, used for validating the server’s TLS host name            |
|           `--tls_client_auth` | string   |        `client-auth-type` | string   |              `alpha`, `zero`              |  TLS client authentication used to validate client connections from external ports |
|             `--tls_node_cert` | string   |             `server-cert` | string   |             `alpha` and `zero`            |         Path and filename of the node certificate (for example, `node.crt`)        |
|              `--tls_node_key` | string   |              `server-key` | string   |             `alpha` and `zero`            |   Path and filename of the node certificate private key (for example, `node.key`)  |
| `--tls_internal_port_enabled` | bool     |           `internal-port` | bool     | `alpha`, `zero`, `bulk`, `backup`, `live` | Makes internal ports (by default, 5080 and 7080) use the REQUIREANDVERIFY setting. |
|                  `--tls_cert` | string   |             `client-cert` | string   | `alpha`, `zero`, `bulk`, `backup`, `live` |               User cert file provided by the client to the Alpha node              |
|                   `--tls_key` | string   |              `client-key` | string   | `alpha`, `zero`, `bulk`, `backup`, `live` |           User private key file provided by the client to the Alpha node           |

### Trace superflag

|              Old flag | Old type | New superflag and options | New type |    Applies to   |                   Notes                   |
| --------------------: | :------- | ------------------------: | :------- | :-------------: | :---------------------------------------: |
|                       |          |             **`--trace`** |          |                 | [Tracing](/dgraph/admin/traces) superflag |
|             `--trace` | float64  |                   `ratio` | float64  | `alpha`, `zero` |       The ratio of queries to trace       |
|  `--jaeger.collector` | string   |                  `jaeger` | string   | `alpha`, `zero` |  URL of Jaeger to send OpenCensus traces  |
| `--datadog.collector` | string   |                 `datadog` | string   | `alpha`, `zero` |  URL of Datadog to send OpenCensus traces |

### Vault superflag

|                Old flag | Old type | New superflag and options | New type |                 Applies to                 |                                            Notes                                            |
| ----------------------: | :------- | ------------------------: | :------- | :----------------------------------------: | :-----------------------------------------------------------------------------------------: |
|                         |          |             **`--vault`** |          |                                            |                                       Vault superflag                                       |
|          `--vault_addr` | string   |                    `addr` | string   | `alpha`, `bulk`, `backup`, `live`, `debug` |                Vault server address, formatted as of `http://ip-address:port`               |
|   `--vault_roleid_file` | string   |            `role-id-file` | string   | `alpha`, `bulk`, `backup`, `live`, `debug` |               File containing Vault `role-id` used for AppRole authentication               |
| `--vault_secretid_file` | string   |          `secret-id-file` | string   | `alpha`, `bulk`, `backup`, `live`, `debug` |              File containing Vault `secret-id` used for AppRole authentication              |
|          `--vault_path` | string   |                    `path` | string   | `alpha`, `bulk`, `backup`, `live`, `debug` | Vault key=value store path (example: `secret/data/dgraph` for kv-v2, `kv/dgraph` for kv-v1) |
|         `--vault_field` | string   |                   `field` | string   | `alpha`, `bulk`, `backup`, `live`, `debug` |         Vault key=value store field whose value is the base64 encoded encryption key        |
|        `--vault_format` | string   |                  `format` | string   | `alpha`, `bulk`, `backup`, `live`, `debug` |                            Vault field format (`raw` or `base64`)                           |

To learn more about each superflag and its options, see the `--help` output of
the Dgraph CLI commands listed in the following section.

## Dgraph CLI command help listing

The Dgraph CLI includes the root `dgraph` command and its subcommands. The CLI
help for these commands is replicated inline below for your reference, or you
can find help by calling these commands (or their subcommands) using the
`--help` flag.

<Note>
  Although many of the commands listed below have subcommands, only `dgraph` and
  subcommands of `dgraph` are included in this listing.
</Note>

The Dgraph CLI has several commands, which are organized into the following
groups:

* [Dgraph core](#dgraph-core-commands)
* [Data loading](#data-loading-commands)
* [Dgraph security](#dgraph-security-commands)
* [Dgraph debug](#dgraph-debug-commands)
* [Dgraph tools](#dgraph-tools-commands)

The commands in these groups are shown in the following table:

| Group           | Command                                  | Note                                                                                                            |
| --------------- | ---------------------------------------- | --------------------------------------------------------------------------------------------------------------- |
| (root)          | [`dgraph`](#dgraph-root-command)         | Root command for Dgraph CLI                                                                                     |
| Dgraph core     | [`alpha`](#dgraph-alpha)                 | Dgraph Alpha database node commands                                                                             |
| Dgraph core     | [`zero`](#dgraph-zero)                   | Dgraph Zero management node commands                                                                            |
| Data loading    | [`bulk`](#dgraph-bulk)                   | Dgraph [Bulk Loader](/dgraph/admin/bulk-loader) commands                                                        |
| Data loading    | [`live`](#dgraph-live)                   | Dgraph [Live Loader](/dgraph/admin/live-loader) commands                                                        |
| Data loading    | [`restore`](#dgraph-restore)             | Command used to restore backups created using Dgraph Enterprise Edition                                         |
| Dgraph security | [`acl`](#dgraph-acl)                     | Dgraph [Access Control List (ACL)](/dgraph/enterprise/access-control-lists) commands                            |
| Dgraph security | [`audit`](#dgraph-audit)                 | Decrypt audit files                                                                                             |
| Dgraph security | [`cert`](#dgraph-cert)                   | Configure TLS and manage TLS certificates                                                                       |
| Dgraph debug    | [`debug`](#dgraph-debug)                 | Used to debug issues with Dgraph                                                                                |
| Dgraph debug    | [`debuginfo`](#dgraph-debuginfo)         | Generates information about the current node for use in debugging issues with Dgraph clusters                   |
| Dgraph tools    | [`completion`](#dgraph-completion)       | Generates shell completion scripts for `bash` and `zsh`                                                         |
| Dgraph tools    | [`conv`](#dgraph-conv)                   | Converts geographic files into RDF so that they can be consumed by Dgraph                                       |
| Dgraph tools    | [`decrypt`](#dgraph-decrypt)             | Decrypts an export file created by an encrypted Dgraph Cluster                                                  |
| Dgraph tools    | [`export_backup`](#dgraph-export_backup) | Converts a binary backup created using Dgraph Enterprise Edition into an exported folder.                       |
| Dgraph tools    | [`increment`](#dgraph-increment)         | Increments a counter transactionally to confirm that a Dgraph Alpha node can handle query and mutation requests |
| Dgraph tools    | [`lsbackup`](#dgraph-lsbackup)           | Lists information on backups in a given location                                                                |
| Dgraph tools    | [`migrate`](#dgraph-migrate)             | Migrates data from a MySQL database to Dgraph                                                                   |
| Dgraph tools    | [`raftmigrate`](#dgraph-raftmigrate)     | Dgraph Raft migration tool                                                                                      |
| Dgraph tools    | [`upgrade`](#dgraph-upgrade)             | Upgrades Dgraph to a newer version                                                                              |

### `dgraph` root command

This command is the root for all commands in the Dgraph CLI. Key information
from the help listing for `dgraph --help` is shown below:

```shell
Usage:
  dgraph [command]

Generic:
 help          Help about any command
 version       Prints the dgraph version details

Available Commands:

Dgraph Core:
  alpha         Run Dgraph Alpha database server
  zero          Run Dgraph Zero management server

Data Loading:
  bulk          Run Dgraph Bulk Loader
  live          Run Dgraph Live Loader
  restore       Restore backup from Dgraph Enterprise Edition

Dgraph Security:
  acl           Run the Dgraph Enterprise Edition ACL tool
  audit         Dgraph audit tool
  cert          Dgraph TLS certificate management

Dgraph Debug:
  debug         Debug Dgraph instance
  debuginfo     Generate debug information on the current node

Dgraph Tools:
  completion    Generates shell completion scripts for bash or zsh
  conv          Dgraph Geo file converter
  decrypt       Run the Dgraph decryption tool
  export_backup Export data inside single full or incremental backup
  increment     Increment a counter transactionally
  lsbackup      List info on backups in a given location
  migrate       Run the Dgraph migration tool from a MySQL database to Dgraph
  raftmigrate   Run the Raft migration tool
  upgrade       Run the Dgraph upgrade tool

Flags:
      --alsologtostderr                  log to standard error as well as files
      --bindall                          Use 0.0.0.0 instead of localhost to bind to all addresses on local machine. (default true)
      --block_rate int                   Block profiling rate. Must be used along with block profile_mode
      --config string                    Configuration file. Takes precedence over default values, but is overridden to values set with environment variables and flags.
      --cwd string                       Change working directory to the path specified. The parent must exist.
      --expose_trace                     Allow trace endpoint to be accessible from remote
  -h, --help                             help for dgraph
      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)
      --log_dir string                   If non-empty, write log files in this directory
      --logtostderr                      log to standard error instead of files
      --profile_mode string              Enable profiling mode, one of [cpu, mem, mutex, block]
  -v, --v Level                          log level for V logs
      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging

```

### Dgraph core commands

Dgraph core commands provide core deployment and management functionality for
the Dgraph Alpha database nodes and Dgraph Zero management nodes in your
deployment.

#### `dgraph alpha`

This command is used to configure and run the Dgraph Alpha database nodes in
your deployment. The following replicates the help listing for
`dgraph alpha --help`:

```shell
A Dgraph Alpha instance stores the data. Each Dgraph Alpha is responsible for
storing and serving one data group. If multiple Alphas serve the same group,
they form a Raft group and provide synchronous replication.

Usage:
  dgraph alpha [flags]

Flags:
      --acl string                 [Enterprise Feature] ACL options
                                       access-ttl=6h; The TTL for the access JWT.
                                       refresh-ttl=30d; The TTL for the refresh JWT.
                                       secret-file=; The file that stores the HMAC secret, which is used for signing the JWT and should have at least 32 ASCII characters. Required to enable ACLs.
                                    (default "access-ttl=6h; refresh-ttl=30d; secret-file=;")
      --audit string               Audit options
                                       compress=false; Enables the compression of old audit logs.
                                       days=10; The number of days audit logs will be preserved.
                                       encrypt-file=; The path to the key file to be used for audit log encryption.
                                       output=; [stdout, /path/to/dir] This specifies where audit logs should be output to.
                                         "stdout" is for standard output. You can also specify the directory where audit logs
                                         will be saved. When stdout is specified as output other fields will be ignored.
                                       size=100; The audit log max size in MB after which it will be rolled over.
                                    (default "compress=false; days=10; size=100; dir=; output=; encrypt-file=;")
      --badger string              Badger options
                                       compression=snappy; [none, zstd:level, snappy] Specifies the compression algorithm and
                                         compression level (if applicable) for the postings directory."none" would disable
                                         compression, while "zstd:1" would set zstd compression at level 1.
                                       numgoroutines=8; The number of goroutines to use in badger.Stream.
                                       max-retries=-1; Commits to disk will give up after these number of retries to prevent locking the worker in a failed state. Use -1 to retry infinitely.
                                    (default "compression=snappy; numgoroutines=8; max-retries=-1;")
      --cache string               Cache options
                                       percentage=0,65,35; Cache percentages summing up to 100 for various caches (FORMAT: PostingListCache,PstoreBlockCache,PstoreIndexCache)
                                       size-mb=1024; Total size of cache (in MB) to be used in Dgraph.
                                    (default "size-mb=1024; percentage=0,65,35;")
      --cdc string                 Change Data Capture options
                                       ca-cert=; The path to CA cert file for TLS encryption.
                                       client-cert=; The path to client cert file for TLS encryption.
                                       client-key=; The path to client key file for TLS encryption.
                                       file=; The path where audit logs will be stored.
                                       kafka=; A comma separated list of Kafka hosts.
                                       sasl-password=; The SASL password for Kafka.
                                       sasl-user=; The SASL username for Kafka.
                                    (default "file=; kafka=; sasl_user=; sasl_password=; ca_cert=; client_cert=; client_key=;")
      --custom_tokenizers string   Comma separated list of tokenizer plugins for custom indices.
      --encryption string          [Enterprise Feature] Encryption At Rest options
                                       key-file=; The file that stores the symmetric key of length 16, 24, or 32 bytes. The key size determines the chosen AES cipher (AES-128, AES-192, and AES-256 respectively).
                                    (default "key-file=;")
      --export string              Folder in which to store exports. (default "export")
      --graphql string             GraphQL options
                                       debug=false; Enables debug mode in GraphQL. This returns auth errors to clients, and we do not recommend turning it on for production.
                                       extensions=true; Enables extensions in GraphQL response body.
                                       introspection=true; Enables GraphQL schema introspection.
                                       lambda-url=; The URL of a lambda server that implements custom GraphQL Javascript resolvers.
                                       poll-interval=1s; The polling interval for GraphQL subscription.
                                    (default "introspection=true; debug=false; extensions=true; poll-interval=1s; lambda-url=;")
  -h, --help                       help for alpha
      --limit string               Limit options
                                       disallow-drop=false; Set disallow-drop to true to block drop-all and drop-data operation. It still allows dropping attributes and types.
                                       mutations-nquad=1000000; The maximum number of nquads that can be inserted in a mutation request.
                                       mutations=allow; [allow, disallow, strict] The mutations mode to use.
                                       normalize-node=10000; The maximum number of nodes that can be returned in a query that uses the normalize directive.
                                       query-edge=1000000; The maximum number of edges that can be returned in a query. This applies to shortest path and recursive queries.
                                       query-timeout=0ms; Maximum time after which a query execution will fail. If set to 0, the timeout is infinite.
                                       txn-abort-after=5m; Abort any pending transactions older than this duration. The liveness of a transaction is determined by its last mutation.
                                       max-pending-queries=10000; Number of maximum pending queries before we reject them as too many requests.
                                    (default "mutations=allow; query-edge=1000000; normalize-node=10000; mutations-nquad=1000000; disallow-drop=false; query-timeout=0ms; txn-abort-after=5m; max-pending-queries=10000")
      --my string                  addr:port of this server, so other Dgraph servers can talk to this.
  -o, --port_offset int            Value added to all listening port numbers. [Internal=7080, HTTP=8080, Grpc=9080]
  -p, --postings string            Directory to store posting lists. (default "p")
      --raft string                Raft options
                                       group=; Provides an optional Raft Group ID that this Alpha would indicate to Zero to join.
                                       idx=; Provides an optional Raft ID that this Alpha would use to join Raft groups.
                                       learner=false; Make this Alpha a "learner" node. In learner mode, this Alpha will not participate in Raft elections. This can be used to achieve a read-only replica.
                                       pending-proposals=256; Number of pending mutation proposals. Useful for rate limiting.
                                       snapshot-after-duration=30m; Frequency at which we should create a new raft snapshots. Set to 0 to disable duration based snapshot.
                                       snapshot-after-entries=10000; Create a new Raft snapshot after N number of Raft entries. The lower this number, the more frequent snapshot creation will be. Snapshots are created only if both snapshot-after-duration and snapshot-after-entries threshold are crossed.
                                    (default "learner=false; snapshot-after-entries=10000; snapshot-after-duration=30m; pending-proposals=256; idx=; group=;")
      --security string            Security options
                                       token=; If set, all Admin requests to Dgraph will need to have this token. The token can be passed as follows: for HTTP requests, in the X-Dgraph-AuthToken header. For Grpc, in auth-token key in the context.
                                       whitelist=; A comma separated list of IP addresses, IP ranges, CIDR blocks, or hostnames you wish to whitelist for performing admin actions (i.e., --security "whitelist=144.142.126.254,127.0.0.1:127.0.0.3,192.168.0.0/16,host.docker.internal").
                                    (default "token=; whitelist=;")
      --survive string             Choose between "process" or "filesystem".
                                       If set to "process", there would be no data loss in case of process crash, but the behavior would be nondeterministic in case of filesystem crash.
                                       If set to "filesystem", blocking sync would be called after every write, hence guaranteeing no data loss in case of hard reboot.
                                       Most users should be OK with choosing "process". (default "process")
      --telemetry string           Telemetry (diagnostic) options
                                       reports=true; Send anonymous telemetry data to Dgraph devs.
                                       sentry=true; Send crash events to Sentry.
                                    (default "reports=true; sentry=true;")
      --tls string                 TLS Server options
                                       ca-cert=; The CA cert file used to verify server certificates. Required for enabling TLS.
                                       client-auth-type=VERIFYIFGIVEN; The TLS client authentication method.
                                       client-cert=; (Optional) The client Cert file which is needed to connect as a client with the other nodes in the cluster.
                                       client-key=; (Optional) The private client Key file which is needed to connect as a client with the other nodes in the cluster.
                                       internal-port=false; (Optional) Enable inter-node TLS encryption between cluster nodes.
                                       server-cert=; The server Cert file which is needed to initiate the server in the cluster.
                                       server-key=; The server Key file which is needed to initiate the server in the cluster.
                                       use-system-ca=true; Includes System CA into CA Certs.
                                    (default "use-system-ca=true; client-auth-type=VERIFYIFGIVEN; internal-port=false;")
      --tmp string                 Directory to store temporary buffers. (default "t")
      --trace string               Trace options
                                       datadog=; URL of Datadog to send OpenCensus traces. As of now, the trace exporter does not support annotation logs and discards them.
                                       jaeger=; URL of Jaeger to send OpenCensus traces.
                                       ratio=0.01; The ratio of queries to trace.
                                    (default "ratio=0.01; jaeger=; datadog=;")
      --vault string               Vault options
                                       acl-field=; Vault field containing ACL key.
                                       acl-format=base64; ACL key format, can be 'raw' or 'base64'.
                                       addr=http://localhost:8200; Vault server address (format: http://ip:port).
                                       enc-field=; Vault field containing encryption key.
                                       enc-format=base64; Encryption key format, can be 'raw' or 'base64'.
                                       path=secret/data/dgraph; Vault KV store path (e.g. 'secret/data/dgraph' for KV V2, 'kv/dgraph' for KV V1).
                                       role-id-file=; Vault RoleID file, used for AppRole authentication.
                                       secret-id-file=; Vault SecretID file, used for AppRole authentication.
                                    (default "addr=http://localhost:8200; role-id-file=; secret-id-file=; path=secret/data/dgraph; acl-field=; acl-format=base64; enc-field=; enc-format=base64")
  -w, --wal string                 Directory to store raft write-ahead logs. (default "w")
  -z, --zero string                Comma separated list of Dgraph Zero addresses of the form IP_ADDRESS:PORT. (default "localhost:5080")

Use "dgraph alpha [command] --help" for more information about a command.
```

#### `dgraph zero`

This command is used to configure and run the Dgraph Zero management nodes in
your deployment. The following replicates the help listing shown when you run
`dgraph zero --help`:

```shell
A Dgraph Zero instance manages the Dgraph cluster.  Typically, a single Zero
instance is sufficient for the cluster; however, one can run multiple Zero
instances to achieve high-availability.

Usage:
  dgraph zero [flags]

Flags:
      --audit string                  Audit options
                                          compress=false; Enables the compression of old audit logs.
                                          days=10; The number of days audit logs will be preserved.
                                          encrypt-file=; The path to the key file to be used for audit log encryption.
                                          output=; [stdout, /path/to/dir] This specifies where audit logs should be output to.
                                            "stdout" is for standard output. You can also specify the directory where audit logs
                                            will be saved. When stdout is specified as output other fields will be ignored.
                                          size=100; The audit log max size in MB after which it will be rolled over.
                                       (default "compress=false; days=10; size=100; dir=; output=; encrypt-file=;")
      --enterprise_license string     Path to the enterprise license file.
  -h, --help                          help for zero
      --limit string                  Limit options
                                          disable-admin-http=false; Turn on/off the administrative endpoints exposed over Zero's HTTP port.
                                          refill-interval=30s; The interval after which the tokens for UID lease are replenished.
                                          uid-lease=0; The maximum number of UIDs that can be leased by namespace (except default namespace)
                                            in an interval specified by refill-interval. Set it to 0 to remove limiting.
                                       (default "uid-lease=0; refill-interval=30s; disable-admin-http=false;")
      --my string                     addr:port of this server, so other Dgraph servers can talk to this.
      --peer string                   Address of another dgraphzero server.
  -o, --port_offset int               Value added to all listening port numbers. [Grpc=5080, HTTP=6080]
      --raft string                   Raft options
                                          idx=1; Provides an optional Raft ID that this Alpha would use to join Raft groups.
                                          learner=false; Make this Zero a "learner" node. In learner mode, this Zero will not participate in Raft elections. This can be used to achieve a read-only replica.
                                       (default "idx=1; learner=false;")
      --rebalance_interval duration   Interval for trying a predicate move. (default 8m0s)
      --replicas int                  How many Dgraph Alpha replicas to run per data shard group. The count includes the original shard. (default 1)
      --survive string                Choose between "process" or "filesystem".
                                          If set to "process", there would be no data loss in case of process crash, but the behavior would be nondeterministic in case of filesystem crash.
                                          If set to "filesystem", blocking sync would be called after every write, hence guaranteeing no data loss in case of hard reboot.
                                          Most users should be OK with choosing "process". (default "process")
      --telemetry string              Telemetry (diagnostic) options
                                          reports=true; Send anonymous telemetry data to Dgraph devs.
                                          sentry=true; Send crash events to Sentry.
                                       (default "reports=true; sentry=true;")
      --tls string                    TLS Server options
                                          ca-cert=; The CA cert file used to verify server certificates. Required for enabling TLS.
                                          client-auth-type=VERIFYIFGIVEN; The TLS client authentication method.
                                          client-cert=; (Optional) The client Cert file which is needed to connect as a client with the other nodes in the cluster.
                                          client-key=; (Optional) The private client Key file which is needed to connect as a client with the other nodes in the cluster.
                                          internal-port=false; (Optional) Enable inter-node TLS encryption between cluster nodes.
                                          server-cert=; The server Cert file which is needed to initiate the server in the cluster.
                                          server-key=; The server Key file which is needed to initiate the server in the cluster.
                                          use-system-ca=true; Includes System CA into CA Certs.
                                       (default "use-system-ca=true; client-auth-type=VERIFYIFGIVEN; internal-port=false;")
      --trace string                  Trace options
                                          datadog=; URL of Datadog to send OpenCensus traces. As of now, the trace exporter does not support annotation logs and discards them.
                                          jaeger=; URL of Jaeger to send OpenCensus traces.
                                          ratio=0.01; The ratio of queries to trace.
                                       (default "ratio=0.01; jaeger=; datadog=;")
  -w, --wal string                    Directory storing WAL. (default "zw")

Use "dgraph zero [command] --help" for more information about a command.
```

### Data loading commands

#### `dgraph bulk`

This command is used to bulk load data with the Dgraph
[Bulk Loader](/dgraph/admin/bulk-loader) tool. The following replicates the help
listing shown when you run `dgraph bulk --help`:

```shell
 Run Dgraph Bulk Loader
Usage:
  dgraph bulk [flags]

Flags:
      --badger string              Badger options (Refer to badger documentation for all possible options)
                                       compression=snappy; Specifies the compression algorithm and compression level (if applicable) for the postings directory. "none" would disable compression, while "zstd:1" would set zstd compression at level 1.
                                       numgoroutines=8; The number of goroutines to use in badger.Stream.
                                    (default "compression=snappy; numgoroutines=8;")
      --cleanup_tmp                Clean up the tmp directory after the loader finishes. Setting this to false allows the bulk loader can be re-run while skipping the map phase. (default true)
      --custom_tokenizers string   Comma separated list of tokenizer plugins
      --encrypted                  Flag to indicate whether schema and data files are encrypted. Must be specified with --encryption or vault option(s).
      --encrypted_out              Flag to indicate whether to encrypt the output. Must be specified with --encryption or vault option(s).
      --encryption string          [Enterprise Feature] Encryption At Rest options
                                       key-file=; The file that stores the symmetric key of length 16, 24, or 32 bytes. The key size determines the chosen AES cipher (AES-128, AES-192, and AES-256 respectively).
                                    (default "key-file=;")
  -f, --files string               Location of *.rdf(.gz) or *.json(.gz) file(s) to load.
      --force-namespace uint       Namespace onto which to load the data. If not set, will preserve the namespace. (default 18446744073709551615)
      --format string              Specify file format (rdf or json) instead of getting it from filename.
  -g, --graphql_schema string      Location of the GraphQL schema file.
  -h, --help                       help for bulk
      --http string                Address to serve http (pprof). (default "localhost:8080")
      --ignore_errors              ignore line parsing errors in rdf files
      --map_shards int             Number of map output shards. Must be greater than or equal to the number of reduce shards. Increasing allows more evenly sized reduce shards, at the expense of increased memory usage. (default 1)
      --mapoutput_mb int           The estimated size of each map file output. Increasing this increases memory usage. (default 2048)
      --new_uids                   Ignore UIDs in load files and assign new ones.
  -j, --num_go_routines int        Number of worker threads to use. MORE THREADS LEAD TO HIGHER RAM USAGE. (default 1)
      --out string                 Location to write the final dgraph data directories. (default "./out")
      --partition_mb int           Pick a partition key every N megabytes of data. (default 4)
      --reduce_shards int          Number of reduce shards. This determines the number of dgraph instances in the final cluster. Increasing this potentially decreases the reduce stage runtime by using more parallelism, but increases memory usage. (default 1)
      --reducers int               Number of reducers to run concurrently. Increasing this can improve performance, and must be less than or equal to the number of reduce shards. (default 1)
      --replace_out                Replace out directory and its contents if it exists.
  -s, --schema string              Location of schema file.
      --skip_map_phase             Skip the map phase (assumes that map output files already exist).
      --store_xids                 Generate an xid edge for each node.
      --tls string                 TLS Client options
                                       ca-cert=; The CA cert file used to verify server certificates. Required for enabling TLS.
                                       client-cert=; (Optional) The Cert file provided by the client to the server.
                                       client-key=; (Optional) The private Key file provided by the clients to the server.
                                       internal-port=false; (Optional) Enable inter-node TLS encryption between cluster nodes.
                                       server-name=; Used to verify the server hostname.
                                       use-system-ca=true; Includes System CA into CA Certs.
                                    (default "use-system-ca=true; internal-port=false;")
      --tmp string                 Temp directory used to use for on-disk scratch space. Requires free space proportional to the size of the RDF file and the amount of indexing used. (default "tmp")
      --vault string               Vault options
                                       acl-field=; Vault field containing ACL key.
                                       acl-format=base64; ACL key format, can be 'raw' or 'base64'.
                                       addr=http://localhost:8200; Vault server address (format: http://ip:port).
                                       enc-field=; Vault field containing encryption key.
                                       enc-format=base64; Encryption key format, can be 'raw' or 'base64'.
                                       path=secret/data/dgraph; Vault KV store path (e.g. 'secret/data/dgraph' for KV V2, 'kv/dgraph' for KV V1).
                                       role-id-file=; Vault RoleID file, used for AppRole authentication.
                                       secret-id-file=; Vault SecretID file, used for AppRole authentication.
                                    (default "addr=http://localhost:8200; role-id-file=; secret-id-file=; path=secret/data/dgraph; acl-field=; acl-format=base64; enc-field=; enc-format=base64")
      --version                    Prints the version of Dgraph Bulk Loader.
      --xidmap string              Directory to store xid to uid mapping
  -z, --zero string                gRPC address for Dgraph zero (default "localhost:5080")

Use "dgraph bulk [command] --help" for more information about a command.
```

#### `dgraph live`

This command is used to load live data with the Dgraph
[Live Loader](/dgraph/admin/live-loader) tool. The following replicates the help
listing shown when you run `dgraph live --help`:

```shell
 Run Dgraph Live Loader
Usage:
  dgraph live [flags]

Flags:
  -a, --alpha string                 Comma-separated list of Dgraph alpha gRPC server addresses (default "127.0.0.1:9080")
  -t, --auth_token string            The auth token passed to the server for Alter operation of the schema file. If used with --slash_grpc_endpoint, then this should be set to the API token issuedby Slash GraphQL
  -b, --batch int                    Number of N-Quads to send as part of a mutation. (default 1000)
  -m, --bufferSize string            Buffer for each thread (default "100")
  -c, --conc int                     Number of concurrent requests to make to Dgraph (default 10)
      --creds string                 Various login credentials if login is required.
                                       user defines the username to login.
                                       password defines the password of the user.
                                       namespace defines the namespace to log into.
                                       Sample flag could look like --creds user=username;password=mypass;namespace=2
      --encryption string            [Enterprise Feature] Encryption At Rest options
                                         key-file=; The file that stores the symmetric key of length 16, 24, or 32 bytes. The key size determines the chosen AES cipher (AES-128, AES-192, and AES-256 respectively).
                                      (default "key-file=;")
  -f, --files string                 Location of *.rdf(.gz) or *.json(.gz) file(s) to load
      --force-namespace int          Namespace onto which to load the data.Only guardian of galaxy should use this for loading data into multiple namespaces or somespecific namespace. Setting it to negative value will preserve the namespace.
      --format string                Specify file format (rdf or json) instead of getting it from filename
  -h, --help                         help for live
      --http string                  Address to serve http (pprof). (default "localhost:6060")
      --new_uids                     Ignore UIDs in load files and assign new ones.
  -s, --schema string                Location of schema file
      --slash_grpc_endpoint string   Path to Slash GraphQL gRPC endpoint. If --slash_grpc_endpoint is set, all other TLS options and connection options will beignored
      --tls string                   TLS Client options
                                         ca-cert=; The CA cert file used to verify server certificates. Required for enabling TLS.
                                         client-cert=; (Optional) The Cert file provided by the client to the server.
                                         client-key=; (Optional) The private Key file provided by the clients to the server.
                                         internal-port=false; (Optional) Enable inter-node TLS encryption between cluster nodes.
                                         server-name=; Used to verify the server hostname.
                                         use-system-ca=true; Includes System CA into CA Certs.
                                      (default "use-system-ca=true; internal-port=false;")
      --tmp string                   Directory to store temporary buffers. (default "t")
  -U, --upsertPredicate string       run in upsertPredicate mode. the value would be used to store blank nodes as an xid
  -C, --use_compression              Enable compression on connection to alpha server
      --vault string                 Vault options
                                         acl-field=; Vault field containing ACL key.
                                         acl-format=base64; ACL key format, can be 'raw' or 'base64'.
                                         addr=http://localhost:8200; Vault server address (format: http://ip:port).
                                         enc-field=; Vault field containing encryption key.
                                         enc-format=base64; Encryption key format, can be 'raw' or 'base64'.
                                         path=secret/data/dgraph; Vault KV store path (e.g. 'secret/data/dgraph' for KV V2, 'kv/dgraph' for KV V1).
                                         role-id-file=; Vault RoleID file, used for AppRole authentication.
                                         secret-id-file=; Vault SecretID file, used for AppRole authentication.
                                      (default "addr=http://localhost:8200; role-id-file=; secret-id-file=; path=secret/data/dgraph; acl-field=; acl-format=base64; enc-field=; enc-format=base64")
      --verbose                      Run the live loader in verbose mode
  -x, --xidmap string                Directory to store xid to uid mapping
  -z, --zero string                  Dgraph zero gRPC server address (default "127.0.0.1:5080")

Use "dgraph live [command] --help" for more information about a command.
```

#### `dgraph restore`

This command loads objects from available backups. The following replicates the
help listing shown when you run `dgraph restore --help`:

```shell
Restore loads objects created with the backup feature in Dgraph Enterprise Edition (EE).

Backups taken using the GraphQL API can be restored using CLI restore
command. Restore is intended to be used with new Dgraph clusters in offline state.

The --location flag indicates a source URI with Dgraph backup objects. This URI supports all
the schemes used for backup.

Source URI formats:
  [scheme]://[host]/[path]?[args]
  [scheme]:///[path]?[args]
  /[path]?[args] (only for local or NFS)

Source URI parts:
  scheme - service handler, one of: "s3", "minio", "file"
    host - remote address. ex: "dgraph.s3.amazonaws.com"
    path - directory, bucket or container at target. ex: "/dgraph/backups/"
    args - specific arguments that are ok to appear in logs.

The --posting flag sets the posting list parent dir to store the loaded backup files.

Using the --zero flag will use a Dgraph Zero address to update the start timestamp using
the restored version. Otherwise, the timestamp must be manually updated through Zero's HTTP
'assign' command.

Dgraph backup creates a unique backup object for each node group, and restore will create
a posting directory 'p' matching the backup group ID. Such that a backup file
named '.../r32-g2.backup' will be loaded to posting dir 'p2'.

Usage examples:

# Restore from local dir or NFS mount:
$ dgraph restore -p . -l /var/backups/dgraph

# Restore from S3:
$ dgraph restore -p /var/db/dgraph -l s3://s3.us-west-2.amazonaws.com/srfrog/dgraph

# Restore from dir and update Ts:
$ dgraph restore -p . -l /var/backups/dgraph -z localhost:5080


Usage:
  dgraph restore [flags]

Flags:
      --backup_id string    The ID of the backup series to restore. If empty, it will restore the latest series.
  -b, --badger string       Badger options
                                compression=snappy; Specifies the compression algorithm and compression level (if applicable) for the postings directory. "none" would disable compression, while "zstd:1" would set zstd compression at level 1.
                                goroutines=; The number of goroutines to use in badger.Stream.
                             (default "compression=snappy; numgoroutines=8;")
      --encryption string   [Enterprise Feature] Encryption At Rest options
                                key-file=; The file that stores the symmetric key of length 16, 24, or 32 bytes. The key size determines the chosen AES cipher (AES-128, AES-192, and AES-256 respectively).
                             (default "key-file=;")
      --force_zero          If false, no connection to a zero in the cluster will be required. Keep in mind this requires you to manually update the timestamp and max uid when you start the cluster. The correct values are printed near the end of this command's output. (default true)
  -h, --help                help for restore
  -l, --location string     Sets the source location URI (required).
  -p, --postings string     Directory where posting lists are stored (required).
      --tls string          TLS Client options
                                ca-cert=; The CA cert file used to verify server certificates. Required for enabling TLS.
                                client-cert=; (Optional) The Cert file provided by the client to the server.
                                client-key=; (Optional) The private Key file provided by the clients to the server.
                                internal-port=false; (Optional) Enable inter-node TLS encryption between cluster nodes.
                                server-name=; Used to verify the server hostname.
                                use-system-ca=true; Includes System CA into CA Certs.
                             (default "use-system-ca=true; internal-port=false;")
      --vault string        Vault options
                                acl-field=; Vault field containing ACL key.
                                acl-format=base64; ACL key format, can be 'raw' or 'base64'.
                                addr=http://localhost:8200; Vault server address (format: http://ip:port).
                                enc-field=; Vault field containing encryption key.
                                enc-format=base64; Encryption key format, can be 'raw' or 'base64'.
                                path=secret/data/dgraph; Vault KV store path (e.g. 'secret/data/dgraph' for KV V2, 'kv/dgraph' for KV V1).
                                role-id-file=; Vault RoleID file, used for AppRole authentication.
                                secret-id-file=; Vault SecretID file, used for AppRole authentication.
                             (default "addr=http://localhost:8200; role-id-file=; secret-id-file=; path=secret/data/dgraph; acl-field=; acl-format=base64; enc-field=; enc-format=base64")
  -z, --zero string         gRPC address for Dgraph zero. ex: localhost:5080

Use "dgraph restore [command] --help" for more information about a command.
```

### Dgraph security commands

Dgraph security commands let you manage access control lists (ACLs), manage
certificates, and audit database usage.

#### `dgraph acl`

This command runs the Dgraph Enterprise Edition ACL tool. The following
replicates the help listing shown when you run `dgraph acl --help`:

```shell
Run the Dgraph Enterprise Edition ACL tool
Usage:
 dgraph acl [command]

Available Commands:
 add         Run Dgraph acl tool to add a user or group
 del         Run Dgraph acl tool to delete a user or group
 info        Show info about a user or group
 mod         Run Dgraph acl tool to modify a user's password, a user's group list, or agroup's predicate permissions

Flags:
 -a, --alpha string            Dgraph Alpha gRPC server address (default "127.0.0.1:9080")
     --guardian-creds string   Login credentials for the guardian
                                 user defines the username to login.
                                 password defines the password of the user.
                                 namespace defines the namespace to log into.
                                 Sample flag could look like --guardian-creds user=username;password=mypass;namespace=2
 -h, --help                    help for acl
     --tls string              TLS Client options
                                   ca-cert=; The CA cert file used to verify server certificates. Required for enabling TLS.
                                   client-cert=; (Optional) The Cert file provided by the client to the server.
                                   client-key=; (Optional) The private Key file provided by the clients to the server.
                                   internal-port=false; (Optional) Enable inter-node TLS encryption between cluster nodes.
                                   server-name=; Used to verify the server hostname.
                                   use-system-ca=true; Includes System CA into CA Certs.
                                (default "use-system-ca=true; internal-port=false;")

Use "dgraph acl [command] --help" for more information about a command.
```

#### `dgraph audit`

This command decrypts audit files. These files are created using the `--audit`
when you run the `dgraph alpha` command. The following replicates the help
listing shown when you run `dgraph audit --help`:

```shell
Dgraph audit tool
Usage:
 dgraph audit [command]

Available Commands:
 decrypt     Run Dgraph Audit tool to decrypt audit files

Flags:
 -h, --help   help for audit

Use "dgraph audit [command] --help" for more information about a command.
```

#### `dgraph cert`

This command lets you manage
[TLS certificates](/dgraph/self-managed/tls-configuration). The following
replicates the help listing shown when you run `dgraph cert --help`:

```shell
Dgraph TLS certificate management
Usage:
 dgraph cert [flags]
 dgraph cert [command]

Available Commands:
 ls          lists certificates and keys

Flags:
 -k, --ca-key string           path to the CA private key (default "ca.key")
 -c, --client string           create cert/key pair for a client name
 -d, --dir string              directory containing TLS certs and keys (default "tls")
     --duration int            duration of cert validity in days (default 365)
 -e, --elliptic-curve string   ECDSA curve for private key. Values are: "P224", "P256", "P384", "P521".
     --force                   overwrite any existing key and cert
 -h, --help                    help for cert
 -r, --keysize int             RSA key bit size for creating new keys (default 2048)
 -n, --nodes strings           creates cert/key pair for nodes
     --verify                  verify certs against root CA when creating (default true)

Use "dgraph cert [command] --help" for more information about a command.
```

### Dgraph debug commands

Dgraph debug commands provide support for debugging issues with Dgraph
deployments. To learn more, see [Using the Debug Tool](/dgraph/admin/debug).

#### `dgraph debug`

This command is used to debug issues with a Dgraph database instance. The
following replicates the help listing shown when you run `dgraph debug --help`:

```shell
 Debug Dgraph instance
Usage:
  dgraph debug [flags]

Flags:
      --at uint             Set read timestamp for all txns. (default 18446744073709551615)
      --encryption string   [Enterprise Feature] Encryption At Rest options
                                key-file=; The file that stores the symmetric key of length 16, 24, or 32 bytes. The key size determines the chosen AES cipher (AES-128, AES-192, and AES-256 respectively).
                             (default "key-file=;")
  -h, --help                help for debug
      --histogram           Show a histogram of the key and value sizes.
  -y, --history             Show all versions of a key.
      --item                Output item meta as well. Set to false for diffs. (default true)
      --jepsen string       Disect Jepsen output. Can be linear/binary.
  -l, --lookup string       Hex of key to lookup.
      --nokeys              Ignore key_. Only consider amount when calculating total.
      --only-summary        If true, only show the summary of the p directory.
  -p, --postings string     Directory where posting lists are stored.
  -r, --pred string         Only output specified predicate.
      --prefix string       Uses a hex prefix.
  -o, --readonly            Open in read only mode. (default true)
      --rollup string       Hex of key to rollup.
  -s, --snap string         Set snapshot term,index,readts to this. Value must be comma-separated list containing the value for these vars in that order.
  -t, --truncate uint       Remove data from Raft entries until but not including this index.
      --vals                Output values along with keys.
      --vault string        Vault options
                                acl-field=; Vault field containing ACL key.
                                acl-format=base64; ACL key format, can be 'raw' or 'base64'.
                                addr=http://localhost:8200; Vault server address (format: http://ip:port).
                                enc-field=; Vault field containing encryption key.
                                enc-format=base64; Encryption key format, can be 'raw' or 'base64'.
                                path=secret/data/dgraph; Vault KV store path (e.g. 'secret/data/dgraph' for KV V2, 'kv/dgraph' for KV V1).
                                role-id-file=; Vault RoleID file, used for AppRole authentication.
                                secret-id-file=; Vault SecretID file, used for AppRole authentication.
                             (default "addr=http://localhost:8200; role-id-file=; secret-id-file=; path=secret/data/dgraph; acl-field=; acl-format=base64; enc-field=; enc-format=base64")
  -w, --wal string          Directory where Raft write-ahead logs are stored.

Use "dgraph debug [command] --help" for more information about a command.
```

#### `dgraph debuginfo`

This command generates information about the current node that is useful for
debugging. The following replicates the help listing shown when you run
`dgraph debuginfo --help`:

```shell
Generate debug information on the current node
Usage:
 dgraph debuginfo [flags]

Flags:
 -a, --alpha string       Address of running dgraph alpha. (default "localhost:8080")
 -x, --archive            Whether to archive the generated report (default true)
 -d, --directory string   Directory to write the debug info into.
 -h, --help               help for debuginfo
 -p, --profiles strings   List of pprof profiles to dump in the report. (default [goroutine,heap,threadcreate,block,mutex,profile,trace])
 -s, --seconds uint32     Duration for time-based profile collection. (default 15)
 -z, --zero string        Address of running dgraph zero.

Use "dgraph debuginfo [command] --help" for more information about a command.
```

### Dgraph tools commands

Dgraph tools provide a variety of tools to make it easier for you to deploy and
manage Dgraph.

#### `dgraph completion`

This command generates shell completion scripts for `bash` and `zsh` CLIs. The
following replicates the help listing shown when you run
`dgraph completion --help`:

```shell
Generates shell completion scripts for bash or zsh
Usage:
 dgraph completion [command]

Available Commands:
 bash        bash shell completion
 zsh         zsh shell completion

Flags:
 -h, --help   help for completion

Use "dgraph completion [command] --help" for more information about a command.
```

#### `dgraph conv`

This command runs the Dgraph geographic file converter, which converts
geographic files into RDF so that they can be consumed by Dgraph. The following
replicates the help listing shown when you run `dgraph conv --help`:

```shell
Dgraph Geo file converter
Usage:
 dgraph conv [flags]

Flags:
     --geo string       Location of geo file to convert
     --geopred string   Predicate to use to store geometries (default "loc")
 -h, --help             help for conv
     --out string       Location of output rdf.gz file (default "output.rdf.gz")

Use "dgraph conv [command] --help" for more information about a command.
```

#### `dgraph decrypt`

This command lets you decrypt an export file created by an encrypted Dgraph
Cluster. The following replicates the help listing shown when you run
`dgraph decrypt --help`:

```shell
 A tool to decrypt an export file created by an encrypted Dgraph cluster
Usage:
  dgraph decrypt [flags]

Flags:
      --encryption string   [Enterprise Feature] Encryption At Rest options
                                key-file=; The file that stores the symmetric key of length 16, 24, or 32 bytes. The key size determines the chosen AES cipher (AES-128, AES-192, and AES-256 respectively).
                             (default "key-file=;")
  -f, --file string         Path to file to decrypt.
  -h, --help                help for decrypt
  -o, --out string          Path to the decrypted file.
      --vault string        Vault options
                                acl-field=; Vault field containing ACL key.
                                acl-format=base64; ACL key format, can be 'raw' or 'base64'.
                                addr=http://localhost:8200; Vault server address (format: http://ip:port).
                                enc-field=; Vault field containing encryption key.
                                enc-format=base64; Encryption key format, can be 'raw' or 'base64'.
                                path=secret/data/dgraph; Vault KV store path (e.g. 'secret/data/dgraph' for KV V2, 'kv/dgraph' for KV V1).
                                role-id-file=; Vault RoleID file, used for AppRole authentication.
                                secret-id-file=; Vault SecretID file, used for AppRole authentication.
                             (default "addr=http://localhost:8200; role-id-file=; secret-id-file=; path=secret/data/dgraph; acl-field=; acl-format=base64; enc-field=; enc-format=base64")

Use "dgraph decrypt [command] --help" for more information about a command.
```

#### `dgraph export_backup`

This command is used to convert a
[binary backup](/dgraph/enterprise/binary-backups) created using Dgraph
Enterprise Edition into an exported folder. The following replicates key
information from the help listing shown when you run
`dgraph export_backup --help`:

```shell
Export data inside single full or incremental backup
Usage:
  dgraph export_backup [flags]

Flags:
  -d, --destination string   The folder to which export the backups.
      --encryption string    [Enterprise Feature] Encryption At Rest options
                                 key-file=; The file that stores the symmetric key of length 16, 24, or 32 bytes. The key size determines the chosen AES cipher (AES-128, AES-192, and AES-256 respectively).
                              (default "key-file=;")
  -f, --format string        The format of the export output. Accepts a value of either rdf or JSON (default "rdf")
  -h, --help                 help for export_backup
  -l, --location string      Sets the location of the backup. Both file URIs and s3 are supported.
                                 This command will take care of all the full + incremental backups present in the location.
      --upgrade              If true, retrieve the CORS from DB and append at the end of GraphQL schema.
                                 It also deletes the deprecated types and predicates.
                                 Use this option when exporting a backup of 20.11 for loading onto 21.03.
      --vault string         Vault options
                                 acl-field=; Vault field containing ACL key.
                                 acl-format=base64; ACL key format, can be 'raw' or 'base64'.
                                 addr=http://localhost:8200; Vault server address (format: http://ip:port).
                                 enc-field=; Vault field containing encryption key.
                                 enc-format=base64; Encryption key format, can be 'raw' or 'base64'.
                                 path=secret/data/dgraph; Vault KV store path (e.g. 'secret/data/dgraph' for KV V2, 'kv/dgraph' for KV V1).
                                 role-id-file=; Vault RoleID file, used for AppRole authentication.
                                 secret-id-file=; Vault SecretID file, used for AppRole authentication.
                              (default "addr=http://localhost:8200; role-id-file=; secret-id-file=; path=secret/data/dgraph; acl-field=; acl-format=base64; enc-field=; enc-format=base64")

Use "dgraph export_backup [command] --help" for more information about a command.
```

#### `dgraph increment`

This command increments a counter transactionally, so that you can confirm that
an Alpha node is able to handle both query and mutation requests. To learn more,
see [Using the Increment Tool](/dgraph/admin/increment-tool). The following
replicates the help listing shown when you run `dgraph increment --help`:

```shell
Increment a counter transactionally
Usage:
 dgraph increment [flags]

Flags:
     --alpha string    Address of Dgraph Alpha. (default "localhost:9080")
     --be              Best-effort. Read counter value without retrieving timestamp from Zero.
     --creds string    Various login credentials if login is required.
                         user defines the username to login.
                         password defines the password of the user.
                         namespace defines the namespace to log into.
                         Sample flag could look like --creds user=username;password=mypass;namespace=2
 -h, --help            help for increment
     --jaeger string   Send opencensus traces to Jaeger.
     --num int         How many times to run. (default 1)
     --pred string     Predicate to use for storing the counter. (default "counter.val")
     --retries int     How many times to retry setting up the connection. (default 10)
     --ro              Read-only. Read the counter value without updating it.
     --tls string      TLS Client options
                           ca-cert=; The CA cert file used to verify server certificates. Required for enabling TLS.
                           client-cert=; (Optional) The Cert file provided by the client to the server.
                           client-key=; (Optional) The private Key file provided by the clients to the server.
                           internal-port=false; (Optional) Enable inter-node TLS encryption between cluster nodes.
                           server-name=; Used to verify the server hostname.
                           use-system-ca=true; Includes System CA into CA Certs.
                        (default "use-system-ca=true; internal-port=false;")
     --wait duration   How long to wait.

Use "dgraph increment [command] --help" for more information about a command.
```

#### `dgraph lsbackup`

This command lists information on backups in a given location for Dgraph
Enterprise Edition. To learn more, see
[Backup List Tool](/dgraph/enterprise/lsbackup). The following replicates the
help listing shown when you run `dgraph lsbackup --help`:

```shell
List info on backups in a given location
Usage:
 dgraph lsbackup [flags]

Flags:
 -h, --help              help for lsbackup
 -l, --location string   Sets the source location URI (required).
     --verbose           Outputs additional info in backup list.

Use "dgraph lsbackup [command] --help" for more information about a command.
```

#### `dgraph migrate`

This command runs the Dgraph [migration tool](/dgraph/admin/import-mysql) to
move data from a MySQL database to Dgraph. The following replicates the help
listing shown when you run `dgraph migrate --help`:

```shell
Run the Dgraph migration tool from a MySQL database to Dgraph
Usage:
 dgraph migrate [flags]

Flags:
     --db string              The database to import
 -h, --help                   help for migrate
     --host string            The hostname or IP address of the database server. (default "localhost")
 -o, --output_data string     The data output file (default "sql.rdf")
 -s, --output_schema string   The schema output file (default "schema.txt")
     --password string        The password used for logging in
     --port string            The port of the database server. (default "3306")
 -q, --quiet                  Enable quiet mode to suppress the warning logs
 -p, --separator string       The separator for constructing predicate names (default ".")
     --tables string          The comma separated list of tables to import, an empty string means importing all tables in the database
     --user string            The user for logging in

Use "dgraph migrate [command] --help" for more information about a command.
```

#### `dgraph upgrade`

This command helps you to upgrade from an earlier Dgraph release to a newer
release. The following replicates the help listing shown when you run
`dgraph upgrade --help`:

```shell
This tool is supported only for the mainstream release versions of Dgraph, not for the beta releases.
Usage:
 dgraph upgrade [flags]

Flags:
     --acl               upgrade ACL from v1.2.2 to >=v20.03.0
 -a, --alpha string      Dgraph Alpha gRPC server address (default "127.0.0.1:9080")
 -d, --deleteOld         Delete the older ACL types/predicates (default true)
     --dry-run           dry-run the upgrade
 -f, --from string       The version string from which to upgrade, e.g.: v1.2.2
 -h, --help              help for upgrade
 -p, --password string   Password of ACL user
 -t, --to string         The version string till which to upgrade, e.g.: v20.03.0
 -u, --user string       Username of ACL user

Use "dgraph upgrade [command] --help" for more information about a command.
```


# Shell Completion
Source: https://docs.hypermode.com/dgraph/cli/completion

Dgraph supports command-line completion, a common feature provided by shells like bash or zsh that helps you to type commands in a fast and easy way

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Command-line completion is a common feature provided by shells like `bash` or
`zsh` that lets you type commands in a fast and easy way. This functionality
automatically fills in partially typed commands when the user press the

<kbd>tab</kbd> key.

## Completion script

The command-line interpreter requires a completion script to define which
completion suggestions can be displayed for a given executable.

Using the `dgraph completion` command you can generate a file that can be added
to your shell configuration. Once added, you will be able to auto-complete any
`dgraph` command.

<Note>
  Dgraph command completion currently supports `bash` and `zsh` shells.
</Note>

First, you need to know which shell you are running. If you don't know, you can
execute the following command:

```sh
echo $0
```

and the output should look like:

```sh
user@workstation:~/dgraph$ echo $0
bash
```

## Bash shell

To generate a `dgraph-completion.sh` configuration file for your `bash` shell,
run the `completion` command as follows:

```sh
dgraph completion bash > ~/dgraph-completion.sh
```

The file content should look like:

```sh
[Decoder]: Using assembly version of decoder
Page Size: 4096
# bash completion for dgraph                               -*- shell-script -*-

__dgraph_debug()
{
    if [[ -n ${BASH_COMP_DEBUG_FILE} ]]; then
        echo "$*" >> "${BASH_COMP_DEBUG_FILE}"
    fi
}
...
..
.
```

Currently, the generated file has 2 lines at the beginning that need to be
removed, or else the script won't run properly. You can comment them out with a
`#`, or you can easily remove them with the following command:

```sh
sed -i.bak '1d;2d' ~/dgraph-completion.sh
```

Next, you have to make that file executable by running the following command
(your system might require `sudo` to run it):

```sh
chmod +x ~/dgraph-completion.sh
```

Now open the `.bashrc` file with any text editor (you might need `sudo` to apply
changes). For example:

```sh
nano ~/.bashrc
```

Once opened, add the path to `dgraph-completion.sh` using the following syntax
and save:

```sh
. path/to/dgraph-completion.sh
```

Finally, reload the `bashrc` settings with the following command:

```sh
source ~/.bashrc
```

Now you can start typing `dgraph` and press <kbd>tab</kbd> to get
auto-completion and suggestions:

```txt
user@workstation:~/dgraph$ dgraph
acl            cert           debug          increment      migrate        tool           zero
alpha          completion     debuginfo      live           raftmigrate    upgrade
bulk           conv           export_backup  lsbackup       restore        version
```


# Access Control Lists
Source: https://docs.hypermode.com/dgraph/concepts/acl



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Access Control Lists (ACL) are a typical mechanism to list who can access what,
specifying either users or roles and what they can access. ACLs help determine
who is "authorized" to access what.

Dgraph Access Control Lists (ACLs) are sets of permissions for which
`Relationships` a user may access. Recall that Dgraph is "predicate based" so
all data is stored in and is implicit in relationships. This allows
relationship-based controls to be very powerful in restricting a graph based on
roles, known as Relationship-Based Access Control (RBAC).

Note that the Dgraph multi-tenancy feature relies on ACLs to ensure each tenant
can only see their own data in one server.

Using ACLs requires a client to authenticate (log in) differently and specify
credentials that drive which relationships are visible in their view of the
graph database.


# Badger
Source: https://docs.hypermode.com/dgraph/concepts/badger



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

[Badger](/badger/overview) is a key-value store developed and maintained by
Dgraph. It is also open source, and it's the backing store for Dgraph data.

It is largely transparent to users that Dgraph uses Badger to store data
internally. Badger is packaged into the Dgraph binary, and is the persistence
layer. However, various configuration settings and log messages may reference
Badger, such as cache sizes.

Badger values are `Posting Lists` and indexes. Badger Keys are formed by
concatenating `<RelationshipName>+<NodeUID>`.


# Dgraph Clients
Source: https://docs.hypermode.com/dgraph/concepts/clients



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

A client is a program that calls dgraph. Broadly, there are stand alone clients
such as Ratel, which is a graphical web-based app, and programmatic client
libraries which are embedded in larger programs to efficiently call Dgraph.

GraphQL is an open standard with many clients (graphical and libraries) also,
and GraphQL clients work with Dgraph.

Dgraph provides [client libraries](/dgraph/sdks/overview) for many languages.
These clients send DQL queries, and perform useful functions such as logging in.

Note that Dgraph doesn't force or insist on any particular GraphQL client. Any
GraphQL client, GUI, tool, or library works well with Dgraph, and it's the
users' choice which to choose. Dgraph only provides clients for the proprietary
DQL query language. GraphQL clients are available for free from many
organizations.


# Consistency Model
Source: https://docs.hypermode.com/dgraph/concepts/consistency-model



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

### Dgraph supports MVCC, Read Snapshots and Distributed ACID transactions

Multi-version concurrency control (MVCC) is a technique where many versions of
data are written (but never modified) on disk, so many versions exist. This
helps control concurrency because the database is queried at a particular
"timestamp" for the duration of one query to provide snapshot isolation and
ensure data is consistent for that transaction. (Note that MVCC is losely
related to LSM trees - in LSM parlance, data is "logged" to write-only files,
which are later merged via Log Compaction.)

Writes are faster with MVCC because data is always written by flushing a larger
in-memory buffer (a memtable) to new, contiguous files (SST files), and newer
data obscures or replaces older data. Consistent updates from each transaction
share a logical commit timestamp (a 64 bit, increasing number loosely correlated
to wall clock time), and all reads occur "at a point in time" meaning any read
accesses a known, stable set of committed data using these same commit
timestamps. New or in-process commits are associated with a later timestamp so
they do not affect running queries at earlier timestamps. This allows pure
queries (reads) to execute without any locks.

One special set of structures are "memtables" which are also referred to as
being Level 0 of the LSM tree. These are buffers for fast writes, which later
are flushed to on-disk files called SSTs.

### Dgraph transactions are cluster-wide (not key-only, or any other non-ACID version of transactions)

Dgraph uses the Raft protocol to synchronize updates and ensure updates are
durably written to a majority of alpha nodes in a cluster before the transaction
is considered successful. Raft ensures true, distributed, cluster wide
transactions across multiple nodes, keys, edges, indexes and facets. Dgraph
provides true ACID transactions, and does not impose limitations on what can be
in a transaction: a transaction can involve multiple predicates, multiple nodes,
multiple keys and even multiple shards.

### Transactions are lockless

Dgraph transactoins do not use locks, allowing fast, distributed transactions.

For reads, queries execute at a particular timestamp based on snapshot
isolation, which isolates reads from any concurrent write activity. All reads
access snapshots across the entire cluster, seeing all previously committed
transactions in full, regardless of which alpha node received earlier queries.

Writes use optimistic lock semantics, where a transaction will be aborted if
another (concurrent) transaction updates exactly the same data (same edge on the
same node) first. This will be reported as an "aborted" transaction to the
caller.

Dgraph ensures monotonically increasing transaction timestamps to sequence all
updates in the database. This provides serializability: if any transaction Tx1
commits before Tx2 starts, then Ts\_commit(Tx1) \< Ts\_start(Tx2), and in turn a
read at any point in time can never see Tx1 changes but not Tx2 changes.

Dgraph also ensures proper read-after-write semantics. Any commit at timestamp
Tc is guaranteed to be seen by a read at timestamp Tr by any client, if Tr >=
Tc.

### Terminology

* **Snapshot isolation:** all reads see a consistent view of the database at the
  point in time when the read was submitted
* **Oracle:** a logical process that tracks timestamps and which data (keys,
  predicates, etc.) has been committed or is being modified. The oracle hands
  out timestamps and aborts transactions if another transaction has modified its
  data.
* **Raft:** a well-known consistency algorithm to ensure distributed processes
  durably store data
* **Write-Ahead Log:** Also WAL. A fast log of updates on each alpha that
  ensures buffered in-memory structures are persisted.
* **Proposal:** A process within the Raft algorithm to track possible updates
  during the consensus process.
* **SST:** Persistent files comprising the LSM tree, together with memtables.
* **Memtable:** An in-memory version of an SST, supporting fast updates.
  Memtables are mutable, and SSTs are immutable.
* **Log Compaction:** The process of combining SSTs into newer SSTs while
  eliminating obsolte data and reclaiming disk space.
* **Timestamp:** Or point in time. A numeric counter representing the sequential
  order of all transactions, and indicating when a transaction became valid and
  query-able.
* **Optimistic Lock:** a logical process whereby all transactions execute
  without blocking on other transactions, and are aborted if there is a
  conflict. Aborted transactions should typically be retried if they occur.
* **Pessimistic Lock:** a process, not used in Dgraph, where all concurrent
  transactions mutating the same data except one block and wait for each other
  to complete.
* **ACID** An acronym representing attributes of true transactions: Atomic,
  Consistent, Isolated, and Durable


# Discovery
Source: https://docs.hypermode.com/dgraph/concepts/discovery



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

### New servers and discovery

Dgraph clusters detects new machines allocated to the
[cluster](/dgraph/self-managed/cluster-setup), establish connections, and
transfer data to the new server based on the group the new machine is in.


# DQL
Source: https://docs.hypermode.com/dgraph/concepts/dql



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

DQL is the "Dgraph Query Language" and is based on GraphQL. It is neither a
superset nor subset of GraphQL, but is generally more powerful than GraphQL. DQL
coexists nicely with GraphQL so many users perform most access using GraphQL and
only "drop down" into DQL when there is a particular query mechanism needed that
isn't supported in the GraphQL spec. E.g. @recurse query operations are only in
DQL. Other customers simply use DQL. DQL supports both queries and mutations, as
well as hybrid "upsert" operations.


# DQL and GraphQL
Source: https://docs.hypermode.com/dgraph/concepts/dql-graphql-layering



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## Dgraph schemas

Dgraph natively supports GraphQL, including `GraphQL Schema`s. GraphQL schemas
"sit on top of" DQL schemas, in the sense that when a GraphQL schema is added to
Dgraph, a corresponding `DQL Schema` is automatically created.

## Dgraph queries, mutations and upserts

Similarly, GraphQL mutations are implemented on top of DQL in the sense that a
GraphQL query is converted internally into a DQL query, which is then executed.
This translation isn't particularly complex, since DQL is based on GraphQL, with
some syntax changes and some extensions.

This is generally transparent to all callers, however users should be aware that

1. Anything done in GraphQL can also be done in DQL if needed. Some small
   exceptions include the enforcement of non-null constraints and other checks
   done before Dgraph transpiles GraphQL to DQL and executes it.
2. Some logging including Request Logging and OpenTrace (Jaeger) tracing may
   show DQL converted from the GraphQL.


# Facets
Source: https://docs.hypermode.com/dgraph/concepts/facets



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph allows a set of properties to be associated with any `Relationship`. E.g.
if there is a "worksFor" relationships between Node "Bob" and Node "Google",
this relationship may have facet values of "since": 2002-05-05 and "position":
"Engineer".

Facets can always be replaced by adding a new Node representing the relationship
and storing the facet data as attributes of the new Node.

The term "facet" is also common in database and search engine technology, and
indicates a dimension or classification of data. One way to use facets it to
indicate a relationship type.


# GraphQL
Source: https://docs.hypermode.com/dgraph/concepts/graphql



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

`GraphQL` is a query and update standard defined at
[GraphQL.org](https://graphql.org/). `GraphQL` is natively supported by Dgraph,
without requiring additional servers, data mappings or resolvers. Typically,
"resolving" a data field in GraphQL simply corresponds to walking that
relationship in Dgraph.

Dgraph also auto-generates access functions for any `GraphQL Schema`, allowing
users to get up and running in minutes with Dgraph + a GraphQL schema. The APIs
are auto-generated.

GraphQL is internally converted to the (similar-but-different) `DQL` query
language before being executed. We can think of GraphQL as "sitting on top" of
DQL.


# Group
Source: https://docs.hypermode.com/dgraph/concepts/group



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

A group is a set of 1 or 3 or more servers that work together and have a single
`leader` in the sense defined by the Raft protocol.

## Alpha group

An Alpha `Group` in Dgraph is a shard of data, and may or may not be highly
available (HA). An HA group typically has three Dgraph instances (servers or K8s
pods), and a non-HA group is a single instance. Every Alpha instance belongs to
one group, and each group is responsible for serving a particular set of tablets
(relations). In an HA configuration, the three or more instances in a single
group replicate the same data to every instance to ensure redundancy of data.

In a sharded Dgraph cluster, tablets are automatically assigned to each group,
and dynamically relocated as sizes change to keep the groups balanced.
Predicates can also be moved manually if desired.

To avoid confusion, remember that you may have many Dgraph Alpha instances due
to either sharding, or due to HA configuration. If you have both sharding and
HA, you have 3\*N groups:

| Configuration | Non-HA            | HA                       |
| ------------- | ----------------- | ------------------------ |
| Non-sharded   | 1 Alpha total     | 3 Alphas total           |
| Sharded       | 1 Alpha per group | 3\*N Alphas for N groups |

## Zero group

Group Zero is a lightweight server or group of servers which helps control the
overall cluster. It manages timestamps and UIDs, determines when data should be
rebalanced among shards, and other functions. The servers in this group are
generally called "Zeros."


# Index and Tokenizer
Source: https://docs.hypermode.com/dgraph/concepts/index-tokenize



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

### Indexing

An index is an optimized data structure, stored on disk and loaded into memory,
that speeds or optimizes query processing. It is created and stored in addition
to the primary data. E.g. a "hasName" property or relation is the primary
storage structure for a graph in Dgraph, but may also have an additional index
structure configured.

Typically, Dgraph query access is optimized for forward access. When other
access is needed, an index may speed up queries. Indexes are large structures
that hold all values for some Relation (vs `Posting Lists`, which are typically
smaller, per-Node structures).

### Tokenizers

Tokenizers are simply small algorithms that create indexed values from some Node
property. E.g. if a Book Node has a Title attribute, and you add a "term" index,
each word (term) in the text will be indexed. The word "Tokenizer" derives its
name from tokenizing operations to create this index type.

Similary if the Book has a publicationDateTime you can add a day or year index.
The "tokenizer" here extracts the value to be indexed, which may be the day or
hour of the dateTime, or only the year.


# Lambdas
Source: https://docs.hypermode.com/dgraph/concepts/lambda



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph Lambdas are JavaScript functions that can be used during query or
mutation processing to extend GraphQL or DQL queries and mutations. Lambdas are
not related at all to AWS Lambdas.


# Minimal Network Calls
Source: https://docs.hypermode.com/dgraph/concepts/minimizing-network-calls



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Compared to RAM or SSD access, network calls are slow, so Dgraph is built from
the ground up to minimize them. For graph databases which store sub-graphs on
different shards, this is difficult or impossible, but predicate-based
(relationship-based) sharding allows fast distributed query with Dgraph.

### Predicate-based storage and sharding

Dgraph is unique in its use of predicate-based sharding, which allows complex
and deep distributed queries to run without incurring high network overhead and
associated delays.

Rather than store and shard by putting different *nodes* (aka
entities<sup>\*</sup>) on different servers, Dgraph stores predicates or triples
of the form `<node1> <predicateRelation> <node2>`. The nodes are therefore
implicit in the predicate storage, rather than vice versa.

This makes querying much different and particularly allows network optimizations
in a distributed database.

### Example

To explain how this works, let's use an example query:

`Find all posts liked by friends of friends of mine over the last year, written by a popular author A.`

### SQL/NoSQL

In a distributed SQL database or (non-graph) NoSQL database, this query requires
retrieval of a lot of data. Consider two approaches:

Approach 1:

* Find all the friends (\~ 338
  [friends](https://www.pewresearch.org/fact-tank/2014/02/03/what-people-like-dislike-about-facebook/)).
* Find all their friends (\~ 338 \* 338 = 40,000 people).
* Find all the posts liked by these people over the last year (resulting set in
  the millions).
* Intersect these posts with posts authored by person A.

Approach 2:

* Find all posts written by popular author A over the last year (possibly
  thousands).
* Find all people who liked those posts (easily millions) (call this
  `result set 1`).
* Find all your friends.
* Find all their friends (call this `result set 2`).
* Intersect `result set 1` with `result set 2`.

Both approaches wouild result in a lot of data moving back and forth between
database and app; would be slow to execute, and may require running an offline
job.

### Dgraph Approach

This is how it would run in Dgraph:

Sharding assumptions (which predicates live where):

* Assume Server X contains the predicate `friends` representing all friend
  relations.
* Assume Server Y contains the predicate `posts_liked` representing who likes
  each post.
* Assume Server Z contains the predicate `author` representing all who authored
  each post.
* Assume Server W contains the predicate `title` representing the uid->string
  title property of posts.

Algorithm:

* Server X
  * If the request was not sent to Server X, route it to Server X where the
    friends predicate lives. **(1 RPC)**.
  * Seek to my uid within predicate (tablet) `friends` and retrieve a list of my
    friends as a list of uids.
  * Still on Server X, use the friends predicate again to get friends for all of
    those uids, generating a list of my friends of friends. Call this
    `result set myFOF`.
* Server Y
  * Send result set myFOF to Server Y, which holds the posts\_liked predicate
    **(1 RPC)**.
  * Retrieve all posts liked by my friends-of-friends. Call this
    `result set postsMyFOFLiked`.
* Server Z
  * Send postsMyFOFLiked result set to Server Z **(1 RPC)**.
  * Retrieve all posts authored by A. Call this `result set authoredByA`.
  * Still on Server Z, intersect the two sorted lists to get posts that are both
    liked and authored by A: `result set postsMyFOFLiked` intersect
    `result set authoredByA`. Call this `result set postsMyFOFLikedByA`
  * at this point we have done the hard work, but have the uids of the posts,
    instead of the post titles.
* Server W
  * Send `result set postsMyFOFLikedByA` to Server W which holds the title
    predicate **(1 RPC)**.
  * Convert uids to names by looking up the title for each uid.
    `result set postUidsAndTitles`
* Respond to caller with `result set postUidsAndTitles`.

## Net Result - predictable distributed graph scaling

In at most 4 RPCs, we have figured out all the posts liked by friends of
friends, written by popular author X, with titles. Typically, all four
predicates will not live on four different Servers, so this is a worst-case
scenario. Dgraph network activity is limited to the level of query join depth,
rather than increasing arbitrarily according to the number of nodes in the
graph, and how they are broken up across servers.

There is no way we are aware of that a node-based sharding database can avoid
high network RPC counts during arbitrary queries because "node-hopping" does not
mix well with a graph that is segmented across servers.

***

<sup>\*</sup> *Throughout this note, we call entities in a graph "nodes" which
is a standard terminology when talking about nodes and predicates. These may be
confused with Raft or Kubernetes nodes in some contexts, but generally we mean
nodes in a graph*.


# Namespace and Tenant
Source: https://docs.hypermode.com/dgraph/concepts/namespace-tenant



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

A Dgraph `Namespace` (or tenant) is a logical separation for data within a
Dgraph cluster. A Dgraph cluster can host many Namespaces. Each user must then
into their own namespace using namespace-specific own credentials, and sees only
their own data. Note that this usually requires an extra or specific login.

There is no mechanism to query in a way that combines data from two namespaces,
which simplifies and enforces security in use cases where this is the
requirement. An API layer or client would have to pull data from multiple
namespaces using different authenticated queries if data needed to be combined.


# Posting List and Tablet
Source: https://docs.hypermode.com/dgraph/concepts/posting-list



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Posting lists and tablets are internal storage mechanisms and are generally
hidden from users or developers, but logs, core product code, blog posts and
discussions about Dgraph may use the terms "posting list" and "tablet."

Posting lists are a form of inverted index. Posting lists correspond closely to
the RDF concept of a graph, where the entire graph is a collection of triples,
`<subject> <predicate> <object>`. In this view, a posting list is a list of all
triples that share a `<subject>+<predicate>` pair.

(Note that in Dgraph docs, we typically use the term "relationship" rather than
predicate, but here we will refer to predicates explicitly.)

The posting lists are grouped by predicate into `tablets`. A tablet therefore
has all data for a predicate, for all subject UIDs.

Tablets are the basis for data shards in Dgraph. In the near future, Dgraph may
split a single tablet into two shards, but currently every data shard is a
single predicate. Every server then hosts and stores a set of tablets. Dgraph
will move or allocate different tablets to different servers to achieve balance
across a sharded cluster.

### Example

If we're storing friendship relationships among four people, we may have four
posting lists represented by the four tables below:

| Node    | Attribute | Value   |
| ------- | --------- | ------- |
| person1 | friend    | person2 |
| person1 | friend    | person4 |

 

| Node    | Attribute | Value   |
| ------- | --------- | ------- |
| person2 | friend    | person1 |

 

| Node    | Attribute | Value   |
| ------- | --------- | ------- |
| person3 | friend    | person2 |
| person3 | friend    | person4 |

 

| Node    | Attribute | Value   |
| ------- | --------- | ------- |
| person4 | friend    | person2 |
| person4 | friend    | person1 |
| person4 | friend    | person3 |

 

The corrsponding posting lists would be something like:

```
person1UID+friend->[person2UID, person4UID]
person2UID+friend->[person1UID]
person3UID+friend->[person2UID, person4UID]
person4UID+friend->[person1UID, person2UID, person3UID]
```

 

Similarly, a posting list will also hold all literal value properties for every
node. E.g. consider the names of people in these three tables:

| Node    | Attribute | Value   |
| ------- | --------- | ------- |
| person1 | name      | "James" |
| person1 | name      | "Jimmy" |
| person1 | name      | "Jim"   |

 

| Node    | Attribute | Value   |
| ------- | --------- | ------- |
| person2 | name      | "Rajiv" |

 

| Node    | Attribute | Value    |
| ------- | --------- | -------- |
| person3 | name      | "Rachel" |

  The posting lists would look like:

```
person1UID+name->["James", "Jimmy", "Jim"]
person2UID+friend->["Rajiv"]
person3UID+friend->["Rachel"]
```

 

Note that person4 has no name attribute specified, so that posting list would
not exist.

In these examples, two predicates (relations) are defined, and therefore two
tablets will exist.

The tablet for the `friend` predicate will hold all posting lists for all
"friend" relationships in the entire graph. The tablet for the `name` property
will hold all posting lists for `name` in the graph.

If other types such as Pets or Cities also have a name property, their data will
be in the same tablet as the Person names.

### Performance implications

A key advantage of grouping data into predicate-based shards is that we have all
the data to do one join in one `tablet` on one server/shard. This means, one RPC
to the machine serving that `tablet` will be adequate, as documented in
[How Dgraph Minmizes Network Calls](./minimizing-network-calls).

Posting lists are the unit of data access and caching in Dgraph. The underlying
key-value store stores and retrieves posting lists as a unit. Queries that
access larger posting lists will use more cache and may incur more disk access
for un-cached posting lists.


# Protocol Buffers
Source: https://docs.hypermode.com/dgraph/concepts/protocol-buffers



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

All data in Dgraph that is stored or transmitted among the Dgraph instances
(servers) is converted into space-optimized byte arrays using
[Protocol Buffers](https://developers.google.com/protocol-buffers/). Protocol
Buffers are a standard, optimized technology to speed up network communications.


# Query Process
Source: https://docs.hypermode.com/dgraph/concepts/queries-process



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

To understand how query execution works, look at an example.

```
{
    me(func: uid(0x1)) {
      rel_A
      rel_B {
        rel_B1
        rel_B2
      }
      rel_C {
        rel_C1
        rel_C2 {
          rel_C2_1
      }
      }
  }
}

```

Let's assume we have 3 Alpha instances, and instance id=2 receives this query.
These are the steps:

* This query specifies the exact UID list (one UID) to start with, so there is
  no root query clause.
* Retreive posting lists using keys = `0x1::rel_A`, `0x1::rel_B`, and
  `0x1::rel_C`.
  * At worst, these predicates could belong to 3 different groups if the DB is
    sharded, so this would incur at most 3 network calls.
* The above posting lists would include three lists of UIDs or values.
  * The UID results (id1, id2, ..., idn) for `rel_B` are converted into queries
    for `id1::rel_B1` `id2::rel_B1`, etc., and for `id1::rel_B2` `id2::rel_B2`,
    etc.
  * Similarly, results for rel\_C will be used to get the next set of UIDs from
    posting list keys like `id::rel_C1` and `id::rel_C2`.
* This process continues recursively for `rel_C2_1` as well, and as deep as any
  query requires.

More complex queries may do filtering operations, or intersections and unions of
UIDs, but this recursive walk to execute a number of (often parallel) `Tasks` to
retrieve UIDs characterizes Dgraph querying.

If the query was run via HTTP interface `/query`, the resulting subgraph then
gets converted into JSON for replying back to the client. If the query was run
via [gRPC](https://grpc.io/) interface using the language [clients](./clients),
the subgraph gets converted to
[protocol buffer](https://developers.google.com/protocol-buffers/) format and
similarly returned to the client.


# Raft
Source: https://docs.hypermode.com/dgraph/concepts/raft



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph uses Raft whenever consensus among a distributed set of servers is
required, such as ensuring that a transaction has been properly committed, or
determining the proper timestamp for a read or write. Each zero or alpha `group`
uses raft to elect leaders.

This section aims to explain the Raft consensus algorithm in simple terms. The
idea is to give you just enough to make you understand the basic concepts,
without going into explanations about why it works accurately. For a detailed
explanation of Raft, please read the original thesis paper by
[Diego Ongaro](https://github.com/ongardie/dissertation).

## Term

Each election cycle is considered a **term**, during which there is a single
leader *(just like in a democracy)*. When a new election starts, the term number
is increased. This is straightforward and obvious but is a critical factor for
the accuracy of the algorithm.

In rare cases, if no leader could be elected within an `ElectionTimeout`, that
term can end without a leader.

## Server States

Each server in cluster can be in one of the following three states:

* Leader
* Follower
* Candidate

Generally, the servers are in leader or follower state. When the leader crashes
or the communication breaks down, the followers will wait for election timeout
before converting to candidates. The election timeout is randomized. This would
allow one of them to declare candidacy before others. The candidate would vote
for itself and wait for the majority of the cluster to vote for it as well. If a
follower hears from a candidate with a higher term than the current (*dead in
this case*) leader, it would vote for it. The candidate who gets majority votes
wins the election and becomes the leader.

The leader then tells the rest of the cluster about the result
(<tt>Heartbeat</tt> [Communication](./#communication)) and the other candidates
then become followers. Again, the cluster goes back into leader-follower model.

A leader could revert to being a follower without an election, if it finds
another leader in the cluster with a higher [Term](./#term)). This might happen
in rare cases (network partitions).

## Communication

There is unidirectional RPC communication, from the leader to all/any followers.
The followers never ping the leader. The leader sends `AppendEntries` messages
to the followers with logs containing state updates. When the leader sends
`AppendEntries` with zero logs (updates), that's considered a

<tt>Heartbeat</tt>. The leader sends all followers <tt>Heartbeats</tt> at
regular intervals.

If a follower doesn't receive a <tt>Heartbeat</tt> for `ElectionTimeout`
duration (generally between 150ms to 300ms), the leader may be down, so it
converts it's state to candidate (as mentioned in
[Server States](./#server-states)). It then requests for votes by sending a
`RequestVote` call to other servers. If it gets votes from the majority, the
candidate becomes the leader. On becoming leader, it sends <tt>Heartbeats</tt>
to all other servers to establish its authority.

Every communication request contains a term number. If a server receives a
request with a stale term number, it rejects the request.

## Log Entries

Dgraph uses LSM Trees, so we call commits or updates "Log Entries." Log Entries
are numbered sequentially and contain a term number. An Entry is considered
**committed** if it has been replicated (and stored) by a majority of the
servers.

On being notified of the results of a client request (which is often processed
on other servers), the leader does four things to coordinate Raft consensus
(this is also called Log Replication):

* Appends and persists to its log.
* Issue `AppendEntries` in parallel to other servers.
* Monitors for the majority to report it's replicated, after which it considers
  the entry committed and applies it to the leader's state machine.
* Notifies followers that the entry is committed so that they can apply it to
  their state machines.

A leader never overwrites or deletes its entries. Raft guarantees that if an
entry is committed, all future leaders will have it. A leader can, however,
force overwrite the followers' logs, so they match leader's logs if necessary.

## Voting

Each server persists its current term and vote, so it doesn't end up voting
twice in the same term. On receiving a `RequestVote` RPC, the server denies its
vote if its log is more up-to-date than the candidate. It would also deny a
vote, if a minimum `ElectionTimeout` hasn't passed since the last

<tt>Heartbeat</tt> from the leader. Otherwise, it gives a vote and resets its
`ElectionTimeout` timer.

Up-to-date property of logs is determined as follows:

* Term number comparison
* Index number or log length comparison

<Tip>
  To understand the above sections better, you can see this [interactive
  visualization](http://thesecretlivesofdata.com/raft).
</Tip>

## Cluster membership

Raft only allows single-server changes, i.e. only one server can be added or
deleted at a time. This is achieved by cluster configuration changes. Cluster
configurations are communicated using special entries in `AppendEntries`.

The significant difference in how cluster configuration changes are applied
compared to how typical [Log Entries](./#log-entries) are applied is that the
followers don't wait for a commitment confirmation from the leader before
enabling it.

A server can respond to both `AppendEntries` and `RequestVote`, without checking
current configuration. This mechanism allows new servers to participate without
officially being part of the cluster. Without this feature, things won't work.

When a new server joins, it won't have any logs, and they need to be streamed.
To ensure cluster availability, Raft allows this server to join the cluster as a
non-voting member. Once it's caught up, voting can be enabled. This also allows
the cluster to remove this server in case it's too slow to catch up, before
giving voting rights *(sort of like getting a green card to allow assimilation
before citizenship is awarded providing voting rights)*.

<Tip>
  If you want to add a few servers and remove a few servers, do the addition
  before the removal. To bootstrap a cluster, start with one server to allow it
  to become the leader, and then add servers to the cluster one-by-one.
</Tip>

## Snapshots

One of the ways to do this is snapshotting. As soon as the state machine is
synced to disk, the logs can be discarded.

Snapshots are taken by default after 10000 Raft entries, with a frequency of 30
minutes. The frequency indicates the time between two subsequent snapshots.
These numbers can be adjusted using the `--raft`
[superflag](/dgraph/cli/command-reference)'s `snapshot-after-entries` and
`snapshot-after-duration` options respectively. Snapshots are created only when
conditions set by both of these options have been met.

## Clients

Clients must locate the cluster to interact with it. Various approaches can be
used for discovery.

A client can randomly pick up any server in the cluster. If the server isn't a
leader, the request should be rejected, and the leader information passed along.
The client can then re-route it's query to the leader. Alternatively, the server
can proxy the client's request to the leader.

When a client first starts up, it can register itself with the cluster using
`RegisterClient` RPC. This creates a new client id, which is used for all
subsequent RPCs.

## Linearizable Semantics

Servers must filter out duplicate requests. They can do this via session
tracking where they use the client id and another request UID set by the client
to avoid reprocessing duplicate requests. Raft also suggests storing responses
along with the request UIDs to reply back in case it receives a duplicate
request.

Linearizability requires the results of a read to reflect the latest committed
write. Serializability, on the other hand, allows stale reads.

## Read-only queries

To ensure linearizability of read-only queries run via leader, leader must take
these steps:

* Leader must have at least one committed entry in its term. This would allow
  for up-to-dated-ness. *(C'mon! Now that you're in power do something at
  least!)*
* Leader stores it's latest commit index.
* Leader sends <tt>Heartbeats</tt> to the cluster and waits for ACK from
  majority. Now it knows that it's the leader. *(No successful coup. Yup, still
  the democratically elected dictator I was before!)*
* Leader waits for its state machine to advance to readIndex.
* Leader can now run the queries against state machine and reply to clients.

Read-only queries can also be serviced by followers to reduce the load on the
leader. But this could lead to stale results unless the follower confirms that
its leader is the real leader(network partition). To do so, it would have to
send a query to the leader, and the leader would have to do steps 1-3. Then the
follower can do 4-5.

Read-only queries would have to be batched up, and then RPCs would have to go to
the leader for each batch, who in turn would have to send further RPCs to the
whole cluster. *(This is not scalable without considerable optimizations to deal
with latency.)*

**An alternative approach** would be to have the servers return the index
corresponding to their state machine. The client can then keep track of the
maximum index it has received from replies so far. And pass it along to the
server for the next request. If a server's state machine hasn't reached the
index provided by the client, it will not service the request. This approach
avoids inter-server communication and is a lot more scalable. *(This approach
does not guarantee linearizability, but should converge quickly to the latest
write.)*


# Relationships
Source: https://docs.hypermode.com/dgraph/concepts/relationships



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph stores `relationships` among `nodes` to represent graph structures, and
also stores literal properties of `nodes`.

This makes it easy for Dgraph to ingest the RDF
[N-Quad](https://www.w3.org/TR/n-quads/) format, where each line represents

* `Node, RelationName, Node, Label` or
* `Node, RelationName, ValueLiteral, Label`

The first represents relations among entities (nodes in graph terminology) and
the second represents the relationship of a Node to all it's named attributes.

Often, the optional `Label` is omitted, and therefore the N-Quad data is also
referred to as "triples." When it's included, it represents which `Tenant` or
`Namespace` the data lives in within Dgraph.

<Tip>
  Dgraph can automatically generate a reverse relation. If the user wants to run
  queries in that direction, they would need to define the [reverse
  relationship](/dgraph/dql/schema#reverse-edges)
</Tip>

For `Relationships`, the subject and object are represented as 64-bit numeric
UIDs and the relationship name itself links them:
`<subjectUID> <relationshipName> <objectUID>`.

For literal attributes of a `Node`, the subject must still (and always) be a
numeric UID, but the Object will be a primitive value. These can be thought of
as `<subjectUID> <relationshipName> <value>`, where value is not a 64-bit UID,
and is instead a: string, float, int, dateTime, geopoint, or boolean.


# High Availability Replication
Source: https://docs.hypermode.com/dgraph/concepts/replication



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Each Highly-Available (HA) group will be served by at least 3 instances (or two
if one is temporarily unavailable). In the case of an alpha instance failure,
other alpha instances in the same group still handle the load for data in that
group. In case of a zero instance failure, the remaining two zeros in the zero
group will continue to hand out timestamps and perform other zero functions.

In addition, Dgraph `Learner Nodes` are alpha instances that hold replicas of
data, but this replication is to support read replicas, often in a different
geography from the master cluster. This replication is implemented the same way
as HA replication, but the learner nodes do not participate in quorum, and do
not take over from failed nodes to provide high availability.


# Transaction and Mutation
Source: https://docs.hypermode.com/dgraph/concepts/transaction-mutation



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Borrowing from GraphQL, Dgraph calls writes to the database `Mutations`. As
noted elsewhere (MVCC, LSM Trees and Write Ahead Log sections) writes are
written persistently to the Write Ahead Log, and ephemerally to a memtable.

Data is queried from the combination of persistent SST files and ephemeral
memtable data structures. The mutations therefore always go into the memtables
first (though are also written durably to the WAL). The memtables are the "Level
0" in the LSM Tree, and conceptually sit on top of the immutable SST files.


# ACID Transactions
Source: https://docs.hypermode.com/dgraph/concepts/transactions



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

ACID is an acronym for

* Atomic
* Consistent
* Isolated
* Durable

If these properties are maintained, there is a guarantee that data updates will
not be lost, corrupted or unpredictable. Broadly, an ACID database safely and
reliably stores data, but other databases have failure modes where data can be
lost or corrupted.

### ACID in Dgraph

Dgraph supports distributed ACID transactions through snapshot isolation and the
Raft consensus protocol. Dgraph is fully transactional, and is tested via Jepsen
tests, which is a gold standard to verify transactional consistency.

Dgraph ensure snapshot isolation plus realtime safety: if transaction T1 commits
before T2 begins, than the commit timestamp of T1 is strictly less than the
start timestamp of T2. This ensures that the sequence of writes on shared data
by many processes is reflected in database state.

Snapshot isolation is ensured by maintaining a consistent view of the database
at any (relatively recent) point in time. Every read (query) takes place at the
point-in-time it was submitted, accesses a consistent snapshot that does not
change or include any partial updates due to concurrent writes that are
processing or committing.


# WAL and Memtable
Source: https://docs.hypermode.com/dgraph/concepts/wal-memtable



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Per the Raft (and MVCC) approach, transactions write data to a `Write-Ahead Log`
(WAL) to ensure it's durably stored. Soon after commit, data is also updated in
the `memtables` which are memory buffers holding recently-updated data. The
`memtables` are mutable, unlike the SST files written to disk which hold most
data. Once full, memtables are flushed to disk and become SST files. See Log
Compaction for more details on this process.

In the event of a system crash, the persistent data in the Write Ahead Logs is
replayed to rebuild the memtables and restore the full system state from before
the crash.


# Workers
Source: https://docs.hypermode.com/dgraph/concepts/workers



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

### Workers and Worker Pools

Dgraph maintains a fixed set of worker processes (much like threads or
goroutines) that retrieve and execute queries in parallel as they are sent over
HTTP or gRPC. Dgraph also parallelizes Tasks within a single query execution, to
maximize parallelism and more fully utilize system resources. Dgraph is written
in the go language, which supports high numbers of parallel goroutines, enabling
this approach without creating large numbers of OS threads which would be
slower.


# Aggregation
Source: https://docs.hypermode.com/dgraph/dql/aggregation



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Syntax Example: `AG(val(varName))`

For `AG` replaced with

* `min` : select the minimum value in the value variable `varName`
* `max` : select the maximum value
* `sum` : sum all values in value variable `varName`
* `avg` : calculate the average of values in `varName`

Schema Types:

| Aggregation   | Schema Types                                    |
| :------------ | :---------------------------------------------- |
| `min` / `max` | `int`, `float`, `string`, `dateTime`, `default` |
| `sum` / `avg` | `int`, `float`                                  |

Aggregation can only be applied to
[value variables](./variables#value-variables). An index is not required (the
values have already been found and stored in the value variable mapping).

An aggregation is applied at the query block enclosing the variable definition.
As opposed to query variables and value variables, which are global, aggregation
is computed locally. For example:

```dql
A as predicateA {
  ...
  B as predicateB {
    x as ...some value...
  }
  min(val(x))
}
```

Here, `A` and `B` are the lists of all UIDs that match these blocks. Value
variable `x` is a mapping from UIDs in `B` to values. The aggregation
`min(val(x))`, however, is computed for each UID in `A`. That is, it has a
semantics of: for each UID in `A`, take the slice of `x` that corresponds to
`A`'s outgoing `predicateB` edges and compute the aggregation for those values.

Aggregations can themselves be assigned to value variables, making a UID to
aggregation map.

## Min

### Usage at root

Query Example: Get the min initial release date for any Harry Potter movie.

The release date is assigned to a variable, then it's aggregated and fetched in
an empty block.
`json { var(func: allofterms(name@en, "Harry Potter")) { d as initial_release_date } me() { min(val(d)) } } `

### Usage at other levels

Query Example: Directors called Steven and the date of release of their first
movie, in ascending order of first movie.

```json
{
  stevens as var(func: allofterms(name@en, "steven")) {
    director.film {
      ird as initial_release_date # ird is a value variable mapping a film UID to its release date
    }
    minIRD as min(val(ird)) # minIRD is a value variable mapping a director UID to their first release date
  }

  byIRD(func: uid(stevens), orderasc: val(minIRD)) {
    name@en firstRelease:val(minIRD)
  }
}
```

## Max

### Usage at root

Query Example: Get the max initial release date for any Harry Potter movie.

The release date is assigned to a variable, then it's aggregated and fetched in
an empty block.
`json { var(func: allofterms(name@en, "Harry Potter")) { d as initial_release_date } me() { max(val(d)) } } `

### Usage at other levels

Query Example: Quentin Tarantino's movies and date of release of the most recent
movie.

```json
{
  director(func: allofterms(name@en, "Quentin Tarantino")) {
    director.film { name@en x as initial_release_date } max(val(x))
  }
}
```

## Sum and Avg

### Usage at root

Query Example: Get the sum and average of number of count of movies directed by
people who have Steven or Tom in their name.

```json
{
  var(func: anyofterms(name@en, "Steven Tom")) { a as count(director.film) }

  me() { avg(val(a)) sum(val(a)) }
}
```

### Usage at other levels

Query Example: Steven Spielberg's movies, with the number of recorded genres per
movie, and the total number of genres and average genres per movie.

```json
{
  director(func: eq(name@en, "Steven Spielberg")) {
    name@en director.film { name@en numGenres : g as count(genre) }
    totalGenres : sum(val(g)) genresPerMovie : avg(val(g))
  }
}
```

## Aggregating Aggregates

Aggregations can be assigned to value variables, and so these variables can in
turn be aggregated.

Query Example: For each actor in a Peter Jackson film, find the number of roles
played in any movie. Sum these to find the total number of roles ever played by
all actors in the movie. Then sum the lot to find the total number of roles ever
played by actors who have appeared in Peter Jackson movies. Note that this
demonstrates how to aggregate aggregates; the answer in this case isn't quite
precise though, because actors that have appeared in multiple Peter Jackson
movies are counted more than once.

```json
{
  PJ as var(func:allofterms(name@en, "Peter Jackson")) {
    director.film {
      starring { # starring an actor performance.actor
        { movies as count(actor.film) # number of roles for this actor
      }

      perf_total as sum(val(movies))
    }

    movie_total as sum(val(perf_total)) # total roles for all actors in this movie
    }
    gt as sum(val(movie_total))
  }

  PJmovies(func: uid(PJ)) {
    name@en director.film (orderdesc: val(movie_total), first: 5) {
      name@en totalRoles : val(movie_total)
    }

    grandTotal : val(gt)
  }
}
```


# Aliases
Source: https://docs.hypermode.com/dgraph/dql/alias



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Syntax Examples:

* `aliasName : predicate`
* `aliasName : predicate { ... }`
* `aliasName : varName as ...`
* `aliasName : count(predicate)`
* `aliasName : max(val(varName))`

An alias provides an alternate name in results. Predicates, variables, and
aggregates can be aliased by prefixing with the alias name and `:`. Aliases do
not have to be different to the original predicate name, but, within a block, an
alias must be distinct from predicate names and other aliases returned in the
same block. Aliases can be used to return the same predicate multiple times
within a block.

Query Example: directors with `name` matching term `Steven`, their UID, English
name, average number of actors per movie, total number of films, and the name of
each film in English and French.

```json
{
  ID as var(func: allofterms(name@en, "Steven")) @filter(has(director.film)) {
    director.film {
      num_actors as count(starring)
      average as avg(val(num_actors))
    }
  }

films(func: uid(ID)) { director_id : uid english_name : name@en average_actors :
val(average) num_films : count(director.film)

    films : director.film {
      name : name@en
      english_name : name@en
      french_name : name@fr
    }
  }
}
```


# cascade
Source: https://docs.hypermode.com/dgraph/dql/cascade



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

With the `@cascade` directive, nodes that don't have all predicates specified in
the query are removed. This can be useful in cases where some filter was applied
or if nodes might not have all listed predicates.

Query Example: Harry Potter movies, with each actor and characters played. With
`@cascade`, any character not played by an actor called Warwick is removed, as
is any Harry Potter movie without any actors called Warwick. Without `@cascade`,
every character is returned, but only those played by actors called Warwick also
have the actor name.

```dql
{
  HP(func: allofterms(name@en, "Harry Potter")) @cascade {
    name@en
    starring {
      performance.character {
        name@en
      }
      performance.actor @filter(allofterms(name@en, "Warwick")) {
        name@en
      }
    }
  }
}
```

You can apply `@cascade` on inner query blocks as well.

```json
{
  HP(func: allofterms(name@en, "Harry Potter")) {
    name@en
    genre {
      name@en
    }
    starring @cascade {
      performance.character {
        name@en
      }
      performance.actor @filter(allofterms(name@en, "Warwick")) {
        name@en
      }
    }
  }
}
```

## Parameterized `@cascade`

The `@cascade` directive can optionally take a list of fields as an argument.
This changes the default behavior, considering only the supplied fields as
mandatory instead of all the fields for a type. Listed fields are automatically
cascaded as a required argument to nested selection sets. A parameterized
cascade works on levels (e.g. on the root function or on lower levels), so you
need to specify `@cascade(param)` on the exact level where you want it to be
applied.

<Tip>
  The rule with `@cascade(predicate)` is that the predicate needs to be in the
  query at the same level `@cascade` is.
</Tip>

Take the following query as an example:

```json
{
  nodes(func: allofterms(name@en, "jones indiana")) {
    name@en
    genre @filter(anyofterms(name@en, "action adventure")) {
      name@en
    }
    produced_by {
      name@en
    }
  }
}
```

This query gets nodes that have all the terms *"jones indiana"* and then
traverses to `genre` and `produced_by`. It also adds an additional filter for
`genre`, to only get the ones that either have *"action"* or *"adventure"* in
the name. The results include nodes that have no `genre` and nodes that have no
`genre` and no `producer`.

If you apply a regular `@cascade` without a parameter, you'll lose the ones that
had `genre` but no `producer`.

To get the nodes that have the traversed `genre` but possibly not `produced_by`,
you can parameterize the cascade:

```json
{
  nodes(func: allofterms(name@en, "jones indiana")) @cascade(genre) {
    name@en
    genre @filter(anyofterms(name@en, "action adventure")) {
      name@en
    }
    produced_by {
      name@en
    }
    written_by {
      name@en
    }
  }
}
```

If you want to check for multiple fields, just comma separate them. For example,
to cascade over `produced_by` and `written_by`:

```json
{
  nodes(func: allofterms(name@en, "jones indiana")) @cascade(produced_by,written_by) {
    name@en
    genre @filter(anyofterms(name@en, "action adventure")) {
      name@en
    }
    produced_by {
      name@en
    }
    written_by {
      name@en
    }
  }
}
```

### Nesting and parameterized cascade

The cascading nature of field selection is overwritten by a nested `@cascade`.

The previous example can be cascaded down the chain as well, and be overridden
on each level as needed.

For example, if you only want the *"Indiana Jones movies that were produced by
the same person who produced a Jurassic World movie"*:

```json
{
  nodes(func: allofterms(name@en, "jones indiana")) @cascade(produced_by) {
    name@en
    genre @filter(anyofterms(name@en, "action adventure")) {
      name@en
    }
    produced_by @cascade(producer.film) {
      name@en
      producer.film @filter(allofterms(name@en, "jurassic world")) {
        name@en
      }
    }
    written_by {
      name@en
    }
  }
}
```

Another nested example: *"Find the Indiana Jones movie that was written by the
same person who wrote a Star Wars movie and was produced by the same person who
produced Jurassic World"*:

```json
{
  nodes(func: allofterms(name@en, "jones indiana")) @cascade(produced_by,written_by) {
    name@en
    genre @filter(anyofterms(name@en, "action adventure")) {
      name@en
    }
    produced_by @cascade(producer.film) {
      name@en
      producer.film @filter(allofterms(name@en, "jurassic world")) {
        name@en
      }
    }
    written_by @cascade(writer.film) {
      name@en
      writer.film @filter(allofterms(name@en, "star wars")) {
        name@en
      }
    }
  }
}
```

## Cascade Performance

The `@cascade` directive processes the nodes after the query, but before Dgraph
returns query results. This means that all of the nodes that would normally be
returned if there was no `@cascade` applied are still touched in the internal
query process. If you see slower-than-expected performance when using the
`@cascade` directive, it's probably because the internal query process returns a
large set of nodes but the cascade reduces those to a small set of nodes in
query results. To improve the performance of queries that use the `@cascade`
directive, you might want to use `var` blocks or `has` filters, as described
below.

### Cascade with `var` blocks

The performance impact of using `var` blocks is that it reduces the graph that
is touched to generate the final query results. For example, many of the
previous examples could be replaced entirely using
[`var` blocks](./query#variable-var-blocks) instead of utilizing `@cascade`.

The following query provides an alternative way to structure the query shown
above, *"Find the Indiana Jones movie that was written by the same person who
wrote a Star Wars movie and was produced by the same person who produced
Jurassic World"*, without using the `@cascade` directive:

```json
{
  var(func: allofterms(name@en, "jurassic world")) {
    produced_by { ProducedBy as producer.film }
  }
  var(func: allofterms(name@en, "star wars")) {
    written_by { WrittenBy as writer.film }
  }
  nodes(func: allofterms(name@en,"indiana jones")) @filter(uid(ProducedBy) AND uid(WrittenBy)) {
    name@en
    genre {
      name@en
    }
  }
}
```

The performance impact of building queries with multiple `var` blocks versus
using `@cascade` depends on the nodes touched to reach the end results.
Depending on the size of your data set and distribution between nodes,
refactoring a query with `var` blocks instead of `@cascade` might actually
decrease performance if the query must touch more nodes as a result of the
refactor.

### Cascade with `has` filter

In cases where only a small set of nodes have the predicates where `@cascade` is
applied, it might be beneficial to query performance to include a `has` filter
for those predicates.

For example, you could run a query like *"Find movies that have a sequel whose
name contains the term **Star Wars**"* as follows:

```json
{
  nodes(func: has(sequel)) @filter(type(Film)) @cascade {
    count(uid)
    name@en
    sequel @filter(allofterms(name@en,"Star Wars")) {
      name@en
    }
  }
}
```

By using a `has` filter in the root function instead of `type(Movie)`, you can
reduce the root graph from `275,195` nodes down to `7,747` nodes. Reducing the
root graph before the post-query cascade process results in a higher-performing
query.


# count
Source: https://docs.hypermode.com/dgraph/dql/count



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Syntax Examples:

* `count(predicate)`
* `count(uid)`

The form `count(predicate)` counts how many `predicate` edges lead out of a
node.

The form `count(uid)` counts the number of UIDs matched in the enclosing block.

Query Example: the number of films acted in by each actor with `Orlando` in
their name.

`````json
{
  me(func: allofterms(name@en, "Orlando"))
  @filter(has(actor.film)) {
    name@en
    count(actor.film)
  }
}

Count can be used at root and [aliased](./alias).

Query Example: count of directors who have directed more than five films. When
used at the query root, the [count index](./schema#count-index) is
required.

````json
{
  directors(func: gt(count(director.film), 5)) {
    totalDirectors : count(uid)
  }
}

Count can be assigned to a
[value variable](./variables#value-variables).

Query Example: the actors of Ang Lee's "Eat Drink Man Woman" ordered by the
number of movies acted in.

```json
{
  var(func: allofterms(name@en, "eat drink man woman")) {
    starring {
      actors as performance.actor {
        totalRoles as count(actor.film)
      }
    }
  }

  edmw(func: uid(actors), orderdesc: val(totalRoles)) {
    name@en
    name@zh
    totalRoles : val(totalRoles)
  }
}
```
`````


# debug
Source: https://docs.hypermode.com/dgraph/dql/debug



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

For the purposes of debugging, you can attach a query parameter `debug=true` to
a query. Attaching this parameter lets you retrieve the `uid` attribute for all
the entities along with the `server_latency` and `start_ts` information under
the `extensions` key of the response.

* `parsing_ns`: Latency in nanoseconds to parse the query.
* `processing_ns`: Latency in nanoseconds to process the query.
* `encoding_ns`: Latency in nanoseconds to encode the JSON response.
* `start_ts`: The logical start timestamp of the transaction.

Query with debug as a query parameter

```sh
curl -H "Content-Type: application/dql" http://localhost:8080/query?debug=true -XPOST -d $'{
  tbl(func: allofterms(name@en, "The Big Lebowski")) {
    name@en
  }
}' | python -m json.tool | less
```

Returns `uid` and `server_latency`

```json
{
  "data": {
    "tbl": [
      {
        "uid": "0x41434",
        "name@en": "The Big Lebowski"
      },
      {
        "uid": "0x145834",
        "name@en": "The Big Lebowski 2"
      },
      {
        "uid": "0x2c8a40",
        "name@en": "Jeffrey \"The Big\" Lebowski"
      },
      {
        "uid": "0x3454c4",
        "name@en": "The Big Lebowski"
      }
    ],
    "extensions": {
      "server_latency": {
        "parsing_ns": 18559,
        "processing_ns": 802990982,
        "encoding_ns": 1177565
      },
      "txn": {
        "start_ts": 40010
      }
    }
  }
}
```

<Note>
  GraphQL+- has been renamed to Dgraph Query Language (DQL). While
  `application/dql` is the preferred value for the `Content-Type` header, we
  will continue to support `Content-Type: application/graphql+-` to avoid making
  breaking changes.
</Note>


# expand
Source: https://docs.hypermode.com/dgraph/dql/expand



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The `expand()` function can be used to expand the predicates out of a node. To
use `expand()`, the [type system](./schema) is required. Refer to the section on
the type system to check how to set the types nodes. The rest of this section
assumes familiarity with that section.

There are two ways to use the `expand` function.

* Types can be passed to `expand()` to expand all the predicates in the type.

Query example: List the movies from the Harry Potter series:

```json
{
  all(func: eq(name@en, "Harry Potter")) @filter(type(Series)) {
    name@en expand(Series) { name@en expand(Film) }
  }
}
```

* If `_all_` is passed as an argument to `expand()`, the predicates to be
  expanded will be the union of fields in the types assigned to a given node.

The `_all_` keyword requires that the nodes have types. Dgraph will look for all
the types that have been assigned to a node, query the types to check which
attributes they have, and use those to compute the list of predicates to expand.

For example, consider a node that has types `Animal` and `Pet`, which have the
following definitions:

```
type Animal { name species dob }

type Pet { owner veterinarian }
```

When `expand(_all_)` is called on this node, Dgraph will first check which types
the node has (`Animal` and `Pet`). Then it will get the definitions of `Animal`
and `Pet` and build a list of predicates from their type definitions.

```
name species dob owner veterinarian
```

<Note>
  {" "}

  For `string` predicates, `expand` only returns values not tagged with a
  language (see [language preference](./language-support)). So it's often
  required to add `name@fr` or `name@.` as well to an expand query.
</Note>

## Filtering during expand

Expand queries support filters on the type of the outgoing edge. For example,
`expand(_all_) @filter(type(Person))` will expand on all the predicates but will
only include edges whose destination node is of type Person. Since only nodes of
type `uid` can have a type, this query will filter out any scalar values.

Please note that other type of filters and directives are not currently
supported with the expand function. The filter needs to use the `type` function
for the filter to be allowed. Logical `AND` and `OR` operations are allowed. For
example, `expand(_all_) @filter(type(Person) OR type(Animal))` will only expand
the edges that point to nodes of either type.

```
```


# Facets and Edge Attributes
Source: https://docs.hypermode.com/dgraph/dql/facets



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph supports facets --- **key value pairs on edges** --- as an extension to
RDF triples. That is, facets add properties to edges, rather than to nodes. For
example, a `friend` edge between two nodes may have a Boolean property of
`close` friendship. Facets can also be used as `weights` for edges.

Though you may find yourself leaning towards facets many times, they should not
be misused. It wouldn't be correct modeling to give the `friend` edge a facet
`date_of_birth`. That should be an edge for the friend. However, a facet like
`start_of_friendship` might be appropriate. Facets are however not first class
citizen in Dgraph like predicates.

Facet keys are strings and values can be `string`, `bool`, `int`, `float` and
`dateTime`. For `int` and `float`, only 32-bit signed integers and 64-bit floats
are accepted.

The following mutation is used throughout this section on facets. The mutation
adds data for some peoples and, for example, records a `since` facet in `mobile`
and `car` to record when Alice bought the car and started using the mobile
number.

First we add some schema.

```sh
curl localhost:8080/alter -XPOST -d $'
    name: string @index(exact, term) .
    rated: [uid] @reverse @count .
' | python -m json.tool | less
```

```sh
curl -H "Content-Type: application/rdf" localhost:8080/mutate?commitNow=true -XPOST -d $'
{
  set {

    # -- Facets on scalar predicates
    _:alice <name> "Alice" .
    _:alice <dgraph.type> "Person" .
    _:alice <mobile> "040123456" (since=2006-01-02T15:04:05) .
    _:alice <car> "MA0123" (since=2006-02-02T13:01:09, first=true) .

    _:bob <name> "Bob" .
    _:bob <dgraph.type> "Person" .
    _:bob <car> "MA0134" (since=2006-02-02T13:01:09) .

    _:charlie <name> "Charlie" .
    _:charlie <dgraph.type> "Person" .
    _:dave <name> "Dave" .
    _:dave <dgraph.type> "Person" .


    # -- Facets on UID predicates
    _:alice <friend> _:bob (close=true, relative=false) .
    _:alice <friend> _:charlie (close=false, relative=true) .
    _:alice <friend> _:dave (close=true, relative=true) .


    # -- Facets for variable propagation
    _:movie1 <name> "Movie 1" .
    _:movie1 <dgraph.type> "Movie" .
    _:movie2 <name> "Movie 2" .
    _:movie2 <dgraph.type> "Movie" .
    _:movie3 <name> "Movie 3" .
    _:movie3 <dgraph.type> "Movie" .

    _:alice <rated> _:movie1 (rating=3) .
    _:alice <rated> _:movie2 (rating=2) .
    _:alice <rated> _:movie3 (rating=5) .

    _:bob <rated> _:movie1 (rating=5) .
    _:bob <rated> _:movie2 (rating=5) .
    _:bob <rated> _:movie3 (rating=5) .

    _:charlie <rated> _:movie1 (rating=2) .
    _:charlie <rated> _:movie2 (rating=5) .
    _:charlie <rated> _:movie3 (rating=1) .
  }
}' | python -m json.tool | less
```

## Facets on scalar predicates

Querying `name`, `mobile` and `car` of Alice gives the same result as without
facets.

```json
{
  data(func: eq(name, "Alice")) {
    name mobile car
  }
}
```

The syntax `@facets(facet-name)` is used to query facet data. For Alice the
`since` facet for `mobile` and `car` are queried as follows.

```json
{
  data(func: eq(name, "Alice")) {
    name mobile @facets(since) car @facets(since)
  }
}
```

Facets are returned at the same level as the corresponding edge and have keys
like edge|facet.

All facets on an edge are queried with `@facets`.

```json
{
  data(func: eq(name, "Alice")) {
    name mobile @facets car @facets
  }
}
```

## Facets i18n

Facets keys and values can use language-specific characters directly when
mutating. But facet keys need to be enclosed in angle brackets `<>` when
querying. This is similar to predicates. See
[Predicates i18n](./schema#predicates-i18n) for more info.

<Note>
  Dgraph supports [Internationalized Resource
  Identifiers](https://en.wikipedia.org/wiki/Internationalized_Resource_Identifier)
  (IRIs) for facet keys when querying.
</Note>

Example:

```json
{
  set {
    _:person1 <name> "Daniel" (वंश="स्पेनी", ancestry="Español") .
_:person1 <dgraph.type> "Person" . _:person2 <name> "Raj" (वंश="हिंदी",
ancestry="हिंदी") . _:person2 <dgraph.type> "Person" . _:person3 <name> "Zhang
Wei" (वंश="चीनी", ancestry="中文") . _:person3 <dgraph.type> "Person" . } }
. _:person2 <dgraph.type> "Person" . _:person3 <name> "Zhang Wei" (वंश="चीनी",
ancestry="中文") . _:person3 <dgraph.type> "Person" .
  }
}
```

Query, notice the `<>`'s:

```
{ q(func: has(name)) { name @facets(<वंश>) } }
```

## Alias with facets

Alias can be specified while requesting specific predicates. Syntax is similar
to how would request alias for other predicates. `orderasc` and `orderdesc` are
not allowed as alias as they have special meaning. Apart from that anything else
can be set as alias.

Here we set `car_since`, `close_friend` alias for `since`, `close` facets
respectively.

```json
{
  data(func: eq(name, "Alice")) { name mobile car @facets(car_since: since)
    friend @facets(close_friend: close) { name }
  }
}
```

## Facets on UID predicates

Facets on UID edges work similarly to facets on value edges.

For example, `friend` is an edge with facet `close`. It was set to true for
friendship between Alice and Bob and false for friendship between Alice and
Charlie.

A query for friends of Alice.

```json
{
  data(func: eq(name, "Alice")) {
    name friend { name }
  }
}
```

A query for friends and the facet `close` with `@facets(close)`.

```json
{
  data(func: eq(name, "Alice")) {
    name friend @facets(close) { name }
  }
}
```

For uid edges like `friend`, facets go to the corresponding child under the key
edge|facet. In the above example you can see that the `close` facet on the edge
between Alice and Bob appears with the key `friend|close` along with Bob's
results.

```json
{
  data(func: eq(name, "Alice")) {
    name friend @facets { name car @facets }
  }
}
```

Bob has a `car` and it has a facet `since`, which, in the results, is part of
the same object as Bob under the key car|since. Also, the `close` relationship
between Bob and Alice is part of Bob's output object. Charlie does not have
`car` edge and thus only UID facets.

## Filtering on facets

Dgraph supports filtering edges based on facets. Filtering works similarly to
how it works on edges without facets and has the same available functions.

Find Alice's close friends

```json
{
  data(func: eq(name, "Alice")) { friend @facets(eq(close, true)) { name }
  }
}
```

To return facets as well as filter, add another `@facets(<facetname>)` to the
query.

```json
{
  data(func: eq(name, "Alice")) {
    friend @facets(eq(close, true)) @facets(relative) { # filter close
      friends and give relative status name
    }
  }
}
```

Facet queries can be composed with `AND`, `OR` and `NOT`.

```json
{
  data(func: eq(name, "Alice")) {
    friend @facets(eq(close, true) AND eq(relative, true)) @facets(relative) {
      # filter close friends in my relation name
    }
  }
}
```

## Sorting using facets

Sorting is possible for a facet on a uid edge. Here we sort the movies rated by
Alice, Bob and Charlie by their `rating` which is a facet.

```json
{
  me(func: anyofterms(name, "Alice Bob Charlie")) { name rated
@facets(orderdesc: rating) { name } }
}
```

## Assigning Facet values to a variable

Facets on UID edges can be stored in
[value variables](./variables#value-variables). The variable is a map from the
edge target to the facet value.

Alice's friends reported by variables for `close` and `relative`.

```json
{
  var(func: eq(name, "Alice")) { friend @facets(a as close, b as relative) }

  friend(func: uid(a)) { name val(a) }

  relative(func: uid(b)) { name val(b) }
}
```

## Facets and Variable Propagation

Facet values of `int` and `float` can be assigned to variables and thus the
[values propagate](./variables#variable-propagation).

Alice, Bob and Charlie each rated every movie. A value variable on facet
`rating` maps movies to ratings. A query that reaches a movie through multiple
paths sums the ratings on each path. The following sums Alice, Bob and Charlie's
ratings for the three movies.

```json
{
  var(func: anyofterms(name, "Alice Bob Charlie")) {
    num_raters as math(1) rated @facets(r as rating) {
      total_rating as math(r) # sum of the 3 ratings average_rating as math(total_rating / num_raters)
    }
  }
  data(func: uid(total_rating)) { name val(total_rating) val(average_rating) }
}
```

## Facets and Aggregation

Facet values assigned to value variables can be aggregated.

```json
{
  data(func: eq(name, "Alice")) { name rated @facets(r as rating) { name }
    avg(val(r))
  }
}
```

Note though that `r` is a map from movies to the sum of ratings on edges in the
query reaching the movie. Hence, the following does not correctly calculate the
average ratings for Alice and Bob individually --- it calculates 2 times the
average of both Alice and Bob's ratings.

```json
{
  data(func: anyofterms(name, "Alice Bob")) { name rated @facets(r as rating) {
    name
  }
  avg(val(r))
}
}
```

Calculating the average ratings of users requires a variable that maps users to
the sum of their ratings.

```json
{
  var(func: has(rated)) {
    num_rated as math(1) rated @facets(r as rating) {
      avg_rating as math(r / num_rated)
    }
  }

  data(func: uid(avg_rating)) { name val(avg_rating) }
}
```


# filter
Source: https://docs.hypermode.com/dgraph/dql/filter



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Within `@filter` multiple functions can be used with boolean connectives.

## AND, OR, and NOT

Connectives `AND`, `OR`, and `NOT` join filters and can be built into
arbitrarily complex filters, such as `(NOT A OR B) AND (C AND NOT (D OR E))`.
Note that, `NOT` binds more tightly than `AND` which binds more tightly than
`OR`.

Query Example: All Steven Spielberg movies that contain either both "indiana"
and "jones" OR both "jurassic" and "park".

```json
{
  me(func: eq(name@en, "Steven Spielberg")) @filter(has(director.film)) {
    name@en
    director.film @filter(allofterms(name@en, "jones indiana") OR allofterms(name@en, "jurassic park")) {
      uid
      name@en
    }
  }
}
```


# fragment
Source: https://docs.hypermode.com/dgraph/dql/fragment



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The `fragment` keyword lets you define new fragments that can be referenced in a
query, per the
[Fragments section of the GraphQL specification](http://spec.graphql.org/June2018/#sec-Language.Fragments).
Fragments allow for the reuse of common repeated selections of fields, reducing
duplicated text in the DQL documents. Fragments can be nested inside fragments,
but no cycles are allowed in such cases. For example:

```sh
curl -H "Content-Type: application/dql" localhost:8080/query -XPOST -d $'
query {
  debug(func: uid(1)) {
    name@en
    ...TestFrag
  }
}
fragment TestFrag {
  initial_release_date
  ...TestFragB
}
fragment TestFragB {
  country
}' | python -m json.tool | less
```

<Note>
  GraphQL+- has been renamed to Dgraph Query Language (DQL). While
  `application/dql` is the preferred value for the `Content-Type` header, we
  will continue to support `Content-Type: application/graphql+-` to avoid making
  breaking changes.
</Note>


# Functions
Source: https://docs.hypermode.com/dgraph/dql/functions



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Functions allow filtering based on properties of nodes or
[variables](./variables#value-variables). Functions can be applied in the query
root or in filters.

Comparison functions (`eq`, `ge`, `gt`, `le`, `lt`) in the query root (`func:`)
can only be applied on [indexed predicates](./schema#indexing). Comparison
functions can be used on [@filter](./filter) directives even on predicates that
haven't been indexed. Filtering on non-indexed predicates can be slow for large
datasets, as they require iterating over all of the possible values at the level
where the filter is being used.

All other functions, in the query root or in the filter can only be applied to
indexed predicates.

For functions on string valued predicates, if no language preference is given,
the function is applied to all languages and strings without a language tag. If
a language preference is given, the function is applied only to strings of the
given language.

## Term matching

### `allofterms`

Syntax Example: `allofterms(predicate, "space-separated term list")`

Schema Types: `string`

Index Required: `term`

Matches strings that have all specified terms in any order, case-insensitive.

#### Usage at root

Query Example: all nodes that have `name` containing terms `indiana` and
`jones`, returning the English name and genre in English.

```json
{
  me(func: allofterms(name@en, "jones indiana")) {
    name@en
    genre { name@en }
  }
}
```

#### Usage as filter

Query Example: all Steven Spielberg films that contain the words `indiana` and
`jones`. The `@filter(has(director.film))` removes nodes with name Steven
Spielberg that aren't the director --- the data also contains a character in a
film called Steven Spielberg.

```json
{
  me(func: eq(name@en, "Steven Spielberg"))
  @filter(has(director.film)) {
    name@en
    director.film
    @filter(allofterms(name@en, "jones indiana")) {
      name@en
    }
  }
}
```

### `anyofterms`

Syntax Example: `anyofterms(predicate, "space-separated term list")`

Schema Types: `string`

Index Required: `term`

Matches strings that have any of the specified terms in any order; case
insensitive.

#### Usage at root

Query Example: All nodes that have a `name` containing either `poison` or
`peacock`. Many of the returned nodes are movies, but people like Joan Peacock
also meet the search terms because without a [cascade directive](./cascade) the
query doesn't require a genre.

```json
{
  me(func: anyofterms(name@en, "poison peacock")) {
    name@en
    genre { name@en }
  }
}
```

#### Usage as filter

Query Example: All Steven Spielberg movies that contain `war` or `spies`. The
`@filter(has(director.film))` removes nodes with name Steven Spielberg that
aren't the director --- the data also contains a character in a film called
Steven Spielberg.

```json
{
  me(func: eq(name@en, "Steven Spielberg"))
  @filter(has(director.film)) {
    name@en
    director.film
    @filter(anyofterms(name@en, "war spies")) { name@en }
  }
}
```

## Regular expressions

Syntax Examples: `regexp(predicate, /regular-expression/)` or case insensitive
`regexp(predicate, /regular-expression/i)`

Schema Types: `string`

Index Required: `trigram`

Matches strings by regular expression. The regular expression language is that
of [go regular expressions](https://golang.org/pkg/regexp/syntax/).

Query Example: At root, match nodes with `Steven Sp` at the start of `name`,
followed by any characters. For each such matched UID, match the films
containing `ryan`. Note the difference with `allofterms`, which would match only
`ryan` but regular expression search also matches within terms, such as `bryan`.

```json
{
  directors(func: regexp(name@en, /^Steven Sp.\*$/)) {
    name@en
    director.film
    @filter(regexp(name@en, /ryan/i)) { name@en }
  }
}
```

### Technical details

A Trigram is a substring of three continuous runes. For example, `Dgraph` has
trigrams `Dgr`, `gra`, `rap`, `aph`.

To ensure efficiency of regular expression matching, Dgraph uses
[trigram indexing](https://swtch.com/~rsc/regexp/regexp4.html). Dgraph converts
the regular expression to a trigram query, uses the trigram index and trigram
query to find possible matches and applies the full regular expression search
only to the possibles.

### Writing efficient regular expressions and limitations

Keep the following in mind when designing regular expression queries.

* At least one trigram must be matched by the regular expression (patterns
  shorter than 3 runes aren't supported) since Dgraph requires regular
  expressions that can be converted to a trigram query.
* The number of alternative trigrams matched by the regular expression should be
  as small as possible (`[a-zA-Z][a-zA-Z][0-9]` isn't a good idea). Many
  possible matches means the full regular expression is checked against many
  strings; where as, if the expression enforces more trigrams to match, Dgraph
  can make better use of the index and check the full regular expression against
  a smaller set of possible matches.
* Thus, the regular expression should be as precise as possible. Matching longer
  strings means more required trigrams, which helps to effectively use the
  index.
* If repeat specifications (`*`, `+`, `?`, `{n,m}`) are used, the entire regular
  expression must not match the *empty* string or *any* string: for example, `*`
  may be used like `[Aa]bcd*` but not like `(abcd)*` or `(abcd)|((defg)*)`
* Repeat specifications after bracket expressions (e.g. `[fgh]{7}`, `[0-9]+` or
  `[a-z]{3,5}`) are often considered as matching any string because they match
  too many trigrams.
* If the partial result (for subset of trigrams) exceeds 1000000 UIDs during
  index scan, the query is stopped to prohibit expensive queries.

## Fuzzy matching

Syntax: `match(predicate, string, distance)`

Schema Types: `string`

Index Required: `trigram`

Matches predicate values by calculating the
[Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) to
the string, also known as *fuzzy matching*. The distance parameter must be
greater than zero (0). Using a greater distance value can yield more but less
accurate results.

Query Example: At root, fuzzy match nodes similar to `Stephen`, with a distance
value of less than or equal to 8.

```json
{
  directors(func: match(name@en, Stephen, 8)) {
    name@en
  }
}
```

Same query with a Levenshtein distance of 3.

```json
{
  directors(func: match(name@en, Stephen, 3)) {
    name@en
  }
}
```

## Vector Similarity Search

Syntax Examples: `similar_to(predicate, 3, "[0.9, 0.8, 0, 0]")`

Alternatively the vector can be passed as a variable:
`similar_to(predicate, 3, $vec)`

This function finds the nodes that have `predicate` close to the provided
vector. The search is based on the distance metric specified in the index
(`cosine`, `euclidean`, or `dotproduct`). The shorter distance indicates more
similarity. The second parameter, `3` specifies that top 3 matches be returned.

Schema Types: `float32vector`

Index Required: `hnsw`

## Full-Text Search

Syntax Examples: `alloftext(predicate, "space-separated text")` and
`anyoftext(predicate, "space-separated text")`

Schema Types: `string`

Index Required: `fulltext`

Apply full-text search with stemming and stop words to find strings matching all
or any of the given text.

The following steps are applied during index generation and to process full-text
search arguments:

1. Tokenization (according to Unicode word boundaries).
2. Conversion to lowercase.
3. Unicode-normalization (to
   [Normalization Form KC](http://unicode.org/reports/tr15/#Norm_Forms)).
4. Stemming using language-specific stemmer (if supported by language).
5. Stop words removal (if supported by language).

Dgraph uses [bleve](https://github.com/blevesearch/bleve) for its full-text
search indexing. See also the bleve language specific
[stop word lists](https://github.com/blevesearch/bleve/tree/master/analysis/lang).

Following table contains all supported languages, corresponding country-codes,
stemming and stop words filtering support.

|  Language  | Country Code | Stemming | Stop words |
| :--------: | :----------: | :------: | :--------: |
|   Arabic   |      ar      |     ✓    |      ✓     |
|  Armenian  |      hy      |          |      ✓     |
|   Basque   |      eu      |          |      ✓     |
|  Bulgarian |      bg      |          |      ✓     |
|   Catalan  |      ca      |          |      ✓     |
|   Chinese  |      zh      |     ✓    |      ✓     |
|    Czech   |      cs      |          |      ✓     |
|   Danish   |      da      |     ✓    |      ✓     |
|    Dutch   |      nl      |     ✓    |      ✓     |
|   English  |      en      |     ✓    |      ✓     |
|   Finnish  |      fi      |     ✓    |      ✓     |
|   French   |      fr      |     ✓    |      ✓     |
|   Gaelic   |      ga      |          |      ✓     |
|  Galician  |      gl      |          |      ✓     |
|   German   |      de      |     ✓    |      ✓     |
|    Greek   |      el      |          |      ✓     |
|    Hindi   |      hi      |     ✓    |      ✓     |
|  Hungarian |      hu      |     ✓    |      ✓     |
| Indonesian |      id      |          |      ✓     |
|   Italian  |      it      |     ✓    |      ✓     |
|  Japanese  |      ja      |     ✓    |      ✓     |
|   Korean   |      ko      |     ✓    |      ✓     |
|  Norwegian |      no      |     ✓    |      ✓     |
|   Persian  |      fa      |          |      ✓     |
| Portuguese |      pt      |     ✓    |      ✓     |
|  Romanian  |      ro      |     ✓    |      ✓     |
|   Russian  |      ru      |     ✓    |      ✓     |
|   Spanish  |      es      |     ✓    |      ✓     |
|   Swedish  |      sv      |     ✓    |      ✓     |
|   Turkish  |      tr      |     ✓    |      ✓     |

Query Example: All names that have `dog`, `dogs`, `bark`, `barks`, `barking`,
etc. Stop word removal eliminates `the` and `which`.

```json
{
  movie(func: alloftext(name@en, "the dog which barks")) {
    name@en
  }
}
```

## Inequality

### equal to

Syntax Examples:

* `eq(predicate, value)`
* `eq(val(varName), value)`
* `eq(predicate, val(varName))`
* `eq(count(predicate), value)`
* `eq(predicate, [val1, val2, ..., valN])`
* `eq(predicate, [$var1, "value", ..., $varN])`

Schema Types: `int`, `float`, `bool`, `string`, `dateTime`

Index Required: An index is required for the `eq(predicate, ...)` forms (see
table below) when used at query root. For `count(predicate)` at the query root,
the `@count` index is required. For variables the values have been calculated as
part of the query, so no index is required.

| Type       | Index Options                       |
| :--------- | :---------------------------------- |
| `int`      | `int`                               |
| `float`    | `float`                             |
| `bool`     | `bool`                              |
| `string`   | `exact`, `hash`, `term`, `fulltext` |
| `dateTime` | `dateTime`                          |

Test for equality of a predicate or variable to a value or find in a list of
values.

The boolean constants are `true` and `false`, so with `eq` this becomes, for
example, `eq(boolPred, true)`.

Query Example: Movies with exactly thirteen genres.

```json
{
  me(func: eq(count(genre), 13)) {
    name@en genre { name@en }
  }
}
```

Query Example: Directors called Steven who have directed 1,2 or 3 movies.

```json
{
  steve as var(func: allofterms(name@en, "Steven")) {
    films as count(director.film)
  }

  stevens(func: uid(steve)) @filter(eq(val(films), [1,2,3])) { name@en numFilms :
  val(films)
  }
}
```

### less than, less than or equal to, greater than and greater than or equal to

Syntax Examples: for inequality `IE`

* `IE(predicate, value)`
* `IE(val(varName), value)`
* `IE(predicate, val(varName))`
* `IE(count(predicate), value)`

With `IE` replaced by

* `le` less than or equal to
* `lt` less than
* `ge` greater than or equal to
* `gt` greater than

Schema Types: `int`, `float`, `string`, `dateTime`

Index required: An index is required for the `IE(predicate, ...)` forms (see
table below) when used at query root. For `count(predicate)` at the query root,
the `@count` index is required. For variables the values have been calculated as
part of the query, so no index is required.

| Type       | Index Options |
| :--------- | :------------ |
| `int`      | `int`         |
| `float`    | `float`       |
| `string`   | `exact`       |
| `dateTime` | `dateTime`    |

Query Example: Ridley Scott movies released before 1980.

```json
{
  me(func: eq(name@en, "Ridley Scott")) {
    name@en director.film
    @filter(lt(initial_release_date, "1980-01-01")) { initial_release_date name@en
    }
  }
}
```

Query Example: Movies with directors with `Steven` in `name` and have directed
more than `100` actors.

```json
{
  ID as var(func: allofterms(name@en, "Steven")) {
    director.film { num_actors as count(starring) }
    total as sum(val(num_actors))
  }

  dirs(func: uid(ID)) @filter(gt(val(total), 100)) { name@en total_actors :
  val(total)
  }
}
```

Query Example: A movie in each genre that has over 30000 movies. Because there
is no order specified on genres, the order will be by UID. The
[count index](./count) records the number of edges out of nodes and makes such
queries more.

```json
{
  genre(func: gt(count(~genre), 30000)) {
    name@en ~genre (first:1) { name@en }
  }
}
```

Query Example: Directors called Steven and their movies which have
`initial_release_date` greater than that of the movie Minority Report.

```json
{
  var(func: eq(name@en,"Minority Report")) {
    d as initial_release_date
  }

  me(func: eq(name@en, "Steven Spielberg")) {
    name@en director.film
    @filter(ge(initial_release_date, val(d))) { initial_release_date name@en
  }
}
```

## between

Syntax Example: `between(predicate, startDateValue, endDateValue)`

Schema Types: Scalar types, including `dateTime`, `int`, `float` and `string`

Index Required: `dateTime`, `int`, `float`, and `exact` on strings

Returns nodes that match an inclusive range of indexed values. The `between`
keyword performs a range check on the index to improve query efficiency, helping
to prevent a wide-ranging query on a large set of data from running slowly.

A common use case for the `between` keyword is to search within a dataset
indexed by `dateTime`. The following example query demonstrates this use case.

Query Example: Movies initially released in 1977, listed by genre.

```json
{
  me(func: between(initial_release_date, "1977-01-01", "1977-12-31")) {
    name@en genre { name@en }
  }
}
```

## uid

Syntax Examples:

* `q(func: uid(<uid>))`
* `predicate @filter(uid(<uid1>, ..., <uidn>))`
* `predicate @filter(uid(a))` for variable `a`
* `q(func: uid(a,b))` for variables `a` and `b`
* `q(func: uid($uids))` for multiple uids in DQL Variables. You have to set the
  value of this variable as a string (e.g`"[0x1, 0x2, 0x3]"`) in queryWithVars.

Filters nodes at the current query level to only nodes in the given set of UIDs.

For query variable `a`, `uid(a)` represents the set of UIDs stored in `a`. For
value variable `b`, `uid(b)` represents the UIDs from the UID to value map. With
two or more variables, `uid(a,b,...)` represents the union of all the variables.

`uid(<uid>)`, like an identity function, will return the requested UID even if
the node does not have any edges.

<Tip>
  If the UID of a node is known, values for the node can be read directly.
</Tip>

Query Example: The films of Priyanka Chopra by known UID.

```json
{
  films(func: uid(0x2c964)) {
    name@hi actor.film { performance.film { name@hi } }
  }
}
```

Query Example: The films of Taraji Henson by genre.

```json
{
  var(func: allofterms(name@en, "Taraji Henson")) {
    actor.film { F as performance.film { G as genre } }
  }

  Taraji_films_by_genre(func: uid(G)) { genre_name : name@en films : ~genre
  @filter(uid(F)) { film_name : name@en } }
}
```

Query Example: Taraji Henson films ordered by number of genres, with genres
listed in order of how many films Taraji has made in each genre.

```json
{
  var(func: allofterms(name@en, "Taraji Henson")) {
    actor.film {
      F as performance.film { G as count(genre) genre { C as count(~genre
      @filter(uid(F))) } } }
  }
}

Taraji_films_by_genre_count(func: uid(G), orderdesc: val(G)) { film_name :
name@en genres : genre (orderdesc: val(C)) { genre_name : name@en } } }
```

## uid\_in

Syntax Examples:

* `q(func: ...) @filter(uid_in(predicate, <uid>))`
* `predicate1 @filter(uid_in(predicate2, <uid>))`
* `predicate1 @filter(uid_in(predicate2, [<uid1>, ..., <uidn>]))`
* `predicate1 @filter(uid_in(predicate2, uid(myVariable) ))`

Schema Types: UID

Index Required: none

While the `uid` function filters nodes at the current level based on UID,
function `uid_in` allows looking ahead along an edge to check that it leads to a
particular UID. This can often save an extra query block and avoids returning
the edge.

`uid_in` cannot be used at root. It accepts multiple UIDs as its argument, and
it accepts a UID variable (which can contain a map of UIDs).

Query Example: The collaborations of Marc Caro and Jean-Pierre Jeunet (UID
0x99706). If the UID of Jean-Pierre Jeunet is known, querying this way removes
the need to have a block extracting his UID into a variable and the extra edge
traversal and filter for `~director.film`.

```json
{
  caro(func: eq(name@en, "Marc Caro")) {
    name@en director.film
    @filter(uid_in(~director.film, 0x99706)) { name@en }
  }
}
```

You can also query for Jean-Pierre Jeunet if you don't know his UID and use it
in a UID variable.

```json
{
  getJeunet as q(func: eq(name@fr, "Jean-Pierre Jeunet"))

  caro(func: eq(name@en, "Marc Caro")) {
    name@en director.film
    @filter(uid_in(~director.film, uid(getJeunet))) { name@en }
  }
}
```

## type

Query Example: all nodes of type "Animal"

```json
{
  q(func: type(Animal)) { uid name }
}
```

`type(Animal)` equivalent to `eq(dgraph.type,"Animal")`

type() can also be used as a filter:

```json
{
  q(func: has(parent)) { uid parent @filter(type(Person)) { uid name } }
}
```

## has

Syntax Examples: `has(predicate)`

Schema Types: all

Determines if a node has a particular predicate.

Query Example: First five directors and all their movies that have a release
date recorded. Directors have directed at least one film --- equivalent
semantics to `gt(count(director.film), 0)`.

```json
{
  me(func: has(director.film), first: 5) {
    name@en director.film
    @filter(has(initial_release_date)) { initial_release_date name@en }
  }
}
```

## Geolocation

<Note>
  As of now we only support indexing Point, Polygon and MultiPolygon [geometry
  types](https://github.com/twpayne/go-geom#geometry-types). However, Dgraph can
  store other types of gelocation data.
</Note>

Note that for geo queries, any polygon with holes is replace with the outer
loop, ignoring holes. Also, as for version 0.7.7 polygon containment checks are
approximate.

### Mutations

To make use of the geo functions you would need an index on your predicate.

```
loc: geo @index(geo) .
```

Here is how you would add a `Point`.

```json
{
  set {
    <_:0xeb1dde9c> <loc>
      "{\"type\":\"Point\",\"coordinates\":[-122.4220186,37.772318]}"^^<geo:geojson>
      . <_:0xeb1dde9c> <name> "Hamon Tower" . <\_:0xeb1dde9c> <dgraph.type> "Location"
      .
  }
}
```

Here is how you would associate a `Polygon` with a node. Adding a `MultiPolygon`
is also similar.

```json
{
  set {
    <_:0xf76c276b> <loc>
      "{\"type\":\"Polygon\",\"coordinates\":[[[-122.409869,37.7785442],[-122.4097444,37.7786443],[-122.4097544,37.7786521],[-122.4096334,37.7787494],[-122.4096233,37.7787416],[-122.4094004,37.7789207],[-122.4095818,37.7790617],[-122.4097883,37.7792189],[-122.4102599,37.7788413],[-122.409869,37.7785442]],[[-122.4097357,37.7787848],[-122.4098499,37.778693],[-122.4099025,37.7787339],[-122.4097882,37.7788257],[-122.4097357,37.7787848]]]}"^^<geo:geojson>
      . <_:0xf76c276b> <name> "Best Western Americana Hotel" . <\_:0xf76c276b>
      <dgraph.type> "Location" .
  }
}
```

The above examples have been picked from our
[SF Tourism](https://github.com/hypermodeinc/dgraph-benchmarks/blob/main/data/sf.tourism.gz?raw=true)
dataset.

### Query

#### near

Syntax Example: `near(predicate, [long, lat], distance)`

Schema Types: `geo`

Index Required: `geo`

Matches all entities where the location given by `predicate` is within
`distance` meters of geojson coordinate `[long, lat]`.

Query Example: Tourist destinations within 1000 meters (1 kilometer) of a point
in Golden Gate Park in San Francisco.

```json
{
  tourist(func: near(loc, [-122.469829, 37.771935], 1000)) { name }
}
```

#### within

Syntax Example: `within(predicate, [[[long1, lat1], ..., [longN, latN]]])`

Schema Types: `geo`

Index Required: `geo`

Matches all entities where the location given by `predicate` lies within the
polygon specified by the geojson coordinate array.

Query Example: Tourist destinations within the specified area of Golden Gate
Park, San Francisco.

```json
{
  tourist(func: within(loc, [[[-122.47266769409178, 37.769018558337926 ], [ -122.47266769409178, 37.773699921075135 ], [ -122.4651575088501, 37.773699921075135 ], [ -122.4651575088501, 37.769018558337926 ], [ -122.47266769409178, 37.769018558337926]]])) {
    name
  }
}
```

#### contains

Syntax Examples: `contains(predicate, [long, lat])` or
`contains(predicate, [[long1, lat1], ..., [longN, latN]])`

Schema Types: `geo`

Index Required: `geo`

Matches all entities where the polygon describing the location given by
`predicate` contains geojson coordinate `[long, lat]` or given geojson polygon.

Query Example : All entities that contain a point in the flamingo enclosure of
San Francisco Zoo.

```json
{
  tourist(func: contains(loc, [-122.50326097011566, 37.73353615592843])) {
    name
  }
}
```

#### intersects

Syntax Example: `intersects(predicate, [[[long1, lat1], ..., [longN, latN]]])`

Schema Types: `geo`

Index Required: `geo`

Matches all entities where the polygon describing the location given by
`predicate` intersects the given geojson polygon.

```json
{
  tourist(func: intersects(loc, [[[-122.503325343132, 37.73345766902749 ], [ -122.503325343132, 37.733903134117966 ], [ -122.50271648168564, 37.733903134117966 ], [ -122.50271648168564, 37.73345766902749 ], [ -122.503325343132, 37.73345766902749]]])) {
    name
  }
}
```


# groupby
Source: https://docs.hypermode.com/dgraph/dql/groupby



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Syntax Examples:

* `q(func: ...) @groupby(predicate) { min(...) }`
* `predicate @groupby(pred) { count(uid) }`

A `groupby` query aggregates query results given a set of properties on which to
group elements. For example, a query containing the block
`friend @groupby(age) { count(uid) }`, finds all nodes reachable along the
friend edge, partitions these into groups based on age, then counts how many
nodes are in each group. The returned result is the grouped edges and the
aggregations.

Inside a `groupby` block, only aggregations are allowed and `count` may only be
applied to `uid`.

If the `groupby` is applied to a `uid` predicate, the resulting aggregations can
be saved in a variable (mapping the grouped UIDs to aggregate values) and used
elsewhere in the query to extract information other than the grouped or
aggregated edges.

Query Example: For Steven Spielberg movies, count the number of movies in each
genre and for each of those genres return the genre name and the count. The name
can't be extracted in the `groupby` because it's not an aggregate, but `uid(a)`
can be used to extract the UIDs from the UID to value map and thus organize the
`byGenre` query by genre UID.

```json
{
  var(func: allofterms(name@en, "steven spielberg")) {
    director.film @groupby(genre) {
      a as count(uid)
    }
  }

  byGenre(func: uid(a), orderdesc: val(a)) {
    name@en
    total_movies : val(a)
  }
}
```

Query Example: Actors from Tim Burton movies and how many roles they have played
in Tim Burton movies.

```json
{
  var(func: allofterms(name@en, "Tim Burton")) {
    director.film {
      starring @groupby(performance.actor) {
        a as count(uid) # a is an actor UID to count value variable
      }
    }
  }

  byActor(func: uid(a), orderdesc: val(a)) {
    name@en
    val(a)
  }
}
```


# ignorereflex
Source: https://docs.hypermode.com/dgraph/dql/ignorereflex



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The `@ignorereflex` directive forces the removal of child nodes that are
reachable from themselves as a parent, through any path in the query result

Query Example: all the co-actors of Rutger Hauer. Without `@ignorereflex`, the
result would also include Rutger Hauer for every movie.

```json
{
  coactors(func: eq(name@en, "Rutger Hauer")) @ignorereflex {
    actor.film {
      performance.film {
        starring {
          performance.actor {
            name@en
          }
        }
      }
    }
  }
}
```


# Indexes
Source: https://docs.hypermode.com/dgraph/dql/indexes



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Filtering on a predicate by applying a [function](./functions) requires an
index.

Indices are defined in the [Dgraph types schema](./schema) using `@index`
directive.

Here are some examples:

```dql
name: string @index(term) .
release_date: datetime @index(year) .
description_vector: float32vector @index(hnsw(metric:"cosine")) .
```

When filtering by applying a function, Dgraph uses the index to make the search
through a potentially large dataset efficient.

All scalar types can be indexed.

Types `int`, `float`, `bool` and `geo` have only a default index each: with
tokenizers named `int`, `float`, `bool` and `geo`.

Types `string` and `dateTime` have a number of indices.

Type `float32vector` supports `hnsw` index.

## String Indices

The indices available for strings are as follows.

| Dgraph function            | Required index / tokenizer             | Notes                                                                                                                                                                                                        |
| :------------------------- | :------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `eq`                       | `hash`, `exact`, `term`, or `fulltext` | The most performant index for `eq` is `hash`. Only use `term` or `fulltext` if you also require term or full-text search. If you're already using `term`, there is no need to use `hash` or `exact` as well. |
| `le`, `ge`, `lt`, `gt`     | `exact`                                | Allows faster sorting.                                                                                                                                                                                       |
| `allofterms`, `anyofterms` | `term`                                 | Allows searching by a term in a sentence.                                                                                                                                                                    |
| `alloftext`, `anyoftext`   | `fulltext`                             | Matching with language specific stemming and stopwords.                                                                                                                                                      |
| `regexp`                   | `trigram`                              | Regular expression matching. Can also be used for equality checking.                                                                                                                                         |

<Warning>
  Incorrect index choice can impose performance penalties and an increased
  transaction conflict rate. Use only the minimum number of and simplest indexes
  that your app needs.
</Warning>

## Vector Indices

The indices available for `float32vector` are as follows.

| Dgraph function | Required index / tokenizer | Notes                                                   |
| :-------------- | :------------------------- | :------------------------------------------------------ |
| `similar_to`    | `hnsw`                     | HNSW index supports parameters `metric` and `exponent`. |

`hnsw` (**Hierarchical Navigable Small World**) index supports the following
parameters

* metric : indicate the metric to use to compute vector similarity. One of
  `cosine`, `euclidean`, and `dotproduct`. Default is `euclidean`.

* exponent : An integer, represented as a string, roughly representing the
  number of vectors expected in the index in power of 10. The exponent value,is
  used to set "reasonable defaults" for HNSW internal tuning parameters. Default
  is "4" (10^4 vectors).

Here are some examples:

```
simple_vector: float32vector @index(hnsw) .
description_vector: float32vector @index(hnsw(metric:"cosine")) .
large_vector: float32vector @index(hnsw(metric:"euclidean",exponent:"6")) .
```

## DateTime Indices

The indices available for `dateTime` are as follows.

| Index name / Tokenizer | Part of date indexed               |
| :--------------------- | :--------------------------------- |
| `year`                 | index on year (default)            |
| `month`                | index on year and month            |
| `day`                  | index on year, month and day       |
| `hour`                 | index on year, month, day and hour |

The choices of `dateTime` index allow selecting the precision of the index.
Apps, such as the movies examples in these docs, that require searching over
dates but have relatively few nodes per year may prefer the `year` tokenizer;
apps that are dependent on fine grained date searches, such as real-time sensor
readings, may prefer the `hour` index.

All the `dateTime` indices are sortable.

## Sortable Indices

Not all the indices establish a total order among the values that they index.
Sortable indices allow inequality functions and sorting.

* Indexes `int` and `float` are sortable.
* `string` index `exact` is sortable.
* All `dateTime` indices are sortable.

For example, given an edge `name` of `string` type, to sort by `name` or perform
inequality filtering on names, the `exact` index must have been specified. In
which case a schema query would return at least the following tokenizers.

```
{
  "predicate": "name",
  "type": "string",
  "index": true,
  "tokenizer": [
    "exact"
  ]
}
```

## Count index

For predicates with the `@count` Dgraph indexes the number of edges out of each
node. This enables fast queries of the form:

```
{
  q(func: gt(count(pred), threshold)) {
    ...
  }
}
```

## List Type

Predicate with scalar types can also store a list of values if specified in the
schema. The scalar type needs to be enclosed within `[]` to indicate that its a
list type.

```
occupations: [string] .
score: [int] .
```

* A set operation adds to the list of values. The order of the stored values is
  non-deterministic.
* A delete operation deletes the value from the list.
* Querying for these predicates would return the list in an array.
* Indexes can be applied on predicates which have a list type and you can use
  [Functions](./functions) on them.
* Sorting is not allowed using these predicates.
* These lists are like an unordered set. For example: `["e1", "e1", "e2"]` may
  get stored as `["e2", "e1"]`, i.e., duplicate values will not be stored and
  order may not be preserved.

## Filtering on list

Dgraph supports filtering based on the list. Filtering works similarly to how it
works on edges and has the same available functions.

For example, `@filter(eq(occupations, "Teacher"))` at the root of the query or
the parent edge will display all the occupations from a list of each node in an
array but will only include nodes which have `Teacher` as one of the
occupations. However, filtering on value edge is not supported.

## Reverse Edges

A graph edge is unidirectional. For node-node edges, sometimes modeling requires
reverse edges. If only some subject-predicate-object triples have a reverse,
these must be manually added. But if a predicate always has a reverse, Dgraph
computes the reverse edges if `@reverse` is specified in the schema.

The reverse edge of `anEdge` is `~anEdge`.

For existing data, Dgraph computes all reverse edges. For data added after the
schema mutation, Dgraph computes and stores the reverse edge for each added
triple.

```
type Person {
  name
}
type Car {
  regnbr
  owner
}
owner: uid @reverse .
regnbr: string @index(exact) .
name: string @index(exact) .
```

This makes it possible to query Persons and their cars by using:

```
q(func: type(Person)) {
  name
  ~owner { regnbr }
}
```

To get a different key than `~owner` in the result, the query can be written
with the wanted label (`cars` in this case):

```
q(func: type(Person)) {
  name
  cars: ~owner { regnbr }
}
```

This also works if there are multiple "owners" of a `car`:

```
owner [uid] @reverse .
```

In both cases the `owner` edge should be set on the `Car`:

```
_:p1 <name> "Mary" .
_:p1 <dgraph.type> "Person" .
_:c1 <regnbr> "ABC123" .
_:c1 <dgraph.type> "Car" .
_:c1 <owner> _:p1 .
```

## Querying Schema

A schema query queries for the whole schema:

```
schema {}
```

<Note>
  Unlike regular queries, the schema query is not surrounded by curly braces.
  Also, schema queries and regular queries cannot be combined.
</Note>

You can query for particular schema fields in the query body.

```
schema {
  type
  index
  reverse
  tokenizer
  list
  count
  upsert
  lang
}
```

You can also query for particular predicates:

```
schema(pred: [name, friend]) {
  type
  index
  reverse
  tokenizer
  list
  count
  upsert
  lang
}
```

<Note>
  If ACL is enabled, then the schema query returns only the predicates for which
  the logged-in ACL user has read access.
</Note>

Types can also be queried. Below are some example queries.

```
schema(type: Movie) {}
schema(type: [Person, Animal]) {}
```

Note that type queries do not contain anything between the curly braces. The
output will be the entire definition of the requested types.

## Indexing with custom tokenizers

For advanced indexing, you can use
[custom tokenizers](./indexing-custom-tokenizers).


# Indexing with Custom Tokenizers
Source: https://docs.hypermode.com/dgraph/dql/indexing-custom-tokenizers



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph comes with a large toolkit of builtin indexes, but sometimes for niche
use cases they're not always enough.

Dgraph allows you to implement custom tokenizers via a plugin system in order to
fill the gaps.

## Caveats

The plugin system uses Go's [`pkg/plugin`](https://golang.org/pkg/plugin/). This
brings some restrictions to how plugins can be used.

* Plugins must be written in Go.

* As of Go 1.9, `pkg/plugin` only works on Linux. Therefore, plugins will only
  work on Dgraph instances deployed in a Linux environment.

* The version of Go used to compile the plugin should be the same as the version
  of Go used to compile Dgraph itself. Dgraph always uses the latest version of
  Go (and so should you!).

## Implementing a plugin

<Note>
  You should consider Go's [plugin](https://golang.org/pkg/plugin/)
  documentation to be supplementary to the documentation provided here.
</Note>

Plugins are implemented as their own main package. They must export a particular
symbol that allows Dgraph to hook into the custom logic the plugin provides.

The plugin must export a symbol named `Tokenizer`. The type of the symbol must
be `func() interface{}`. When the function is called the result returned should
be a value that implements the following interface:

```
type PluginTokenizer interface {
    // Name is the name of the tokenizer. It should be unique among all
    // builtin tokenizers and other custom tokenizers. It identifies the
    // tokenizer when an index is set in the schema and when search/filter
    // is used in queries.
    Name() string

    // Identifier is a byte that uniquely identifiers the tokenizer.
    // Bytes in the range 0x80 to 0xff (inclusive) are reserved for
    // custom tokenizers.
    Identifier() byte

    // Type is a string representing the type of data that is to be
    // tokenized. This must match the schema type of the predicate
    // being indexed. Allowable values are shown in the table below.
    Type() string

    // Tokens should implement the tokenization logic. The input is
    // the value to be tokenized, and will always have a concrete type
    // corresponding to Type(). The return value should be a list of
    // the tokens generated.
    Tokens(interface{}) ([]string, error)
}
```

The return value of `Type()` corresponds to the concrete input type of
`Tokens(interface{})` in the following way:

| `Type()` return value | `Tokens(interface{})` input type |
| --------------------- | -------------------------------- |
| `"int"`               | `int64`                          |
| `"float"`             | `float64`                        |
| `"string"`            | `string`                         |
| `"bool"`              | `bool`                           |
| `"datetime"`          | `time.Time`                      |

## Building the plugin

The plugin has to be built using the `plugin` build mode so that an `.so` file
is produced instead of a regular executable. For example:

```sh
go build -buildmode=plugin -o myplugin.so ~/go/src/myplugin/main.go
```

## Running Dgraph with plugins

When starting Dgraph, use the `--custom_tokenizers` flag to tell Dgraph which
tokenizers to load. It accepts a comma separated list of plugins. E.g.

```sh
dgraph ...other-args... --custom_tokenizers=plugin1.so,plugin2.so
```

<Note>
  Plugin validation is performed on startup. If a problem is detected, Dgraph
  will refuse to initialize.
</Note>

## Adding the index to the schema

To use a tokenization plugin, an index has to be created in the schema.

The syntax is the same as adding any built-in index. To add an custom index
using a tokenizer plugin named `foo` to a `string` predicate named
`my_predicate`, use the following in the schema:

```sh
my_predicate: string @index(foo) .
```

## Using the index in queries

There are two functions that can use custom indexes:

| Mode    | Behavior                                                  |
| ------- | --------------------------------------------------------- |
| `anyof` | Returns nodes that match on *any* of the tokens generated |
| `allof` | Returns nodes that match on *all* of the tokens generated |

The functions can be used either at the query root or in filters.

There behavior here an analogous to `anyofterms`/`allofterms` and
`anyoftext`/`alloftext`.

## Examples

The following examples should make the process of writing a tokenization plugin
more concrete.

### Unicode Characters

This example shows the type of tokenization that is similar to term tokenization
of full-text search. Instead of being broken down into terms or stem words, the
text is instead broken down into its constituent unicode codepoints (in Go
terminology these are called *runes*).

<Note>
  This tokenizer would create a very large index that would be expensive to
  manage and store. That's one of the reasons that text indexing usually occurs
  at a higher level; stem words for full-text search or terms for term search.
</Note>

The implementation of the plugin looks like this:

```go
package main

import "encoding/binary"

func Tokenizer() interface{} { return RuneTokenizer{} }

type RuneTokenizer struct{}

func (RuneTokenizer) Name() string     { return "rune" }
func (RuneTokenizer) Type() string     { return "string" }
func (RuneTokenizer) Identifier() byte { return 0xfd }

func (t RuneTokenizer) Tokens(value interface{}) ([]string, error) {
  var toks []string
  for _, r := range value.(string) {
    var buf [binary.MaxVarintLen32]byte
    n := binary.PutVarint(buf[:], int64(r))
    tok := string(buf[:n])
    toks = append(toks, tok)
  }
  return toks, nil
}
```

**Hints and tips:**

* Inside `Tokens`, you can assume that `value` will have concrete type
  corresponding to that specified by `Type()`. It is safe to do a type
  assertion.

* Even though the return value is `[]string`, you can always store non-unicode
  data inside the string. See [this blogpost](https://blog.golang.org/strings)
  for some interesting background how string are implemented in Go and why they
  can be used to store non-textual data. By storing arbitrary data in the
  string, you can make the index more compact. In this case, varints are stored
  in the return values.

Setting up the indexing and adding data:

```
name: string @index(rune) .
```

```
{
  set{
    _:ad <name> "Adam" .
    _:ad <dgraph.type> "Person" .
    _:aa <name> "Aaron" .
    _:aa <dgraph.type> "Person" .
    _:am <name> "Amy" .
    _:am <dgraph.type> "Person" .
    _:ro <name> "Ronald" .
    _:ro <dgraph.type> "Person" .
  }
}
```

Now queries can be performed.

The only person that has all of the runes `A` and `n` in their `name` is Aaron:

```
{
  q(func: allof(name, rune, "An")) {
    name
  }
}
=>
{
  "data": {
    "q": [
      { "name": "Aaron" }
    ]
  }
}
```

But there are multiple people who have both of the runes `A` and `m`:

```
{
  q(func: allof(name, rune, "Am")) {
    name
  }
}
=>
{
  "data": {
    "q": [
      { "name": "Amy" },
      { "name": "Adam" }
    ]
  }
}
```

Case is taken into account, so if you search for all names containing `"ron"`,
you would find `"Aaron"`, but not `"Ronald"`. But if you were to search for
`"no"`, you would match both `"Aaron"` and `"Ronald"`. The order of the runes in
the strings doesn't matter.

It is possible to search for people that have *any* of the supplied runes in
their names (rather than *all* of the supplied runes). To do this, use `anyof`
instead of `allof`:

```
{
  q(func: anyof(name, rune, "mr")) {
    name
  }
}
=>
{
  "data": {
    "q": [
      { "name": "Adam" },
      { "name": "Aaron" },
      { "name": "Amy" }
    ]
  }
}
```

`"Ronald"` doesn't contain `m` or `r`, so isn't found by the search.

<Note>
  Understanding what's going on under the hood can help you intuitively understand how `Tokens` method should be implemented.

  When Dgraph sees new edges that are to be indexed by your tokenizer, it will
  tokenize the value. The resultant tokens are used as keys for posting lists. The
  edge subject is then added to the posting list for each token.

  When a query root search occurs, the search value is tokenized. The result of
  the search is all of the nodes in the union or intersection of the corresponding
  posting lists (depending on whether `anyof` or `allof` was used).
</Note>

### CIDR Range

Tokenizers don't always have to be about splitting text up into its constituent
parts. This example indexes
[IP addresses into their CIDR ranges](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing).
This allows you to search for all IP addresses that fall into a particular CIDR
range.

The plugin code is more complicated than the rune example. The input is an IP
address stored as a string, e.g. `"100.55.22.11/32"`. The output are the CIDR
ranges that the IP address could possibly fall into. There could be up to 32
different outputs (`"100.55.22.11/32"` does indeed have 32 possible ranges, one
for each mask size).

```go
package main

import "net"

func Tokenizer() interface{} { return CIDRTokenizer{} }

type CIDRTokenizer struct{}

func (CIDRTokenizer) Name() string     { return "cidr" }
func (CIDRTokenizer) Type() string     { return "string" }
func (CIDRTokenizer) Identifier() byte { return 0xff }

func (t CIDRTokenizer) Tokens(value interface{}) ([]string, error) {
  _, ipnet, err := net.ParseCIDR(value.(string))
  if err != nil {
    return nil, err
  }
  ones, bits := ipnet.Mask.Size()
  var toks []string
  for i := ones; i >= 1; i-- {
    m := net.CIDRMask(i, bits)
    tok := net.IPNet{
      IP:   ipnet.IP.Mask(m),
      Mask: m,
    }
    toks = append(toks, tok.String())
  }
  return toks, nil
}
```

An example of using the tokenizer:

Setting up the indexing and adding data:

```
ip: string @index(cidr) .

```

```
{
  set{
    _:a <ip> "100.55.22.11/32" .
    _:b <ip> "100.33.81.19/32" .
    _:c <ip> "100.49.21.25/32" .
    _:d <ip> "101.0.0.5/32" .
    _:e <ip> "100.176.2.1/32" .
  }
}
```

```
{
  q(func: allof(ip, cidr, "100.48.0.0/12")) {
    ip
  }
}
=>
{
  "data": {
    "q": [
      { "ip": "100.55.22.11/32" },
      { "ip": "100.49.21.25/32" }
    ]
  }
}
```

The CIDR ranges of `100.55.22.11/32` and `100.49.21.25/32` are both
`100.48.0.0/12`. The other IP addresses in the database aren't included in the
search result, since they have different CIDR ranges for 12 bit masks
(`100.32.0.0/12`, `101.0.0.0/12`, `100.154.0.0/12` for `100.33.81.19/32`,
`101.0.0.5/32`, and `100.176.2.1/32` respectively).

Note that we're using `allof` instead of `anyof`. Only `allof` will work
correctly with this index. Remember that the tokenizer generates all possible
CIDR ranges for an IP address. If we were to use `anyof` then the search result
would include all IP addresses under the 1 bit mask (in this case, `0.0.0.0/1`,
which would match all IPs in this dataset).

### Anagram

Tokenizers don't always have to return multiple tokens. If you just want to
index data into groups, have the tokenizer just return an identifying member of
that group.

In this example, we want to find groups of words that are
[anagrams](https://en.wikipedia.org/wiki/Anagram) of each other.

A token to correspond to a group of anagrams could just be the letters in the
anagram in sorted order, as implemented below:

```go
package main

import "sort"

func Tokenizer() interface{} { return AnagramTokenizer{} }

type AnagramTokenizer struct{}

func (AnagramTokenizer) Name() string     { return "anagram" }
func (AnagramTokenizer) Type() string     { return "string" }
func (AnagramTokenizer) Identifier() byte { return 0xfc }

func (t AnagramTokenizer) Tokens(value interface{}) ([]string, error) {
  b := []byte(value.(string))
  sort.Slice(b, func(i, j int) bool { return b[i] < b[j] })
  return []string{string(b)}, nil
}
```

In action:

Setting up the indexing and adding data:

```
word: string @index(anagram) .
```

```
{
  set{
    _:1 <word> "airmen" .
    _:2 <word> "marine" .
    _:3 <word> "beat" .
    _:4 <word> "beta" .
    _:5 <word> "race" .
    _:6 <word> "care" .
  }
}
```

```
{
  q(func: allof(word, anagram, "remain")) {
    word
  }
}
=>
{
  "data": {
    "q": [
      { "word": "airmen" },
      { "word": "marine" }
    ]
  }
}
```

Since a single token is only ever generated, it doesn't matter if `anyof` or
`allof` is used. The result will always be the same.

### Integer prime factors

All of the custom tokenizers shown previously have worked with strings. However,
other data types can be used as well. This example is contrived, but nonetheless
shows some advanced usages of custom tokenizers.

The tokenizer creates a token for each prime factor in the input.

```
package main

import (
    "encoding/binary"
    "fmt"
)

func Tokenizer() interface{} { return FactorTokenizer{} }

type FactorTokenizer struct{}

func (FactorTokenizer) Name() string     { return "factor" }
func (FactorTokenizer) Type() string     { return "int" }
func (FactorTokenizer) Identifier() byte { return 0xfe }

func (FactorTokenizer) Tokens(value interface{}) ([]string, error) {
    x := value.(int64)
    if x <= 1 {
        return nil, fmt.Errorf("Cannot factor int <= 1: %d", x)
    }
    var toks []string
    for p := int64(2); x > 1; p++ {
        if x%p == 0 {
            toks = append(toks, encodeInt(p))
            for x%p == 0 {
                x /= p
            }
        }
    }
    return toks, nil

}

func encodeInt(x int64) string {
    var buf [binary.MaxVarintLen64]byte
    n := binary.PutVarint(buf[:], x)
    return string(buf[:n])
}
```

<Note>
  Notice that the return of `Type()` is `"int"`, corresponding to the concrete
  type of the input to `Tokens` (which is `int64`).
</Note>

This allows you do things like search for all numbers that share prime factors
with a particular number.

In particular, we search for numbers that contain any of the prime factors of
15, that's any numbers that are divisible by either 3 or 5.

Setting up the indexing and adding data:

```
num: int @index(factor) .
```

```
{
  set{
    _:2 <num> "2"^^<xs:int> .
    _:3 <num> "3"^^<xs:int> .
    _:4 <num> "4"^^<xs:int> .
    _:5 <num> "5"^^<xs:int> .
    _:6 <num> "6"^^<xs:int> .
    _:7 <num> "7"^^<xs:int> .
    _:8 <num> "8"^^<xs:int> .
    _:9 <num> "9"^^<xs:int> .
    _:10 <num> "10"^^<xs:int> .
    _:11 <num> "11"^^<xs:int> .
    _:12 <num> "12"^^<xs:int> .
    _:13 <num> "13"^^<xs:int> .
    _:14 <num> "14"^^<xs:int> .
    _:15 <num> "15"^^<xs:int> .
    _:16 <num> "16"^^<xs:int> .
    _:17 <num> "17"^^<xs:int> .
    _:18 <num> "18"^^<xs:int> .
    _:19 <num> "19"^^<xs:int> .
    _:20 <num> "20"^^<xs:int> .
    _:21 <num> "21"^^<xs:int> .
    _:22 <num> "22"^^<xs:int> .
    _:23 <num> "23"^^<xs:int> .
    _:24 <num> "24"^^<xs:int> .
    _:25 <num> "25"^^<xs:int> .
    _:26 <num> "26"^^<xs:int> .
    _:27 <num> "27"^^<xs:int> .
    _:28 <num> "28"^^<xs:int> .
    _:29 <num> "29"^^<xs:int> .
    _:30 <num> "30"^^<xs:int> .
  }
}
```

```
{
  q(func: anyof(num, factor, 15)) {
    num
  }
}
=>
{
  "data": {
    "q": [
      { "num": 3 },
      { "num": 5 },
      { "num": 6 },
      { "num": 9 },
      { "num": 10 },
      { "num": 12 },
      { "num": 15 },
      { "num": 18 }
      { "num": 20 },
      { "num": 21 },
      { "num": 25 },
      { "num": 24 },
      { "num": 27 },
      { "num": 30 },
    ]
  }
}
```


# JSON Data Format
Source: https://docs.hypermode.com/dgraph/dql/json



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph supports [Mutations](./mutation) in JSON or [RDF](./rdf) format. When
using JSON format Dgraph creates nodes and relationships from the JSON structure
and assigns UIDs to nodes.

## Specifying node UIDs

For example, if you run this mutation:

```dql
 {
   "set": [
     {
      "name": "diggy",
      "dgraph.type": "Mascot"
     }
   ]
 }
```

You see that Dgraph responds with

```json
{
  "data": {
    "code": "Success",
    "message": "Done",
    "queries": null,
    "uids": {
      "dg.3162278161.22055": "0xfffd8d72745f0650"
    }
  }
}
```

Meaning that Dgraph has created one node from the JSON. It has used the
identifier `dg.3162278161.22055` during the transaction. And the final UID value
for this node is `0xfffd8d72745f0650`.

You can control the identifier name by specifying a `uid` field in your JSON
data and using the notation: `"uid" : "_:<your-identifier>"`

In this mutation, there are two JSON objects and because they refer to the same
identifier, Dgraph creates only one node:

```dql
   {
   "set": [
     {
      "uid": "_:diggy",
      "name": "diggy",
      "dgraph.type": "Mascot"
     },
     {
      "uid": "_:diggy",
      "specie": "badger"
     }
   ]
 }
```

When you run this mutation, you can see that Dgraph returns the UID of the node
that was created with the `diggy` identifier:

```json
{
  "data": {
    "code": "Success",
    "message": "Done",
    "queries": null,
    "uids": { "diggy": "0xfffd8d72745f0691" }
  }
}
```

Note that the `specie` field is added to the node already created with `name`
and `dgraph.type` information.

### Referencing existing nodes

You can use the `"uid"` field to reference an existing node. To do so, you must
specify the UID value of the node.

For example:

```dql
   {
   "set": [
     {
      "uid": "0xfffd8d72745f0650",
      "specie": "badger"
     }
   ]
 }
```

Adds the `specie` information to the node that was created earlier.

## Language support

To set a string value for a specific language, append the language tag to the
field name. In case, `specie` predicate has the @lang directive, the JSON
mutation

```dql
   {
   "set": [
     {
      "uid": "_:diggy",
      "name": "diggy",
      "dgraph.type": "Mascot",
      "specie@en" : "badger",
      "specie@fr" : "blaireau"
     }
   ]
 }
```

Dgraph sets the `specie` string predicate in English and in French.

## Geolocation support

Geo-location data must be specified using keys `type` and `coordinates` in the
JSON document. The supported types are `Point`, `Polygon`, or `MultiPolygon` .

```dql
 {
   "set": [
     {
      "name": "diggy",
      "dgraph.type": "Mascot",
      "home" : {
          "type": "Point",
          "coordinates": [-122.475537, 37.769229 ]
       }
     }
   ]
 }
```

## Relationships

Relationships are simply created from the nested structure of JSON.

For example:

```dql
 {
   "set": [
     {
      "uid": "_:diggy",
      "name": "diggy",
      "dgraph.type": "Mascot",
      "food" : [
        {
          "uid":"_:f1",
          "name": "earthworms"
        },
        {
          "uid":"_:f2",
          "name": "apples"
        }]
     }
   ]
 }

```

This result in the creation of three nodes and the `food` predicate as a
relationship.

```json
{
  "data": {
    "code": "Success",
    "message": "Done",
    "queries": null,
    "uids": {
      "diggy": "0xfffd8d72745f06d7",
      "f1": "0xfffd8d72745f06d8",
      "f2": "0xfffd8d72745f06d9"
    }
  }
}
```

You can use references to existing nodes at any level of your nested JSON.

## Deleting literal values

To delete node predicates, specify the UID of the node you are changing and
set\
the predicates to delete to the JSON value `null`.

For example, to remove the predicate `name` from node `0xfffd8d72745f0691` :

```dql
{
   "delete": [
     {
      "uid": "0xfffd8d72745f0691",
      "name": null
     }
   ]
}
```

## Deleting relationship

A relationship can be defined with a cardinality of 1 or many (list). Setting a
relationship to `null` removes all the relationships.

```JSON
{
  "uid": "0xfffd8d72745f06d7",
  "food": null
}
```

To delete a single relationship in a list, you must specify the target node of
the relationship.

```dql
{
   "delete": [
      {
      "uid": "0xfffd8d72745f06d7",
      "food": {
          "uid": "0xfffd8d72745f06d9"
        }
      }
   ]
}

```

deletes only one `food` relationship.

To delete all predicates of a given node:

* make sure the node has a `dgraph.type` predicate
* the type is defined in the [Dgraph types schema](./schema)
* run a delete mutation specifying only the UID field

```JSON
{
   "delete": [
      {
        "uid": "0x123"
      }
   ]
}
```

## Handling arrays

To create a predicate as a list of string:

```JSON
{
   "set": [
    {
      "testList": [
        "Grape",
        "Apple",
        "Strawberry",
        "Banana",
        "watermelon"
      ]
    }
   ]
}
```

For example, if `0x06` is the UID of the node created.

To remove one value from the list:

```JSON
{
  "delete": {
    "uid": "0x6", #UID of the list.
    "testList": "Apple"
  }
}
```

To remove multiple values:

```JSON
{
  "delete": {
    "uid": "0x6",
    "testList": [
          "Strawberry",
          "Banana",
          "watermelon"
        ]
  }
}
```

To add a value:

```JSON
{
   "uid": "0x6", #UID of the list.
   "testList": "Pineapple"
}
```

## Adding facets

Facets can be created by using the `|` character to separate the predicate and
facet key in a JSON object field name. This is the same encoding schema used to
show facets in query results. E.g.

```JSON
{
  "name": "Carol",
  "name|initial": "C",
  "dgraph.type": "Person",
  "friend": {
    "name": "Daryl",
    "friend|close": "yes",
    "dgraph.type": "Person"
  }
}
```

Facets don't contain type information but Dgraph tries to guess a type from the
input. If the value of a facet can be parsed to a number, it is converted to
either a float or an int. If it can be parsed as a Boolean, it is stored as a
Boolean. If the value is a string, it is stored as a datetime if the string
matches one of the time formats that Dgraph recognizes (`YYYY`, `MM-YYYY`,
`DD-MM-YYYY`, RFC339, etc.) and as a double-quoted string otherwise. If you do
not want to risk the chance of your facet data being misinterpreted as a time
value, it's best to store numeric data as either an int or a float.

## Deleting facets

To delete a `Facet`, overwrite it. When you run a mutation for the same entity
without a `Facet`, the existing `Facet` is deleted automatically.

## Facets in list

Schema:

```sh
<name>: string @index(exact).
<nickname>: [string] .
```

To create a List-type predicate you need to specify all value in a single list.
Facets for all predicate values should be specified together. It is done in map
format with index of predicate values inside list being map key and their
respective facets value as map values. Predicate values that don't have facets
values are excluded from the facets map.

```JSON
{
  "set": [
    {
      "uid": "_:Julian",
      "name": "Julian",
      "nickname": ["Jay-Jay", "Jules", "JB"],
      "nickname|kind": {
        "0": "first",
        "1": "official",
        "2": "CS-GO"
      }
    }
  ]
}
```

Above you see that there are three values ​​to enter the list with their
respective facets. You can run this query to check the list with facets:

```graphql
{
   q(func: eq(name,"Julian")) {
    uid
    nickname @facets
   }
}
```

Later, if you want to add more values ​​with facets, just do the same procedure,
but this time instead of using Blank-node you must use the actual node's UID.

```JSON
{
  "set": [
    {
      "uid": "0x3",
      "nickname|kind": "Internet",
      "nickname": "@JJ"
    }
  ]
}
```

And the final result is:

```JSON
{
  "data": {
    "q": [
      {
        "uid": "0x3",
        "nickname|kind": {
          "0": "first",
          "1": "Internet",
          "2": "official",
          "3": "CS-GO"
        },
        "nickname": [
          "Jay-Jay",
          "@JJ",
          "Jules",
          "JB"
        ]
      }
    ]
  }
}
```

## Reserved values

The string values `uid(...)`, `val(...)` aren't accepted.


# Language Support
Source: https://docs.hypermode.com/dgraph/dql/language-support



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

<Note>
  A `@lang` directive must be specified in the schema to query or mutate
  predicates with language tags.
</Note>

Dgraph supports UTF-8 strings.

In a query, for a string valued edge `edge`, the syntax

```dql
edge@lang1:...:langN
```

specifies the preference order for returned languages, with the following rules.

* At most one result is returned (except in the case where the language list is
  set to \*).
* The preference list is considered left to right: if a value in given language
  isn't found, the next language from the list is considered.
* If there are no values in any of the specified languages, no value is
  returned.
* A final `.` means that a value without a specified language is returned or if
  there is no value without language, a value in ''some'' language is returned.
* Setting the language list value to \* returns all the values for that
  predicate along with their language. Values without a language tag are also
  returned.

For example:

* `name` => Look for an untagged string; return nothing if no untagged value
  exits.
* `name@.` => Look for an untagged string, then any language.
* `name@en` => Look for `en` tagged string; return nothing if no `en` tagged
  string exists.
* `name@en:.` => Look for `en`, then untagged, then any language.
* `name@en:pl` => Look for `en`, then `pl`, otherwise nothing.
* `name@en:pl:.` => Look for `en`, then `pl`, then untagged, then any language.
* `name@*` => Look for all the values of this predicate and return them along
  with their language. For example, if there are two values with languages en
  and hi, this query returns two keys named "name\@en" and "name\@hi".

<Note>
  In functions, language lists (including the `@*` notation) aren't allowed.
  Untagged predicates, Single language tags, and `.` notation work as described
  above.

  ***

  In [full-text search functions](./functions#full-text-search) (`alloftext`,
  `anyoftext`), when no language is specified (untagged or `@.`), the default
  (English) full-text tokenizer is used. This does not mean that the value with
  the `en` tag will be searched when querying the untagged value, but that
  untagged values will be treated as English text. If you don't want that to be
  the case, use the appropriate tag for the desired language, both for mutating
  and querying the value.
</Note>

Query Example: some of Bollywood director and actor Farhan Akhtar's movies have
a name stored in Russian as well as Hindi and English, others do not.

```json
query {
  q(func: allofterms(name@en, "Farhan Akhtar")) {
    name@hi
    name@en

    director.film {
      name@ru:hi:en
      name@en
      name@hi
      name@ru
    }
  }
}
```


# Mutation
Source: https://docs.hypermode.com/dgraph/dql/mutation



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph Query Language (DQL) is Dgraph's proprietary language to add, modify,
delete and fetch data.

Fetching data is done through [DQL Queries](./query). Adding, modifying, or
deleting data is done through ***DQL Mutations***.

This overview explains the structure of DQL Mutations and provides links to the
appropriate DQL reference documentation.

DQL mutations support [JSON](./json) or [RDF](./rdf) format.

## Set block

In DQL, you add data using a set mutation, identified by the `set` keyword.

<Tabs>
  <Tab title="JSON">
    ```dql
       {
       "set": [
         {
           "name":"Star Wars: Episode IV - A New Hope",
           "release_date": "1977-05-25",
           "director": {
             "name": "George Lucas",
             "dgraph.type": "Person"
           },
           "starring" : [
             {
               "name": "Luke Skywalker"
             },
             {
               "name": "Princess Leia"
             },
             {
               "name": "Han Solo"
             }
           ]
         },
         {
           "name":"Star Trek: The Motion Picture",
           "release_date": "1979-12-07"
         }
       ]
     }
    ```
  </Tab>

  <Tab title="RDF">
    ```dql
    {
      set {
        # triples in here
        _:n1 <name> "Star Wars: Episode IV - A New Hope" .
        _:n1 <release_date>  "1977-05-25" .
        _:n1 <director> _:n2 .
        _:n2 <name> "George Lucas" .

      }
    }
    ```

    triples are in [RDF](./rdf) format.

    ### Node reference

    A mutation can include a blank nodes as an identifier for the subject or object,
    or a known UID.

    ```dql
    {
      set {
        # triples in here
        <0x632ea2> <release_date>  "1977-05-25" .
      }
    }
    ```

    adds the `release_date` information to the node identified by UID `0x632ea2`.

    ### Language support

    ```dql
    {
      set {
        # triples in here
        <0x632ea2> <name>  "Star Wars, épisode IV : Un nouvel espoir"@fr .
      }
    }
    ```
  </Tab>
</Tabs>

## Delete block

A delete mutation, identified by the `delete` keyword, removes [triples](./rdf)
from the store.

For example, if the store contained the following:

```RDF
<0xf11168064b01135b> <name> "Lewis Carrol"
<0xf11168064b01135b> <died> "1998"
<0xf11168064b01135b> <dgraph.type> "Person" .
```

Then, the following delete mutation deletes the specified erroneous data, and
removes it from any indexes:

```sh
{
  delete {
     <0xf11168064b01135b> <died> "1998" .
  }
}
```

### Wildcard delete

In many cases you need to delete multiple types of data for a predicate. For a
particular node `N`, all data for predicate `P` (and all corresponding indexing)
is removed with the pattern `S P *`.

```sh
{
  delete {
     <0xf11168064b01135b> <author.of> * .
  }
}
```

The pattern `S * *` deletes all the known edges out of a node, any reverse edges
corresponding to the removed edges, and any indexing for the removed data.

<Note>
  For mutations that fit the `S * *` pattern, only predicates that are among the
  types associated with a given node (using `dgraph.type`) are deleted. Any
  predicates that don't match one of the node's types remains after an `S * *`
  delete mutation.
</Note>

```sh
{
  delete {
     <0xf11168064b01135b> * * .
  }
}
```

If the node `S` in the delete pattern `S * *` has only a few predicates with a
type defined by `dgraph.type`, then only those triples with typed predicates are
deleted. A node that contains un-typed predicates still exists after a `S * *`
delete mutation.

<Note>
  The patterns `* P O` and `* * O` aren't supported because it's inefficient to
  store and find all the incoming edges.
</Note>

### Deletion of non-list predicates

Deleting the value of a non-list predicate (i.e a 1-to-1 relationship) can be
done in two ways.

* Using the [wildcard delete](#wildcard-delete) (star notation) mentioned in the
  last section.
* Setting the object to a specific value. If the value passed isn't the current
  value, the mutation succeeds but has no effect. If the value passed is the
  current value, the mutation succeeds and deletes the non-list predicate.

For language-tagged values, the following special syntax is supported:

```dql
{
  delete {
    <0x12345> <name@es> * .
  }
}
```

In this example, the value of the `name` field that's tagged with the language
tag `es` is deleted. Other tagged values are left untouched.

## Upsert block

Upsert is an operation where:

1. A node is searched for, and then
2. Depending on if it's found or not, either:
   * Updating some of its attributes, or
   * Creating a new node with those attributes.

The upsert block allows performing queries and mutations in a single request.
The upsert block contains one query block and mutation blocks.

The structure of the upsert block is as follows:

```dql
upsert {
  query <query block>
  mutation <mutation block 1>
  [mutation <mutation block 2>]
  ...
}
```

Execution of an upsert block also returns the response of the query executed on
the state of the database *before mutation was executed*. To get the latest
result, you have to execute another query after the transaction is committed.

Variables defined in the query block can be used in the mutation blocks using
the [UID](./upsert#uid-function-in-upsert) and
[val](./upsert#val-function-in-upsert) functions.

## Conditional upsert

The upsert block also allows specifying conditional mutation blocks using an
`@if` directive. The mutation is executed only when the specified condition is
true. If the condition is false, the mutation is silently ignored. The general
structure of Conditional Upsert looks like as follows:

```dql
upsert {
  query <query block>
  [fragment <fragment block>]
  mutation [@if(<condition>)] <mutation block 1>
  [mutation [@if(<condition>)] <mutation block 2>]
  ...
}
```

The `@if` directive accepts a condition on variables defined in the query block
and can be connected using `AND`, `OR` and `NOT`.

## Example of conditional Upsert

Let's say in our previous example, we know the `company1` has less than 100
employees. For safety, we want the mutation to execute only when the variable
`v` stores less than 100 but greater than 50 UIDs in it. This can be achieved as
follows:

```sh
curl -H "Content-Type: application/rdf" -X POST localhost:8080/mutate?commitNow=true -d  $'
upsert {
  query {
    v as var(func: regexp(email, /.*@company1.io$/))
  }

  mutation @if(lt(len(v), 100) AND gt(len(v), 50)) {
    delete {
      uid(v) <name> * .
      uid(v) <email> * .
      uid(v) <age> * .
    }
  }
}' | jq
```

We can achieve the same result using `json` dataset as follows:

```sh
curl -H "Content-Type: application/json" -X POST localhost:8080/mutate?commitNow=true -d '{
  "query": "{ v as var(func: regexp(email, /.*@company1.io$/)) }",
  "cond": "@if(lt(len(v), 100) AND gt(len(v), 50))",
  "delete": {
    "uid": "uid(v)",
    "name": null,
    "email": null,
    "age": null
  }
}' | jq
```


# normalize
Source: https://docs.hypermode.com/dgraph/dql/normalize



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

With the `@normalize` directive, Dgraph returns only aliased predicates and
flattens the result to remove nesting.

Query Example: film name, country, and first two actors (by UID order) of every
Steven Spielberg movie, without `initial_release_date` because no alias is
given, and flattened by `@normalize`.

```json
{ director(func:allofterms(name@en, "steven spielberg")) @normalize {
  director: name@en
  director.film {
    film: name@en initial_release_date starring(first: 2) {
        performance.actor { actor: name@en }
        performance.character { character: name@en }
      }
      country { country: name@en }
    }
  }
}
```

You can also apply `@normalize` on nested query blocks. It works similarly but
only flatten the result of the nested query block where `@normalize` has been
applied. `@normalize` returns a list irrespective of the type of attribute on
which it's applied.

```json
{ director(func:allofterms(name@en, "steven spielberg")) {
  director: name@en
  director.film {
    film: name@en initial_release_date starring(first: 2) @normalize {
      performance.actor { actor: name@en }
      performance.character { character: name@en }
    }
    country { country: name@en }
  }
}
```


# Pagination
Source: https://docs.hypermode.com/dgraph/dql/pagination



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Pagination allows returning only a portion, rather than the whole, result set.
This can be useful for top-k style queries as well as to reduce the size of the
result set for client side processing or to allow paged access to results.

Pagination is often used with [sorting](./sorting).

<Note>
  Without a sort order specified, the results are sorted by `uid`, which is
  assigned randomly. So the ordering, while deterministic, might not be what you
  expected.
</Note>

## First

Syntax Examples:

* `q(func: ..., first: N)`
* `predicate (first: N) { ... }`
* `predicate @filter(...) (first: N) { ... }`

For positive `N`, `first: N` retrieves the first `N` results, by sorted or UID
order.

For negative `N`, `first: N` retrieves the last `N` results, by sorted or UID
order. Currently, negative is only supported when no order is applied. To
achieve the effect of a negative with a sort, reverse the order of the sort and
use a positive `N`.

Query Example: last two films, by UID order, directed by Steven Spielberg and
the first three genres of those movies, sorted alphabetically by English name.

```dql
{ me(func: allofterms(name@en, "Steven Spielberg")) {
  director.film (first: -2) {
    name@en
    initial_release_date
    genre (orderasc: name@en) (first: 3) {
      name@en
    }
  }
}
}
```

Query Example: the three directors named Steven who have directed the most
actors of all directors named Steven.

```dql
{
  ID as var(func: allofterms(name@en, "Steven")) @filter(has(director.film)) {
    director.film {
      stars as count(starring)
    }
    totalActors as sum(val(stars))
  }

  directors(func: uid(ID), orderdesc: val(totalActors), first: 3) {
    name@en
  }
}
```

## Offset

Syntax Examples:

* `q(func: ..., offset: N)`
* `predicate (offset: N) { ... }`
* `predicate (first: M, offset: N) { ... }`
* `predicate @filter(...) (offset: N) { ... }`

With `offset: N` the first `N` results aren't returned. Used in combination with
first, `first: M, offset: N` skips over `N` results and returns the following
`M`.

<Note>
  Skipping over `N` results takes time proportional to `N` (complexity `O(N)`).
  In other words, the larger `N`, the longer it takes to compute the result set.
  Prefer [after](./#after) over `offset`.
</Note>

Query Example: order Hark Tsui's films by English title, skip over the first 4
and return the following 6.

```dql
{
  me(func: allofterms(name@en, "Hark Tsui")) {
    name@zh
    name@en
    director.film (orderasc: name@en) (first:6, offset:4) {
      genre {
        name@en
      }
      name@zh
      name@en
      initial_release_date
    }
  }
}
```

## After

Syntax Examples:

* `q(func: ..., after: UID)`
* `predicate (first: N, after: UID) { ... }`
* `predicate @filter(...) (first: N, after: UID) { ... }`

Another way to get results after skipping over some results is to use the
default UID ordering and skip directly past a node specified by UID. For
example, a first query could be of the form `predicate (after: 0x0, first: N)`,
or just `predicate (first: N)`, with subsequent queries of the form
`predicate(after: <uid of last entity in last result>, first: N)`.

<Note>
  Skipping over results with `after` takes constant time (complexity `O(1)`). In
  other words, no matter how many results are skipped, no extra time adds to
  computing the result set. This should be preferred over [offset](./#offset).
</Note>

Query Example: the first five of Baz Luhrmann's films, sorted by UID order.

```dql
{
  me(func: allofterms(name@en, "Baz Luhrmann")) {
    name@en
    director.film (first:5) {
      uid
      name@en
    }
  }
}
```

The fifth movie is the Australian movie classic Strictly Ballroom. It has UID
`0x99e44`. The results after Strictly Ballroom can now be obtained with `after`.

```dql
{
  me(func: allofterms(name@en, "Baz Luhrmann")) {
    name@en
    director.film (first:5, after: 0x99e44) {
      uid
      name@en
    }
  }
}
```


# Query Structure
Source: https://docs.hypermode.com/dgraph/dql/query



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Fetching data with Dgraph Query Language (DQL), is done through **DQL Queries**.
Adding, modifying or deleting data is done through [DQL Mutations](./mutation).

This overview explains the structure of DQL Queries and provides links to the
appropriate DQL reference documentation.

## DQL query structure

DQL is **declarative**, which means that queries return a response back in a
similar shape to the query. It gives the client app the control of what it gets:
the request return exactly what you ask for, nothing less and nothing more. In
this, DQL is similar to GraphQL from which it's inspired.

A DQL query finds nodes based on search criteria, matches patterns in the graph
and returns the node attributes, relationships specified in the query.

A DQL query has

* an optional parameterization, ie a name and a list of parameters
* an opening curly bracket
* at least one [query block](./#query-block), but can contain many blocks
* optional var blocks
* a closing curly bracket

![DQL Query with parameterization](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/query-syntax-1.png)

## Query parameterization

**Parameters**

* must have a name starting with a `$` symbol.
* must have a type `int`, `float`, `bool` or `string`.
* may have a default value. In the example below, `$age` has a default value of
  `95`
* may be mandatory by suffixing the type with a `!`. Mandatory parameters can't
  have a default value.

Variables can be used in the query where a string, float, int or bool value are
needed.

You can also use a variable holding `uids` by using a string variable and by
providing the value as a quoted list in square brackets:\
`query title($uidsParam: string = "[0x1, 0x2, 0x3]") { ... }`.

**Error handling** When submitting a query using parameters, Dgraph responds
with errors if

* A parameter value is not parsable to the given type.
* The query is using a parameter that is not declared.
* A mandatory parameter is not provided

The query parameterization is optional. If you don't use parameters you can omit
it and send only the query blocks.

![DQL Query without parameters](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/query-syntax-2.png)

<Note>
  The current documentation is usually using example of queries without
  parameters.
</Note>

If you execute this query in our [Movies demo database](./schema#sample-schema)
you can see that Dgraph will return a JSON structure similar to the request :
![DQL response structure](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/query-syntax-3.png)

## Query block

A query block specifies information to retrieve from Dgraph.

A query block

* must have name
* must have a node criteria defined by the keyword `func:`
* may have ordering and pagination information
* may have a combination of filters (to apply to the root nodes)
* must provide the list of attributes and relationships to fetch for each node
  matching the root nodes.

Refer to [pagination](./pagination) and [ordering](./sorting) for more
information.

For each relationships to fetch, the query is using a nested block.

A nested block

* may specify filters to apply on the related nodes
* may specify criteria on the relationships attributes using
  [filtering on facets](./facets#filtering-on-facets))
* provides the list of relationship attributes ([facets](./facets))) to fetch.
* provides the list of attributes and relationships to fetch for the related
  nodes.

A nested block may contain another nested block, and such at any level.

## Multiple query blocks

Inside a single query, multiple query blocks are allowed, and each block can
have a name. Multiple query blocks are executed in parallel, and they don't need
to be related in any way.

Query Example: *"All of Angelina Jolie's films, with genres, and Peter Jackson's
films since 2008"*

```json
{
  AngelinaInfo(func: allofterms(name@en, "angelina jolie")) {
    name@en actor.film { performance.film { genre { name@en } } }
  }

  DirectorInfo(func: eq(name@en, "Peter Jackson")) {
    name@en director.film
    @filter(ge(initial_release_date, "2008")) { Release_date: initial_release_date
    Name: name@en }
  }
}
```

If queries contain some overlap in answers, the result sets are still
independent.

Query Example: *"The movies Mackenzie Crook has acted in and the movies Jack
Davenport has acted in"*

The results sets overlap because both have acted in the *Pirates of the
Caribbean* movies, but the results are independent and both contain the full
answers sets.

```json
{
  Mackenzie(func:allofterms(name@en, "Mackenzie Crook")) {
    name@en actor.film { performance.film { uid name@en } performance.character {
    name@en } }
  }

  Jack(func:allofterms(name@en, "Jack Davenport")) { name@en actor.film {
  performance.film { uid name@en } performance.character { name@en } } }
}
```

## Variable (`var`) blocks

Variable blocks (`var` blocks) start with the keyword `var` and are not returned
in the query results, but do affect the contents of query results.

Query Example: *"Angelina Jolie's movies ordered by genre"*

```json
{
  var(func:allofterms(name@en, "angelina jolie")) {
    name@en actor.film { A AS performance.film { B AS genre } }
  }

  films(func: uid(B), orderasc: name@en) { name@en ~genre @filter(uid(A)) {
  name@en }
  }
}
```

## Multiple `var` blocks

You can also use multiple `var` blocks within a single query operation. You can
use variables from one `var` block in any of the subsequent blocks, but not
within the same block.

Query Example: *"Movies containing both Angelina Jolie and Morgan Freeman sorted
by name"*

```json
{
  var(func:allofterms(name@en, "angelina jolie")) {
    name@en actor.film { A AS performance.film }
  }

  var(func:allofterms(name@en, "morgan freeman")) {
    name@en actor.film { B as performance.film @filter(uid(A)) }
  }

  films(func: uid(B), orderasc: name@en) {
    name@en
  }
}
```

### Combining multiple `var` blocks

You could get the same query results by logically combining both `var` blocks in
the films block, as follows:

```json
{
  var(func:allofterms(name@en, "angelina jolie")) {
    name@en actor.film { A AS performance.film }
  }

  var(func:allofterms(name@en, "morgan freeman")) {
    name@en actor.film { B as performance.film }
  }
  films(func: uid(A,B), orderasc: name@en) @filter(uid(A) AND uid(B)) { name@en }
}
```

The root `uid` function unions the `uid`s from `var` `A` and `B`, so you need a
filter to intersect the `uid`s from `var` `A` and `B`.

### Escape characters in predicate names

If your predicate has special characters, wrap it with angular brackets `< >` in
the query.

E.g. `<https://myschema.org#name>  `

### Formatting options

Dgraph returns the attributes and relationships that you specified in the query.
You can specify an alternate name for the result by using \[

You can flatten the response structure at any level using
[@normalize](./normalize) directive.

Entering the list of all the attributes you want to fetch could be fastidious
for large queries or repeating blocks : you may take advantage of
[fragments](./fragment) and the [expand function](./expand).

### Node criteria (used by root function or by filter)

Root criteria and filters are using [functions](./functions) applied to nodes
attributes or variables.

Dgraph offers functions for

* testing string attributes
  * term matching : [allofterms](./functions#allofterms),
    [anyofterms](./functions#anyofterms)
  * regular Expression : [regexp](./functions#regular-expressions)
  * fuzzy match : [match](./functions#fuzzy-matching)
  * full-text search : [alloftext](./functions#full-text-search)
* testing attribute value
  * equality : [eq](./functions#equal-to)
  * inequalities :
    [le,lt,ge,gt](./functions#less-than-less-than-or-equal-to-greater-than-and-greater-than-or-equal-to)
  * range : [between](./functions#between)
* testing if a node
  * has a particular predicate (an attribute or a relation) :
    [has](./functions#has)
  * has a given UID : `[uid]`(./functions#uid)
  * has a relationship to a given node : [uid\_in](./functions#uid_in)
  * is of a given type : type()
* testing the number of node relationships
  * equality : [eq](./functions#equal-to)
  * inequalities :
    [le,lt,ge,gt](./functions#less-than-less-than-or-equal-to-greater-than-and-greater-than-or-equal-to)
* testing geolocation attributes
  * if geo location is within distance : [near](./functions#near)
  * if geo location lies within a given area : [within](./functions#within)
  * if geo area contains a given location : [contains](./functions#contains)
  * if geo area intersects a given are : [intersects](./functions#intersects)

### Variable (`var`) block

Variable blocks (`var` blocks) start with the keyword `var` instead of a block
name.

var blocks are not reflected in the query result. They are used to compute
[query-variables](./variables#query-variables) which are lists of node UIDs, or
[value-variables](./variables#value-variables) which are maps from node UIDs to
the corresponding scalar values.

Note that query-variables and value-variables can also be computed in query
blocks. In that case, the query block is used to fetch and return data, and to
define some variables which must be used in other blocks of the same query.

Variables may be used as functions parameters in filters or root criteria in
other blocks.

### Summarizing functions

When dealing with array attributes or with relationships to many node, the query
may use summary functions [count](./count) , [min](./aggregation#min),
[max](./aggregation#max), [avg](./aggregation#sum-and-avg) or
[sum](./aggregation#sum-and-avg).

The query may also contain
[mathematical functions](./variables#math-on-value-variables) on value
variables.

Summary functions can be used in conjunction with [@grouby](./groupby) directive
to create aggregated value variables.

The query may contain **anonymous block** to return computed values. **Anonymous
block** don't have a root criteria as they are not used to search for nodes but
only to returned computed values.

### Graph traversal

When you specify nested blocks and filters you basically describe a way to
traverse the graph.

[@recurse](./recurse) and [@ignorereflex](./ignorereflex) are directives used to
optionally configure the graph traversal.

### Pattern matching

Queries with nested blocks with filters may be turned into pattern matching
using [@cascade](./cascade) directive : nodes that don’t have all attributes and
all relationships specified in the query at any sub level are not considered in
the result. So only nodes "matching" the complete query structure are returned.

### Graph algorithms

The query can ask for the shortest path between a source (from) node and
destination (to) node using the [shortest](./shortest) query block.

### Comments

Anything on a line following a `#` is a comment

```
```


# RDF Data Format
Source: https://docs.hypermode.com/dgraph/dql/rdf



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph natively supports Resource Description Framework (RDF) when creating,
importing and exporting data. Dgraph Client libraries can be used to query RDF
as well.

[RDF 1.1](https://www.w3.org/RDF/) is a Semantic Web Standard for data
interchange defined by the W3C. It expresses statements about resources. The
format of these statements is simple and in the form of triples.

A triple has the form

```dql
<subject> <predicate> <object> .
```

In RDF terminology, each triple represents one fact about a node.

In Dgraph, the `<subject>` of a triple is always a node, and must be a numeric
UID. The `<object>` of a triple may be another node or a literal value:

```dql
<0x01> <name> "Alice" .
<0x01> <knows> <0x02> .
```

The first triple specifies that a node has a name property of “Alice”. The
subject is the UID of the first node, the predicate is `name`, and the object is
the literal value string: `"Alice"`. The second triple specifies that Alice
knows Bob. The subject is again the UID of a node (the "alice" node), the
predicate is `knows`, and the object of this triple is the UID of the other node
(the "bob" node). When the object is a UID, the triple represents a relationship
in Dgraph.

Each triple representation in RDF ends with a period.

### Blank nodes in mutations

When creating nodes in Dgraph, you often let Dgraph assign the node
[UID](/dgraph/glossary#uid) by specifying a blank node starting with `_:`. All
references to the same blank node, such as `_:identifier123`, identify the same
node within a mutation. Dgraph creates a UID identifying each blank node.

### Language for string values

Languages are written using `@lang`. For example

```dql
<0x01> <name> "Adelaide"@en .
<0x01> <name> "Аделаида"@ru .
<0x01> <name> "Adélaïde"@fr .
<0x01> <dgraph.type> "Person" .
```

See also
[how language strings are handled in queries](/dgraph/dql/language-support#language-support).

### Types

Dgraph understands standard RDF types specified in RDF using the `^^` separator.
For example

```dql
<0x01> <age> "32"^^<xs:int> .
<0x01> <birthdate> "1985-06-08"^^<xs:dateTime> .
```

The supported
[RDF data types](https://www.w3.org/TR/rdf11-concepts/#section-Datatypes) and
the corresponding internal Dgraph type are as follows.

| Storage Type                                                                                            | Dgraph type |
| ------------------------------------------------------------------------------------------------------- | :---------: |
| \<xs:string>                                                                                            |   `string`  |
| \<xs:dateTime>                                                                                          |  `dateTime` |
| \<xs:date>                                                                                              |  `datetime` |
| \<xs:int>                                                                                               |    `int`    |
| \<xs:integer>                                                                                           |    `int`    |
| \<xs:boolean>                                                                                           |    `bool`   |
| \<xs:double>                                                                                            |   `float`   |
| \<xs:float>                                                                                             |   `float`   |
| \<geo:geojson>                                                                                          |    `geo`    |
| \<xs:password>                                                                                          |  `password` |
| \<[http://www.w3.org/2001/XMLSchema#string](http://www.w3.org/2001/XMLSchema#string)>                   |   `string`  |
| \<[http://www.w3.org/2001/XMLSchema#dateTime](http://www.w3.org/2001/XMLSchema#dateTime)>               |  `dateTime` |
| \<[http://www.w3.org/2001/XMLSchema#date](http://www.w3.org/2001/XMLSchema#date)>                       |  `dateTime` |
| \<[http://www.w3.org/2001/XMLSchema#int](http://www.w3.org/2001/XMLSchema#int)>                         |    `int`    |
| \<[http://www.w3.org/2001/XMLSchema#positiveInteger](http://www.w3.org/2001/XMLSchema#positiveInteger)> |    `int`    |
| \<[http://www.w3.org/2001/XMLSchema#integer](http://www.w3.org/2001/XMLSchema#integer)>                 |    `int`    |
| \<[http://www.w3.org/2001/XMLSchema#boolean](http://www.w3.org/2001/XMLSchema#boolean)>                 |    `bool`   |
| \<[http://www.w3.org/2001/XMLSchema#double](http://www.w3.org/2001/XMLSchema#double)>                   |   `float`   |
| \<[http://www.w3.org/2001/XMLSchema#float](http://www.w3.org/2001/XMLSchema#float)>                     |   `float`   |

### Facets

Dgraph is more expressive than RDF in that it allows properties to be stored on
every relation. These properties are called Facets in Dgraph, and dgraph allows
an extension to RDF where facet values are included in any triple.

#### Creating a list with facets

The following set operation uses a sequence of RDF statements with additional
facet information:

```sh
{
  set {
    _:Julian <name> "Julian" .
    _:Julian <nickname> "Jay-Jay" (kind="first") .
    _:Julian <nickname> "Jules" (kind="official") .
    _:Julian <nickname> "JB" (kind="CS-GO") .
  }
}
```

```graphql
{
  q(func: eq(name,"Julian")){
    name
    nickname @facets
  }
}
```

Result:

```JSON
{
  "data": {
    "q": [
      {
        "name": "Julian",
        "nickname|kind": {
          "0": "first",
          "1": "official",
          "2": "CS-GO"
        },
        "nickname": [
          "Jay-Jay",
          "Jules",
          "JB"
        ]
      }
    ]
  }
}
```

<Tip>
  Dgraph can automatically generate a reverse relation. If the user wants to run
  queries in that direction, they would define the [reverse
  relationship](./schema#reverse-edges)
</Tip>

## N-quads format

While most RDF data uses only triples (with three parts) an optional fourth part
is allowed. This fourth component in RDF is called a graph label, and in Dgraph
it must be the UID of the namespace that the data should go into as described in
[Multi-tenancy](/dgraph/enterprise/multitenancy).

## Processing RDF to comply with Dgraph syntax for subjects

While it's valid RDF to specify subjects that are IRI strings, Dgraph requires a
numeric UID or a blank node as the subject. If a string IRI is required, Dgraph
support them via [xid properties](./upsert#external-ids-and-upsert-block). When
importing RDF from another source that does not use numeric UID subjects, it
will be required to replace arbitrary subject IRIs with blank node IRIs.

Typically this is done simply by prepending "\_:" to the start of the original
IRI. So a triple such as:

`<http://abc.org/schema/foo#item1> <http://abc.org/hasRelation> "somevalue"^^xs:string`

may be rewritten as

`<_:http://abc.org/schema/foo#item1> <http://abc.org/hasRelation> "somevalue"^^xs:string`

Dgraph will create a consistent UID for all references to the uniquely-named
blank node. To maintain this uniqueness over multiple data loads, use the
[dgraph live](/dgraph/glossary#uid) utility with the xid option, or use specific
UIDs such as the hash of the IRI in the source RDF directly.


# recurse
Source: https://docs.hypermode.com/dgraph/dql/recurse



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

`Recurse` queries let you traverse a set of predicates (with filter, facets,
etc.) until we reach all leaf nodes or we reach the maximum depth which is
specified by the `depth` parameter.

To get 10 movies from a genre that has more than 30000 films and then get two
actors for those movies we'd do something as follows:

```json
query {
  me(func: gt(count(~genre), 30000), first: 1)
    @recurse(depth: 5, loop: true) {
      name@en
      ~genre (first: 10)
        @filter(gt(count(starring), 2))
        starring (first: 2)
        performance.actor
    }
}
```

Some points to keep in mind while using recurse queries are:

* You can specify only one level of predicates after root. These would be
  traversed recursively. Both scalar and entity-nodes are treated similarly.
* Only one recurse block is advised per query.
* Be careful as the result size could explode quickly and an error would be
  returned if the result set gets too large. In such cases use more filters,
  limit results using pagination, or provide a depth parameter at root as shown
  in the example above.
* The `loop` parameter can be set to false, in which case paths which lead to a
  loop would be ignored while traversing.
* If not specified, the value of the `loop` parameter defaults to false.
* If the value of the `loop` parameter is false and depth is not specified,
  `depth` will default to `math.MaxUint64`, which means that the entire graph
  might be traversed until all the leaf nodes are reached.


# Schema
Source: https://docs.hypermode.com/dgraph/dql/schema



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Here is an example of Dgraph types schema:

```json
name: string @index(term) .
release_date: datetime @index(year) .
revenue: float .
running_time: int .
starring: [uid] .
director: [uid] .
description: string .

description_vector: float32vector @index(hnsw(metric:"cosine")) .

type Person {
  name
}

type Film {
  name
  release_date
  revenue
  running_time
  starring
  director
  description
  description_vector
}
```

The schema contains information about [predicate types](#predicate-types) and
[node types](#node-types).

A [predicate](/dgraph/glossary#predicate) is the smallest piece of information
about an object. A predicate can hold a literal value or a relation to another
entity:

* when we store that an entity name is "Alice". The predicate is `name` and
  predicate value is the string "Alice".
* when we store that Alice knows Bob, we may use a predicate `knows` with the
  node representing Alice. The value of this predicate would be the
  [UID](/dgraph/glossary#uid) of the node representing Bob. In that case,
  `knows` is a relationship.

Dgraph maintains a list of all predicates names and their type in the **Dgraph
types schema**.

## Predicates declaration

The Dgraph cluster schema mode defines if the Dgraph types must be declared
before allowing mutations or not:

* In `strict` mode, you must declare the predicates
  ([Update Dgraph types](/dgraph/admin/update-types) ) before you can run a
  mutation using those predicates.
* In `flexible` mode (which is the default behavior), you can run a mutation
  without declaring the predicate in the DQL Schema.

<Note>
  When you deploy a [GraphQL API schema](/dgraph/graphql/schema/overview),
  Dgraph generates all the underlying Dgraph types. Refer to [GraphQL and DQL
  schemas](/dgraph/graphql/schema/graphql-dql) for use cases using both
  approaches.
</Note>

For example, you can run the following mutation (using the [RDF](./rdf)
notation):

```dql
{
  set {
    <_:jedi1> <character_name> "Luke Skywalker" .
    <_:leia> <character_name> "Leia" .
    <_:sith1> <character_name> "Anakin" (aka="Darth Vader",villain=true).
    <_:sith1> <has_for_child> <_:jedi1> .
    <_:sith1> <has_for_child> <_:leia> .
  }
}
```

In `strict` mode, the mutation returns an error if the predicates aren't present
in the Dgraph types schema.

In `flexible` mode, Dgraph executes the mutation and adds the predicates
`character_name` and `has_for_child` to the Dgraph types.

## Predicate types

All predicate types used in a Dgraph cluster are declared in the Dgraph schema.

The Dgraph types schema is the way to specify predicates types and cardinality
(if it's a list or not), to instruct Dgraph how to index predicates, and to
declare if Dgraph needs to maintain different languages for a string predicate.

A predicate type is either created

* by altering the Dgraph types schema (See
  [Update Dgraph types](/dgraph/admin/update-types) ) or
* during a mutation, if the Dgraph Cluster schema mode is `flexible` and the
  predicate used isn't yet declared.

  If a predicate type isn't declared in the schema, then the type is inferred
  from the first mutation and added to the schema.

  If the mutation is using [RDF format](./#rdf-types) with an RDF type, Dgraph
  uses this information to infer the predicate type.

  If no type can be inferred, the predicate type is set to `default`.

A predicate can hold a literal value ([Scalar type](#scalar-types)) or a
relation to another entity ([UID type](#uid-type)).

### Scalar types

For all triples with a predicate of scalar types the object is a literal.

| Dgraph Type | Go type                                                                                                                  |
| ----------- | :----------------------------------------------------------------------------------------------------------------------- |
| `default`   | string                                                                                                                   |
| `int`       | int64                                                                                                                    |
| `float`     | float                                                                                                                    |
| `string`    | string                                                                                                                   |
| `bool`      | bool                                                                                                                     |
| `dateTime`  | time.Time (RFC3339 format \[Optional timezone] eg: 2006-01-02T15:04:05.999999999+10:00 or 2006-01-02T15:04:05.999999999) |
| `geo`       | [go-geom](https://github.com/twpayne/go-geom)                                                                            |
| `password`  | string (encrypted)                                                                                                       |

<Note>
  Dgraph supports date and time formats for `dateTime` scalar type only if they
  are RFC 3339 compatible which is different from ISO 8601(as defined in the RDF
  spec). You should convert your values to RFC 3339 format before sending them
  to Dgraph.
</Note>

### Vector type

The `float32vector` type denotes a vector of floating point numbers, i.e an
ordered array of float32. A node type can contain more than one vector
predicate.

Vectors are normally used to store embeddings obtained from other information
through an ML model. When a `float32vector` is [indexed](#predicate-indexing),
the DQL [`similar_to`](./functions#vector-similarity-search) function can be
used for similarity search.

### UID type

The `uid` type denotes a relationship. Internally each node is identified by
it's UID which is a `uint64`.

### Predicate name rules

Any alphanumeric combination of a predicate name is permitted. Dgraph also
supports
[Internationalized Resource Identifiers](https://en.wikipedia.org/wiki/Internationalized_Resource_Identifier)
(IRIs). You can read more in [Predicates i18n](#predicates-i18n).

<Note>
  You can't define type names starting with `dgraph.`, it's reserved as the
  namespace for Dgraph's internal types/predicates. For example, defining
  `dgraph.Student` as a type is invalid.
</Note>

### Special characters

Following characters are accepted if prefixed/suffixed with alphanumeric
characters.

```txt
][&*()_-+=!#$%
```

<Note>
  You aren't restricted to use @ suffix, but the suffix character gets ignored.
</Note>

The special characters below aren't accepted.

```txt
^}|{`\~
```

### Predicates i18n

Dgraph supports
[Internationalized Resource Identifiers](https://en.wikipedia.org/wiki/Internationalized_Resource_Identifier)
(IRIs) for predicate names and values.

If your predicate is a URI or has language-specific characters, then enclose it
with angle brackets `<>` when executing the schema mutation.

Schema syntax:

```dql
<职业>: string @index(exact) .
<年龄>: int @index(int) .
<地点>: geo @index(geo) .
<公司>: string .
```

This syntax allows for internationalized predicate names, but full-text indexing
still defaults to English. To use the right tokenizer for your language, you
need to use the `@lang` directive and enter values using your language tag.

Schema:

```dql
<公司>: string @index(fulltext) @lang .
```

Mutation:

```dql
{
  set {
    _:a <公司> "Dgraph Labs Inc"@en .
    _:b <公司> "夏新科技有限责任公司"@zh .
    _:a <dgraph.type> "Company" .
  }
}
```

Query:

```dql
{
  q(func: alloftext(<公司>@zh, "夏新科技有限责任公司")) {
    uid
    <公司>@.
  }
}
```

### Unique directive

The unique constraint enables us to guarantee that all values of a predicate are
distinct. To implement the @unique directive for a predicate, you should define
it in the schema and create an index on the predicate based on its type. If a
user doesn't add the proper index to the predicate, then Dgraph returns an
error.

Dgraph automatically includes the @upsert directive for the predicate. To
enforce this uniqueness constraint, a predicate must have an index, as explained
below. Currently, we only support the @unique directive on newly created
predicates with the data types string and integer.

| Data Type | Index       |
| --------- | ----------- |
| string    | hash, exact |
| int       | int         |

This is how you define the unique directive for a predicate.

```dql
email: string @unique @index(exact)  .
```

### Upsert directive

To use [upsert operations](./upsert) on a predicate, specify the `@upsert`
directive in the schema.

When committing transactions involving predicates with the `@upsert` directive,
Dgraph checks index keys for conflicts, helping to enforce uniqueness
constraints when running concurrent upserts.

This is how you specify the upsert directive for a predicate.

```dql
email: string @index(exact) @upsert .
```

### NoConflict directive

The NoConflict directive prevents conflict detection at the predicate level.
This is an experimental feature and not a recommended directive but exists to
help avoid conflicts for predicates that don't have high correctness
requirements. This can cause data loss, especially when used for predicates with
count index.

This is how you specify the `@noconflict` directive for a predicate.

```dql
email: string @index(exact) @noconflict .
```

### Predicate types from RDF Types

As well as implying a schema type for a first mutation, an RDF type can override
a schema type for storage. Dgraph supports a number of [RDF](./rdf) types.

If a predicate has a schema type and a mutation has an RDF type with a different
underlying Dgraph type, the convertibility to schema type is checked, and an
error is thrown if incompatible, but the value is stored in the RDF type's
corresponding Dgraph type. Query results are always returned in schema type.

For example, if no schema is set for the `age` predicate. Given the mutation

```dql
{
 set {
  _:a <age> "15"^^<xs:int> .
  _:b <age> "13" .
  _:c <age> "14"^^<xs:string> .
  _:d <age> "14.5"^^<xs:string> .
  _:e <age> "14.5" .
 }
}
```

Dgraph:

* sets the schema type to `int`, as implied by the first triple,
* converts `"13"` to `int` on storage,
* checks `"14"` can be converted to `int`, but stores as `string`,
* throws an error for the remaining two triples, because `"14.5"` can't be
  converted to `int`.

### Password type

A password for an entity is set with setting the schema for the attribute to be
of type `password`. Passwords can't be queried directly, only checked for a
match using the `checkpwd` function. The passwords are encrypted using
[`bcrypt`](https://en.wikipedia.org/wiki/Bcrypt).

For example: to set a password, first set schema, then the password:

```dql
pass: password .
```

```dql
{
  set {
    <0x123> <name> "Password Example" .
    <0x123> <pass> "ThePassword" .
  }
}
```

to check a password:

```dql
{
  check(func: uid(0x123)) {
    name
    checkpwd(pass, "ThePassword")
  }
}
```

output:

```dql
{
  "data": {
    "check": [
      {
        "name": "Password Example",
        "checkpwd(pass)": true
      }
    ]
  }
}
```

You can also use alias with password type.

```dql
{
  check(func: uid(0x123)) {
    name
    secret: checkpwd(pass, "ThePassword")
  }
}
```

output:

```dql
{
  "data": {
    "check": [
      {
        "name": "Password Example",
        "secret": true
      }
    ]
  }
}
```

## Predicate indexing

The schema is also used to set predicates indexes, which are required to apply
[filtering functions](./functions) in DQL queries.

## Node types

Node types are declared along with [predicate types](#predicate-types) in the
Dgraph types schema.

Node types are optional.

### Node type definition

Node type declares the list of predicates that could be present in a Node of
this type. Node type are defined using the following syntax:

```dql
name: string @index(term) .
dob: datetime .
home_address: string .
friends: [uid] .

type Student {
  name
  dob
  home_address
  friends
}
```

<Note>
  All predicates used in a type must be defined in the Dgraph types schema
  itself.
</Note>

<Tip>Different node types can use the same predicates.</Tip>

### Reverse predicates

Reverse predicates can also be included inside a type definition. For example,
the following schema, declares that a node of type Child may have a `~children`
inverse relationship.

```dql
children: [uid] @reverse .
name: string @index(term) .
type Parent {
  name
  children
}
type Child {
  name
  <~children>
}
brackets `<>` </Tip>
```

### Node type attribution

A node is given a type by setting the `dgraph.type` predicate value to the type
name.

A node may be given many types, `dgraph.type` is an array of strings.

<Note>
  DQL types are only declarative and aren't enforced by Dgraph. In DQL, you can
  always add node without a `dgraph.type` predicate.
</Note>

Here's an example of mutation to set the types of a node:

```dql
{
  set {
    _:a <name> "Garfield" .
    _:a <dgraph.type> "Pet" .
    _:a <dgraph.type> "Animal" .
  }
}
```

### When to use node types

Node types are optional, but there are two use cases where actually knowing the
list of potential predicates of a node is necessary:

* deleting all the information about a node: this is the
  `delete { <uid> * * . }` mutation.
* retrieving all the predicates of a given node: this is done using the
  [expand(*all*)](./expand) feature of DQL.

The Dgraph node types are used in those 2 use cases: when executing the
`delete all predicates` mutation or the `expand all` query, Dgraph checks if the
node has a `dgraph.type` predicate. If so, the engine is using the declared type
to find the list of predicates and apply the delete or the expand on all of
them.

When nodes have a type (i.e have a `dgraph.type` predicate), then you can use
the function [type()](./query#node-criteria-used-by-root-function-or-by-filter)
in queries.

<Warning>
  `delete { <uid> * * . }` only deletes the
  predicates declared in the type. You may have added other predicates by running
  DQL mutation on this node: the node may still exist after the operation if it
  holds predicates not declared in the node type.
</Warning>

## Sample schema

The following pages are the language reference for DQL.

They contain examples that you can run interactively using a database of 21
million triples about movies and actors.

The queries are executed on an instance of Dgraph running at
[https://play.dgraph.io/](https://play.dgraph.io/).

The example movie database uses the following schema:

```dql
# Define Directives and index

director.film: [uid] @reverse .
actor.film: [uid] @count .
genre: [uid] @reverse .
initial_release_date: dateTime @index(year) .
name: string @index(exact, term) @lang .
starring: [uid] .
performance.film: [uid] .
performance.character_note: string .
performance.character: [uid] .
performance.actor: [uid] .
performance.special_performance_type: [uid] .
type: [uid] .

# Define Types

type Person {
    name
    director.film
    actor.film
}

type Movie {
    name
    initial_release_date
    genre
    starring
}

type Genre {
    name
}

type Performance {
    performance.film
    performance.character
    performance.actor
}
```


# shortest
Source: https://docs.hypermode.com/dgraph/dql/shortest



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The shortest path between a source (`from`) node and destination (`to`) node can
be found using the keyword `shortest` for the query block name. It requires the
source node UID, destination node UID and the predicates (at least one) that
have to be considered for traversal. A `shortest` query block returns the
shortest path under `_path_` in the query response. The path can also be stored
in a variable which is used in other query blocks.

## K-Shortest Path queries

By default the shortest path is returned. With `numpaths: k`, and `k > 1`, the
k-shortest paths are returned. Cyclical paths are pruned out from the result of
k-shortest path query. With `depth: n`, the paths up to `n` depth away are
returned.

<Note>
  If no predicates are specified in the `shortest` block, no path can be fetched
  as no edge is traversed.
</Note>

<Note>
  If you're seeing queries take a long time, you can set a [gRPC
  deadline](https://grpc.io/blog/deadlines) to stop the query after a certain
  amount of time.
</Note>

For example:

```sh
curl localhost:8080/alter -XPOST -d $'
    name: string @index(exact) .
' | python -m json.tool | less
```

```graphql
{
  set {
    _:a <friend> _:b (weight=0.1) .
    _:b <friend> _:c (weight=0.2) .
    _:c <friend> _:d (weight=0.3) .
    _:a <friend> _:d (weight=1) .
    _:a <name> "Alice" .
    _:a <dgraph.type> "Person" .
    _:b <name> "Bob" .
    _:b <dgraph.type> "Person" .
    _:c <name> "Tom" .
    _:c <dgraph.type> "Person" .
    _:d <name> "Mallory" .
    _:d <dgraph.type> "Person" .
  }
}
```

The shortest path between Alice and Mallory (assuming UIDs `0x2` and `0x5`
respectively) can be found with this query:

```graphql
{
 path as shortest(from: 0x2, to: 0x5) {
  friend
 }
 path(func: uid(path)) {
   name
 }
}
```

Which returns the following results.

<Note>
  without considering the `weight` facet, each edges' weight is considered as
  `1`
</Note>

```
{
  "data": {
    "path": [
      {
        "name": "Alice"
      },
      {
        "name": "Mallory"
      }
    ],
    "_path_": [
      {
        "uid": "0x2",
        "friend": [
          {
            "uid": "0x5"
          }
        ]
      }
    ]
  }
}
```

We can return more paths by specifying `numpaths`. Setting `numpaths: 2` returns
the shortest two paths:

```graphql
{

 A as var(func: eq(name, "Alice"))
 M as var(func: eq(name, "Mallory"))

 path as shortest(from: uid(A), to: uid(M), numpaths: 2) {
  friend
 }
 path(func: uid(path)) {
   name
 }
}
```

<Note>
  In the query above, instead of using UID literals, we query both people using
  var blocks and the `uid()` function. You can also combine it with [GraphQL
  Variables](./variables).
</Note>

## Edge weight

The shortest path implementation in Dgraph relies on facets to provide weights.
Using `facets` on the edges let you define the edges' weight as follows:

<Note>
  Only one facet per predicate is allowed in the shortest query block.
</Note>

```graphql
{
 path as shortest(from: 0x2, to: 0x5) {
  friend @facets(weight)
 }

 path(func: uid(path)) {
  name
 }
}
```

```
{
  "data": {
    "path": [
      {
        "name": "Alice"
      },
      {
        "name": "Bob"
      },
      {
        "name": "Tom"
      },
      {
        "name": "Mallory"
      }
    ],
    "_path_": [
      {
        "uid": "0x2",
        "friend": [
          {
            "uid": "0x3",
            "friend|weight": 0.1,
            "friend": [
              {
                "uid": "0x4",
                "friend|weight": 0.2,
                "friend": [
                  {
                    "uid": "0x5",
                    "friend|weight": 0.3
                  }
                ]
              }
            ]
          }
        ]
      }
    ]
  }
}
```

### Traverse example

Here is a graph traversal example that allows you to find the shortest path
between friends using a `Car` or a `Bus`.

<Tip>
  Car and Bus movement for each relation is modeled as facets and specified in
  the shortest query
</Tip>

```graphql
{
  set {
    _:a <friend> _:b (weightCar=10, weightBus=1 ) .
    _:b <friend> _:c (weightCar=20, weightBus=1) .
    _:c <friend> _:d (weightCar=11, weightBus=1.1) .
    _:a <friend> _:d (weightCar=70, weightBus=2) .
    _:a <name> "Alice" .
    _:a <dgraph.type> "Person" .
    _:b <name> "Bob" .
    _:b <dgraph.type> "Person" .
    _:c <name> "Tom" .
    _:c <dgraph.type> "Person" .
    _:d <name> "Mallory" .
    _:d <dgraph.type> "Person" .
  }
}
```

Query to find the shortest path relying on `Car` and `Bus`:

```graphql
{

 A as var(func: eq(name, "Alice"))
 M as var(func: eq(name, "Mallory"))

 sPathBus as shortest(from: uid(A), to: uid(M)) {
  friend
  @facets(weightBus)
 }

 sPathCar as shortest(from: uid(A), to: uid(M)) {
  friend
  @facets(weightCar)
 }

 pathBus(func: uid(sPathBus)) {
   name
 }

 pathCar(func: uid(sPathCar)) {
   name
 }
}
```

The response contains the following paths conforming to the specified weights:

```
    "pathBus": [
      {
        "name": "Alice"
      },
      {
        "name": "Mallory"
      }
    ],
    "pathCar": [
      {
        "name": "Alice"
      },
      {
        "name": "Bob"
      },
      {
        "name": "Tom"
      },
      {
        "name": "Mallory"
      }
    ]
```

## Constraints

Constraints can be applied to the intermediate nodes as follows.

```graphql
{
  path as shortest(from: 0x2, to: 0x5) {
    friend @filter(not eq(name, "Bob")) @facets(weight)
    relative @facets(liking)
  }

  relationship(func: uid(path)) {
    name
  }
}
```

The k-shortest path algorithm (used when `numpaths` > 1) also accepts the
arguments `minweight` and `maxweight`, which take a float as their value. When
they are passed, only paths within the weight range `[minweight, maxweight]`
will be considered as valid paths. This can be used, for example, to query the
shortest paths that traverse between 2 and 4 nodes.

```graphql
{
 path as shortest(from: 0x2, to: 0x5, numpaths: 2, minweight: 2, maxweight: 4) {
  friend
 }
 path(func: uid(path)) {
   name
 }
}
```

## Notes

Some points to keep in mind for shortest path queries:

* Weights must be non-negative. Dijkstra's algorithm is used to calculate the
  shortest paths.
* Only one facet per predicate in the shortest query block is allowed.
* Only one `shortest` path block is allowed per query. Only one `_path_` is
  returned in the result. For queries with `numpaths` > 1, `_path_` contains all
  the paths.
* Cyclical paths are not included in the result of k-shortest path query.
* For k-shortest paths (when `numpaths` > 1), the result of the shortest path
  query variable will only return a single path which will be the shortest path
  among the k paths. All k paths are returned in `_path_`.


# Sorting
Source: https://docs.hypermode.com/dgraph/dql/sorting



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Syntax Examples:

* `q(func: ..., orderasc: predicate)`
* `q(func: ..., orderdesc: val(varName))`
* `predicate (orderdesc: predicate) { ... }`
* `predicate @filter(...) (orderasc: N) { ... }`
* `q(func: ..., orderasc: predicate1, orderdesc: predicate2)`

Sortable Types: `int`, `float`, `String`, `dateTime`, `default`

Results can be sorted in ascending order (`orderasc`) or descending order
(`orderdesc`) by a predicate or variable.

For sorting on predicates with [sortable indices](./schema#sortable-indices),
Dgraph sorts on the values and with the index in parallel and returns whichever
result is computed first.

<Note>
  Dgraph returns `null` values at the end of the results, irrespective of their
  sort. This behavior is consistent across indexed and non-indexed sorts.
</Note>

<Tip>
  Sorted queries retrieve up to 1000 results by default. This can be changed
  with [first](./pagination#first).
</Tip>

Query Example: French director Jean-Pierre Jeunet's movies sorted by release
date.

```json
{
  me(func: allofterms(name@en, "Jean-Pierre Jeunet")) {
    name@fr
    director.film(orderasc: initial_release_date) {
      name@fr name@en
      initial_release_date
    }
  }
}
```

Sorting can be performed at root and on value variables.

Query Example: All genres sorted alphabetically and the five movies in each
genre with the most genres.

```json
{
  genres as var(func: has(~genre)) {
    ~genre { numGenres as count(genre) }
  }

  genres(func: uid(genres), orderasc: name@en) {
    name@en ~genre (orderdesc: val(numGenres), first: 5) { name@en genres : val(numGenres) }
  }
}
```

Sorting can also be performed by multiple predicates as shown below. If the
values are equal for the first predicate, then they are sorted by the second
predicate and so on.

Query Example: Find all nodes which have type Person, sort them by their
first\_name and among those that have the same first\_name sort them by last\_name
in descending order.

```json
{
  me(func: type("Person"), orderasc: first_name, orderdesc: last_name) {
    first_name last_name
  }
}
```

```
```


# Tips and Tricks
Source: https://docs.hypermode.com/dgraph/dql/tips



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## Get sample data

Use the `has` function to get some sample nodes.

```json
{
  result(func: has(director.film), first: 10) {
    uid
    expand(_all_)
  }
}
```

## Count number of connecting nodes

Use `expand(_all_)` to expand the nodes' edges, then assign them to a variable.
The variable can now be used to iterate over the unique neighboring nodes. Then
use `count(uid)` to count the number of nodes in a block.

```json
{
  uids(func: has(director.film), first: 1) {
    uid
    expand(_all_) {
      u as uid
    }
  }

  result(func: uid(u)) {
    count(uid)
  }
}
```

## Search on non-indexed predicates

Use the `has` function among the value variables to search on non-indexed
predicates.

```json
{
  var(func: has(festival.date_founded)) {
    p as festival.date_founded
  }

  query(func: eq(val(p), "1961-01-01T00:00:00Z")) {
    uid
    name@en
    name@ru
    name@pl
    festival.date_founded
    festival.focus {
      name@en
    }
    festival.individual_festivals {
      total : count(uid)
    }
  }
}
```

## Sort edge by nested node values

Dgraph [sorting](./sorting) is based on a single level of the subgraph. To sort
a level by the values of a deeper level, use
[query variables](./variables#query-variables) to bring nested values up to the
level of the edge to be sorted.

Example: get all actors from a Steven Spielberg movie sorted alphabetically. The
actor's name isn't accessed from a single traversal from the `starring` edge.
The name is accessible via `performance.actor`.

```json
{
  spielbergMovies as var(func: allofterms(name@en, "steven spielberg")) {
    name@en
    director.film (orderasc: name@en, first: 1) {
      starring {
        performance.actor {
          ActorName as name@en
        }
      }
      Stars as min(val(ActorName))
    }
  }

  movies(func: uid(spielbergMovies)) @cascade {
    name@en
    director.film (orderasc: name@en, first: 1) {
      name@en
      starring (orderasc: val(Stars)) {
        performance.actor {
          name@en
        }
      }
    }
  }
}
```

## Obtain unique results by using variables

To obtain unique results, assign the node's edge to a variable. The variable can
now be used to iterate over the unique nodes.

Example: get all unique genres from all of the movies directed by Steven
Spielberg.

```json
{
  var(func: eq(name@en, "Steven Spielberg")) {
    director.film {
      genres as genre
    }
  }

  q(func: uid(genres)) {
    name@.
  }
}
```

## Usage of `checkpwd` boolean

Store the result of `checkpwd` in a query variable and then match it against `1`
(`checkpwd` is `true`) or `0` (`checkpwd` is `false`).

```json
{
  exampleData(func: has(email)) {
    uid
    email
    check as checkpwd(pass, "1bdfhJHb!fd")
  }

  userMatched(func: eq(val(check), 1)) {
    uid
    email
  }

  userIncorrect(func: eq(val(check), 0)) {
    uid
    email
  }
}
```


# Upsert
Source: https://docs.hypermode.com/dgraph/dql/upsert



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Upsert-style operations are operations where:

1. A node is searched for, and then
2. Depending on if it's found or not, either:
   * Updating some of its attributes, or
   * Creating a new node with those attributes.

The upsert has to be an atomic operation such that either a new node is created,
or an existing node is modified. Two concurrent upserts can't both create a new
node.

There are many examples where upserts are useful. Most examples involve the
creation of a 1 to 1 mapping between two different entities. For example,
associating email addresses with user accounts.

Upserts are common in both traditional RDBMSs and newer NoSQL databases. Dgraph
is no exception.

## Upsert procedure

In Dgraph, upsert-style behavior can be implemented by users on top of
transactions. The steps are as follows:

1. Create a new transaction.

2. Query for the node. This usually is as simple as
   `{ q(func: eq(email,    "bob@example.com")) { uid }}`. If a `uid` result is
   returned, then that's the `uid` for the existing node. If no results are
   returned, then the user account doesn't exist.

3. In the case where the user account doesn't exist, then a new node has to be
   created. This is done in the usual way by making a mutation (inside the
   transaction), e.g. the RDF `_:newAccount <email> "bob@example.com" .`. The
   `uid` assigned can be accessed by looking up the blank node name `newAccount`
   in the `Assigned` object returned from the mutation.

4. Now that you have the `uid` of the account (either new or existing), you can
   modify the account (using additional mutations) or perform queries on it in
   whichever way you wish.

## Upserts in DQL and GraphQL

You can also use the `Upsert Block` in DQL to achieve the upsert procedure in a
single mutation. The request contains both the query and the mutation as
explained [here](/dgraph/dql/mutation#upsert-block).

In GraphQL, you can use the `upsert` input variable in an `add` mutation, as
explained [here](/dgraph/graphql/mutation/upsert).

## Conflicts

Upsert operations are intended to be run concurrently, as per the needs of the
app. As such, it's possible that two concurrently running operations could try
to add the same node at the same time. For example, both try to add a user with
the same email address. If they do, then one of the transactions will fail with
an error indicating that the transaction was aborted.

If this happens, the transaction is rolled back and it's up to the user's app
logic to retry the whole operation. The transaction has to be retried in its
entirety, all the way from creating a new transaction.

The choice of index placed on the predicate is important for performance. **Hash
is almost always the best choice of index for equality checking.**

<Note>
  It is the *index* that typically causes upsert conflicts to occur. The index
  is stored as many key/value pairs, where each key is a combination of the
  predicate name and some function of the predicate value (e.g. its hash for the
  hash index). If two transactions modify the same key concurrently, then one
  will fail.
</Note>

## `uid` function in upsert

The upsert block contains one query block and mutation blocks. Variables defined
in the query block can be used in the mutation blocks using the `uid` and `val`
function.

The `uid` function allows extracting UIDs from variables defined in the query
block. There are two possible outcomes based on the results of executing the
query block:

* If the variable is empty i.e. no node matched the query, the `uid` function
  returns a new UID in case of a `set` operation and is thus treated similar to
  a blank node. On the other hand, for `delete/del` operation, it returns no
  UID, and thus the operation becomes a no-op and is silently ignored. A blank
  node gets the same UID across all the mutation blocks.
* If the variable stores one or more than one UIDs, the `uid` function returns
  all the UIDs stored in the variable. In this case, the operation is performed
  on all the UIDs returned, one at a time.

### Example: `uid` function

Consider an example with the following schema:

```sh
curl localhost:8080/alter -X POST -d $'
  name: string @index(term) .
  email: string @index(exact, trigram) @upsert .
  age: int @index(int) .' | jq
```

Now, let's say we want to create a new user with `email` and `name` information.
We also want to make sure that one email has exactly one corresponding user in
the database. To achieve this, we need to first query whether a user exists in
the database with the given email. If a user exists, we use its UID to update
the `name` information. If the user doesn't exist, we create a new user and
update the `email` and `name` information.

We can do this using the upsert block as follows:

```sh
curl -H "Content-Type: application/rdf" -X POST localhost:8080/mutate?commitNow=true -d $'
upsert {
  query {
    q(func: eq(email, "user@company1.io")) {
      v as uid
      name
    }
  }

  mutation {
    set {
      uid(v) <name> "first last" .
      uid(v) <email> "user@company1.io" .
    }
  }
}' | jq
```

Result:

```json
{
  "data": {
    "q": [],
    "code": "Success",
    "message": "Done",
    "uids": {
      "uid(v)": "0x1"
    }
  },
  "extensions": {...}
}
```

The query part of the upsert block stores the UID of the user with the provided
email in the variable `v`. The mutation part then extracts the UID from variable
`v`, and stores the `name` and `email` information in the database. If the user
exists, the information is updated. If the user doesn't exist, `uid(v)` is
treated as a blank node and a new user is created as explained above.

If we run the same mutation again, the data would just be overwritten, and no
new uid is created. Note that the `uids` map is empty in the result when the
mutation is executed again and the `data` map (key `q`) contains the uid that
was created in the previous upsert.

```json
{
  "data": {
    "q": [
      {
        "uid": "0x1",
        "name": "first last"
      }
    ],
    "code": "Success",
    "message": "Done",
    "uids": {}
  },
  "extensions": {...}
}
```

We can achieve the same result using `json` dataset as follows:

```sh
curl -H "Content-Type: application/json" -X POST localhost:8080/mutate?commitNow=true -d '
{
  "query": "{ q(func: eq(email, \"user@company1.io\")) {v as uid, name} }",
  "set": {
    "uid": "uid(v)",
    "name": "first last",
    "email": "user@company1.io"
  }
}' | jq
```

Now, we want to add the `age` information for the same user having the same
email `user@company1.io`. We can use the upsert block to do the same as follows:

```sh
curl -H "Content-Type: application/rdf" -X POST localhost:8080/mutate?commitNow=true -d $'
upsert {
  query {
    q(func: eq(email, "user@company1.io")) {
      v as uid
    }
  }

  mutation {
    set {
      uid(v) <age> "28" .
    }
  }
}' | jq
```

Result:

```json
{
  "data": {
    "q": [
      {
        "uid": "0x1"
      }
    ],
    "code": "Success",
    "message": "Done",
    "uids": {}
  },
  "extensions": {...}
}
```

Here, the query block queries for a user with `email` as `user@company1.io`. It
stores the `uid` of the user in variable `v`. The mutation block then updates
the `age` of the user by extracting the uid from the variable `v` using `uid`
function.

We can achieve the same result using `json` dataset as follows:

```sh
curl -H "Content-Type: application/json" -X POST localhost:8080/mutate?commitNow=true -d $'
{
  "query": "{ q(func: eq(email, \\"user@company1.io\\")) {v as uid} }",
  "set":{
    "uid": "uid(v)",
    "age": "28"
  }
}' | jq
```

If we want to execute the mutation only when the user exists, we could use
[Conditional Upsert](./mutation#conditional-upsert).

### Example: Bulk delete

Let's say we want to delete all the users of `company1` from the database. This
can be achieved in just one query using the upsert block as follows:

```sh
curl -H "Content-Type: application/rdf" -X POST localhost:8080/mutate?commitNow=true -d $'
upsert {
  query {
    v as var(func: regexp(email, /.*@company1.io$/))
  }

  mutation {
    delete {
      uid(v) <name> * .
      uid(v) <email> * .
      uid(v) <age> * .
    }
  }
}' | jq
```

We can achieve the same result using `json` dataset as follows:

```sh
curl -H "Content-Type: application/json" -X POST localhost:8080/mutate?commitNow=true -d '{
  "query": "{ v as var(func: regexp(email, /.*@company1.io$/)) }",
  "delete": {
    "uid": "uid(v)",
    "name": null,
    "email": null,
    "age": null
  }
}' | jq
```

## `val` function in upsert

The upsert block allows performing queries and mutations in a single request.
The upsert block contains one query block and one or more than one mutation
blocks. Variables defined in the query block can be used in the mutation blocks
using the `uid` and `val` function.

The `val` function allows extracting values from value variables. Value
variables store a mapping from UIDs to their corresponding values. Hence,
`val(v)` is replaced by the value stored in the mapping for the UID (Subject) in
the N-Quad. If the variable `v` has no value for a given UID, the mutation is
silently ignored. The `val` function can be used with the result of aggregate
variables as well, in which case, all the UIDs in the mutation would be updated
with the aggregate value.

Let's say we want to migrate the predicate `age` to `other`. We can do this
using the following mutation:

```sh
curl -H "Content-Type: application/rdf" -X POST localhost:8080/mutate?commitNow=true -d $'
upsert {
  query {
    v as var(func: has(age)) {
      a as age
    }
  }

  mutation {
    # we copy the values from the old predicate
    set {
      uid(v) <other> val(a) .
    }

    # and we delete the old predicate
    delete {
      uid(v) <age> * .
    }
  }
}' | jq
```

Result:

```json
{
  "data": {
    "code": "Success",
    "message": "Done",
    "uids": {}
  },
  "extensions": {...}
}
```

Here, variable `a` will store a mapping from all the UIDs to their `age`. The
mutation block then stores the corresponding value of `age` for each UID in the
`other` predicate and deletes the `age` predicate.

We can achieve the same result using `json` dataset as follows:

```sh
curl -H "Content-Type: application/json" -X POST localhost:8080/mutate?commitNow=true -d $'{
  "query": "{ v as var(func: regexp(email, /.*@company1.io$/)) }",
  "delete": {
    "uid": "uid(v)",
    "age": null
  },
  "set": {
    "uid": "uid(v)",
    "other": "val(a)"
  }
}' | jq
```

## External IDs and Upsert Block

The upsert block makes managing external IDs easy.

Set the schema.

```
xid: string @index(exact) .
<http://schema.org/name>: string @index(exact) .
<http://schema.org/type>: [uid] @reverse .
```

Set the type first of all.

```
{
  set {
    _:blank <xid> "http://schema.org/Person" .
    _:blank <dgraph.type> "ExternalType" .
  }
}
```

Now you can create a new person and attach its type using the upsert block.

```
   upsert {
      query {
        var(func: eq(xid, "http://schema.org/Person")) {
          Type as uid
        }
        var(func: eq(<http://schema.org/name>, "Robin Wright")) {
          Person as uid
        }
      }
      mutation {
          set {
           uid(Person) <xid> "https://www.themoviedb.org/person/32-robin-wright" .
           uid(Person) <http://schema.org/type> uid(Type) .
           uid(Person) <http://schema.org/name> "Robin Wright" .
           uid(Person) <dgraph.type> "Person" .
          }
      }
    }
```

You can also delete a person and detach the relation between Type and Person
Node. It is the same as above, but you use the keyword "delete" instead of
"set". "`http://schema.org/Person`" will remain but "`Robin Wright`" will be
deleted.

```
   upsert {
      query {
        var(func: eq(xid, "http://schema.org/Person")) {
          Type as uid
        }
        var(func: eq(<http://schema.org/name>, "Robin Wright")) {
          Person as uid
        }
      }
      mutation {
          delete {
           uid(Person) <xid> "https://www.themoviedb.org/person/32-robin-wright" .
           uid(Person) <http://schema.org/type> uid(Type) .
           uid(Person) <http://schema.org/name> "Robin Wright" .
           uid(Person) <dgraph.type> "Person" .
          }
      }
    }
```

Query by user.

```
{
  q(func: eq(<http://schema.org/name>, "Robin Wright")) {
    uid
    xid
    <http://schema.org/name>
    <http://schema.org/type> {
      uid
      xid
    }
  }
}
```


# Variables
Source: https://docs.hypermode.com/dgraph/dql/variables



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## Query variables

Syntax Examples:

* `varName as q(func: ...) { ... }`
* `varName as var(func: ...) { ... }`
* `varName as predicate { ... }`
* `varName as predicate @filter(...) { ... }`

Types : `uid`

Nodes (UIDs) matched at one place in a query can be stored in a variable and
used elsewhere. Query variables can be used in other query blocks or in a child
node of the defining block.

Query variables do not affect the semantics of the query at the point of
definition. Query variables are evaluated to all nodes matched by the defining
block.

In general, query blocks are executed in parallel, but variables impose an
evaluation order on some blocks. Cycles induced by variable dependence are not
permitted.

If a variable is defined, it must be used elsewhere in the query.

A query variable is used by extracting the UIDs in it with `uid(var-name)`.

The syntax `func: uid(A,B)` or `@filter(uid(A,B))` means the union of UIDs for
variables `A` and `B`.

Query Example: the movies of Angelia Jolie and Brad Pitt where both have acted
on movies in the same genre. Note that `B` and `D` match all genres for all
movies, not genres per movie.

```json
{
  var(func:allofterms(name@en, "angelina jolie")) {
    actor.film {
        A AS performance.film # All films acted in by Angelina Jolie
        B As genre # Genres of all the films acted in by Angelina Jolie
    }
  }

  var(func:allofterms(name@en, "brad pitt")) {
    actor.film {
        C AS performance.film # All films acted in by Brad Pitt
        D as genre # Genres of all the films acted in by Brad Pitt
    }
  }

  films(func: uid(D)) @filter(uid(B)) { # Genres from both Angelina and Brad
    name@en ~genre @filter(uid(A, C)) { # Movies in either A or C. name@en }
  }
}
```

## Value variables

Syntax Examples:

* `varName as scalarPredicate`
* `varName as count(predicate)`
* `varName as avg(...)`
* `varName as math(...)`

Types : `int`, `float`, `String`, `dateTime`, `default`, `geo`, `bool`

Value variables store scalar values. Value variables are a map from the UIDs of
the enclosing block to the corresponding values.

It therefore only makes sense to use the values from a value variable in a
context that matches the same UIDs - if used in a block matching different UIDs
the value variable is undefined.

It is an error to define a value variable but not use it elsewhere in the query.

Value variables are used by extracting the values with `val(var-name)`, or by
extracting the UIDs with `uid(var-name)`.

[Facet](./facets) values can be stored in value variables.

Query Example: the number of movie roles played by the actors of the 80's
classic "The Princess Bride". Query variable `pbActors` matches the UIDs of all
actors from the movie. Value variable `roles` is thus a map from actor UID to
number of roles. Value variable `roles` can be used in the `totalRoles` query
block because that query block also matches the `pbActors` UIDs, so the actor to
number of roles map is available.

```json
{
  var(func: allofterms(name@en, "The Princess Bride")) {
    starring {
      pbActors as performance.actor {
        roles as count(actor.film)
      }
    }
  }
  totalRoles(func: uid(pbActors), orderasc: val(roles)) {
    name@en
    numRoles : val(roles)
  }
}
```

Value variables can be used in place of UID variables by extracting the UID list
from the map.

Query Example: the same query as the previous example, but using value variable
`roles` for matching UIDs in the `totalRoles` query block.

````json
{
  var(func: allofterms(name@en, "The Princess Bride")) {
    starring {
      performance.actor {
        roles as count(actor.film)
      }
    }
  }

  totalRoles(func: uid(roles), orderasc: val(roles)) {
    name@en
    numRoles : val(roles)
  }
}

## Variable propagation

Like query variables, value variables can be used in other query blocks and in
blocks nested within the defining block. When used in a block nested within the
block that defines the variable, the value is computed as a sum of the variable
for parent nodes along all paths to the point of use. This is called variable
propagation.

For example:

```json
{
  q(func: uid(0x01)) {
    myscore as math(1) # A
    friends { # B
      ...myscore...
    }
  }
}
````

At line A, a value variable `myscore` is defined as mapping node with UID `0x01`
to value 1. At B, the value for each friend is still 1: there is only one path
to each friend. Traversing the friend edge twice reaches the friends of friends.
The variable `myscore` gets propagated such that each friend of friend receives
the sum of its parents values: if a friend of a friend is reachable from only
one friend, the value is still 1, if they're reachable from two friends, the
value is two and so on. That is, the value of `myscore` for each friend of
friends inside the block marked C will be the number of paths to them.

**The value that a node receives for a propagated variable is the sum of the
values of all its parent nodes.**

This propagation is useful, for example, in normalizing a sum across users,
finding the number of paths between nodes and accumulating a sum through a
graph.

Query Example: for each Harry Potter movie, the number of roles played by actor
Warwick Davis.

```json
{
  num_roles(func: eq(name@en, "Warwick Davis")) @cascade @normalize {
    paths as math(1)  # records number of paths to each character
    paths as math(1)  # records number of paths to each character

    actor : name@en

    actor.film {
      performance.film @filter(allofterms(name@en, "Harry Potter")) {
        film_name : name@en
        characters : math(paths)  # how many paths (i.e. characters) reach this film
      }
    }

}
```

Query Example: each actor who has been in a Peter Jackson movie and the fraction
of Peter Jackson movies they have appeared in.

```json
{
  movie_fraction(func:eq(name@en, "Peter Jackson")) @normalize {

    paths as math(1)
    total_films : num_films as count(director.film)
    director : name@en

    director.film {
      starring {
        performance.actor {
          fraction : math(paths / (num_films/paths))
          actor : name@en
        }
      }
    }

}
```

More examples can be found in two Dgraph blog posts about using variable
propagation for recommendation engines
([post 1](https://hypermode.com/blog/recommendation/),
[post 2](https://hypermode.com/blog/recommendation2/)).

## Math on value variables

Value variables can be combined using mathematical functions. For example, this
could be used to associate a score which is then used to order or perform other
operations, such as might be used in building news feeds, simple recommendation
systems, and so on.

Math statements must be enclosed within `math( <exp> )` and must be stored to a
value variable.

The supported operators are as follows:

|             Operators            |                   Types accepted                  |                          What it does                          |
| :------------------------------: | :-----------------------------------------------: | :------------------------------------------------------------: |
|        `+` `-` `*` `/` `%`       |                   `int`, `float`                  |              performs the corresponding operation              |
|            `min` `max`           | All types except `geo`, `bool` (binary functions) |             selects the min/max value among the two            |
|    `<` `>` `<=` `>=` `==` `!=`   |           All types except `geo`, `bool`          |            Returns true or false based on the values           |
| `floor` `ceil` `ln` `exp` `sqrt` |          `int`, `float` (unary function)          |              performs the corresponding operation              |
|              `since`             |                     `dateTime`                    | Returns the number of seconds in float from the time specified |
|            `pow(a, b)`           |                   `int`, `float`                  |                   Returns `a to the power b`                   |
|          `logbase(a,b)`          |                   `int`, `float`                  |                Returns `log(a)` to the base `b`                |
|          `cond(a, b, c)`         |          first operand must be a Boolean          |               selects `b` if `a` is true else `c`              |

<Note>
  If an integer overflow occurs, or an operand is passed to a math operation
  (such as `ln`, `logbase`, `sqrt`, `pow`) which results in an illegal
  operation, Dgraph will return an error.
</Note>

Query Example: Form a score for each of Steven Spielberg's movies as the sum of
number of actors, number of genres and number of countries. List the top five
such movies in order of decreasing score.

```json
{
  var(func:allofterms(name@en, "steven spielberg")) {
    films as director.film {
        p as count(starring)
        q as count(genre)
        r as count(country)
        score as math(p + q + r)
    }
  }

  TopMovies(func: uid(films), orderdesc: val(score), first: 5){
    name@en
    val(score)
  }
}
```

Value variables and aggregations of them can be used in filters.

Query Example: Calculate a score for each Steven Spielberg movie with a
condition on release date to penalize movies that are more than 20 years old,
filtering on the resulting score.

```json
{
  var(func:allofterms(name@en, "steven spielberg")) {
    films as director.film {
        p as count(starring)
        q as count(genre)
        date as initial_release_date
        years as math(since(date)/(365*24*60\*60))
        score as math(cond(years > 20, 0, ln(p)+q-ln(years)))
    }
  }

  TopMovies(func: uid(films), orderdesc: val(score)) @filter(gt(val(score), 2)){
    name@en val(score) val(date)
  }
}
```

Values calculated with math operations are stored to value variables and so can
be aggregated.

Query Example: Compute a score for each Steven Spielberg movie and then
aggregate the score.

```json
{
  steven as var(func:eq(name@en, "Steven Spielberg")) @filter(has(director.film)) {
    director.film { p as count(starring) q as count(genre) r as count(country)
      score as math(p + q + r)
    }
    directorScore as sum(val(score))
  }
  score(func: uid(steven)) { name@en val(directorScore) }
}
```


# Access Control Lists
Source: https://docs.hypermode.com/dgraph/enterprise/access-control-lists



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

<Note>
  This feature was introduced in
  [v1.1.0](https://github.com/hypermodeinc/dgraph/releases/tag/v1.1.0). The
  `dgraph acl` command is deprecated and will be removed in a future release.
  ACL changes can be made by using the `/admin` GraphQL endpoint on any Alpha
  node.
</Note>

Access Control List (ACL) provides access protection to your data stored in
Dgraph. When the ACL feature is enabled, a client, e.g.
[dgo](https://github.com/hypermodeinc/dgo) or
[dgraph4j](https://github.com/hypermodeinc/dgraph4j), must authenticate with a
username and password before executing any transactions, and is only allowed to
access the data permitted by the ACL rules.

## Enable enterprise ACL feature

1. Generate a data encryption key that is 32 bytes long:

   ```sh
   tr -dc 'a-zA-Z0-9' < /dev/urandom | dd bs=1 count=32 of=enc_key_file
   ```

   <Note>On a macOS you may have to use `LC_CTYPE=C; tr -dc 'a-zA-Z0-9' < /dev/urandom | dd bs=1 count=32 of=enc_key_file`.</Note>

2. To view the secret key value use `cat enc_key_file`.

3. Create a plain text file named `hmac_secret_file`, and store a randomly
   generated `<SECRET KEY VALUE>` in it. The secret key is used by Dgraph Alpha
   nodes to sign JSON Web Tokens (JWT).

   ```sh
   echo '<SECRET KEY VALUE>' > hmac_secret_file
   ```

4. Start all the Dgraph Alpha nodes in your cluster with the option
   `--acl secret-file="/path/to/secret"`, and make sure that they use the same
   secret key file created in Step 1. Alternatively, you can
   [store the secret in HashiCorp Vault](#storing-acl-secret-in-hashicorp-vault).

   ```sh
   dgraph alpha --acl "secret-file=/path/to/secret" --security "whitelist=<permitted-ip-addresses>"
   ```

<Tip>
  In addition to command line flags
  `--acl secret-file="/path/to/secret"` and
  `--security "whitelist=<permitted-ip-addresses>"`, you can also configure Dgraph
  using a configuration file (`config.yaml`, `config.json`). You can also use
  environment variables such as `DGRAPH_ALPHA_ACL="secret-file=</path/to/secret>"`
  and `DGRAPH_ALPHA_SECURITY="whitelist=<permitted-ip-addresses>"`. See
  [Config](/dgraph/self-managed/config) for more information in general about configuring
  Dgraph.
</Tip>

### Example using Dgraph CLI

Here is an example that starts a Dgraph Zero node and a Dgraph Alpha node with
the ACL feature turned on. You can run these commands in a separate terminal
tab:

```sh
## Create ACL secret key file with 32 ASCII characters
echo '<SECRET KEY VALUE>' > hmac_secret_file

## Start Dgraph Zero in different terminal tab or window
dgraph zero --my=localhost:5080 --replicas 1 --raft idx=1

## Start Dgraph Alpha in different terminal tab or window
dgraph alpha --my=localhost:7080 --zero=localhost:5080 \
  --acl secret-file="./hmac_secret_file" \
  --security whitelist="10.0.0.0/8,172.0.0.0/8,192.168.0.0/16"
```

### Example using Docker Compose

If you are using [Docker Compose](https://docs.docker.com/compose/), you can set
up a sample Dgraph cluster using this `docker-compose.yaml` configuration:

```yaml
version: "3.5"
services:
  alpha1:
    command: dgraph alpha --my=alpha1:7080 --zero=zero1:5080
    container_name: alpha1
    environment:
      DGRAPH_ALPHA_ACL: secret-file=/dgraph/acl/hmac_secret_file
      DGRAPH_ALPHA_SECURITY: whitelist=10.0.0.0/8,172.0.0.0/8,192.168.0.0/16
    image: dgraph/dgraph:latest
    ports:
      - "8080:8080"
    volumes:
      - ./hmac_secret_file:/dgraph/acl/hmac_secret_file
  zero1:
    command: dgraph zero --my=zero1:5080 --replicas 1 --raft idx=1
    container_name: zero1
    image: dgraph/dgraph:latest
```

You can run this with:

```sh
## Create ACL secret key file with 32 ASCII characters
echo '<SECRET KEY VALUE>' > hmac_secret_file

## Start Docker Compose
docker-compose up
```

### Example using Kubernetes Helm Chart

If you deploy Dgraph on [Kubernetes](https://kubernetes.io/), you can configure
the ACL feature using the
[Dgraph Helm Chart](https://artifacthub.io/packages/helm/dgraph/dgraph).

The first step is to encode the secret with base64:

```sh
## encode a secret without newline character and copy to the clipboard
printf '<SECRET KEY VALUE>' | base64
```

The next step is that we need to create a [Helm](https://helm.sh/) chart config
values file, e.g. `dgraph_values.yaml`. We want to copy the results of encoded
secret as paste this into the `hmac_secret_file` like the example below:

```yaml
## dgraph_values.yaml
alpha:
  acl:
    enabled: true
    file:
      hmac_secret_file: <SECRET KEY VALUE>
  configFile:
    config.yaml: |
      acl:
        secret_file: /dgraph/acl/hmac_secret_file
      security:
        whitelist: 10.0.0.0/8,172.0.0.0/8,192.168.0.0/16
```

Now with the Helm chart config values created, we can deploy Dgraph:

```sh
helm repo add "dgraph" https://charts.dgraph.io
helm install "my-release" --values ./dgraph_values.yaml dgraph/dgraph
```

## Storing ACL secret in HashiCorp Vault

You can save the ACL secret on [HashiCorp Vault](https://www.vaultproject.io/)
server instead of saving the secret on the local file system.

### Configuring a HashiCorp Vault server

Do the following to set up on the
[HashiCorp Vault](https://www.vaultproject.io/) server for use with Dgraph:

1. Ensure that the Vault server is accessible from Dgraph Alpha and configured
   using URL `http://fqdn[ip]:port`.

2. Enable [AppRole Auth method](https://www.vaultproject.io/docs/auth/approle)
   and enable [KV Secrets Engine](https://www.vaultproject.io/docs/secrets/kv).

3. Save the 256-bits (32 ASCII characters) long ACL secret in a KV Secret path
   ([K/V Version 1](https://www.vaultproject.io/docs/secrets/kv/kv-v1) or
   [K/V Version 2](https://www.vaultproject.io/docs/secrets/kv/kv-v2)). For
   example, you can upload this below to KV Secrets Engine Version 2 path of
   `secret/data/dgraph/alpha`:

   ```json
   {
     "options": {
       "cas": 0
     },
     "data": {
       "hmac_secret_file": "<SECRET KEY VALUE>"
     }
   }
   ```

4. Create or use a role with an attached policy that grants access to the
   secret. For example, the following policy would grant access to
   `secret/data/dgraph/alpha`:

   ```hcl
   path "secret/data/dgraph/*" {
     capabilities = [ "read", "update" ]
   }
   ```

5. Using the `role_id` generated from the previous step, create a corresponding
   `secret_id`, and copy the `role_id` and `secret_id` over to local files, like
   `./dgraph/vault/role_id` and `./dgraph/vault/secret_id`, that will be used by
   Dgraph Alpha nodes.

<Note>
  The key format for the `acl-field` option can be defined using `acl-format`
  with the values `base64` (default) or `raw`.
</Note>

### Example using Dgraph CLI with HashiCorp Vault configuration

Here is an example of using Dgraph with a Vault server that holds the secret
key:

```sh
## Start Dgraph Zero in different terminal tab or window
dgraph zero --my=localhost:5080 --replicas 1 --raft "idx=1"

## Start Dgraph Alpha in different terminal tab or window
dgraph alpha \
  --security whitelist="10.0.0.0/8,172.0.0.0/8,192.168.0.0/16" \
  --vault addr="http://localhost:8200";acl-field="hmac_secret_file";acl-format="raw";path="secret/data/dgraph/alpha";role-id-file="./role_id";secret-id-file="./secret_id"

```

### Example using Docker Compose with HashiCorp Vault configuration

If you are using [Docker Compose](https://docs.docker.com/compose/), you can set
up a sample Dgraph cluster using this `docker-compose.yaml` configuration:

```yaml
version: "3.5"
services:
  alpha1:
    command: dgraph alpha --my=alpha1:7080 --zero=zero1:5080
    container_name: alpha1
    environment:
      DGRAPH_ALPHA_VAULT: addr=http://vault:8200;acl-field=hmac_secret_file;acl-format=raw;path=secret/data/dgraph/alpha;role-id-file=/dgraph/vault/role_id;secret-id-file=/dgraph/vault/secret_id
      DGRAPH_ALPHA_SECURITY: whitelist=10.0.0.0/8,172.0.0.0/8,192.168.0.0/16
    image: dgraph/dgraph:latest
    ports:
      - "8080:8080"
    volumes:
      - ./role_id:/dgraph/vault/role_id
      - ./secret_id:/dgraph/vault/secret_id
  zero1:
    command: dgraph zero --my=zero1:5080 --replicas 1 --raft idx=1
    container_name: zero1
    image: dgraph/dgraph:latest
```

In this example, you will also need to configure a
[HashiCorp Vault](https://www.vaultproject.io/) service named `vault` in the
above `docker-compose.yaml`, and then run through this sequence:

1. Launch `vault` service: `docker-compose up --detach vault`
2. Unseal and Configure `vault` with the required prerequisites (see
   [Configuring a HashiCorp Vault Server](#configuring-a-hashicorp-vault-server)).
3. Save role-id and secret-id as `./role_id` and `secret_id`
4. Launch Dgraph Zero and Alpha: `docker-compose up --detach`

### Example using Kubernetes Helm Chart with HashiCorp Vault configuration

If you deploy Dgraph on [Kubernetes](https://kubernetes.io/), you can configure
the ACL feature using the
[Dgraph Helm Chart](https://artifacthub.io/packages/helm/dgraph/dgraph).

The next step is that we need to create a [Helm](https://helm.sh/) chart config
values file, such as `dgraph_values.yaml`.

```yaml
## dgraph_values.yaml
alpha:
  configFile:
    config.yaml: |
      vault:
        addr: http://vault-headless.default.svc.cluster.local:9200
        acl_field: hmac_secret_file
        acl_format: raw
        path: secret/data/dgraph/alpha
        role_id_file: /dgraph/vault/role_id
        secret_id_file: /dgraph/vault/secret_id
      security:
        whitelist: 10.0.0.0/8,172.0.0.0/8,192.168.0.0/16‘
```

To set up this chart, the [HashiCorp Vault](https://www.vaultproject.io/)
service must be installed and available. You can use the
[HashiCorp Vault Helm Chart](https://www.vaultproject.io/docs/platform/k8s/helm)
and configure it to
[auto unseal](https://learn.hashicorp.com/collections/vault/auto-unseal) so that
the service is immediately available after deployment.

## Accessing secured Dgraph

Before managing users and groups and configuring ACL rules, you will need to
login in order to get a token that is needed to access Dgraph. You will use this
token with the `X-Dgraph-AccessToken` header field.

### Logging In

To login, send a POST request to `/admin` with the GraphQL mutation. For
example, to log in as the root user `groot`:

```graphql
mutation {
  login(userId: "groot", password: "password") {
    response {
      accessJWT
      refreshJWT
    }
  }
}
```

Response:

```json
{
  "data": {
    "accessJWT": "<accessJWT>",
    "refreshJWT": "<refreshJWT>"
  }
}
```

#### Access Token

The response includes the access and refresh JWTs which are used for the
authentication itself and refreshing the authentication token, respectively.
Save the JWTs from the response for later HTTP requests.

You can run authenticated requests by passing the access JWT to a request via
the `X-Dgraph-AccessToken` header. Add the header `X-Dgraph-AccessToken` with
the `accessJWT` value which you got in the login response in the GraphQL tool
which you're using to make the request.

For example, if you were using the GraphQL Playground, you would add this in the
headers section:

```json
{ "X-Dgraph-AccessToken": "<accessJWT>" }
```

And in the main code section, you can add a mutation, such as:

```graphql
mutation {
  addUser(input: [{ name: "alice", password: "whiterabbit" }]) {
    user {
      name
    }
  }
}
```

#### Refresh Token

The refresh token can be used in the `/admin` POST GraphQL mutation to receive
new access and refresh JWTs, which is useful to renew the authenticated session
once the ACL access TTL expires (controlled by Dgraph Alpha's flag
`--acl_access_ttl` which is set to 6h0m0s by default).

```graphql
mutation {
  login(userId: "groot", password: "password", refreshToken: "<refreshJWT>") {
    response {
      accessJWT
      refreshJWT
    }
  }
}
```

### Login using a client

With ACL configured, you need to log in as a user to access data protected by
ACL rules. You can do this using the client's `.login(USER_ID, USER_PASSWORD)`
method.

Here are some code samples using a client:

* **Go** ([dgo client](https://github.com/hypermodeinc/dgo)): example
  `acl_over_tls_test.go`
  ([here](https://github.com/hypermodeinc/dgraph/blob/main/tlstest/acl/acl_over_tls_test.go))
* **Java** ([dgraph4j](https://github.com/hypermodeinc/dgraph4j)): example
  `AclTest.java`
  ([here](https://github.com/hypermodeinc/dgraph4j/blob/main/src/test/java/io/dgraph/AclTest.java))

### Login using curl

If you are using `curl` from the command line, you can use the following with
the above [login mutation](#logging-in) saved to `login.graphql`:

```sh
## Login and save results
JSON_RESULT=$(curl http://localhost:8080/admin --silent --request POST \
  --header "Content-Type: application/graphql" \
  --upload-file login.graphql
)

## Extracting a token using GNU grep, perl, the silver searcher, or jq
TOKEN=$(grep -oP '(?<=accessJWT":")[^"]*' <<< $JSON_RESULT)
TOKEN=$(perl -wln -e '/(?<=accessJWT":")[^"]*/ and print $&;' <<< $JSON_RESULT)
TOKEN=$(ag -o '(?<=accessJWT":")[^"]*' <<< $JSON_RESULT)
TOKEN=$(jq -r '.data.login.response.accessJWT' <<< $JSON_RESULT)

## Run a GraphQL query using the token
curl http://localhost:8080/admin --silent --request POST \
  --header "Content-Type: application/graphql" \
  --header "X-Dgraph-AccessToken: $TOKEN" \
  --upload-file some_other_query.graphql
```

<Tip>
  Parsing JSON results on the command line can be challenging, so you will find
  some alternatives to extract the desired data using popular tools, such as
  [the silver searcher](https://github.com/ggreer/the_silver_searcher) or the
  JSON query tool [jq](https://stedolan.github.io/jq), embedded in this snippet.
</Tip>

## User and group administration

The default configuration comes with a user `groot`, with a password of
`password`. The `groot` user is part of administrative group called `guardians`
that have access to everything. You can add more users to the `guardians` group
as needed.

### Reset the root password

You can reset the root password like this example:

```graphql
mutation {
  updateUser(
    input: {
      filter: { name: { eq: "groot" } }
      set: { password: "$up3r$3cr3t1337p@$$w0rd" }
    }
  ) {
    user {
      name
    }
  }
}
```

### Create a regular user

To create a user `alice`, with password `whiterabbit`, you should execute the
following GraphQL mutation:

```graphql
mutation {
  addUser(input: [{ name: "alice", password: "whiterabbit" }]) {
    user {
      name
    }
  }
}
```

### Create a group

To create a group `dev`, you should execute:

```graphql
mutation {
  addGroup(input: [{ name: "dev" }]) {
    group {
      name
      users {
        name
      }
    }
  }
}
```

### Assign a user to a group

To assign the user `alice` to both the group `dev` and the group `sre`, the
mutation should be

```graphql
mutation {
  updateUser(
    input: {
      filter: { name: { eq: "alice" } }
      set: { groups: [{ name: "dev" }, { name: "sre" }] }
    }
  ) {
    user {
      name
      groups {
        name
      }
    }
  }
}
```

### Remove a user from a group

To remove `alice` from the `dev` group, the mutation should be

```graphql
mutation {
  updateUser(
    input: {
      filter: { name: { eq: "alice" } }
      remove: { groups: [{ name: "dev" }] }
    }
  ) {
    user {
      name
      groups {
        name
      }
    }
  }
}
```

### Delete a User

To delete the user `alice`, you should execute

```graphql
mutation {
  deleteUser(filter: { name: { eq: "alice" } }) {
    msg
    numUids
  }
}
```

### Delete a Group

To delete the group `sre`, the mutation should be

```graphql
mutation {
  deleteGroup(filter: { name: { eq: "sre" } }) {
    msg
    numUids
  }
}
```

## ACL rules configuration

You can set up ACL rules using the Dgraph Ratel UI or by using a GraphQL tool,
such as [Insomnia](https://insomnia.rest/),
[GraphQL Playground](https://github.com/prisma/graphql-playground),
[GraphiQL](https://github.com/skevy/graphiql-app), etc. You can set the
permissions on a predicate for the group using a pattern similar to the UNIX
file permission conventions shown below:

| Permission                  | Value | Binary |
| --------------------------- | ----- | ------ |
| `READ`                      | `4`   | `100`  |
| `WRITE`                     | `2`   | `010`  |
| `MODIFY`                    | `1`   | `001`  |
| `READ` + `WRITE`            | `6`   | `110`  |
| `READ` + `WRITE` + `MODIFY` | `7`   | `111`  |

These permissions represent the following:

* `READ` - group has permission to read read the predicate
* `WRITE` - group has permission to write or update the predicate
* `MODIFY` - group has permission to change the predicate's schema

The following examples will grant full permissions to predicates to the group
`dev`. If there are no rules for a predicate, the default behavior is to block
all (`READ`, `WRITE` and `MODIFY`) operations.

### Assign predicate permissions to a group

Here we assign a permission rule for the `friend` predicate to the group:

```graphql
mutation {
  updateGroup(
    input: {
      filter: { name: { eq: "dev" } }
      set: { rules: [{ predicate: "friend", permission: 7 }] }
    }
  ) {
    group {
      name
      rules {
        permission
        predicate
      }
    }
  }
}
```

In case you have [reverse edges](/dgraph/dql/schema#reverse-edges), they have to
be given the permission to the group as well

```graphql
mutation {
  updateGroup(
    input: {
      filter: { name: { eq: "dev" } }
      set: { rules: [{ predicate: "~friend", permission: 7 }] }
    }
  ) {
    group {
      name
      rules {
        permission
        predicate
      }
    }
  }
}
```

In some cases, it may be desirable to manage permissions for all the predicates
together rather than individual ones. This can be achieved using the
`dgraph.all` keyword.

The following example provides `read+write` access to the `dev` group over all
the predicates of a given namespace using the `dgraph.all` keyword.

```graphql
mutation {
  updateGroup(
    input: {
      filter: { name: { eq: "dev" } }
      set: { rules: [{ predicate: "dgraph.all", permission: 6 }] }
    }
  ) {
    group {
      name
      rules {
        permission
        predicate
      }
    }
  }
}
```

<Note>
  The permissions assigned to a group `dev` is the union of permissions from
  `dgraph.all` and permissions for a specific predicate `name`. So if the group
  is assigned `READ` permission for `dgraph.all` and `WRITE` permission for
  predicate `name` it will have both, `READ` and `WRITE` permissions for the
  `name` predicate, as a result of the union.
</Note>

### Remove a rule from a group

To remove a rule or rules from the group `dev`, the mutation should be:

```graphql
mutation {
  updateGroup(
    input: {
      filter: { name: { eq: "dev" } }
      remove: { rules: ["friend", "~friend"] }
    }
  ) {
    group {
      name
      rules {
        predicate
        permission
      }
    }
  }
}
```

## Querying users and groups

You can set up ACL rules using the Dgraph Ratel UI or by using a GraphQL tool,
such as [Insomnia](https://insomnia.rest/),
[GraphQL Playground](https://github.com/prisma/graphql-playground),
[GraphiQL](https://github.com/skevy/graphiql-app), etc. The permissions can be
set on a predicate for the group using using pattern similar to the UNIX file
permission convention:

You can query and get information for users and groups. These sections show
output that will show the user `alice` and the `dev` group along with rules for
`friend` and `~friend` predicates.

### Query for users

Let's query for the user `alice`:

```graphql
query {
  queryUser(filter: { name: { eq: "alice" } }) {
    name
    groups {
      name
    }
  }
}
```

The output should show the groups that the user has been added to, e.g.

```json
{
  "data": {
    "queryUser": [
      {
        "name": "alice",
        "groups": [
          {
            "name": "dev"
          }
        ]
      }
    ]
  }
}
```

### Get user information

We can obtain information about a user with the following query:

```graphql
query {
  getUser(name: "alice") {
    name
    groups {
      name
    }
  }
}
```

The output should show the groups that the user has been added to, e.g.

```json
{
  "data": {
    "getUser": {
      "name": "alice",
      "groups": [
        {
          "name": "dev"
        }
      ]
    }
  }
}
```

### Query for groups

Let's query for the `dev` group:

```graphql
query {
  queryGroup(filter: { name: { eq: "dev" } }) {
    name
    users {
      name
    }
    rules {
      permission
      predicate
    }
  }
}
```

The output should include the users in the group as well as the permissions, the
group's ACL rules, e.g.

```json
{
  "data": {
    "queryGroup": [
      {
        "name": "dev",
        "users": [
          {
            "name": "alice"
          }
        ],
        "rules": [
          {
            "permission": 7,
            "predicate": "friend"
          },
          {
            "permission": 7,
            "predicate": "~friend"
          }
        ]
      }
    ]
  }
}
```

### Get group information

To check the `dev` group information:

```graphql
query {
  getGroup(name: "dev") {
    name
    users {
      name
    }
    rules {
      permission
      predicate
    }
  }
}
```

The output should include the users in the group as well as the permissions, the
group's ACL rules, e.g.

```json
{
  "data": {
    "getGroup": {
      "name": "dev",
      "users": [
        {
          "name": "alice"
        }
      ],
      "rules": [
        {
          "permission": 7,
          "predicate": "friend"
        },
        {
          "permission": 7,
          "predicate": "~friend"
        }
      ]
    }
  }
}
```

## Reset Groot password

If you have forgotten the password to the `groot` user, then you may reset the
`groot` password (or the password for any user) by following these steps.

1. Stop Dgraph Alpha.

2. Turn off ACLs by removing the `--acl_hmac_secret` config flag in the Alpha
   config. This leaves the Alpha open with no ACL rules, so be sure to restrict
   access, including stopping request traffic to this Alpha.

3. Start Dgraph Alpha.

4. Connect to Dgraph Alpha using Ratel and run the following upsert mutation to
   update the `groot` password to `newpassword` (choose your own secure
   password):

   ```graphql
   upsert {
     query {
       groot as var(func: eq(dgraph.xid, "groot"))
     }
     mutation {
       set {
         uid(groot) <dgraph.password> "newpassword" .
       }
     }
   }
   ```

5. Restart Dgraph Alpha with ACLs turned on by setting the `--acl_hmac_secret`
   config flag.

6. Login as groot with your new password.


# Audit Logging
Source: https://docs.hypermode.com/dgraph/enterprise/audit-logs

With an Enterprise license, Dgraph can generate audit logs that let you track and audit all requests (queries and mutations).

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

As a database administrator, you count on being able to audit access to your
database. With a Dgraph [enterprise license](./license), you can enable audit
logging so that all requests are tracked and available for use in security
audits. When audit logging is enabled, the following information is recorded
about the queries and mutations (requests) sent to your database:

* Endpoint
* Logged-in User Name
* Server host address
* Client Host address
* Request Body (truncated at 4KB)
* Timestamp
* Namespace
* Query Parameters (if provided)
* Response status

## Audit log scope

Most queries and mutations sent to Dgraph Alpha and Dgraph Zero are logged.
Specifically, the following are logged:

* HTTP requests sent over Dgraph Zero's 6080 port and Dgraph Alpha's 8080 port
  (except as noted below)
* gRPC requests sent over Dgraph Zero's 5080 port and Dgraph Alpha's 9080 port
  (except the Raft, health and Dgraph Zero stream endpoints noted below)

The following aren't logged:

* Responses to queries and mutations
* HTTP requests to `/health`, `/state` and `/jemalloc` endpoints
* gRPC requests to Raft endpoints (see [Raft](/dgraph/concepts/raft))
* gRPC requests to health endpoints (`Check` and `Watch`)
* gRPC requests to Dgraph Zero stream endpoints (`StreamMembership`,
  `UpdateMembership`, `Oracle`, `Timestamps`, `ShouldServe` and `Connect`)

{/* We don't have any docs to link to for the endpoints described in the last two bullets. TBD fix this so we are't referencing something not described elsewhere */}

## Audit log files

All audit logs are in JSON format. Dgraph has a "rolling-file" policy for audit
logs, where the current log file is used until it reaches a configurable size
(default: 100 MB), and then is replaced by another current audit log file. Older
audit log files are retained for a configurable number of days (default: 10
days).

For example, by sending this query:

```graphql
{
  q(func: has(actor.film)){
    count(uid)
  }
}
```

You'll get the following JSON audit log entry:

```json
{
  "ts": "2021-03-22T15:03:19.165Z",
  "endpoint": "/query",
  "level": "AUDIT",
  "user": "",
  "namespace": 0,
  "server": "localhost:7080",
  "client": "[::1]:60118",
  "req_type": "Http",
  "req_body": "{\"query\":\"{\\n  q(func: has(actor.film)){\\n    count(uid)\\n  }\\n}\",\"variables\":{}}",
  "query_param": {
    "timeout": ["20s"]
  },
  "status": "OK"
}
```

## Enable audit logging

You can enable audit logging on a Dgraph Alpha or Dgraph Zero node by using the
`--audit` flag to specify semicolon-separated options for audit logging. When
you enable audit logging, a few options are available for you to configure:

* `compress=true` tells Dgraph to use compression on older audit log files
* `days=20` tells Dgraph to retain older audit logs for 20 days, rather than the
  default of 10 days
* `output=/path/to/audit/logs` tells Dgraph which path to use for storing audit
  logs
* `encrypt-file=/encryption/key/path` tells Dgraph to encrypt older log files
  with the specified key
* `size=200` tells Dgraph to store audit logs in 200 MB files, rather than the
  default of 100 MB files

You can see how to use these options in the example commands below.

## Example commands

The commands in this section show you how to enable and configure audit logging.

### Enable audit logging

In the simplest scenario, you can enable audit logging by simply specifying the
directory to store audit logs on a Dgraph Alpha node:

```sh
dgraph alpha --audit output=audit-log-dir
```

You could extend this command a bit to specify larger log files (200 MB, instead
of 100 MB) and retain them for longer (15 days instead of 10 days):

```sh
dgraph alpha --audit "output=audit-log-dir;size=200;days=15"
```

### Enable audit logging with compression

In many cases you want to compress older audit logs to save storage space. You
can do this with a command like the following:

```sh
dgraph alpha --audit "output=audit-log-dir;compress=true"
```

### Enable audit logging with encryption

You can also enable encryption of audit logs to protect sensitive information
that might exist in logged requests. You can do this, along with compression,
with a command like the following:

```sh
dgraph alpha --audit "output=audit-log-dir;compress=true;encrypt-file=/path/to/encrypt/key/file"
```

### Decrypt audit logs

To decrypt encrypted audit logs, you can use the `dgraph audit decrypt` command,
as follows:

```sh
dgraph audit decrypt --encryption_key_file=/path/encrypt/key/file --in /path/to/encrypted/log/file --out /path/to/output/file
```

## Next steps

To learn more about the logging features of Dgraph, see
[Logging](/dgraph/admin/logs).


# Binary Backups
Source: https://docs.hypermode.com/dgraph/enterprise/binary-backups



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Binary backups are full backups of Dgraph that are backed up directly to cloud
storage such as Amazon S3 or any MinIO storage backend. Backups can also be
saved to an on-premise network file system shared by all Alpha servers. These
backups can be used to restore a new Dgraph cluster to the previous state from
the backup. Unlike [exports](/dgraph/admin/export), binary backups are
Dgraph-specific and can be used to restore a cluster quickly.

## Configure backup

Backup is only enabled when a valid license file is supplied to a Zero server OR
within the thirty (30) day trial period, no exceptions.

### Configure Amazon S3 credentials

To backup to Amazon S3, the Alpha server must have the following AWS credentials
set via environment variables:

| Environment Variable                        | Description                                                         |
| ------------------------------------------- | ------------------------------------------------------------------- |
| `AWS_ACCESS_KEY_ID` or `AWS_ACCESS_KEY`     | AWS access key with permissions to write to the destination bucket. |
| `AWS_SECRET_ACCESS_KEY` or `AWS_SECRET_KEY` | AWS access key with permissions to write to the destination bucket. |
| `AWS_SESSION_TOKEN`                         | AWS session token (if required).                                    |

Starting with
[v20.07.0](https://github.com/hypermodeinc/dgraph/releases/tag/v20.07.0) if the
system has access to the S3 bucket, you no longer need to explicitly include
these environment variables.

In AWS, you can accomplish this by doing the following:

1. Create an
   [IAM Role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create.html)
   with an IAM Policy that grants access to the S3 bucket.
2. Depending on whether you want to grant access to an EC2 instance, or to a pod
   running on [EKS](https://aws.amazon.com/eks/), you can do one of these
   options:
   * [Instance Profile](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html)
     can pass the IAM Role to an EC2 Instance
   * [IAM Roles for Amazon EC2](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html)
     to attach the IAM Role to a running EC2 Instance
   * [IAM roles for service accounts](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html)
     to associate the IAM Role to a
     [Kubernetes Service Account](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/).

### Configure MinIO credentials

To backup to MinIO, the Alpha server must have the following MinIO credentials
set via environment variables:

| Environment Variable | Description                                                           |
| -------------------- | --------------------------------------------------------------------- |
| `MINIO_ACCESS_KEY`   | MinIO access key with permissions to write to the destination bucket. |
| `MINIO_SECRET_KEY`   | MinIO secret key with permissions to write to the destination bucket. |

## Create a backup

To create a backup, make an HTTP POST request to `/admin` to a Dgraph Alpha HTTP
address and port (default, "localhost:8080"). Like with all `/admin` endpoints,
this is only accessible on the same machine as the Alpha server unless
[whitelisted for admin operations](/dgraph/self-managed/overview#whitelisting-admin-operations).
You can look at `BackupInput` given below for all the possible options.

```graphql
input BackupInput {
  """
  Destination for the backup: e.g. MinIO or S3 bucket.
  """
  destination: String!

  """
  Access key credential for the destination.
  """
  accessKey: String

  """
  Secret key credential for the destination.
  """
  secretKey: String

  """
  AWS session token, if required.
  """
  sessionToken: String

  """
  Set to true to allow backing up to S3 or MinIO bucket that requires no credentials.
  """
  anonymous: Boolean

  """
  Force a full backup instead of an incremental backup.
  """
  forceFull: Boolean
}
```

Execute the following mutation on /admin endpoint using any GraphQL compatible
client like Insomnia, GraphQL Playground or GraphiQL.

### Backup to Amazon S3

```graphql
mutation {
  backup(
    input: { destination: "s3://s3.us-west-2.amazonaws.com/<bucketname>" }
  ) {
    response {
      message
      code
    }
    taskId
  }
}
```

### Backup to MinIO

```graphql
mutation {
  backup(input: { destination: "minio://127.0.0.1:9000/<bucketname>" }) {
    response {
      message
      code
    }
    taskId
  }
}
```

### Backup using a MinIO Gateway

#### Azure Blob Storage

You can use
[Azure Blob Storage](https://azure.microsoft.com/services/storage/blobs/)
through the
[MinIO Azure Gateway](https://docs.min.io/docs/minio-gateway-for-azure.html).
You need to configure a
[storage account](https://docs.microsoft.com/azure/storage/common/storage-account-overview)
and
a[container](https://docs.microsoft.com/azure/storage/blobs/storage-blobs-introduction#containers)
to organize the blobs.

For MinIO configuration, you will need to
[retrieve storage accounts keys](https://docs.microsoft.com/azure/storage/common/storage-account-keys-manage).
The [MinIO Azure Gateway](https://docs.min.io/docs/minio-gateway-for-azure.html)
will use `MINIO_ACCESS_KEY` and `MINIO_SECRET_KEY` to correspond to Azure
Storage Account `AccountName` and `AccountKey`.

Once you have the `AccountName` and `AccountKey`, you can access Azure Blob
Storage locally using one of these methods:

* Run
  [MinIO Azure Gateway](https://docs.min.io/docs/minio-gateway-for-azure.html)
  using Docker

  ```sh
  docker run --publish 9000:9000 --name gateway \
    --env "MINIO_ACCESS_KEY=<AccountName>" \
    --env "MINIO_SECRET_KEY=<AccountKey>" \
    minio/minio gateway azure
  ```

* Run
  [MinIO Azure Gateway](https://docs.min.io/docs/minio-gateway-for-azure.html)
  using the MinIO Binary

  ```sh
  export MINIO_ACCESS_KEY="<AccountName>"
  export MINIO_SECRET_KEY="<AccountKey>"
  minio gateway azure
  ```

#### Google Cloud Storage

You can use [Google Cloud Storage](https://cloud.google.com/storage) through the
[MinIO GCS Gateway](https://docs.min.io/docs/minio-gateway-for-gcs.html). You
will need to
[create storage buckets](https://cloud.google.com/storage/docs/creating-buckets),
create a Service Account key for GCS and get a credentials file.

Once you have a `credentials.json`, you can access GCS locally using one of
these methods:

* Run [MinIO GCS Gateway](https://docs.min.io/docs/minio-gateway-for-gcs.html)
  using Docker

  ```sh
  docker run --publish 9000:9000 --name gateway \
    --volume /path/to/credentials.json:/credentials.json \
    --env "GOOGLE_APPLICATION_CREDENTIALS=/credentials.json" \
    --env "MINIO_ACCESS_KEY=minioaccountname" \
    --env "MINIO_SECRET_KEY=minioaccountkey" \
    minio/minio gateway gcs <project-id>
  ```

* Run [MinIO GCS Gateway](https://docs.min.io/docs/minio-gateway-for-gcs.html)
  using the MinIO Binary

  ```sh
  export GOOGLE_APPLICATION_CREDENTIALS=/path/to/credentials.json
  export MINIO_ACCESS_KEY=minioaccesskey
  export MINIO_SECRET_KEY=miniosecretkey
  minio gateway gcs <project-id>
  ```

#### Test using MinIO browser

MinIO Gateway comes with an embedded web-based object browser. After using one
of the aforementioned methods to run the MinIO Gateway, you can test that MinIO
Gateway is running, open a web browser, navigate to [http://127.0.0.1:9000](http://127.0.0.1:9000), and
ensure that the object browser is displayed and can access the remote object
storage.

### Disabling HTTPS for S3 and MinIO backups

By default, Dgraph assumes the destination bucket is using HTTPS. The backup
fails when that's not the case. To send a backup to a bucket using HTTP
(insecure), set the query parameter `secure=false` with the destination endpoint
in the `destination` field:

```graphql
mutation {
  backup(
    input: { destination: "minio://127.0.0.1:9000/<bucketname>?secure=false" }
  ) {
    response {
      message
      code
    }
    taskId
  }
}
```

### Overriding credentials

The `accessKey`, `secretKey`, and `sessionToken` parameters can be used to
override the default credentials. Please note that unless HTTPS is used, the
credentials is transmitted in plain text so use these parameters with
discretion. The environment variables should be used by default but these
options are there to allow for greater flexibility.

The `anonymous` parameter can be set to "true" to allow backing up to S3 or
MinIO bucket that requires no credentials (i.e a public bucket).

### Backup to NFS

```graphql
mutation {
  backup(input: { destination: "/path/to/local/directory" }) {
    response {
      message
      code
    }
    taskId
  }
}
```

A local filesystem will work only if all the Alpha servers have access to it
(e.g all the Alpha servers are running on the same filesystems as a normal
process, not a Docker container). However, an NFS is recommended so that backups
work seamlessly across multiple machines and/or containers.

### Forcing a Full Backup

By default, an incremental backup will be created if there's another full backup
in the specified location. To create a full backup, set the `forceFull` field to
`true` in the mutation. Each series of backups can be identified by a unique ID
and each backup in the series is assigned a monotonically increasing number. The
following section contains more details on how to restore a backup series.

```graphql
mutation {
  backup(input: { destination: "/path/to/local/directory", forceFull: true }) {
    response {
      message
      code
    }
    taskId
  }
}
```

## Listing Backups

The GraphQL admin interface includes the `listBackups` endpoint that lists the
backups in the given location along with the information included in the
`manifest.json` file. An example of a request to list the backups in the
`/data/backup` location is included below:

```
query backup() {
  listBackups(input: {location: "/data/backup"}) {
    backupId
    backupNum
    encrypted
    groups {
      groupId
      predicates
    }
    path
    since
    type
  }
}
```

The listBackups input can contain the following fields. Only the `location`
field is required.

```
input ListBackupsInput {
  """
  Destination for the backup: e.g. MinIO or S3 bucket.
  """
  location: String!

  """
  Access key credential for the destination.
  """
  accessKey: String

  """
  Secret key credential for the destination.
  """
  secretKey: String

  """
  AWS session token, if required.
  """
  sessionToken: String

  """
  Whether the destination doesn't require credentials (e.g. S3 public bucket).
  """
  anonymous: Boolean
}
```

The output is of `[Manifest]` type. The fields inside the `Manifest` type
corresponds to the fields in the `manifest.json` file.

```
type Manifest {
  """
  Unique ID for the backup series.
  """
  backupId: String

  """
  Number of this backup within the backup series. The full backup always has a value of one.
  """
  backupNum: Int

  """
  Whether this backup was encrypted.
  """
  encrypted: Boolean

  """
  List of groups and the predicates they store in this backup.
  """
  groups: [BackupGroup]

  """
  Path to the manifest file.
  """
  path: String

  """
  The timestamp at which this backup was taken. The next incremental backup will
  start from this timestamp.
  """
  since: Int

  """
  The type of backup, either full or incremental.
  """
  type: String
}

type BackupGroup {
  """
  The ID of the cluster group.
  """
  groupId: Int

  """
  List of predicates assigned to the group.
  """
  predicates: [String]
}
```

### Automating Backups

You can use the provided endpoint to automate backups, however, there are a few
things to keep in mind.

* The requests should go to a single Alpha server. The Alpha server that
  receives the request is responsible for looking up the location and
  determining from which point the backup should resume.

* Versions of Dgraph starting with v20.07.1, v20.03.5, and v1.2.7 have a way to
  block multiple backup requests going to the same Alpha server. For previous
  versions, keep this in mind and avoid sending multiple requests at once. This
  is for the same reason as the point above.

* You can have multiple backup series in the same location although the feature
  still works if you set up a unique location for each series.

## Export Backups

The `export_backup` tool lets you convert a binary backup into an exported
folder.

If you need to upgrade between two major Dgraph versions that have incompatible
changes, you can use the `export_backup` tool to apply changes (either to the
exported `.rdf` file or to the schema file), and then import back the dataset
into the new Dgraph version.

### Using exports instead of binary backups

An example of this use-case would be to migrate existing schemas from Dgraph
v1.0 to Dgraph latest. You need to update the schema file from an export so all
predicates of type `uid` are changed to `[uid]`. Then use the updated schema
when loading data into Dgraph latest.

For example, for the following schema:

```
name: string .
friend: uid .
```

becomes

```
name: string .
friend: [uid] .
```

Because you have to do a modification to the schema itself, you need an export.
You can use the `export_backup` tool to convert your binary backup into an
export folder.

### Binary Backups and Exports folders

A Binary Backup directory has the following structure:

```sh
backup
├── dgraph.20210102.204757.509
│   └── r9-g1.backup
├── dgraph.20210104.224757.707
│   └── r9-g1.backup
└── manifest.json
```

An Export directory has the following structure:

```sh
dgraph.r9.u0108.1621
├── g01.gql_schema.gz
├── g01.rdf.gz
└── g01.schema.gz
```

If you want to do the changes cited above, you need to edit the `g01.schema.gz`
file.

### Benefits

With the `export_backup` tool you get the speed benefit from the binary backups,
which are faster than regular exports. So if you have a big dataset, you don't
need to wait a long time until an export is completed. Instead, just take a
binary backup and convert it to an export only when needed.

### How to use it

Ensure that you have created a binary backup. The directory tree of a binary
backup usually looks like this:

```sh
backup
├── dgraph.20210104.224757.709
│   └── r9-g1.backup
└── manifest.json
```

Then run the following command:

```sh
dgraph export_backup --location "<location-of-your-binary-backup>" --destination "<destination-of-the-export-dir>"
```

Once completed you will find your export folder (in this case
`dgraph.r9.u0108.1621`). The tree of the directory should look like this:

```sh
dgraph.r9.u0108.1621
├── g01.gql_schema.gz
├── g01.rdf.gz
└── g01.schema.gz
```

## Encrypted Backups

Encrypted backups are an Enterprise feature that are available from `v20.03.1`
and `v1.2.3` and allow you to encrypt your backups and restore them. This
documentation describes how to implement encryption into your binary backups.

Starting with` v20.07.0`, we also added support for Encrypted Backups using
encryption keys sitting on [HashiCorp Vault](https://www.vaultproject.io/).

### New `Encrypted` flag in manifest.json

A new `Encrypted` flag is added to the `manifest.json`. This flag indicates if
the corresponding binary backup is encrypted or not. To be backward compatible,
if this flag is absent, it's presumed that the corresponding backup is not
encrypted.

For a series of full and incremental backups, per the current design, we don't
allow the mixing of encrypted and unencrypted backups. As a result, all full and
incremental backups in a series must either be encrypted fully or not at all.
This flag helps with checking this restriction.

### AES And Chaining with Gzip

If encryption is turned on an Alpha server, then we use the configured
encryption key. The key size (16, 24, 32 bytes) determines AES-128/192/256
cipher chosen. We use the AES CTR mode. Currently, the binary backup is already
gzipped. With encryption, we will encrypt the gzipped data.

During **backup**: the 16 bytes IV is prepended to the Cipher-text data after
encryption.

### Backup

Backup is an online tool, meaning it's available when Dgraph Alpha server is
running. For encrypted backups, the Dgraph Alpha server must be configured with
the `--encryption key-file=value`. Starting with v20.07.0, the Dgraph Alpha
server can alternatively be configured to interface with a
[HashiCorp Vault](https://www.vaultproject.io/) server to obtain keys.

<Note>
  `encryption key-file=value` flag or `vault` superflag was used for
  encryption-at-rest and will now also be used for encrypted backups.
</Note>

## Online restore

To restore from a backup to a live cluster, execute a mutation on the `/admin`
endpoint with the following format:

```graphql
mutation {
  restore(
    input: {
      location: "/path/to/backup/directory"
      backupId: "id_of_backup_to_restore"
    }
  ) {
    message
    code
  }
}
```

Online restores only require you to send this request. The `UID` and timestamp
leases are updated accordingly. The latest backup to be restored should contain
the same number of groups in its `manifest.json` file as the cluster to which it
is being restored.

<Note>
  When using backups made from a Dgraph cluster that uses encryption (so backups
  are encrypted), you need to use the same key from that original cluster when
  doing a restore process. Dgraph's [Encryption at
  Rest](/dgraph/enterprise/encryption-at-rest) uses a symmetric-key algorithm
  where the same key is used for both encryption and decryption, so the
  encryption key from that cluster is needed for the restore process.
</Note>

Online restore can be performed from Amazon S3 / MinIO or from a local
directory. Below is the documentation for the fields inside `RestoreInput` that
can be passed into the mutation.

```graphql
input RestoreInput {
  """
  Destination for the backup: e.g. MinIO or S3 bucket.
  """
  location: String!

  """
  Backup ID of the backup series to restore. This ID is included in the manifest.json file.
  If missing, it defaults to the latest series.
  """
  backupId: String

  """
  Number of the backup within the backup series to be restored. Backups with a greater value
  will be ignored. If the value is zero or is missing, the entire series will be restored.
  """
  backupNum: Int

  """
  Path to the key file needed to decrypt the backup. This file should be accessible
  by all Alpha servers in the group. The backup will be written using the encryption key
  with which the cluster was started, which might be different than this key.
  """
  encryptionKeyFile: String

  """
  Vault server address where the key is stored. This server must be accessible
  by all Alpha servers in the group. Default "http://localhost:8200".
  """
  vaultAddr: String

  """
  Path to the Vault RoleID file.
  """
  vaultRoleIDFile: String

  """
  Path to the Vault SecretID file.
  """
  vaultSecretIDFile: String

  """
  Vault kv store path where the key lives. Default "secret/data/dgraph".
  """
  vaultPath: String

  """
  Vault kv store field whose value is the key. Default "enc_key".
  """
  vaultField: String

  """
  Vault kv store field's format. Must be "base64" or "raw". Default "base64".
  """
  vaultFormat: String

  """
  Access key credential for the destination.
  """
  accessKey: String

  """
  Secret key credential for the destination.
  """
  secretKey: String

  """
  AWS session token, if required.
  """
  sessionToken: String

  """
  Set to true to allow backing up to S3 or MinIO bucket that requires no credentials.
  """
  anonymous: Boolean

  """
  All the backups with num >= incrementalFrom will be restored.
  """
  incrementalFrom: Int

  """
  If `isPartial` is set to true then the cluster is kept in draining mode after
  restore to ensure that the database is not corrupted by any mutations or tablet moves in
  between two restores.
  """
  isPartial: Boolean
}
```

Restore requests returns immediately without waiting for the operation to
finish.

## Incremental Restore

You can use incremental restore to restore a set of incremental backups on a
cluster with a part of the backup already restored. The system does not accept
any mutations made between a normal restore and an incremental restore, because
the cluster is in the draining mode. When the cluster is in a draining mode only
an admin request to bring the cluster back to normal mode is accepted.

Note: Before you start an incremental restore ensure that you set `isPartial` to
`true` in your normal restore.

To incrementally restore from a backup to a live cluster, execute a mutation on
the `/admin` endpoint with the following format:

```graphql
mutation{
  restore(input:{
    incrementalFrom:"incremental_backup_from",
    location: "/path/to/backup/directory",
    backupId: "id_of_backup_to_restore"'
  }){
    message
    code
  }
}
```

## Namespace Aware Restore

You can use namespace-aware restore to restore a single namespace from a backup
that contains multiple namespaces. The created restore will be available in the
default namespace. For example, if you restore namespace 2 using the
restoreTenant API, then after the restore operation is completed, the cluster
will have only the default namespace, and it will contain the data from
namespace 2. Namespace aware restore supports incremental restore.

To restore from a backup to a live cluster, execute a mutation on the `/admin`
endpoint with the following format:

```graphql
mutation {
  restoreTenant(
    input: {
      restoreInput: {
        incrementalFrom: "incremental_backup_from"
        location: "/path/to/backup/directory"
        backupId: "id_of_backup_to_restore"
      }
      fromNamespace: namespaceToBeRestored
    }
  ) {
    message
    code
  }
}
```

Documentation of restoreTenant inputs

```
input RestoreTenantInput {
  """
  restoreInput contains fields that are required for the restore operation,
  i.e., location, backupId, and backupNum
  """
  restoreInput: RestoreInput

  """
  fromNamespace is the namespace of the tenant that needs to be restored into namespace 0 of the new cluster.
  """
  fromNamespace: Int!
}
```

## Offline restore (DEPRECATED)

The restore utility is now a standalone tool. A new flag,
`--encryption key-file=value`, is now part of the restore utility, so you can
use it to decrypt the backup. The file specified using this flag must contain
the same key that was used for encryption during backup. Alternatively, starting
with `v20.07.0`, the `vault` superflag can be used to restore a backup.

You can use the `dgraph restore` command to restore the postings directory from
a previously-created backup to a directory in the local filesystem. This command
restores a backup to a new Dgraph cluster, so it's not designed to restore a
backup to a Dgraph cluster that is currently live. During a restore operation, a
new Dgraph Zero server might run to fully restore the backup state.

You can use the `--location` (`-l`) flag to specify a source URI with Dgraph
backup objects. This URI supports all the schemes used for backup.

You can use the `--postings` (`-p`) flag to set the directory where restored
posting directories are saved. This directory contains a posting directory for
each group in the restored backup.

You can use the `--zero` (`-z`) flag to specify a Dgraph Zero server address to
update the start timestamp and UID lease using the restored version. If no
Dgraph Zero server address is passed, the command will complain unless you set
the value of the `--force_zero` flag to false. If do not pass a zero value to
this command, you need to manually update the timestamp and UID lease using the
Dgraph Zero server's HTTP 'assign' endpoint. The updated values should be those
that are printed near the end of the command's output.

You use the `--backup_id` optional flag to specify the ID of the backup series
to restore. A backup series consists of a full backup and all of the incremental
backups built on top of it. Each time a new full backup is created, a new backup
series with a different ID is started. The backup series ID is stored in each
`manifest.json` file stored in each backup folder.

You use the `--encryption key-file=value` flag in cases where you took the
backup in an encrypted cluster. The string for this flag must point to the
location of the same key file used to run the cluster.

You use the `--vault` [superflag](/dgraph/cli/command-reference) to specify the
[HashiCorp Vault](https://www.vaultproject.io/) server address (`addr`), role id
(`role-id-file`), secret id (`secret-id-file`) and the field that contains the
encryption key (`enc-field`) that was used to encrypt the backup.

The restore feature creates a cluster with as many groups as the original
cluster had at the time of the last backup. For each group, `dgraph restore`
creates a posting directory (`p<N>`) that corresponds to the backup group ID.
For example, a backup for Dgraph Alpha group 2 would have the name
`.../r32-g2.backup` and would be loaded to posting directory `p2`.

After running the restore command, the directories inside the `postings`
directory need to be manually copied over to the machines/containers running the
Dgraph Alpha servers before running the `dgraph alpha` command. For example, in
a database cluster with two Dgraph Alpha groups and one replica each, `p1` needs
to be moved to the location of the first Dgraph Alpha node and `p2` needs to be
moved to the location of the second Dgraph Alpha node.

By default, Dgraph will look for a posting directory with the name `p`, so make
sure to rename the directories after moving them. You can also use the `-p`
option of the `dgraph alpha` command to specify a different path from the
default.

### Restore from Amazon S3

```sh
dgraph restore --postings "/var/db/dgraph" --location "s3://s3.<region>.amazonaws.com/<bucketname>"
```

### Restore from MinIO

```sh
dgraph restore --postings "/var/db/dgraph" --location "minio://127.0.0.1:9000/<bucketname>"
```

### Restore from Local Directory or NFS

```sh
dgraph restore --postings "/var/db/dgraph" --location "/var/backups/dgraph"
```

### Restore and Update Timestamp

Specify the Zero server address and port for the new cluster with `--zero`/`-z`
to update the timestamp.

```sh
dgraph restore --postings "/var/db/dgraph" --location "/var/backups/dgraph" --zero "localhost:5080"
```


# Change Data Capture
Source: https://docs.hypermode.com/dgraph/enterprise/change-data-capture

With a Dgraph enterprise license, you can use Dgraph's change data capture (CDC) capabilities to track data changes over time.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

With a Dgraph [enterprise license](/dgraph/enterprise/license), you can use
change data capture (CDC) to track data changes over time; including mutations
and drops in your database. Dgraph's CDC implementation lets you use Kafka or a
local file as a *sink* to store CDC updates streamed by Dgraph Alpha leader
nodes.

When CDC is enabled, Dgraph streams events for all `set` and `delete` mutations,
except those that affect password fields; along with any drop events. Live
Loader events are recorded by CDC, but Bulk Loader events aren't.

CDC events are based on changes to Raft logs. So, if the sink is not reachable
by the Alpha leader node, then Raft logs expand as events are collected on that
node until the sink is available again. You should enable CDC on all Dgraph
Alpha nodes to avoid interruptions in the stream of CDC events.

## Enable CDC with Kafka sink

Kafka records CDC events under the `dgraph-cdc` topic. The topic must be created
before events are sent to the broker. To enable CDC and sink events to Kafka,
start Dgraph Alpha with the `--cdc` command and the sub-options shown below, as
follows:

```sh
dgraph alpha --cdc "kafka=kafka-hostname:port; sasl-user=tstark; sasl-password=m3Ta11ic"
```

If you use Kafka on the localhost without SASL authentication, you can just
specify the hostname and port used by Kafka, as follows:

```sh
dgraph alpha --cdc "localhost:9092"
```

If the Kafka cluster to which you are connecting requires TLS, the `ca-cert`
option is required. Note that this certificate can be self-signed.

## Enable CDC with file sink

To enable CDC and sink results to a local unencrypted file, start Dgraph Alpha
with the `--cdc` command and the sub-option shown below, as follows:

```sh
dgraph alpha --cdc "file=local-file-path"
```

## CDC command reference

The `--cdc` option includes several sub-options that you can use to configure
CDC when running the `dgraph alpha` command:

| Sub-option       | Example `dgraph alpha` command option                                    | Notes                                                                                                                          |
| ---------------- | ------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------ |
| `tls`            | `--tls=false`                                                            | boolean flag to enable/disable TLS while connecting to Kafka.                                                                  |
| `ca-cert`        | `--cdc "ca-cert=/cert-dir/ca.crt"`                                       | Path and filename of the CA root certificate used for TLS encryption, if not specified, Dgraph uses system certs if `tls=true` |
| `client-cert`    | `--cdc "client-cert=/c-certs/client.crt"`                                | Path and filename of the client certificate used for TLS encryption                                                            |
| `client-key`     | `--cdc "client-cert=/c-certs/client.key"`                                | Path and filename of the client certificate private key                                                                        |
| `file`           | `--cdc "file=/sink-dir/cdc-file"`                                        | Path and filename of a local file sink (alternative to Kafka sink)                                                             |
| `kafka`          | `--cdc "kafka=kafka-hostname; sasl-user=tstark; sasl-password=m3Ta11ic"` | Hostname(s) of the Kafka hosts. May require authentication using the `sasl-user` and `sasl-password` sub-options               |
| `sasl-user`      | `--cdc "kafka=kafka-hostname; sasl-user=tstark; sasl-password=m3Ta11ic"` | SASL username for Kafka. Requires the `kafka` and `sasl-password` sub-options                                                  |
| `sasl-password`  | `--cdc "kafka=kafka-hostname; sasl-user=tstark; sasl-password=m3Ta11ic"` | SASL password for Kafka. Requires the `kafka` and `sasl-username` sub-options                                                  |
| `sasl-mechanism` | `--cdc "kafka=kafka-hostname; sasl-mechanism=PLAIN"`                     | The SASL mechanism for Kafka (PLAIN, SCRAM-SHA-256 or SCRAM-SHA-512). The default is PLAIN                                     |

## CDC data format

CDC events are in JSON format. Most CDC events look like the following example:

```json
{
  "key": "0",
  "value": {
    "meta": { "commit_ts": 5 },
    "type": "mutation",
    "event": {
      "operation": "set",
      "uid": 2,
      "attr": "counter.val",
      "value": 1,
      "value_type": "int"
    }
  }
}
```

The `Meta.Commit_Ts` value (shown above as `"meta":{"commit_ts":5}`) will
increase with each CDC event, so you can use this value to find duplicate events
if those occur due to Raft leadership changes in your Dgraph Alpha group.

### Mutation event examples

A set mutation event updating `counter.val` to 10 would look like the following:

```json
{
  "meta": { "commit_ts": 29 },
  "type": "mutation",
  "event": {
    "operation": "set",
    "uid": 3,
    "attr": "counter.val",
    "value": 10,
    "value_type": "int"
  }
}
```

Similarly, a delete mutation event that removes all values for the `Author.name`
field for a specified node would look like the following:

```json
{
  "meta": { "commit_ts": 44 },
  "type": "mutation",
  "event": {
    "operation": "del",
    "uid": 7,
    "attr": "Author.name",
    "value": "_STAR_ALL",
    "value_type": "default"
  }
}
```

### Drop event examples

CDC drop events look like the following example event for "drop all":

```json
{ "meta": { "commit_ts": 13 }, "type": "drop", "event": { "operation": "all" } }
```

The `operation` field specifies which drop operation (`attribute`, `type`,
specified `data`, or `all` data) is tracked by the CDC event.

## CDC and multi-tenancy

When you enable CDC in a [multi-tenant environment](./multitenancy), CDC events
streamed to Kafka are distributed by the Kafka client. It distributes events
between the available Kafka partitions based on their multi-tenancy namespace.

## Known limitations

CDC has the following known limitations:

* CDC events do not track old values that are updated or removed by mutation or
  drop operations; only new values are tracked
* CDC does not currently track schema updates
* You can only configure or enable CDC when starting Alpha nodes using the
  `dgraph alpha` command
* If a node crashes or the leadership of a Raft group changes, CDC might have
  duplicate events, but no data loss


# Encryption at Rest
Source: https://docs.hypermode.com/dgraph/enterprise/encryption-at-rest



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

<Note>
  For migrating unencrypted data to a new Dgraph cluster with encryption
  enabled, you need to [export the database](/dgraph/admin/export) and [import
  data](/dgraph/admin/import), preferably using the [bulk
  loader](/dgraph/admin/bulk-loader).
</Note>

Encryption at rest refers to the encryption of data stored physically in any
digital form. It ensures that sensitive data on disk isn't readable by any user
or app without a valid key required for decryption. Dgraph provides encryption
at rest as an enterprise feature. If encryption is enabled, Dgraph uses
[Advanced Encryption Standard (AES)](https://en.wikipedia.org/wiki/Advanced_Encryption_Standard)
algorithm to encrypt the data and secure it.

Prior to v20.07.0, the encryption key file must be present on the local file
system. Starting with
[v20.07.0](https://github.com/hypermodeinc/dgraph/releases/tag/v20.07.0), we
have added support for encryption keys sitting on Vault servers. This allows an
alternate way to configure the encryption keys needed for encrypting the data at
rest.

## Set up encryption

To enable encryption, we need to pass a file that stores the data encryption key
with the option `--encryption key-file=value`. The key size must be 16, 24, or
32 bytes long, and the key size determines the corresponding block size for AES
encryption (AES-128, AES-192, and AES-256, respectively).

You can use the following command to create the encryption key file (set *count*
to the desired key size):

```sh
tr -dc 'a-zA-Z0-9' < /dev/urandom | dd bs=1 count=32 of=enc_key_file
```

<Note> On a macOS you may have to use
`LC_CTYPE=C; tr -dc 'a-zA-Z0-9' < /dev/urandom | dd bs=1 count=32 of=enc_key_file`.
To view the key use `cat enc_key_file`. </Note> Alternatively, you can
use the `--vault` [superflag's](/dgraph/cli/command-reference) options to
enable encryption, as
[explained below](#example-using-dgraph-cli-with-hashicorp-vault-configuration).

## Turn on encryption

Here is an example that starts one Zero server and one Alpha server with the
encryption feature turned on:

```sh
dgraph zero --my="localhost:5080" --replicas 1 --raft "idx=1"
dgraph alpha --encryption key-file="./enc_key_file" --my="localhost:7080" --zero="localhost:5080"
```

If multiple Alpha nodes are part of the cluster, you need to pass the
`--encryption key-file` option to each of the Alphas.

Once an Alpha has encryption enabled, the encryption key must be provided in
order to start the Alpha server. If the Alpha server restarts, the
`--encryption key-file` option must be set along with the key to restart
successfully.

### Storing encryption key secret in HashiCorp Vault

You can save the encryption key secret in
[HashiCorp Vault](https://www.vaultproject.io/) K/V Secret instead of as file on
Dgraph Alpha.

To use [HashiCorp Vault](https://www.vaultproject.io/), meet the following
prerequisites for the Vault Server.

1. Ensure that the Vault server is accessible from Dgraph Alpha and configured
   using URL `http://fqdn[ip]:port`.

2. Enable [AppRole Auth method](https://www.vaultproject.io/docs/auth/approle)
   and enable [KV Secrets Engine](https://www.vaultproject.io/docs/secrets/kv).

3. Save the value of the key (16, 24, or 32 bytes long) that Dgraph Alpha uses
   in a KV Secret path
   ([K/V Version 1](https://www.vaultproject.io/docs/secrets/kv/kv-v1) or
   [K/V Version 2](https://www.vaultproject.io/docs/secrets/kv/kv-v2)). For
   example, you can upload this below to KV Secrets Engine Version 2 path of
   `secret/data/dgraph/alpha`:

   ```json
   {
     "options": {
       "cas": 0
     },
     "data": {
       "enc_key": "qIvHQBVUpzsOp74PmMJjHAOfwIA1e6zm%"
     }
   }
   ```

4. Create or use a role with an attached policy that grants access to the
   secret. For example, the following policy would grant access to
   `secret/data/dgraph/alpha`:

   ```hcl
   path "secret/data/dgraph/*" {
     capabilities = [ "read", "update" ]
   }
   ```

5. Using the `role_id` generated from the previous step, create a corresponding
   `secret_id`, and copy the `role_id` and `secret_id` over to local files, like
   `./dgraph/vault/role_id` and `./dgraph/vault/secret_id`, that's used by
   Dgraph Alpha nodes.

<Note>
  The key format for the `enc-field` option can be defined using `enc-format`
  with the values `base64` (default) or `raw`.
</Note>

### Example using Dgraph CLI with HashiCorp Vault configuration

The following example shows how to use Dgraph with a Vault server that holds the
encryption key:

```sh
## Start Dgraph Zero in different terminal tab or window
dgraph zero --my=localhost:5080 --replicas 1 --raft "idx=1"

## Start Dgraph Alpha in different terminal tab or window
dgraph alpha --my="localhost:7080" --zero="localhost:5080" \
  --vault addr="http://localhost:8200";enc-field="enc_key";enc-format="raw";path="secret/data/dgraph/alpha";role-id-file="./role_id";secret-id-file="./secret_id"
```

If multiple Dgraph Alpha nodes are part of the cluster, you must pass the
`--encryption key-file` flag or the `--vault` superflag with appropriate
superflag options to each of the Dgraph Alpha nodes.

After an Alpha node has encryption enabled, you must provide the encryption key
to start the Alpha server. If the Alpha server restarts, the
`--encryption key-file` or the `--vault` superflag's options must be set along
with the key to restart successfully.

## Turn off encryption

You can use [Live Loader](/dgraph/admin/live-loader) or
[Bulk Loader](/dgraph/admin/bulk-loader) to decrypt the data while importing.

## Change encryption key

The master encryption key set by the `--encryption key-file` option (or one used
in Vault KV store) doesn't change automatically. The master encryption key
encrypts underlying *data keys* which are changed on a regular basis
automatically (more info about this is covered on the encryption-at-rest
[blog][encblog] post).

[encblog]: https://hypermode.com/blog/encryption-at-rest-dgraph-badger#one-key-to-rule-them-all-many-keys-to-find-them

Changing the existing key to a new one is called key rotation. You can rotate
the master encryption key by using the `badger rotate` command on both p and w
directories for each Alpha. To maintain availability in HA cluster
configurations, you can do this rotate the key one Alpha at a time in a rolling
manner.

You'll need both the current key and the new key in two different files. Specify
the directory you rotate ("p" or "w") for the `--dir` flag, the old key for the
`--old-key-path` flag, and the new key with the `--new-key-path` flag.

```sh
badger rotate --dir p --old-key-path enc_key_file --new-key-path new_enc_key_file
badger rotate --dir w --old-key-path enc_key_file --new-key-path new_enc_key_file
```

Then, you can start Alpha with the `new_enc_key_file` key file to use the new
key.


# Learner Nodes
Source: https://docs.hypermode.com/dgraph/enterprise/learner-nodes

Learner nodes let you spin-up read-only replica instance to serve best-effort queries faster

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

A Learner node is an enterprise-only feature that allows a user to spin-up a
read-only replica instance across the world without paying a latency cost. When
enabled, a Dgraph cluster using learner nodes can serve best-effort queries
faster.

A "learner node" can still accept write operations. The node forwards them over
to the Alpha group leader and does the writing just like a typical Alpha node.
It will just be slower, depending on the latency between the Alpha node and the
learner node.

<Note>
  A learner node instance can forward `/admin` operations and perform both read
  and write operations, but writing will incur in network call latency to the
  main cluster.
</Note>

## Set up a Learner node

The learner node feature works at the Dgraph Alpha group level. To use it, first
you need to set up an Alpha instance as a learner node. Once the learner
instance is up, this replica can be used to run best-effort queries with zero
latency overhead. Because it's an Enterprise feature, a learner node won't be
able to connect to a Dgraph Zero node until the Zero node has a valid license.

To spin up a learner node, first make sure that you start all the nodes,
including the Dgraph Zero leader and the Dgraph Alpha leader, with the `--my`
flag so that these nodes will be accessible to the learner node. Then, start an
Alpha instance as follows:

```sh
dgraph alpha --raft="learner=true; group=N" --my <learner-node-ip-address>:5080
```

This allows the new Alpha instance to get all the updates from the group "N"
leader without participating in the Raft elections.

<Note>
  You must specify the `--my` flag to set the IP address and port of Dgraph
  Zero, the Dgraph Alpha leader node, and the learner node. If you don't, you
  will get an error similar to the following: `Error during SubscribeForUpdates`
</Note>

## Best-effort Queries

Regular queries use the strict consistency model, and any write operation to the
cluster anywhere would be read immediately.

Best-effort queries apply the eventual consistency model. A write to the cluster
will be seen eventually to the node. In regular conditions, the eventual
consistency is usually achieved quickly.

A best-effort query to a learner node returns any data that is already available
in that learner node. The response is still a valid data snapshot, but at a
timestamp which is not the latest one.

<Note>
  Best-effort queries won't be forwarded to a Zero node to get the latest
  timestamp.
</Note>

You can still send typical read queries (strict consistency) to a learner node.
They would just incur an extra latency cost due to having to reach out the Zero
leader.

<Note>
  If the learner node needs to serve normal queries, at least one Alpha leader
  must be available.
</Note>

## Use-case examples

### Geographic distribution

Consider this scenario:

*You want to achieve low latency for clients in a remote geographical region,
distant from your Dgraph cluster.*

You can address this need by using a learner node to run best-effort queries.
This read-only replica instance can be across distant geographies and you can
use best-effort queries to get instant responses.

Because learner nodes support read and write operations, users in the remote
location can do everything with this learner node, as if they were working with
the full cluster.


# License
Source: https://docs.hypermode.com/dgraph/enterprise/license



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph enterprise features are proprietary licensed under the [Dgraph Community
License][dcl]. All Dgraph releases contain proprietary code for enterprise
features. Enabling these features requires an enterprise contract from
[hello@hypermode.com](mailto:hello@hypermode.com).

**Dgraph enterprise features are enabled by default for 30 days in a new
cluster**. After the trial period of thirty (30) days, the cluster must obtain a
license from Dgraph to continue using the enterprise features released in the
proprietary code.

<Note>
  At the conclusion of your 30-day trial period if a license has not been
  applied to the cluster, access to the enterprise features will be suspended.
  The cluster will continue to operate without enterprise features.
</Note>

When you have an enterprise license key, the license can be applied to the
cluster by including it as the body of a POST request and calling
`/enterpriseLicense` HTTP endpoint on any Zero server.

```sh
curl -X POST localhost:6080/enterpriseLicense --upload-file ./licensekey.txt
```

It can also be applied by passing the path to the enterprise license file (using
the flag `--enterprise_license`) to the `dgraph zero` command used to start the
server. The second option is useful when the process needs to be automated.

```sh
dgraph zero --enterprise_license ./licensekey.txt
```

**Warning messages related to license expiry**

Dgraph will print a warning message in the logs when your license is about to
expire. If you are planning to implement any log monitoring solution, you may
note this pattern and configure suitable alerts for yourself. You can find an
example of this message below:

```sh
Your enterprise license will expire in 6 days from now. To continue using enterprise features after 6 days from now, apply a valid license. To get a new license, contact us at https://dgraph.io/contact.
```

Once your license has expired, you will see the following warning message in the
logs.

```sh
Your enterprise license has expired and enterprise features are disabled. To continue using enterprise features, apply a valid license. To receive a new license, contact us at https://dgraph.io/contact.
```

[dcl]: https://github.com/hypermodeinc/dgraph/blob/main/licenses/DCL.txt


# Backup List Tool
Source: https://docs.hypermode.com/dgraph/enterprise/lsbackup



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The `lsbackup` command-line tool prints information about the stored backups in
a user-defined location.

## Parameters

The `lsbackup` command has two flags:

```txt
Flags:
  -h, --help              help for lsbackup
  -l, --location string   Sets the source location URI (required).
      --verbose           Outputs additional info in backup list.
```

* `--location`: indicates a [source URI](#source-uri) with Dgraph backup
  objects. This URI supports all the schemes used for backup.
* `--verbose`: if enabled will print additional information about the selected
  backup.

For example, you can execute the `lsbackup` command as follows:

```sh
dgraph lsbackup -l <source-location-URI>
```

### Source URI

Source URI formats:

* `[scheme]://[host]/[path]?[args]`
* `[scheme]:///[path]?[args]`
* `/[path]?[args]` (only for local or NFS)

Source URI parts:

* `scheme`: service handler, one of: `s3`, `minio`, `file`
* `host`: remote address; e.g.: `dgraph.s3.amazonaws.com`
* `path`: directory, bucket or container at target; e.g.: `/dgraph/backups/`
* `args`: specific arguments that are ok to appear in logs

## Output

The following snippet is an example output of `lsbackup`:

```json
[
  {
    "path": "/home/user/Dgraph/20.11/backup/manifest.json",
    "since": 30005,
    "backup_id": "reverent_vaughan0",
    "backup_num": 1,
    "encrypted": false,
    "type": "full"
  }
]
```

If the `--verbose` flag was enabled, the output would look like this:

```json
[
  {
    "path": "/home/user/Dgraph/20.11/backup/manifest.json",
    "since": 30005,
    "backup_id": "reverent_vaughan0",
    "backup_num": 1,
    "encrypted": false,
    "type": "full",
    "groups": {
      "1": [
        "dgraph.graphql.schema_created_at",
        "dgraph.graphql.xid",
        "dgraph.drop.op",
        "dgraph.type",
        "dgraph.cors",
        "dgraph.graphql.schema_history",
        "score",
        "dgraph.graphql.p_query",
        "dgraph.graphql.schema",
        "dgraph.graphql.p_sha256hash",
        "series"
      ]
    }
  }
]
```

### Return values

* `path`: Name of the backup

* `since`: is the timestamp at which this backup was taken. It is called Since
  because it will become the timestamp from which to backup in the next
  incremental backup.

* `groups`: is the map of valid groups to predicates at the time the backup was
  created. This is printed only if `--verbose` flag is enabled

* `encrypted`: Indicates whether this backup is encrypted or not

* `type`: Indicates whether this backup is a full or incremental one

* `drop_operation`: lists the various DROP operations that took place since the
  last backup. These are used during restore to redo those operations before
  applying the backup. (This is printed only if `--verbose` flag is enabled)

* `backup_num`: is a monotonically increasing number assigned to each backup in
  a series. The full backup as BackupNum equal to one and each incremental
  backup gets assigned the next available number. This can be used to verify the
  integrity of the data during a restore.

* `backup_id`: is a unique ID assigned to all the backups in the same series.

## Examples

### S3

Checking information about backups stored in an AWS S3 bucket:

```sh
dgraph lsbackup -l s3:///s3.us-west-2.amazonaws.com/dgraph_backup
```

You might need to set up access and secret key environment variables in the
shell (or session) you are going to run the `lsbackup` command. For example:

```
AWS_SECRET_ACCESS_KEY=<paste-your-secret-access-key>
AWS_ACCESS_ID=<paste-your-key-id>
```

### MinIO

Checking information about backups stored in a MinIO bucket:

```sh
dgraph lsbackup -l minio://localhost:9000/dgraph_backup
```

In case the MinIO server is started without `tls`, you must specify that
`secure=false` as it set to `true` by default. You also need to set the
environment variables for the access key and secret key.

In order to get the `lsbackup` running, you should following these steps:

* Set `MINIO_ACCESS_KEY` as an environment variable for the running shell this
  can be done with the following command: (`minioadmin` is the default access
  key, unless is changed by the user)

  ```
  export MINIO_ACCESS_KEY=minioadmin
  ```

* Set MINIO\_SECRET\_KEY as an environment variable for the running shell this can
  be done with the following command: (`minioadmin` is the default secret key,
  unless is changed by the user)

  ```
  export MINIO_SECRET_KEY=minioadmin
  ```

* Add the argument `secure=false` to the `lsbackup command`, that means the
  command will look like: (the double quotes `"` are required)

  ```sh
  dgraph lsbackup -l "minio://localhost:9000/<bucket-name>?secure=false"
  ```

### Local

Checking information about backups stored locally (on disk):

```sh
dgraph lsbackup -l ~/dgraph_backup
```


# Multi-Tenancy
Source: https://docs.hypermode.com/dgraph/enterprise/multitenancy



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Multi-tenancy is an enterprise-only feature that allows various tenants to
co-exist in the same Dgraph Cluster using `uint64` namespaces. With
multi-tenancy, each tenant can only log into their own namespace and operate in
their own namespace.

<Note>
  Multi-tenancy is an enterprise feature and needs [Access Control
  Lists](./access-control-lists) (ACL) enabled to work.
</Note>

## Overview

Multi-tenancy is built upon [Access Control Lists](./access-control-lists)
(ACL), and enables multiple tenants to share a Dgraph Cluster using unique
namespaces. The tenants are logically separated, and their data lies in the same
`p` directory. Each namespace has a group guardian, which has root access to
that namespace.

The default namespace is called a `galaxy`.
[Guardians of the Galaxy](#guardians-of-the-galaxy) get special access to create
or delete namespaces and change passwords of users of other namespaces.

<Note>
  Dgraph provides a timeout limit per query that's configurable using the
  `--limit` superflag's `query-limit` option. There's no time limit for queries
  by default, but you can override it when running Dgraph Alpha. For
  multi-tenant environments a suggested `query-limit` value is 500ms.
</Note>

## FAQ

* How access controls and policies are handled among different tenants?

  In previous versions of Dgraph, the
  [Access Control Lists](./access-control-lists) (ACL) feature offered a unified
  control solution across the entire database. With the new multi-tenancy
  feature, the ACL policies are now scoped down to individual tenants in the
  database.

<Note>
  Only super-admins ([Guardians of the galaxy](#guardians-of-the-galaxy)) have
  access across tenants. The super admin is used only for database
  administration operations, such as exporting data of all tenants. *Guardian*
  of the *Galaxy* group cannot read across tenants.
</Note>

* What's the ACL granularity in a multi-tenancy environment? Is it per tenant?

  The access controls are applied per tenant to either specific predicates or
  all predicates that exist for the tenant. For example, the user `John Smith`
  belonging to the group `Data Approvers` for a tenant `Accounting` may only
  have read-only access over predicates while user `Jane Doe`, belonging to the
  group `Data Editors` within that same tenant, may have access to modify those
  predicates. All the ACL rules need to be defined for each tenant in your
  backend. The level of granularity available allows for defining rules over
  specific predicates or all predicates belonging to that tenant.

* Are tenants a physical separation or a logical one?

  Tenants are a logical separation. In this example, data needs to be written
  twice for 2 different tenants. Each client must authenticate within a tenant,
  and can only modify data within the tenant as allowed by the configured ACLs.

* Can data be copied from one tenant to the other?

  Yes, but not by breaking any ACL or tenancy constraints. This can be done by
  exporting data from one tenant and importing data to another.

## Namespace

A multi-tenancy Namespace acts as a logical silo, so data stored in one
namespace is not accessible from another namespace. Each namespace has a group
guardian (with root access to that namespace), and a unique `uint64` identifier.
Users are members of a single namespace, and cross-namespace queries are not
allowed.

<Note>
  If a user wants to access multiple namespaces, the user needs to be created
  separately for each namespace.
</Note>

The default namespace (`0x00`) is called a `galaxy`. A
[Guardian of the Galaxy](#guardians-of-the-galaxy) has special access to create
or delete namespaces and change passwords of users of other namespaces.

## Access Control Lists

Multi-tenancy defines certain ACL roles for the shared Cluster:

* [Guardians of the Galaxy](#guardians-of-the-galaxy) (Super Admins)

* Guardians of the namespace can perform the following operations:

  * create users and groups within the namespace

  * assign users to groups within the namespace

  * assign predicates to groups within the namespace

  * add users to groups within the namespace

  * export namespac

    e

  * drop data within the namespace

  * query and mutate within the namespace

  <Note>
    Guardians of the namespace cannot query or mutate across namespaces.
  </Note>

* Normal users can perform the following operations:

  * login into a namespace
  * query within the namespace
  * mutate within the namespace

  <Note> Normal users cannot query or mutate across namespaces.</Note>

### Guardians of the Galaxy

A *Guardian of the Galaxy* is a Super Admin of the default namespace (`0x00`).

As a super-admin, a *Guardian of the Galaxy* can:

* [Create](#create-a-namespace) and [delete](#delete-a-namespace) namespaces
* Reset the passwords
* Query and mutate the default namespace (`0x00`)
* Trigger Cluster-wide [backups](#backups) (no namespace-specific backup)
* Trigger Cluster-wide or namespace-specific [exports](#exports) (exports
  contain information about the namespace)

For example, if the user `rocket` is part of the *Guardians of the Galaxy* group
(namespace `0x00`), he can only read/write on namespace `0x00`.

## Create a Namespace

Only members of the [Guardians of the Galaxy](#guardians-of-the-galaxy) group
can create a namespace. A namespace can be created by calling `/admin` with the
`addNamespace` mutation, and will return the assigned number for the new
namespace.

<Note>
  To create a namespace, the *Guardian* must send the JWT access token in the
  `X-Dgraph-AccessToken` header.
</Note>

For example, to create a new namespace:

```graphql
mutation {
  addNamespace(input: { password: "mypass" }) {
    namespaceId
    message
  }
}
```

By sending the mutation above, a namespace is created. A *Guardian group* is
also automatically created for that namespace. A `groot` user with password
`mypass` (default is `password`) is created in the guardian group. You can then
use these credentials to login into the namespace and perform operations like
[`addUser`](./access-control-lists#create-a-regular-user).

## List Namespaces

Only members of the [Guardians of the Galaxy](#guardians-of-the-galaxy) group
can list active namespaces. You can check available namespaces using the
`/state` endpoint.

For example, if you have a multi-tenant Cluster with multiple namespaces, as a
*Guardian of the Galaxy* you can query `state` from GraphQL:

```graphql
query {
  state {
    namespaces
  }
}
```

In the response, namespaces that are available and active are listed.

```json
{
  "data": {
    "state": {
      "namespaces": [2, 1, 0]
    }
  }
}
```

## Delete a Namespace

Only members of the [Guardians of the Galaxy](#guardians-of-the-galaxy) group
can delete a namespace. A namespace can be dropped by calling `/admin` with the
`deleteNamespace` mutation.

<Note>
  To delete a namespace, the *Guardian* must send the JWT access token in the
  `X-Dgraph-AccessToken` header.
</Note>

For example, to drop the namespace `123`:

```graphql
mutation {
  deleteNamespace(input: { namespaceId: 123 }) {
    namespaceId
    message
  }
}
```

<Note>
  Members of `namespace-guardians` can't delete namespaces, they can only
  perform queries and mutations.
</Note>

## Reset passwords

Only members of the *Guardians of the Galaxy* can reset passwords across
namespaces. A password can be reset by calling `/admin` with the `resetPassword`
mutation.

For example, to reset the password for user `groot` from the namespace `100`:

```graphql
mutation {
  resetPassword(
    input: { userId: "groot", password: "newpassword", namespace: 100 }
  ) {
    userId
    message
  }
}
```

## Drop operations

The `drop all` operations can be triggered only by a
[Guardian of the Galaxy](#guardians-of-the-galaxy). They're executed at Cluster
level and delete data across namespaces. All other `drop` operations run at
namespace level and are namespace specific. For information about other drop
operations, see [Alter the database](/dgraph/http#alter-the-database).

<Note>
  `drop all` operation is executed at Cluster level and the operation deletes
  data and schema across namespaces. Guardian of the namespace can trigger `drop
        data` operation within the namespace. The `drop data` operation deletes all
  the data but retains the schema only.
</Note>

For example:

```sh
curl 'http://localhost:8080/alter' \
  -H 'X-Dgraph-AccessToken: <your-access-token>' \
  --data-raw '{"drop_op":"DATA"}' \
  --compressed
```

## Backups

Backups are currently Cluster-wide only, but [exports](#exports) can be created
by namespace. Only a [Guardian of the Galaxy](#guardians-of-the-galaxy) can
trigger a backup.

### Data import

[Initial import](/dgraph/admin/bulk-loader) and
[Live import](/dgraph/admin/live-loader) tools support multi-tenancy.

## Exports

Exports can be generated Cluster-wide or at namespace level. These exported sets
of `.rdf` or `.json` files and schemas include the multi-tenancy namespace
information.

If a *Guardian of the Galaxy* exports the whole Cluster, a single folder
containing the export data of all the namespaces in a single `.rdf` or `.json`
file and a single schema will be generated.

<Note>Guardians of a Namespace can trigger an Export for their namespace.</Note>

A namespace-specific export will contain the namespace value in the generated
`.rdf` file:

```rdf
<0x01> "name" "ibrahim" <0x12> .     -> this belongs to namespace 0x12
<0x01> "name" "ibrahim" <0x0> .      -> this belongs to namespace 0x00
```

For example, when the *Guardian of the Galaxy* user is used to export the
namespace `0x1234` to a folder in the export directory (by default this
directory is `export`):

```graphql
mutation {
  export(input: { format: "rdf", namespace: 1234 }) {
    response {
      message
    }
  }
}
```

When using the *Guardian of the Namespace*, there's no need to specify the
namespace in the GraphQL mutation, as they can only export within their own
namespace:

```graphql
mutation {
  export(input: {format: "rdf") {
    response {
      message
    }
  }
}
```

To export all the namespaces: (this is only valid for *Guardians of the Galaxy*)

```graphql
mutation {
  export(input: { format: "rdf", namespace: -1 }) {
    response {
      message
    }
  }
}
```


# Enterprise Features
Source: https://docs.hypermode.com/dgraph/enterprise/overview



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

<div class="landing">
  <div class="hero">
    <p>
      Exclusive features like ACLs, binary backups, encryption at rest, and more.
    </p>
  </div>

  <div class="item">
    <div class="icon">
      <i class="lni lni-download" aria-hidden="true" />
    </div>

    <a href="./binary-backups.md">
      <h2>Binary Backups</h2>

      <p>
        Full backups of Dgraph that are backed up directly to cloud storage
      </p>
    </a>
  </div>

  <div class="item">
    <div class="icon">
      <i class="lni lni-control-panel" aria-hidden="true" />
    </div>

    <a href="./access-control-lists.md">
      <h2>Access Control Lists</h2>

      <p>
        ACL provides access protection to your data stored in Dgraph
      </p>
    </a>
  </div>

  <div class="item">
    <div class="icon">
      <i class="lni lni-lock-alt" aria-hidden="true" />
    </div>

    <a href="./encryption-at-rest.md">
      <h2>Encryption at Rest</h2>

      <p>
        Ensure that sensitive data on disks is not readable by any user or app
      </p>
    </a>
  </div>

  <div class="item">
    <div class="icon">
      <i class="lni lni-keyword-research" aria-hidden="true" />
    </div>

    <a href="./audit-logs.md">
      <h2>Audit Logging</h2>

      <p>
        All requests are tracked and available for use in security audits.
      </p>
    </a>
  </div>

  <div class="item">
    <div class="icon">
      <i class="lni lni-network" aria-hidden="true" />
    </div>

    <a href="./multitenancy.md">
      <h2>Multi-Tenancy</h2>

      <p>
        Multiple tenants co-exist in the same Dgraph cluster.
      </p>
    </a>
  </div>

  <div class="item">
    <div class="icon">
      <i class="lni lni-files" aria-hidden="true" />
    </div>

    <a href="./learner-nodes.md">
      <h2>Learner Nodes</h2>

      <p>
        Spin-up a read-only replica instance across the world.
      </p>
    </a>
  </div>

  <div class="item">
    <div class="icon">
      <i class="lni lni-cog" aria-hidden="true" />
    </div>

    <a href="./change-data-capture.md">
      <h2>Change Data Capture</h2>

      <p>
        Track data changes over time.
      </p>
    </a>
  </div>
</div>


# Glossary
Source: https://docs.hypermode.com/dgraph/glossary

Dgraph terms

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

### Alpha

A Dgraph cluster consists of [Zero](#zero) and Alpha nodes. Alpha nodes host
relationships (also known as predicates) and indexes. Dgraph scales horizontally
by adding more Alphas.

### Badger

Badger is a fast, open source key-value database written in pure Go that
provides the storage layer for Dgraph. More at
[Badger documentation](/badger/overview)

### DQL

Dgraph Query Language is Dgraph's proprietary language to insert, update, delete
and query data. It is based on GraphQL, but is more expressive.

### Edge

In the mental picture of a graph "bubbles connected by lines," the bubbles are
nodes, the lines are edges. In Dgraph terminology edges are
[relationships](#relationship) i.e an information about the relation between two
nodes.

### Facet

A facet represents a property of a [relationship](#relationship).

### Graph

A graph is a simple structure that maps relations between objects. In Dgraph
terminology, the objects are [nodes](#node) and the connections between them are
[relationships](#relationship).

### GraphQL

[GraphQL](https://graphql.org/) is a declarative language for querying data used
by app developers to get the data they need using GraphQL APIs. GraphQL is an
open standard with a robust ecosystem. Dgraph supports the deployment of a
GraphQL data model (GraphQL schema) and automatically exposes a GraphQL API
endpoint accepting GraphQL queries.

### gRPC

[gRPC](https://grpc.io/) is a high performance Remote Procedure Call (RPC)
framework used by Dgraph to interface with clients. Dgraph has official gRPC
clients for go, C#, Java, JavaScript and Python. Apps written in those language
can perform mutations and queries inside transactions using Dgraph clients.

### Lambda

A Lambda Resolver (Lambda for short) is a GraphQL resolver supported within
Dgraph. A Lambda is a user-defined JavaScript function that performs custom
actions over the GraphQL types, interfaces, queries, and mutations. Dgraph
Lambdas are unrelated to AWS Lambdas.

### Mutation

A mutation is a request to modify the database. Mutations include insert,
update, or delete operations. A Mutation can be combined with a query to form an
[Upsert](#upsert).

### Node

Conceptually, a node is "a thing" or an object of the business domain. For every
node, Dgraph stores and maintains a universal identifier [UID](#uid), a list of
properties, and the [relationships](#relationship) the node has with other
nodes.

The term "node" is also used in software architecture to reference a physical
computer or a virtual machine running a module of Dgraph in a cluster. See
[Alpha node](#alpha) and [Zero node](#zero).

### Predicate

In [RDF](#rdf) terminology, a predicate is the smallest piece of information
about an object. A predicate can hold a literal value or can describe a relation
to another entity :

* When we store that an entity name is "Alice." The predicate is `name` and
  predicate value is the string `Alice`. It becomes a node property.
* When we store that Alice knows Bob, we may use a predicate `knows` with the
  node representing Alice. The value of this predicate would be the [UID](#uid)
  of the node representing Bob. In that case, `knows` is a
  [relationship](#relationship).

### Ratel

Ratel is an open source GUI tool for data visualization and cluster management
that’s designed to work with Dgraph and DQL. See also:
[Ratel Overview](./ratel/overview).

### RDF

RDF 1.1 is a Semantic Web Standard for data interchange. It allows us to make
statements about resources. The format of these statements is simple and in the
form of `<subject>> <predicate> <object>`. Dgraph supports the RDF format to
create, import and export data. Note that Dgraph also supports the JSON format.

### Relationship

A relationship is a named, directed link relating one [node](#node) to another.
It is the Dgraph term similar to [edge](#edge) and [predicate](#predicate). In
Dgraph a relationship may itself have properties representing information about
the relation, such as weight, cost, time frame, or type. In Dgraph the
properties of a relationship are called [facets](#facet).

### Sharding

Sharding is a database architecture pattern to achieve horizontal scale by
distributing data among many servers. Dgraph shards data per relationship, so
all data for one relationship form a single shard, and are stored on one (group
of) servers, an approach referred to as 'predicate-based sharding'.

### Triple

Because RDF statements consist of three elements:
`<subject> <predicate> <object>`, they're called triples. A triple represents a
single atomic statement about a node. The object in an RDF triple can be a
literal value or can point to another node. See
[DQL RDF Syntax](/dgraph/dql/rdf) for more details.

* When we store that a node name is "Alice." The predicate is `name` and
  predicate value is the string `Alice`. The string becomes a node property.
* When we store that Alice knows Bob, we may use a predicate `knows` with the
  node representing Alice. The value of this predicate would be the [UID](#uid)
  of the node representing Bob. In that case, `knows` is a
  [relationship](#relationship).

### UID

A UID is the Universal Identifier of a node. `uid` is a reserved property
holding the UID value for every node. UIDs can either be generated by Dgraph
when creating nodes, or can be set explicitly.

### Upsert

An upsert operation combines a Query with a [Mutation](#mutation). Typically, a
node is searched for, and then depending on if it's found or not, a new node is
created with associated predicates or the existing node relationships are
updated. Upsert operations are important to implement uniqueness of predicates.

### Zero

Dgraph consists of Zero and [Alpha](#alpha) nodes. Zero nodes control the Dgraph
database cluster. It assigns Alpha nodes to groups, re-balances data between
groups, handles transaction timestamp and UID assignment.


# API Endpoints
Source: https://docs.hypermode.com/dgraph/graphql/api

This documentation presents the Admin API and explains how to run a Dgraph database with GraphQL.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

This article presents the Admin API and explains how to run a Dgraph database
with GraphQL.

## Running Dgraph with GraphQL

The simplest way to start with Dgraph GraphQL is to run the all-in-one Docker
image.

```
docker run -it -p 8080:8080 dgraph/standalone:%VERSION_HERE
```

That brings up GraphQL at `localhost:8080/graphql` and `localhost:8080/admin`,
but is intended for quickstart and doesn't persist data.

## Advanced options

Once you've tried out Dgraph GraphQL, you'll need to move past the
`dgraph/standalone` and run and deploy Dgraph instances.

Dgraph is a distributed graph database. It can scale to huge data and shard that
data across a cluster of Dgraph instances. GraphQL is built into Dgraph in its
Alpha nodes. To learn how to manage and deploy a Dgraph cluster, check our
[deployment guide](/dgraph/self-managed/overview).

GraphQL schema introspection is enabled by default, but you can disable it by
setting the `--graphql` superflag's `introspection` option to false
(`--graphql introspection=false`) when starting the Dgraph Alpha nodes in your
cluster.

## Dgraph's schema

Dgraph's GraphQL runs in Dgraph and presents a GraphQL schema where the queries
and mutations are executed in the Dgraph cluster. So the GraphQL schema is
backed by Dgraph's schema.

<Warning>
  This means that if you have a Dgraph instance and change its GraphQL schema,
  the schema of the underlying Dgraph will also be changed!
</Warning>

## Endpoints

When you start Dgraph with GraphQL, two GraphQL endpoints are served.

### /graphql

At `/graphql` you'll find the GraphQL API for the types you've added. That's
what your app would access and is the GraphQL entry point to Dgraph. If you need
to know more about this, see the [quick start](./quickstart) and
[schema docs](./schema/overview).

### /admin

At `/admin` you'll find an admin API for administering your GraphQL instance.
The admin API is a GraphQL API that serves POST and GET as well as compressed
data, much like the `/graphql` endpoint.

Here are the important types, queries, and mutations from the `admin` schema.

```graphql
"""
The Int64 scalar type represents a signed 64‐bit numeric non‐fractional value.
Int64 can represent values in range [-(2^63),(2^63 - 1)].
"""
scalar Int64

"""
The UInt64 scalar type represents an unsigned 64‐bit numeric non‐fractional value.
UInt64 can represent values in range [0,(2^64 - 1)].
"""
scalar UInt64

"""
The DateTime scalar type represents date and time as a string in RFC3339 format.
For example: "1985-04-12T23:20:50.52Z" represents 20 minutes and 50.52 seconds after the 23rd hour of April 12th, 1985 in UTC.
"""
scalar DateTime

"""
Data about the GraphQL schema being served by Dgraph.
"""
type GQLSchema @dgraph(type: "dgraph.graphql") {
  id: ID!

  """
  Input schema (GraphQL types) that was used in the latest schema update.
  """
  schema: String! @dgraph(pred: "dgraph.graphql.schema")

  """
  The GraphQL schema that was generated from the 'schema' field.
  This is the schema that is being served by Dgraph at /graphql.
  """
  generatedSchema: String!
}

type Cors @dgraph(type: "dgraph.cors") {
  acceptedOrigins: [String]
}

"""
A NodeState is the state of an individual node in the Dgraph cluster.
"""
type NodeState {
  """
  Node type : either 'alpha' or 'zero'.
  """
  instance: String

  """
  Address of the node.
  """
  address: String

  """
  Node health status : either 'healthy' or 'unhealthy'.
  """
  status: String

  """
  The group this node belongs to in the Dgraph cluster.
  See : https://docs.hypermode.com/dgraph/self-managed/cluster-setup/.
  """
  group: String

  """
  Version of the Dgraph binary.
  """
  version: String

  """
  Time in nanoseconds since the node started.
  """
  uptime: Int64

  """
  Time in Unix epoch time that the node was last contacted by another Zero or Alpha node.
  """
  lastEcho: Int64

  """
  List of ongoing operations in the background.
  """
  ongoing: [String]

  """
  List of predicates for which indexes are built in the background.
  """
  indexing: [String]

  """
  List of Enterprise Features that are enabled.
  """
  ee_features: [String]
}

type MembershipState {
  counter: UInt64
  groups: [ClusterGroup]
  zeros: [Member]
  maxUID: UInt64
  maxNsID: UInt64
  maxTxnTs: UInt64
  maxRaftId: UInt64
  removed: [Member]
  cid: String
  license: License
  """
  Contains list of namespaces. Note that this is not stored in proto's MembershipState and
  computed at the time of query.
  """
  namespaces: [UInt64]
}

type ClusterGroup {
  id: UInt64
  members: [Member]
  tablets: [Tablet]
  snapshotTs: UInt64
  checksum: UInt64
}

type Member {
  id: UInt64
  groupId: UInt64
  addr: String
  leader: Boolean
  amDead: Boolean
  lastUpdate: UInt64
  clusterInfoOnly: Boolean
  forceGroupId: Boolean
}

type Tablet {
  groupId: UInt64
  predicate: String
  force: Boolean
  space: Int
  remove: Boolean
  readOnly: Boolean
  moveTs: UInt64
}

type License {
  user: String
  maxNodes: UInt64
  expiryTs: Int64
  enabled: Boolean
}

directive @dgraph(
  type: String
  pred: String
) on OBJECT | INTERFACE | FIELD_DEFINITION
directive @id on FIELD_DEFINITION
directive @secret(field: String!, pred: String) on OBJECT | INTERFACE

type UpdateGQLSchemaPayload {
  gqlSchema: GQLSchema
}

input UpdateGQLSchemaInput {
  set: GQLSchemaPatch!
}

input GQLSchemaPatch {
  schema: String!
}

input ExportInput {
  """
  Data format for the export, e.g. "rdf" or "json" (default: "rdf")
  """
  format: String
  """
  Namespace for the export in multi-tenant cluster. Users from guardians of galaxy can export
  all namespaces by passing a negative value or specific namespaceId to export that namespace.
  """
  namespace: Int

  """
  Destination for the export: e.g. Minio or S3 bucket or /absolute/path
  """
  destination: String

  """
  Access key credential for the destination.
  """
  accessKey: String

  """
  Secret key credential for the destination.
  """
  secretKey: String

  """
  AWS session token, if required.
  """
  sessionToken: String

  """
  Set to true to allow backing up to S3 or Minio bucket that requires no credentials.
  """
  anonymous: Boolean
}

input TaskInput {
  id: String!
}
type Response {
  code: String
  message: String
}

type ExportPayload {
  response: Response
  exportedFiles: [String]
}

type DrainingPayload {
  response: Response
}

type ShutdownPayload {
  response: Response
}

type TaskPayload {
  kind: TaskKind
  status: TaskStatus
  lastUpdated: DateTime
}
enum TaskStatus {
  Queued
  Running
  Failed
  Success
  Unknown
}
enum TaskKind {
  Backup
  Export
  Unknown
}
input ConfigInput {
  """
  Estimated memory the caches can take. Actual usage by the process would be
  more than specified here. The caches will be updated according to the
  cache_percentage flag.
  """
  cacheMb: Float

  """
  True value of logRequest enables logging of all the requests coming to alphas.
  False value of logRequest disables above.
  """
  logRequest: Boolean
}

type ConfigPayload {
  response: Response
}

type Config {
  cacheMb: Float
}
input RemoveNodeInput {
  """
  ID of the node to be removed.
  """
  nodeId: UInt64!
  """
  ID of the group from which the node is to be removed.
  """
  groupId: UInt64!
}
type RemoveNodePayload {
  response: Response
}
input MoveTabletInput {
  """
  Namespace in which the predicate exists.
  """
  namespace: UInt64
  """
  Name of the predicate to move.
  """
  tablet: String!
  """
  ID of the destination group where the predicate is to be moved.
  """
  groupId: UInt64!
}
type MoveTabletPayload {
  response: Response
}
enum AssignKind {
  UID
  TIMESTAMP
  NAMESPACE_ID
}
input AssignInput {
  """
  Choose what to assign: UID, TIMESTAMP or NAMESPACE_ID.
  """
  what: AssignKind!
  """
  How many to assign.
  """
  num: UInt64!
}
type AssignedIds {
  """
  The first UID, TIMESTAMP or NAMESPACE_ID assigned.
  """
  startId: UInt64
  """
  The last UID, TIMESTAMP or NAMESPACE_ID assigned.
  """
  endId: UInt64
  """
  TIMESTAMP for read-only transactions.
  """
  readOnly: UInt64
}
type AssignPayload {
  response: AssignedIds
}

input BackupInput {
  """
  Destination for the backup: e.g. Minio or S3 bucket.
  """
  destination: String!
  """
  Access key credential for the destination.
  """
  accessKey: String
  """
  Secret key credential for the destination.
  """
  secretKey: String
  """
  AWS session token, if required.
  """
  sessionToken: String
  """
  Set to true to allow backing up to S3 or Minio bucket that requires no credentials.
  """
  anonymous: Boolean
  """
  Force a full backup instead of an incremental backup.
  """
  forceFull: Boolean
}
type BackupPayload {
  response: Response
  taskId: String
}
input RestoreInput {
  """
  Destination for the backup: e.g. Minio or S3 bucket.
  """
  location: String!
  """
  Backup ID of the backup series to restore. This ID is included in the manifest.json file.
  If missing, it defaults to the latest series.
  """
  backupId: String
  """
  Number of the backup within the backup series to be restored. Backups with a greater value
  will be ignored. If the value is zero or missing, the entire series will be restored.
  """
  backupNum: Int
  """
  Path to the key file needed to decrypt the backup. This file should be accessible
  by all alphas in the group. The backup will be written using the encryption key
  with which the cluster was started, which might be different than this key.
  """
  encryptionKeyFile: String
  """
  Vault server address where the key is stored. This server must be accessible
  by all alphas in the group. Default "http://localhost:8200".
  """
  vaultAddr: String
  """
  Path to the Vault RoleID file.
  """
  vaultRoleIDFile: String
  """
  Path to the Vault SecretID file.
  """
  vaultSecretIDFile: String
  """
  Vault kv store path where the key lives. Default "secret/data/dgraph".
  """
  vaultPath: String
  """
  Vault kv store field whose value is the key. Default "enc_key".
  """
  vaultField: String
  """
  Vault kv store field's format. Must be "base64" or "raw". Default "base64".
  """
  vaultFormat: String
  """
  Access key credential for the destination.
  """
  accessKey: String
  """
  Secret key credential for the destination.
  """
  secretKey: String
  """
  AWS session token, if required.
  """
  sessionToken: String
  """
  Set to true to allow backing up to S3 or Minio bucket that requires no credentials.
  """
  anonymous: Boolean
}
type RestorePayload {
  """
  A short string indicating whether the restore operation was successfully scheduled.
  """
  code: String
  """
  Includes the error message if the operation failed.
  """
  message: String
}
input ListBackupsInput {
  """
  Destination for the backup: e.g. Minio or S3 bucket.
  """
  location: String!
  """
  Access key credential for the destination.
  """
  accessKey: String
  """
  Secret key credential for the destination.
  """
  secretKey: String
  """
  AWS session token, if required.
  """
  sessionToken: String
  """
  Whether the destination doesn't require credentials (e.g. S3 public bucket).
  """
  anonymous: Boolean
}
type BackupGroup {
  """
  The ID of the cluster group.
  """
  groupId: UInt64
  """
  List of predicates assigned to the group.
  """
  predicates: [String]
}
type Manifest {
  """
  Unique ID for the backup series.
  """
  backupId: String
  """
  Number of this backup within the backup series. The full backup always has a value of one.
  """
  backupNum: UInt64
  """
  Whether this backup was encrypted.
  """
  encrypted: Boolean
  """
  List of groups and the predicates they store in this backup.
  """
  groups: [BackupGroup]
  """
  Path to the manifest file.
  """
  path: String
  """
  The timestamp at which this backup was taken. The next incremental backup will
  start from this timestamp.
  """
  since: UInt64
  """
  The type of backup, either full or incremental.
  """
  type: String
}
type LoginResponse {
  """
  JWT token that should be used in future requests after this login.
  """
  accessJWT: String
  """
  Refresh token that can be used to re-login after accessJWT expires.
  """
  refreshJWT: String
}
type LoginPayload {
  response: LoginResponse
}
type User
  @dgraph(type: "dgraph.type.User")
  @secret(field: "password", pred: "dgraph.password") {
  """
  Username for the user.  Dgraph ensures that usernames are unique.
  """
  name: String! @id @dgraph(pred: "dgraph.xid")
  groups: [Group] @dgraph(pred: "dgraph.user.group")
}
type Group @dgraph(type: "dgraph.type.Group") {
  """
  Name of the group.  Dgraph ensures uniqueness of group names.
  """
  name: String! @id @dgraph(pred: "dgraph.xid")
  users: [User] @dgraph(pred: "~dgraph.user.group")
  rules: [Rule] @dgraph(pred: "dgraph.acl.rule")
}
type Rule @dgraph(type: "dgraph.type.Rule") {
  """
  Predicate to which the rule applies.
  """
  predicate: String! @dgraph(pred: "dgraph.rule.predicate")
  """
  Permissions that apply for the rule.  Represented following the UNIX file permission
  convention. That is, 4 (binary 100) represents READ, 2 (binary 010) represents WRITE,
  and 1 (binary 001) represents MODIFY (the permission to change a predicate’s schema).
  The options are:
  * 1 (binary 001) : MODIFY
  * 2 (010) : WRITE
  * 3 (011) : WRITE+MODIFY
  * 4 (100) : READ
  * 5 (101) : READ+MODIFY
  * 6 (110) : READ+WRITE
  * 7 (111) : READ+WRITE+MODIFY
  Permission 0, which is equal to no permission for a predicate, blocks all read,
  write and modify operations.
  """
  permission: Int! @dgraph(pred: "dgraph.rule.permission")
}
input StringHashFilter {
  eq: String
}
enum UserOrderable {
  name
}
enum GroupOrderable {
  name
}
input AddUserInput {
  name: String!
  password: String!
  groups: [GroupRef]
}
input AddGroupInput {
  name: String!
  rules: [RuleRef]
}
input UserRef {
  name: String!
}
input GroupRef {
  name: String!
}
input RuleRef {
  """
  Predicate to which the rule applies.
  """
  predicate: String!
  """
  Permissions that apply for the rule.  Represented following the UNIX file permission
  convention. That is, 4 (binary 100) represents READ, 2 (binary 010) represents WRITE,
  and 1 (binary 001) represents MODIFY (the permission to change a predicate’s schema).
  The options are:
  * 1 (binary 001) : MODIFY
  * 2 (010) : WRITE
  * 3 (011) : WRITE+MODIFY
  * 4 (100) : READ
  * 5 (101) : READ+MODIFY
  * 6 (110) : READ+WRITE
  * 7 (111) : READ+WRITE+MODIFY
  Permission 0, which is equal to no permission for a predicate, blocks all read,
  write and modify operations.
  """
  permission: Int!
}
input UserFilter {
  name: StringHashFilter
  and: UserFilter
  or: UserFilter
  not: UserFilter
}
input UserOrder {
  asc: UserOrderable
  desc: UserOrderable
  then: UserOrder
}
input GroupOrder {
  asc: GroupOrderable
  desc: GroupOrderable
  then: GroupOrder
}
input UserPatch {
  password: String
  groups: [GroupRef]
}
input UpdateUserInput {
  filter: UserFilter!
  set: UserPatch
  remove: UserPatch
}
input GroupFilter {
  name: StringHashFilter
  and: UserFilter
  or: UserFilter
  not: UserFilter
}
input SetGroupPatch {
  rules: [RuleRef!]!
}
input RemoveGroupPatch {
  rules: [String!]!
}
input UpdateGroupInput {
  filter: GroupFilter!
  set: SetGroupPatch
  remove: RemoveGroupPatch
}
type AddUserPayload {
  user: [User]
}
type AddGroupPayload {
  group: [Group]
}
type DeleteUserPayload {
  msg: String
  numUids: Int
}
type DeleteGroupPayload {
  msg: String
  numUids: Int
}
input AddNamespaceInput {
  password: String
}
input DeleteNamespaceInput {
  namespaceId: Int!
}
type NamespacePayload {
  namespaceId: UInt64
  message: String
}
input ResetPasswordInput {
  userId: String!
  password: String!
  namespace: Int!
}
type ResetPasswordPayload {
  userId: String
  message: String
  namespace: UInt64
}
input EnterpriseLicenseInput {
  """
  The contents of license file as a String.
  """
  license: String!
}
type EnterpriseLicensePayload {
  response: Response
}

type Query {
  getGQLSchema: GQLSchema
  health: [NodeState]
  state: MembershipState
  config: Config
  task(input: TaskInput!): TaskPayload

  getUser(name: String!): User
  getGroup(name: String!): Group
  """
  Get the currently logged in user.
  """
  getCurrentUser: User
  queryUser(
    filter: UserFilter
    order: UserOrder
    first: Int
    offset: Int
  ): [User]
  queryGroup(
    filter: GroupFilter
    order: GroupOrder
    first: Int
    offset: Int
  ): [Group]
  """
  Get the information about the backups at a given location.
  """
  listBackups(input: ListBackupsInput!): [Manifest]
}
type Mutation {
  """
  Update the Dgraph cluster to serve the input schema.  This may change the GraphQL
  schema, the types and predicates in the Dgraph schema, and cause indexes to be recomputed.
  """
  updateGQLSchema(input: UpdateGQLSchemaInput!): UpdateGQLSchemaPayload

  """
  Starts an export of all data in the cluster.  Export format should be 'rdf' (the default
  if no format is given), or 'json'.
  See : https://docs.hypermode.com/dgraph/admin/export
  """
  export(input: ExportInput!): ExportPayload

  """
  Set (or unset) the cluster draining mode.  In draining mode no further requests are served.
  """
  draining(enable: Boolean): DrainingPayload

  """
  Shutdown this node.
  """
  shutdown: ShutdownPayload

  """
  Alter the node's config.
  """
  config(input: ConfigInput!): ConfigPayload
  """
  Remove a node from the cluster.
  """
  removeNode(input: RemoveNodeInput!): RemoveNodePayload
  """
  Move a predicate from one group to another.
  """
  moveTablet(input: MoveTabletInput!): MoveTabletPayload
  """
  Lease UIDs, Timestamps or Namespace IDs in advance.
  """
  assign(input: AssignInput!): AssignPayload

  """
  Start a binary backup.  See : https://docs.hypermode.com/dgraph/enterprise/binary-backups/#create-a-backup
  """
  backup(input: BackupInput!): BackupPayload
  """
  Start restoring a binary backup.  See : https://docs.hypermode.com/enterprise/binary-backups/#online-restore
  """
  restore(input: RestoreInput!): RestorePayload
  """
  Login to Dgraph.  Successful login results in a JWT that can be used in future requests.
  If login is not successful an error is returned.
  """
  login(
    userId: String
    password: String
    namespace: Int
    refreshToken: String
  ): LoginPayload
  """
  Add a user.  When linking to groups: if the group doesn't exist it's created; if the group
  exists, the new user is linked to the existing group.  It is possible to both create new
  groups and link to existing groups in the one mutation.
  Dgraph ensures that usernames are unique, hence attempting to add an existing user results
  in an error.
  """
  addUser(input: [AddUserInput!]!): AddUserPayload
  """
  Add a new group and (optionally) set the rules for the group.
  """
  addGroup(input: [AddGroupInput!]!): AddGroupPayload
  """
  Update users, their passwords and groups.  As with AddUser, when linking to groups: if the
  group doesn't exist it's created; if the group exists, the new user is linked to the existing
  group.  If the filter doesn't match any users, the mutation has no effect.
  """
  updateUser(input: UpdateUserInput!): AddUserPayload
  """
  Add or remove rules for groups. If the filter doesn't match any groups,
  the mutation has no effect.
  """
  updateGroup(input: UpdateGroupInput!): AddGroupPayload
  deleteGroup(filter: GroupFilter!): DeleteGroupPayload
  deleteUser(filter: UserFilter!): DeleteUserPayload
  """
  Add a new namespace.
  """
  addNamespace(input: AddNamespaceInput): NamespacePayload
  """
  Delete a namespace.
  """
  deleteNamespace(input: DeleteNamespaceInput!): NamespacePayload
  """
  Reset password can only be used by the Guardians of the galaxy to reset password of
  any user in any namespace.
  """
  resetPassword(input: ResetPasswordInput!): ResetPasswordPayload
  """
  Apply enterprise license.
  """
  enterpriseLicense(input: EnterpriseLicenseInput!): EnterpriseLicensePayload
}
```

You'll notice that the `/admin` schema is very much the same as the schemas
generated by Dgraph GraphQL.

* The `health` query lets you know if everything is connected and if there's a
  schema currently being served at `/graphql`.
* The `state` query returns the current state of the cluster and group
  membership information. For more information about `state` see
  [here](/dgraph/self-managed/dgraph-zero#more-about-the-%2Fstate-endpoint).
* The `config` query returns the configuration options of the cluster set at the
  time of starting it.
* The `getGQLSchema` query gets the current GraphQL schema served at `/graphql`,
  or returns null if there's no such schema.
* The `updateGQLSchema` mutation allows you to change the schema currently
  served at `/graphql`.

## Enterprise features

Enterprise Features like ACL, binary backups are also available using the
GraphQL API at `/admin` endpoint.

* [ACL](/dgraph/enterprise/access-control-lists#accessing-secured-dgraph)
* [Backups](/dgraph/enterprise/binary-backups#create-a-backup)
* [Restore](/dgraph/enterprise/binary-backups#online-restore)

## First start

On first starting with a blank database:

* There's no schema served at `/graphql`.
* Querying the `/admin` endpoint for `getGQLSchema` returns
  `"getGQLSchema": null`.
* Querying the `/admin` endpoint for `health` lets you know that no schema has
  been added.

## Validating a schema

You can validate a GraphQL schema before adding it to your database by sending
your schema definition in an HTTP POST request to the to the
`/admin/schema/validate` endpoint, as shown in the following example:

Request header:

```ssh
path: /admin/schema/validate
method: POST
```

Request body:

```graphql
type Person {
  name: String
}
```

This endpoint returns a JSON response that indicates if the schema is valid or
not, and provides an error if isn't valid. In this case, the schema is valid, so
the JSON response includes the following message: `Schema is valid`.

## Modifying a schema

There are two ways you can modify a GraphQL schema:

* Using `/admin/schema`
* Using the `updateGQLSchema` mutation on `/admin`

<Tip>
  While modifying the GraphQL schema, if you get errors like
  `errIndexingInProgress`, `another operation is already running` or `server is
        not ready`, please wait a moment and then retry the schema update.
</Tip>

### Using `/admin/schema`

The `/admin/schema` endpoint provides a simplified method to add and update
schemas.

To create a schema you only need to call the `/admin/schema` endpoint with the
required schema definition. For example:

```graphql
type Person {
  name: String
}
```

If you have the schema definition stored in a `schema.graphql` file, you can use
`curl` like this:

```
curl -X POST localhost:8080/admin/schema --data-binary '@schema.graphql'
```

On successful execution, the `/admin/schema` endpoint will give you a JSON
response with a success code.

### Using `updateGQLSchema` to add or modify a schema

Another option to add or modify a GraphQL schema is the `updateGQLSchema`
mutation.

For example, to create a schema using `updateGQLSchema`, run this mutation on
the `/admin` endpoint:

```graphql
mutation {
  updateGQLSchema(input: { set: { schema: "type Person { name: String }" } }) {
    gqlSchema {
      schema
      generatedSchema
    }
  }
}
```

## Initial schema

Regardless of the method used to upload the GraphQL schema, on a black database,
adding this schema

```graphql
type Person {
  name: String
}
```

would cause the following:

* The `/graphql` endpoint would refresh and serve the GraphQL schema generated
  from type `type Person { name: String }`.
* The schema of the underlying Dgraph instance would be altered to allow for the
  new `Person` type and `name` predicate.
* The `/admin` endpoint for `health` would return that a schema is being served.
* The mutation would return `"schema": "type Person { name: String }"` and the
  generated GraphQL schema for `generatedSchema` (this is the schema served at
  `/graphql`).
* Querying the `/admin` endpoint for `getGQLSchema` would return the new schema.

## Migrating a schema

Given an instance serving the GraphQL schema from the previous section, updating
the schema to the following

```graphql
type Person {
  name: String @search(by: [regexp])
  dob: DateTime
}
```

would change the GraphQL definition of `Person` and result in the following:

* The `/graphql` endpoint would refresh and serve the GraphQL schema generated
  from the new type.
* The schema of the underlying Dgraph instance would be altered to allow for
  `dob` (predicate `Person.dob: datetime .` is added, and `Person.name` becomes
  `Person.name: string @index(regexp).`) and indexes are rebuilt to allow the
  regexp search.
* The `health` is unchanged.
* Querying the `/admin` endpoint for `getGQLSchema` would return the updated
  schema.

## Removing indexes from a schema

Adding a schema through GraphQL doesn't remove existing data (it only removes
indexes).

For example, starting from the schema in the previous section and modifying it
with the initial schema

```graphql
type Person {
  name: String
}
```

would have the following effects:

* The `/graphql` endpoint would refresh to serve the schema built from this
  type.
* Thus, field `dob` would no longer be accessible, and there would be no search
  available on `name`.
* The search index on `name` in Dgraph would be removed.
* The predicate `dob` in Dgraph would be left untouched (the predicate remains
  and no data is deleted).


# IDEs and Clients
Source: https://docs.hypermode.com/dgraph/graphql/connecting



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

When you deploy a GraphQL schema, Dgraph serves the corresponding
[spec-compliant GraphQL](https://graphql.github.io/graphql-spec/June2018/) API
at the HTTP endpoint `/graphql`. GraphQL requests can be sent via HTTP POST or
HTTP GET requests.

## Getting your GraphQL endpoint

<Tabs>
  <Tab title="Hypermode">
    * access the [Hypermode console](https://hypermode.com/sign-in)
    * the `GraphQL Endpoint` is displayed at the bottom.
    * click on the link button to copy it.
  </Tab>

  <Tab title="Self-Managed">
    `/graphql` is served by the Alpha nodes of the Dgraph cluster on the
    HTTP-external-public port. Refer to
    [ports usage](/dgraph/self-managed/ports-usage).

    For a local install the graphql endpoint would be

    ```
    http://localhost:8080/graphql
    ```

    The URL depends on your configuration and specifically

    * the port offest defined by `--port_offset` option of the dgraph alpha command.
    * the configuration of TLS for https.
    * the usage of a load balancer.
  </Tab>
</Tabs>

## IDEs

As Dgraph serves a
[spec-compliant GraphQL](https://graphql.github.io/graphql-spec/June2018/) API,
you can use your favorite GraphQL IDE.

* Postman
* Insomnia
* GraphiQL
* VSCode with GraphQL extensions

### General IDE setup

* Copy Dgraph GraphQL endpoint.
* Set the security header as required.
* use IDE instrospection capability.

## Clients

You are ready to write GraphQL queries and mutation and to run them against
Dgraph cluster.

When building an app in React, Vue, Svelte, or any of you favorite framework,
using a GraphQL client library is a must.

As Dgraph serves a
[spec-compliant GraphQL](https://graphql.github.io/graphql-spec/June2018/) API
from your schema, supports instropection and GraphQL subscriptions, the
integration with GraphQL UI client libraries is seamless.

Here is a not limited list of popular GraphQL UI clients that you can use with
Dgraph to build apps:

* [graphql-request](https://github.com/jasonkuhrt/graphql-request)
* [URQL](https://github.com/urql-graphql/urql)
* [Apollo client](https://github.com/apollographql/apollo-client)


# HTTP Protocol
Source: https://docs.hypermode.com/dgraph/graphql/http

Get the structure for GraphQL requests and responses, how to enable compression for them, and configuration options for extensions

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## POST request

### Headers

| Header                                  | Optionality                              | Value                                                                                        |
| :-------------------------------------- | :--------------------------------------- | :------------------------------------------------------------------------------------------- |
| Content-Type                            | mandatory                                | `application/graphql` or `application/json`                                                  |
| Content-Encoding                        | optional                                 | `gzip` to send compressed data                                                               |
| Accept-Encoding                         | optional                                 | `gzip` to enabled data compression on response                                               |
| X-Dgraph-AccessToken                    | if `ACL` is enabled                      | pass the access token you got in the login response to access predicates protected by an ACL |
| X-Auth-Token                            | if `anonymous access` is turned off      | admin key or client key                                                                      |
| header as set in `Dgraph.Authorization` | if GraphQL `Dgraph.Authorization` is set | valid JWT used by @auth directives                                                           |

<Note>
  Refer to GraphQL [security](/dgraph/graphql/security/overview) settings for
  explanations about `anonymous access` and `Dgraph.Authorization`.
</Note>

### Payload format

POST requests sent with the Content-Type header `application/graphql` must have
a POST body content as a GraphQL query string. For example, the following is a
valid POST body for a query:

```graphql
query {
  getTask(id: "0x3") {
    id
    title
    completed
    user {
      username
      name
    }
  }
}
```

POST requests sent with the Content-Type header `application/json` must have a
POST body in the following JSON format:

```json
{
  "query": "...",
  "operationName": "...",
  "variables": { "var": "val", ... }
}
```

GraphQL requests can contain one or more operations. Operations include `query`,
`mutation`, or `subscription`. If a request only has one operation, then it can
be unnamed like the following:

### Single operation

The most basic request contains a single anonymous (unnamed) operation. Each
operation can have one or more queries within in. For example, the following
query has `query` operation running the queries `getTask` and `getUser`:

```graphql
query {
  getTask(id: "0x3") {
    id
    title
    completed
  }
  getUser(username: "dgraphlabs") {
    username
  }
}
```

Response:

```json
{
  "data": {
    "getTask": {
      "id": "0x3",
      "title": "GraphQL docs example",
      "completed": true
    },
    "getUser": {
      "username": "dgraphlabs"
    }
  }
}
```

You can optionally name the operation as well, though it's not required if the
request only has one operation as it's clear what needs to be executed.

#### Query shorthand

If a request only has a single query operation, then you can use the short-hand
form of omitting the "query" keyword:

```graphql
{
  getTask(id: "0x3") {
    id
    title
    completed
  }
  getUser(username: "dgraphlabs") {
    username
  }
}
```

This simplifies queries when a query doesn't require an operation name or
[variables](/dgraph/graphql/query/variables).

### Multiple operations

If a request has two or more operations, then each operation must have a name. A
request can only execute one operation, so you must also include the operation
name to execute in the request. Every operation name in a request must be
unique.

For example, in the following request has the operation names `getTaskAndUser`
and `completedTasks`.

```graphql
query getTaskAndUser {
  getTask(id: "0x3") {
    id
    title
    completed
  }
  queryUser(filter: { username: { eq: "dgraphlabs" } }) {
    username
    name
  }
}

query completedTasks {
  queryTask(filter: { completed: true }) {
    title
    completed
  }
}
```

When executing the following request (as an HTTP POST request in JSON format),
specifying the "getTaskAndUser" operation executes the first query:

```json
query getTaskAndUser {
  getTask(id: "0x3") {
    id
    title
    completed
  }
  queryUser(filter: { username: { eq: "dgraphlabs" } }) {
    username
    name
  }
}

query completedTasks {
  queryTask(filter: { completed: true }) {
    title
    completed
  }
}
```

```json
{
  "data": {
    "getTask": {
      "id": "0x3",
      "title": "GraphQL docs example",
      "completed": true
    },
    "queryUser": [
      {
        "username": "dgraphlabs",
        "name": "Dgraph Labs"
      }
    ]
  }
}
```

And specifying the `completedTasks` operation executes the second query:

```json
{
  "query": "query getTaskAndUser { getTask(id: \"0x3\") { id title completed } queryUser(filter: {username: {eq: \"dgraphlabs\"}}) { username name }\n}\n\nquery completedTasks { queryTask(filter: {completed: true}) { title completed }}",
  "operationName": "completedTasks"
}
```

```json
{
  "data": {
    "queryTask": [
      {
        "title": "GraphQL docs example",
        "completed": true
      },
      {
        "title": "Show second operation",
        "completed": true
      }
    ]
  }
}
```

#### Multiple queries execution

When an operation contains multiple queries, they run concurrently and
independently in a Dgraph read-only transaction per query.

When an operation contains multiple mutations, they run serially, in the order
listed in the request, with a transaction per mutation. If a mutation fails, the
following mutations aren't executed and previous mutations aren't rolled back.

### Variables

Variables simplify GraphQL queries and mutations by letting you pass data
separately. A GraphQL request can be split into two sections: one for the query
or mutation, and another for variables.

Variables can be declared after the `query` or `mutation` and are passed like
arguments to a function and begin with `$`.

#### Query example

```graphql
query post($filter: PostFilter) {
  queryPost(filter: $filter) {
    title
    text
    author {
      name
    }
  }
}
```

```graphql
{
  "filter": {
    "title": {
      "eq": "First Post"
    }
  }
}
```

#### Mutation example

```graphql
mutation addAuthor($author: AddAuthorInput!) {
  addAuthor(input: [$author]) {
    author {
      name
      posts {
        title
        text
      }
    }
  }
}
```

```graphql
{
  "author": {
    "name": "A.N. Author",
    "dob": "2000-01-01",
    "posts": [{
      "title": "First Post",
      "text": "Hello world!"
    }]
  }
}
```

### Fragments

A GraphQL fragment is associated with a type and is a reusable subset of the
fields from this type. Here, we declare a `postData` fragment that can be used
with any `Post` object:

```graphql
fragment postData on Post {
  id
  title
  text
  author {
    username
    displayName
  }
}
query allPosts {
  queryPost(order: { desc: title }) {
    ...postData
  }
}
mutation addPost($post: AddPostInput!) {
  addPost(input: [$post]) {
    post {
      ...postData
    }
  }
}
```

### Using fragments with interfaces

It is possible to define fragments on interfaces. Here's an example of a query
that includes in-line fragments:

**Schema**

```graphql
interface Employee {
  ename: String!
}
interface Character {
  id: ID!
  name: String! @search(by: [exact])
}
type Human implements Character & Employee {
  totalCredits: Float
}
type Droid implements Character {
  primaryFunction: String
}
```

**Query**

```graphql
query allCharacters {
  queryCharacter {
    name
    __typename
    ... on Human {
      totalCredits
    }
    ... on Droid {
      primaryFunction
    }
  }
}
```

The `allCharacters` query returns a list of `Character` objects. Since `Human`
and `Droid` implements the `Character` interface, the fields in the result would
be returned according to the type of object.

**Result**

```graphql
{
  "data": {
    "queryCharacter": [
      {
        "name": "Human1",
        "__typename": "Human",
        "totalCredits": 200.23
      },
      {
        "name": "Human2",
        "__typename": "Human",
        "totalCredits": 2.23
      },
      {
        "name": "Droid1",
        "__typename": "Droid",
        "primaryFunction": "Code"
      },
      {
        "name": "Droid2",
        "__typename": "Droid",
        "primaryFunction": "Automate"
      }
    ]
  }
}
```

## GET request

GraphQL request may also be sent using an `HTTP GET` operation.

\GET requests must be sent in the following format. The query, variables, and
operation are sent as URL-encoded query parameters in the URL.

```sh
http://localhost:8080/graphql?query={...}&variables={...}&operationName=...
```

* `query` is mandatory
* `variables` is only required if the query contains GraphQL variables.
* `operationName` is only required if there are multiple operations in the
  query; in which case, operations must also be named.

## Response

All responses, including errors, always return HTTP 200 OK status codes.

The response is a JSON map including the fields `"data"`, `"errors"`, or
`"extensions"` following the GraphQL specification. They follow the following
formats.

Successful queries are in the following format:

```json
{
  "data": { ... },
  "extensions": { ... }
}
```

Queries that have errors are in the following format.

```json
{
  "errors": [ ... ],
}
```

#### Data field

The "data" field contains the result of your GraphQL request. The response has
exactly the same shape as the result. For example, notice that for the following
query, the response includes the data in the exact shape as the query.

Query:

```graphql
query {
  getTask(id: "0x3") {
    id
    title
    completed
    user {
      username
      name
    }
  }
}
```

Response:

```json
{
  "data": {
    "getTask": {
      "id": "0x3",
      "title": "GraphQL docs example",
      "completed": true,
      "user": {
        "username": "dgraphlabs",
        "name": "Dgraph Labs"
      }
    }
  }
}
```

#### Errors field

The "errors" field is a JSON list where each entry has a `"message"` field that
describes the error and optionally has a `"locations"` array to list the
specific line and column number of the request that points to the error
described. For example, here's a possible error for the following query, where
`getTask` needs to have an `id` specified as input:

Query:

```graphql
query {
  getTask() {
    id
  }
}
```

Response:

```json
{
  "errors": [
    {
      "message": "Field \"getTask\" argument \"id\" of type \"ID!\" is required but not provided.",
      "locations": [
        {
          "line": 2,
          "column": 3
        }
      ]
    }
  ]
}
```

#### Error propagation

Before returning query and mutation results, Dgraph uses the types in the schema
to apply GraphQL
[value completion](https://graphql.github.io/graphql-spec/June2018/#sec-Value-Completion)
and
[error handling](https://graphql.github.io/graphql-spec/June2018/#sec-Errors-and-Non-Nullability).
As an example, `null` values for non-nullable fields (such as `String!`) cause
error propagation to parent fields.

In short, the GraphQL value completion and error propagation mean the following.

* Fields marked as nullable (without `!`) can return `null` in the JSON
  response.
* For fields marked as non-nullable (with `!`) Dgraph never returns null for
  that field.
* If an instance of type has a non-nullable field that has evaluated to null,
  the whole instance results in null.
* Reducing an object to null might cause further error propagation. For example,
  querying for a post that has an author with a null name results in null: the
  null name (`name: String!`) causes the author to result in null, and a null
  author causes the post (`author: Author!`) to result in null.
* Error propagation for lists with nullable elements (for example
  `friends [Author]`), can result in nulls inside the result list.
* Error propagation for lists with non-nullable elements results in null for
  `friends [Author!]` and would cause further error propagation for
  `friends [Author!]!`.

Note that, a query that results in no values for a list always returns the empty
list `[]`, not `null`, regardless of whether it is nullable. For example, given
a schema for an author with `posts: [Post!]!`, if an author hasn't posted
anything and we queried for that author, the result for the posts field would be
`posts: []`.

A list can, however, result in null due to GraphQL error propagation. For
example, if the definition is `posts: [Post!]`, and we queried for an author who
has a list of posts. If one of those posts happened to have a null title (title
is non-nullable `title: String!`), then that post would evaluate to null, the
`posts` list can't contain nulls and so the list reduces to null.

#### Extensions field

The "extensions" field contains extra metadata for the request with metrics and
trace information for the request.

* `"touched_uids"`: The number of nodes that were touched to satisfy the
  request. This is a good metric to gauge the complexity of the query.
* `"tracing"`: Displays performance tracing data in [Apollo
  Tracing][apollo-tracing] format. This includes the duration of the whole query
  and the duration of each operation.
* `"dql_query"`: Optional, displays the translated DQL query Dgraph composed.
  This is only output when the GraphQL debug superflag
  `(--graphql "debug=true;")` is set.

[apollo-tracing]: https://github.com/apollographql/apollo-tracing

Here's an example of a query response with the extensions field:

```json
{
  "data": {
    "getTask": {
      "id": "0x3",
      "title": "GraphQL docs example",
      "completed": true,
      "user": {
        "username": "dgraphlabs",
        "name": "Dgraph Labs"
      }
    }
  },
  "extensions": {
    "touched_uids": 9,
    "tracing": {
      "version": 1,
      "startTime": "2020-07-29T05:54:27.784837196Z",
      "endTime": "2020-07-29T05:54:27.787239465Z",
      "duration": 2402299,
      "execution": {
        "resolvers": [
          {
            "path": ["getTask"],
            "parentType": "Query",
            "fieldName": "getTask",
            "returnType": "Task",
            "startOffset": 122073,
            "duration": 2255955,
            "dgraph": [
              {
                "label": "query",
                "startOffset": 171684,
                "duration": 2154290
              }
            ]
          }
        ]
      }
    }
  }
}
```

**Turn off extensions**

To turn off extensions set the `--graphql` superflag's `extensions` option to
false (`--graphql extensions=false`) when running Dgraph Alpha.


# Lambda Fields
Source: https://docs.hypermode.com/dgraph/graphql/lambda/field

Define JavaScript mutation functions and add them as resolvers in your JS source code

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

### Schema

To set up a lambda function, first you need to define it on your GraphQL schema
by using the `@lambda` directive.

For example, to define a lambda function for the `rank` and `bio` fields in
`Author`:

```graphql
type Author {
  id: ID!
  name: String! @search(by: [hash, trigram])
  dob: DateTime @search
  reputation: Float @search
  bio: String @lambda
  rank: Int @lambda
  isMe: Boolean @lambda
}
```

You can also define `@lambda` fields on interfaces, as follows:

```graphql
interface Character {
  id: ID!
  name: String! @search(by: [exact])
  bio: String @lambda
}

type Human implements Character {
  totalCredits: Float
}

type Droid implements Character {
  primaryFunction: String
}
```

### Resolvers

After the schema is ready, you can define your JavaScript mutation function and
add it as a resolver in your JS source code. To add the resolver you can use
either the `addGraphQLResolvers` or `addMultiParentGraphQLResolvers` methods.

<Note>
  A Lambda Field resolver can use a combination of `parents`, `parent`, `dql`,
  or `graphql` inside the function.
</Note>

<Tip>
  This example uses `parent` for the resolver function. You can find additional
  resolver examples using `dql` in the [Lambda queries article](./query), and
  using `graphql` in the [Lambda mutations article](./mutation).
</Tip>

For example, to define JavaScript lambda functions for

* `Author`
* `Character`
* `Human`
* `Droid`

and add them as resolvers, do the following

```javascript
const authorBio = ({ parent: { name, dob } }) =>
  `My name is ${name} and I was born on ${dob}.`
const characterBio = ({ parent: { name } }) => `My name is ${name}.`
const humanBio = ({ parent: { name, totalCredits } }) =>
  `My name is ${name}. I have ${totalCredits} credits.`
const droidBio = ({ parent: { name, primaryFunction } }) =>
  `My name is ${name}. My primary function is ${primaryFunction}.`

self.addGraphQLResolvers({
  "Author.bio": authorBio,
  "Character.bio": characterBio,
  "Human.bio": humanBio,
  "Droid.bio": droidBio,
})
```

For example, you can add a resolver for `rank` using a `graphql` call, as
follows:

```javascript
async function rank({ parents }) {
  const idRepList = parents.map(function (parent) {
    return { id: parent.id, rep: parent.reputation }
  })
  const idRepMap = {}
  idRepList
    .sort((a, b) => (a.rep > b.rep ? -1 : 1))
    .forEach((a, i) => (idRepMap[a.id] = i + 1))
  return parents.map((p) => idRepMap[p.id])
}

self.addMultiParentGraphQLResolvers({
  "Author.rank": rank,
})
```

The following example demonstrates using the client-provided JWT to return
`true` if the custom claim for `USER` from the JWT matches the `id` of the
`Author`.

```javascript
async function isMe({ parent, authHeader }) {
  if (!authHeader) return false
  if (!authHeader.value) return false
  const headerValue = authHeader.value
  if (headerValue === "") return false
  const base64Url = headerValue.split(".")[1]
  const base = base64Url.replace(/-/g, "+").replace(/_/g, "/")
  const allClaims = JSON.parse(atob(base64))
  if (!allClaims["https://my.app.io/jwt/claims"]) return false
  const customClaims = allClaims["https://my.app.io/jwt/claims"]
  return customClaims.USER === parent.id
}

self.addGraphQLResolvers({
  "Author.isMe": isMe,
})
```

### Example

For example, if you execute the following GraphQL query:

```graphql
query {
  queryAuthor {
    name
    bio
    rank
    isMe
  }
}
```

You should see a response such as the following:

```json
{
  "queryAuthor": [
    {
      "name": "Ann Author",
      "bio": "My name is Ann Author and I was born on 2000-01-01T00:00:00Z.",
      "rank": 3,
      "isMe": false
    }
  ]
}
```

In the same way, if you execute the following GraphQL query on the `Character`
interface:

```graphql
query {
  queryCharacter {
    name
    bio
  }
}
```

You should see a response such as the following:

```json
{
  "queryCharacter": [
    {
      "name": "Han",
      "bio": "My name is Han."
    },
    {
      "name": "R2-D2",
      "bio": "My name is R2-D2."
    }
  ]
}
```

<Note>
  The `Human` and `Droid` types inherit the `bio` lambda field from the
  `Character` interface.
</Note>

For example, if you execute a `queryHuman` query with a selection set containing
`bio`, then the lambda function registered for `Human.bio` is executed, as
follows:

```graphql
query {
  queryHuman {
    name
    bio
  }
}
```

This query generates the following response:

```json
{
  "queryHuman": [
    {
      "name": "Han",
      "bio": "My name is Han. I have 10 credits."
    }
  ]
}
```


# Lambda Mutations
Source: https://docs.hypermode.com/dgraph/graphql/lambda/mutation

Ready to use lambdas for mutations? This documentation takes you through the schemas, resolvers, and examples.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

### Schema

To set up a lambda mutation, first you need to define it on your GraphQL schema
by using the `@lambda` directive.

<Note>
  `add`, `update`, and `delete` are reserved prefixes and they can't be used to
  define Lambda mutations.
</Note>

For example, to define a lambda mutation for `Author` that creates a new author
with a default `reputation` of `3.0` given just the `name`:

```graphql
type Author {
  id: ID!
  name: String! @search(by: [hash, trigram])
  dob: DateTime
  reputation: Float
}

type Mutation {
  newAuthor(name: String!): ID! @lambda
}
```

### Resolver

Once the schema is ready, you can define your JavaScript mutation function and
add it as resolver in your JS source code. To add the resolver you can use
either the `addGraphQLResolvers` or `addMultiParentGraphQLResolvers` methods.

<Note>
  A Lambda Mutation resolver can use a combination of `parents`, `args`, `dql`,
  or `graphql` inside the function.
</Note>

<Tip>
  This example uses `graphql` for the resolver function. You can find additional
  resolver examples using `dql` in the [Lambda queries article](./query), and
  using `parent` in the [Lambda fields article](./field).
</Tip>

For example, to define the JavaScript `newAuthor()` lambda function and add it
as resolver:

```javascript
async function newAuthor({ args, graphql }) {
  // lets give every new author a reputation of 3 by default
  const results = await graphql(
    `
      mutation ($name: String!) {
        addAuthor(input: [{ name: $name, reputation: 3.0 }]) {
          author {
            id
            reputation
          }
        }
      }
    `,
    { name: args.name },
  )
  return results.data.addAuthor.author[0].id
}

self.addGraphQLResolvers({
  "Mutation.newAuthor": newAuthor,
})
```

Alternatively, you can use `dql.mutate` to achieve the same results:

```javascript
async function newAuthor({ args, dql, graphql }) {
  // lets give every new author a reputation of 3 by default
  const res = await dql.mutate(`{
        set {
            _:newAuth <Author.name> "${args.name}" .
            _:newAuth <Author.reputation> "3.0" .
            _:newAuth <dgraph.type> "Author" .
        }
    }`)
  return res.data.uids.newAuth
}
```

### Example

Finally, if you execute this lambda mutation a new author `Ken Addams` with
`reputation=3.0` should be added to the database:

```graphql
mutation {
  newAuthor(name: "Ken Addams")
}
```

Afterwards, if you query the GraphQL database for `Ken Addams`, you would see:

```json
{
  "getAuthor": {
    "name": "Ken Addams",
    "reputation": 3.0
  }
}
```


# Dgraph Lambda Overview
Source: https://docs.hypermode.com/dgraph/graphql/lambda/overview

Lambda provides a way to write custom logic in JavaScript, integrate it with your GraphQL schema, and execute it using the GraphQL API in a few easy steps.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Lambda provides a way to write your custom logic in JavaScript, integrate it
with your GraphQL schema, and execute it using the GraphQL API in a few easy
steps:

1. Set up a Dgraph cluster with a working lambda server
2. Declare lambda queries, mutations, and fields in your GraphQL schema as
   needed
3. Define lambda resolvers for them in a JavaScript file

This also simplifies the job of developers, as they can build a complex backend
that's rich with business logic, without setting up multiple different services.
Also, you can build your backend in JavaScript, which means you can build both
your frontend and backend using the same language.

Dgraph doesn't execute your custom logic itself. It makes external HTTP requests
to a user-defined lambda server.

<Tip>
  If you want to deploy your own lambda server, you can find the implementation
  of Dgraph Lambda in our [open source
  repository](https://github.com/dgraph-io/dgraph-lambda).
</Tip>

## Declaring lambda in a GraphQL schema

There are three places where you can use the `@lambda` directive and thus tell
Dgraph where to apply custom JavaScript logic.

* You can add lambda fields to your types and interfaces, as follows:

```graphql
type MyType {
  ...
  customField: String @lambda
}
```

* You can add lambda queries to the Query type, as follows:

```graphql
type Query {
  myCustomQuery(...): QueryResultType @lambda
}
```

* You can add lambda mutations to the Mutation type, as follows:

```graphql
type Mutation {
  myCustomMutation(...): MutationResult @lambda
}
```

## Defining lambda resolvers in JavaScript

A lambda resolver is a user-defined JavaScript function that performs custom
actions over the GraphQL types, interfaces, queries, and mutations. There are
two methods to register JavaScript resolvers:

* `self.addGraphQLResolvers`
* `self.addMultiParentGraphQLResolvers`

<Tip>
  Functions `self.addGraphQLResolvers` and `self.addMultiParentGraphQLResolvers`
  can be called multiple times in your resolver code.
</Tip>

### addGraphQLResolvers

The `self.addGraphQLResolvers` method takes an object as an argument, which maps
a resolver name to the resolver function that implements it. The resolver
functions registered using `self.addGraphQLResolvers` receive
`{ parent, args, graphql, dql }` as argument:

* `parent`, the parent object for which to resolve the current lambda field
  registered using `addGraphQLResolver`. The `parent` receives all immediate
  fields of that object, whether or not they were actually queried. Available
  only for types and interfaces (`null` for queries and mutations)
* `args`, the set of arguments for lambda queries and mutations
* `graphql`, a function to execute auto-generated GraphQL API calls from the
  lambda server. The user's auth header is passed back to the `graphql`
  function, so this can be used securely
* `dql`, provides an API to execute DQL from the lambda server
* `authHeader`, provides the JWT key and value of the auth header passed from
  the client

The `addGraphQLResolvers` can be represented with the following TypeScript
types:

```TypeScript
type GraphQLResponse {
  data?: Record<string, any>
  errors?: { message: string }[]
}

type AuthHeader {
  key: string
  value: string
}

type GraphQLEventWithParent = {
  parent: Record<string, any> | null
  args: Record<string, any>
  graphql: (query: string, vars?: Record<string, any>, authHeader?: AuthHeader) => Promise<GraphQLResponse>
  dql: {
    query: (dql: string, vars?: Record<string, any>) => Promise<GraphQLResponse>
    mutate: (dql: string) => Promise<GraphQLResponse>
  }
  authHeader: AuthHeader
}

function addGraphQLResolvers(resolvers: {
  [key: string]: (e: GraphQLEventWithParent) => any;
}): void
```

<Tip>
  `self.addGraphQLResolvers` is the default choice for registering resolvers
  when the result of the lambda for each parent is independent of other parents.
</Tip>

Each resolver function should return data in the exact format as the return type
of GraphQL field, query, or mutation for which it's being registered.

In the following example, the resolver function `myTypeResolver` registered for
the `customField` field in `MyType` returns a string because the return type of
that field in the GraphQL schema is `String`:

```javascript
const myTypeResolver = ({ parent: { customField } }) =>
  `My value is ${customField}.`

self.addGraphQLResolvers({
  "MyType.customField": myTypeResolver,
})
```

Another resolver example using a `graphql` call:

```javascript
async function todoTitles({ graphql }) {
  const results = await graphql("{ queryTodo { title } }")
  return results.data.queryTodo.map((t) => t.title)
}

self.addGraphQLResolvers({
  "Query.todoTitles": todoTitles,
})
```

### addMultiParentGraphQLResolvers

The `self.addMultiParentGraphQLResolvers` is useful in scenarios where you want
to perform computations involving all the parents returned from Dgraph for a
lambda field. This is useful in two scenarios:

* When you want to perform a computation between parents
* When you want to execute a complex query, and want to optimize it by firing a
  single query for all the parents

This method takes an object as an argument, which maps a resolver name to the
resolver function that implements it. The resolver functions registered using
this method receive `{ parents, args, graphql, dql }` as argument:

* `parents`, a list of parent objects for which to resolve the current lambda
  field registered using `addMultiParentGraphQLResolvers`. Available only for
  types and interfaces (`null` for queries and mutations)
* `args`, the set of arguments for lambda queries and mutations (`null` for
  types and interfaces)
* `graphql`, a function to execute auto-generated GraphQL API calls from the
  lambda server
* `dql`, provides an API to execute DQL from the lambda server
* `authHeader`, provides the JWT key and value of the auth header passed from
  the client

The `addMultiParentGraphQLResolvers` can be represented with the following
TypeScript types:

```TypeScript
type GraphQLResponse {
  data?: Record<string, any>
  errors?: { message: string }[]
}

type AuthHeader {
  key: string
  value: string
}

type GraphQLEventWithParents = {
  parents: (Record<string, any>)[] | null
  args: Record<string, any>
  graphql: (query: string, vars?: Record<string, any>, authHeader?: AuthHeader) => Promise<GraphQLResponse>
  dql: {
    query: (dql: string, vars?: Record<string, any>) => Promise<GraphQLResponse>
    mutate: (dql: string) => Promise<GraphQLResponse>
  }
  authHeader: AuthHeader
}

function addMultiParentGraphQLResolvers(resolvers: {
  [key: string]: (e: GraphQLEventWithParents) => any;
}): void
```

<Note>
  This method shouldn't be used for lambda queries or lambda mutations.
</Note>

Each resolver function should return data as a list of the return type of
GraphQL field for which it's being registered.

In the following example, the resolver function `rank()` registered for the
`rank` field in `Author`, returns a list of integers because the return type of
that field in the GraphQL schema is `Int`:

```graphql
type Author {
  id: ID!
  name: String! @search(by: [hash, trigram])
  reputation: Float @search
  rank: Int @lambda
}
```

```javascript
import { sortBy } from "lodash"

/* 
This function computes the rank of each author based on the reputation of the author relative to other authors.
*/
async function rank({ parents }) {
  const idRepMap = {}
  sortBy(parents, "reputation").forEach(
    (parent, i) => (idRepMap[parent.id] = parents.length - i),
  )
  return parents.map((p) => idRepMap[p.id])
}

self.addMultiParentGraphQLResolvers({
  "Author.rank": rank,
})
```

<Note>
  Scripts containing import packages (such as this example) require compilation
  using Webpack.
</Note>

The following example resolver uses a `dql` call:

```javascript
async function reallyComplexDql({ parents, dql }) {
  const ids = parents.map((p) => p.id)
  const someComplexResults = await dql.query(
    `really-complex-query-here with ${ids}`,
  )
  return parents.map((parent) => someComplexResults[parent.id])
}

self.addMultiParentGraphQLResolvers({
  "MyType.reallyComplexProperty": reallyComplexDql,
})
```

The following resolver example uses a `graphql` call and manually overrides the
`authHeader` provided by the client:

```javascript
async function secretGraphQL({ parents, graphql }) {
  const ids = parents.map((p) => p.id);
  const secretResults = await graphql(
    `query myQueryName ($ids: [ID!]) {
      queryMyType(filter: { id: $ids }) {
        id
        controlledEdge {
          myField
        }
      }
    }`,
    { ids },
    {
      key: 'X-My-App-Auth'
      value: 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJodHRwczovL215LmFwcC5pby9qd3QvY2xhaW1zIjp7IlVTRVIiOiJmb28ifSwiZXhwIjoxODAwMDAwMDAwLCJzdWIiOiJ0ZXN0IiwibmFtZSI6IkpvaG4gRG9lIDIiLCJpYXQiOjE1MTYyMzkwMjJ9.wI3857KzwjtZAtOjng6MnzKVhFSqS1vt1SjxUMZF4jc'
    }
  );
  return parents.map((parent) => {
    const secretRes = secretResults.data.find(res => res.id === parent.id)
    parent.secretProperty = null
    if (secretRes) {
      if (secretRes.controlledEdge) {
        parent.secretProperty = secretRes.controlledEdge.myField
      }
    }
    return parent
  });
}
self.addMultiParentGraphQLResolvers({
  "MyType.secretProperty": secretGraphQL,
});
```

## Example

For example, if you execute the following lambda query:

```graphql
query {
  queryMyType {
    customField
  }
}
```

You should see a response such as the following:

```json
{
  "queryMyType": [
    {
      "customField": "My value is Lambda Example"
    }
  ]
}
```

## Learn more

To learn more about the `@lambda` directive, see:

* [Lambda fields](./field)
* [Lambda queries](./query)
* [Lambda mutations](./mutation)
* [Lambda webhook](./webhook)


# Lambda Queries
Source: https://docs.hypermode.com/dgraph/graphql/lambda/query

Get started with the @lambda directive for queries. This documentation takes you through the schemas, resolvers, and examples.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

### Schema

To set up a lambda query, first you need to define it on your GraphQL schema by
using the `@lambda` directive.

<Note>
  `get`, `query`, and `aggregate` are reserved prefixes and they can't be used
  to define Lambda queries.
</Note>

For example, to define a lambda query for `Author` that finds out authors given
an author's `name`:

```graphql
type Author {
  id: ID!
  name: String! @search(by: [hash, trigram])
  dob: DateTime
  reputation: Float
}

type Query {
  authorsByName(name: String!): [Author] @lambda
}
```

### Resolver

Once the schema is ready, you can define your JavaScript query function and add
it as resolver in your JS source code. To add the resolver you can use either
the `addGraphQLResolvers` or `addMultiParentGraphQLResolvers` methods.

<Note>
  A Lambda Query resolver can use a combination of `parents`, `args`, `dql`, or
  `graphql` inside the function.
</Note>

<Tip>
  This example uses `dql` for the resolver function. You can find additional
  resolver examples using `parent` in the [Lambda fields article](./query), and
  using `graphql` in the [Lambda mutations article](./mutation).
</Tip>

For example, to define the JavaScript `authorsByName()` lambda function and add
it as resolver:

```javascript
async function authorsByName({ args, dql }) {
  const results = await dql.query(
    `query queryAuthor($name: string) {
        queryAuthor(func: type(Author)) @filter(eq(Author.name, $name)) {
            name: Author.name
            dob: Author.dob
            reputation: Author.reputation
        }
    }`,
    { $name: args.name },
  )
  return results.data.queryAuthor
}

self.addGraphQLResolvers({
  "Query.authorsByName": authorsByName,
})
```

### Example

Finally, if you execute this lambda query

```graphql
query {
  authorsByName(name: "Ann Author") {
    name
    dob
    reputation
  }
}
```

You should see a response such as

```json
{
  "authorsByName": [
    {
      "name": "Ann Author",
      "dob": "2000-01-01T00:00:00Z",
      "reputation": 6.6
    }
  ]
}
```


# Lambda Webhooks
Source: https://docs.hypermode.com/dgraph/graphql/lambda/webhook

Ready to use lambdas for webhooks? This documentation takes you through the schemas, resolvers, and examples.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

### Schema

To set up a lambda webhook, you need to define it in your GraphQL schema by
using the `@lambdaOnMutate` directive along with the mutation events
(`add`/`update`/`delete`) you want to listen on.

<Note>
  Lambda webhooks only listen for events from the root mutation. You can create
  a schema that's capable of creating deeply nested objects, but only the parent
  level webhooks are invoked for the mutation.
</Note>

For example, to define a lambda webhook for all mutation events
(`add`/`update`/`delete`) on any `Author` object:

```graphql
type Author @lambdaOnMutate(add: true, update: true, delete: true) {
  id: ID!
  name: String! @search(by: [hash, trigram])
  dob: DateTime
  reputation: Float
}
```

### Resolver

Once the schema is ready, you can define your JavaScript functions and add those
as resolvers in your JS source code. To add the resolvers you should use the
`addWebHookResolvers`method.

<Note>
  A Lambda Webhook resolver can use a combination of `event`, `dql`, `graphql`
  or `authHeader` inside the function.
</Note>

#### Event object

You also have access to the `event` object within the resolver. Depending on the
value of `operation` field, only one of the fields (`add`/`update`/`delete`) is
part of the `event` object. The definition of `event` is as follows:

```json
"event": {
    "__typename": "<Typename>",
    "operation": "<one-of: add/update/delete>",
    "commitTs": <uint64, the commitTs of the mutation>
    "add": {
      "rootUIDs": [<list-of-UIDs-that-were-created-for-root-nodes-in-this-mutation>],
      "input": [<AddTypeInput: i.e. all the data that was received as part of the `input` argument>]
    },
    "update": {
      "rootUIDs": [<list-of-UIDs-of-root-nodes-for-which-something-was-set/removed-in-this-mutation>],
      "setPatch": <TypePatch: the object that was received as the patch for set>,
      "removePatch": <TypePatch: the object that was received as the patch for remove>
    },
    "delete": {
      "rootUIDs": [<list-of-UIDs-of-root-nodes-which-were-deleted-in-this-mutation>]
    }
```

#### Resolver examples

For example, to define JavaScript lambda functions for each mutation event for
which `@lambdaOnMutate` is enabled and add those as resolvers:

```javascript
async function addAuthorWebhook({ event, dql, graphql, authHeader }) {
  // execute what you want on addition of an author
  // maybe send a welcome mail to the author
}

async function updateAuthorWebhook({ event, dql, graphql, authHeader }) {
  // execute what you want on update of an author
  // maybe send a mail to the author informing that few details have been updated
}

async function deleteAuthorWebhook({ event, dql, graphql, authHeader }) {
  // execute what you want on deletion of an author
  // maybe mail the author saying they have been removed from the platform
}

self.addWebHookResolvers({
  "Author.add": addAuthorWebhook,
  "Author.update": updateAuthorWebhook,
  "Author.delete": deleteAuthorWebhook,
})
```

### Example

Finally, if you execute an `addAuthor` mutation, the `add` operation mapped to
the `addAuthorWebhook` resolver is triggered:

```graphql
mutation {
  addAuthor(input: [{ name: "Ken Addams" }]) {
    author {
      id
      name
    }
  }
}
```


# Add Mutations
Source: https://docs.hypermode.com/dgraph/graphql/mutation/add

Add mutations allows you to add new objects of a particular type. Dgraph automatically generates input and return types in the schema for the add mutation.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Add mutations allow you to add new objects of a particular type.

We use the following schema to demonstrate some examples.

**Schema**:

```graphql
type Author {
  id: ID!
  name: String! @search(by: [hash])
  dob: DateTime
  posts: [Post]
}

type Post {
  postID: ID!
  title: String! @search(by: [term, fulltext])
  text: String @search(by: [fulltext, term])
  datePublished: DateTime
}
```

Dgraph automatically generates input and return types in the schema for the
`add` mutation, as shown below:

```graphql
addPost(input: [AddPostInput!]!): AddPostPayload

input AddPostInput {
  title: String!
  text: String
  datePublished: DateTime
}

type AddPostPayload {
  post(filter: PostFilter, order: PostOrder, first: Int, offset: Int): [Post]
  numUids: Int
}
```

**Example**: add mutation on single type with embedded value

```graphql
mutation {
  addAuthor(input: [{ name: "A.N. Author", posts: [] }]) {
    author {
      id
      name
    }
  }
}
```

**Example**: add mutation on single type using variables

```graphql
mutation addAuthor($author: [AddAuthorInput!]!) {
  addAuthor(input: $author) {
    author {
      id
      name
    }
  }
}
```

Variables:

```json
{ "author": { "name": "A.N. Author", "dob": "2000-01-01", "posts": [] } }
```

<Note>
  You can convert an `add` mutation to an `upsert` mutation by setting the value
  of the input variable `upsert` to `true`. For more information, see [Upsert
  Mutations](./upsert).
</Note>

## Examples

You can refer to the following
[link](https://github.com/hypermodeinc/dgraph/blob/main/graphql/resolve/add_mutation_test.yaml)
for more examples.


# Deep Mutations
Source: https://docs.hypermode.com/dgraph/graphql/mutation/deep

Perform deep mutations at multiple levels. Deep mutations don't alter linked objects, but add nested new objects or link to existing objects.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

You can perform deep mutations at multiple levels. Deep mutations don't alter
linked objects, but they can add deeply nested new objects or link to existing
objects. To update an existing nested object, use the update mutation for its
type.

We use the following schema to demonstrate some examples.

## Schema

```graphql
type Author {
  id: ID!
  name: String! @search(by: [hash])
  dob: DateTime
  posts: [Post]
}

type Post {
  postID: ID!
  title: String! @search(by: [term, fulltext])
  text: String @search(by: [fulltext, term])
  datePublished: DateTime
}
```

### Example: Adding deeply nested post with new author mutation using variables

```graphql
mutation addAuthorWithPost($author: addAuthorInput!) {
  addAuthor(input: [$author]) {
    author {
      id
      name
      posts {
        title
        text
      }
    }
  }
}
```

Variables:

```json
{
  "author": {
    "name": "A.N. Author",
    "dob": "2000-01-01",
    "posts": [
      {
        "title": "New post",
        "text": "A really new post"
      }
    ]
  }
}
```

### **Example**: Update mutation on deeply nested post and link to an existing author using variables

The following example assumes that the post with the postID of `0x456` already
exists, and isn't currently nested under the author having the id of `0x123`.

<Note>
  This syntax doesn't remove any other existing posts, it just adds the existing
  post to any that may already be nested.
</Note>

```graphql
mutation updateAuthorWithExistingPost($patch: UpdateAuthorInput!) {
  updateAuthor(input: $patch) {
    author {
      id
      posts {
        title
        text
      }
    }
  }
}
```

Variables:

```json
{
  "patch": {
    "filter": {
      "id": ["0x123"]
    },
    "set": {
      "posts": [
        {
          "postID": "0x456"
        }
      ]
    }
  }
}
```

This example query can't modify the existing post's title or text. To modify the
post's title or text, use the `updatePost` mutation either alongside the this
mutation, or as a separate transaction.


# Delete Mutations
Source: https://docs.hypermode.com/dgraph/graphql/mutation/delete



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Delete Mutations allow you to delete objects of a particular type.

We use the following schema to demonstrate some examples.

**Schema**:

```graphql
type Author {
  id: ID!
  name: String! @search(by: [hash])
  dob: DateTime
  posts: [Post]
}

type Post {
  postID: ID!
  title: String! @search(by: [term, fulltext])
  text: String @search(by: [fulltext, term])
  datePublished: DateTime
}
```

Dgraph automatically generates input and return types in the schema for the
`delete` mutation. Delete mutations take `filter` as an input to select specific
objects and returns the state of the objects before deletion.

```graphql
deleteAuthor(filter: AuthorFilter!): DeleteAuthorPayload

type DeleteAuthorPayload {
  author(filter: AuthorFilter, order: AuthorOrder, first: Int, offset: Int): [Author]
  msg: String
  numUids: Int
}
```

**Example**: delete mutation using variables

```graphql
mutation deleteAuthor($filter: AuthorFilter!) {
  deleteAuthor(filter: $filter) {
    msg
    author {
      name
      dob
    }
  }
}
```

Variables:

```json
{ "filter": { "name": { "eq": "A.N. Author" } } }
```

## Examples

You can refer to the following
[link](https://github.com/hypermodeinc/dgraph/blob/main/graphql/resolve/delete_mutation_test.yaml)
for more examples.


# Mutations Overview
Source: https://docs.hypermode.com/dgraph/graphql/mutation/overview

Mutations can be used to insert, update, or delete data. Dgraph automatically generates GraphQL mutation for each type that you define in your schema.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Mutations allow you to modify server-side data, and it also returns an object
based on the operation performed. It can be used to insert, update, or delete
data. Dgraph automatically generates GraphQL mutations for each type that you
define in your schema. The mutation field returns an object type that allows you
to query for nested fields. This can be useful for fetching an object's new
state after an add/update, or to get the old state of an object before a delete.

```graphql
type Author {
  id: ID!
  name: String! @search(by: [hash])
  dob: DateTime
  posts: [Post]
}

type Post {
  postID: ID!
  title: String! @search(by: [term, fulltext])
  text: String @search(by: [fulltext, term])
  datePublished: DateTime
}
```

The following mutations would be generated from the schema.

```graphql
type Mutation {
  addAuthor(input: [AddAuthorInput!]!): AddAuthorPayload
  updateAuthor(input: UpdateAuthorInput!): UpdateAuthorPayload
  deleteAuthor(filter: AuthorFilter!): DeleteAuthorPayload
  addPost(input: [AddPostInput!]!): AddPostPayload
  updatePost(input: UpdatePostInput!): UpdatePostPayload
  deletePost(filter: PostFilter!): DeletePostPayload
}

type AddAuthorPayload {
  author(
    filter: AuthorFilter
    order: AuthorOrder
    first: Int
    offset: Int
  ): [Author]
  numUids: Int
}

type AddPostPayload {
  post(filter: PostFilter, order: PostOrder, first: Int, offset: Int): [Post]
  numUids: Int
}

type DeleteAuthorPayload {
  author(
    filter: AuthorFilter
    order: AuthorOrder
    first: Int
    offset: Int
  ): [Author]
  msg: String
  numUids: Int
}

type DeletePostPayload {
  post(filter: PostFilter, order: PostOrder, first: Int, offset: Int): [Post]
  msg: String
  numUids: Int
}

type UpdateAuthorPayload {
  author(
    filter: AuthorFilter
    order: AuthorOrder
    first: Int
    offset: Int
  ): [Author]
  numUids: Int
}

type UpdatePostPayload {
  post(filter: PostFilter, order: PostOrder, first: Int, offset: Int): [Post]
  numUids: Int
}
```

## Input objects

Mutations require input data, such as the data, to create a new object or an
object's ID to delete. Dgraph auto-generates the input object type for every
type in the schema.

```graphql
input AddAuthorInput {
  name: String!
  dob: DateTime
  posts: [PostRef]
}

mutation {
  addAuthor(
    input: {
      name: "A.N. Author",
      lastName: "2000-01-01",
    }
  )
  {
    ...
  }
}
```

## Return fields

Each mutation provides a set of fields that can be returned in the response.
Dgraph auto-generates the return payload object type for every type in the
schema.

```graphql
type AddAuthorPayload {
  author(
    filter: AuthorFilter
    order: AuthorOrder
    first: Int
    offset: Int
  ): [Author]
  numUids: Int
}
```

## Multiple fields in mutations

A mutation can contain multiple fields, just like a query. While query fields
are executed in parallel, mutation fields run in series, one after the other.
This means that if we send two `updateAuthor` mutations in one request, the
first is guaranteed to finish before the second begins. This ensures that we
don't end up with a race condition with ourselves. If one of the mutations is
aborted due error like transaction conflict, we continue performing the next
mutations.

**Example**: mutation on multiple types

```graphql
mutation ($post: AddPostInput!, $author: AddAuthorInput!) {
  addAuthor(input: [$author]) {
    author {
      name
    }
  }
  addPost(input: [$post]) {
    post {
      postID
      title
      text
    }
  }
}
```

Variables:

```json
{
  "author": {
    "name": "A.N. Author",
    "dob": "2000-01-01",
    "posts": []
  },
  "post": {
    "title": "Exciting post",
    "text": "A really good post",
    "author": {
      "name": "A.N. Author"
    }
  }
}
```

## Union mutations

Mutations can be used to add a node to a `union` field in a type.

For the following schema,

```graphql
enum Category {
  Fish
  Amphibian
  Reptile
  Bird
  Mammal
  InVertebrate
}

interface Animal {
  id: ID!
  category: Category @search
}

type Dog implements Animal {
  breed: String @search
}

type Parrot implements Animal {
  repeatsWords: [String]
}

type Human {
  name: String!
  pets: [Animal!]!
}

union HomeMember = Dog | Parrot | Human

type Home {
  id: ID!
  address: String
  members: [HomeMember]
}
```

This is the mutation for adding `members` to the `Home` type:

```graphql
mutation {
  addHome(input: [
        {
          "address": "United Street",
          "members": [
            { "dogRef": { "category": Mammal, "breed": "German Shepherd"} },
            { "parrotRef": { "category": Bird, "repeatsWords": ["squawk"]} },
            { "humanRef": { "name": "Han Solo"} }
          ]
        }
      ]) {
    home {
      address
      members {
        ... on Dog {
          breed
        }
        ... on Parrot {
          repeatsWords
        }
        ... on Human {
          name
        }
      }
    }
  }
}
```

## Vector embedding mutations

For types with vector embeddings Dgraph automatically generates the add
mutation. For this example of add mutation we use the following schema.

```graphql
type User {
  userID: ID!
  name: String!
  name_v: [Float!]
    @embedding
    @search(by: ["hnsw(metric: euclidean, exponent: 4)"])
}

mutation {
  addUser(
    input: [
      {
        name: "iCreate with a Mini iPad"
        name_v: [0.12, 0.53, 0.9, 0.11, 0.32]
      }
      { name: "Resistive Touchscreen", name_v: [0.72, 0.89, 0.54, 0.15, 0.26] }
      { name: "Fitness Band", name_v: [0.56, 0.91, 0.93, 0.71, 0.24] }
      { name: "Smart Ring", name_v: [0.38, 0.62, 0.99, 0.44, 0.25] }
    ]
  ) {
    project {
      id
      name
      name_v
    }
  }
}
```

Note: the embeddings are generated outside of Dgraph using any suitable machine
learning model.

## Examples

You can refer to the following
[link](https://github.com/hypermodeinc/dgraph/tree/main/graphql/schema/testdata/schemagen)
for more examples.


# Update Mutations
Source: https://docs.hypermode.com/dgraph/graphql/mutation/update

Update mutations let you update existing objects of a particular type, by filtering nodes and setting or removing any field belonging to that type.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Update mutations let you update existing objects of a particular type. With
update mutations, you can filter nodes and set or remove any field belonging to
a type.

We use the following schema to demonstrate some examples.

**Schema**:

```graphql
type Author {
  id: ID!
  name: String! @search(by: [hash])
  dob: DateTime
  posts: [Post]
}

type Post {
  postID: ID!
  title: String! @search(by: [term, fulltext])
  text: String @search(by: [fulltext, term])
  datePublished: DateTime
}
```

Dgraph automatically generates input and return types in the schema for the
`update` mutation. Update mutations take `filter` as an input to select specific
objects. You can specify `set` and `remove` operations on fields belonging to
the filtered objects. It returns the state of the objects after updating.

<Note>
  Executing an empty `remove {}` or an empty `set{}` doesn't have any effect on
  the update mutation.
</Note>

```graphql
updatePost(input: UpdatePostInput!): UpdatePostPayload

input UpdatePostInput {
  filter: PostFilter!
  set: PostPatch
  remove: PostPatch
}

type UpdatePostPayload {
  post(filter: PostFilter, order: PostOrder, first: Int, offset: Int): [Post]
  numUids: Int
}
```

### Set

For example, an update `set` mutation using variables:

```graphql
mutation updatePost($patch: UpdatePostInput!) {
  updatePost(input: $patch) {
    post {
      postID
      title
      text
    }
  }
}
```

Variables:

```json
{
  "patch": {
    "filter": {
      "postID": ["0x123", "0x124"]
    },
    "set": {
      "text": "updated text"
    }
  }
}
```

### Remove

For example an update `remove` mutation using variables:

```graphql
mutation updatePost($patch: UpdatePostInput!) {
  updatePost(input: $patch) {
    post {
      postID
      title
      text
    }
  }
}
```

Variables:

```json
{
  "patch": {
    "filter": {
      "postID": ["0x123", "0x124"]
    },
    "remove": {
      "text": "delete this text"
    }
  }
}
```

### Examples

You can refer to the following
[link](https://github.com/hypermodeinc/dgraph/blob/main/graphql/resolve/update_mutation_test.yaml)
for more examples.


# Upsert Mutations
Source: https://docs.hypermode.com/dgraph/graphql/mutation/upsert

Upsert mutations allow you to perform `add` or `update` operations based on whether a particular ID exists in the database.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Upsert mutations allow you to perform `add` or `update` operations based on
whether a particular `ID` exists in the database. The IDs must be external IDs,
defined using the `@id` directive in the schema.

For example, to demonstrate how upserts work in GraphQL, take the following
schema:

## Schema

```graphql
type Author {
  id: String! @id
  name: String! @search(by: [hash])
  posts: [Post] @hasInverse(field: author)
}

type Post {
  postID: String! @id
  title: String! @search(by: [term, fulltext])
  text: String @search(by: [fulltext, term])
  author: Author!
}
```

Dgraph automatically generates input and return types in the schema for the
`add` mutation, as shown below:

```graphql
addPost(input: [AddPostInput!]!, upsert: Boolean): AddPostPayload

input AddPostInput {
  postID: String!
  title: String!
  text: String
  author: AuthorRef!
}
```

Suppose you want to update the `text` field of a post with the ID `mm2`. But you
also want to create a new post with that ID in case it doesn't already exist. To
do this, you use the `addPost` mutation, but with an additional input variable
`upsert`.

This is a `Boolean` variable. Setting it to `true` results in an upsert
operation.

It performs an `update` mutation and carry out the changes you specify in your
request if the particular ID exists. Otherwise, it falls back to a default `add`
operation and create a new `Post` with that ID and the details you provide.

Setting `upsert` to `false` is the same as using a plain `add` operation—it'll
either fail or succeed, depending on whether the ID exists or not.

**Example**: add mutation with `upsert` true:

```graphql
mutation ($post: [AddPostInput!]!) {
  addPost(input: $post, upsert: true) {
    post {
      postID
      title
      text
      author {
        id
      }
    }
  }
}
```

With variables:

```json
{
  "post": {
    "postID": "mm2",
    "title": "Second Post",
    "text": "This is my second post, and updated with some new information.",
    "author": {
      "id": "micky"
    }
  }
}
```

If a post with the ID `mm2` exists, it updates the post with the new details.
Otherwise, it'll create a new `Post` with that ID and the values you provided.
In either case, you'll get the following response back:

```graphql
"data": {
    "addPost": {
      "post": [
        {
          "postID": "mm2",
          "title": "Second Post",
          "text": "This is my second post, and updated with some new information.",
          "author": {
            "id": "micky"
          }
        }
      ]
    }
  }
```

<Note>The default value of `upsert` is `false`.</Note>

<Note>
  The current behavior of `Add` and `Update` mutations is such that they don't
  update deep level nodes. So Add mutations with `upsert` set to `true` only
  updates values at the root level.
</Note>

## Examples

You can refer to the following
[link](https://github.com/hypermodeinc/dgraph/blob/main/graphql/resolve/add_mutation_test.yaml)
for more examples.


# GraphQL API
Source: https://docs.hypermode.com/dgraph/graphql/overview

Generate a GraphQL API and a graph backend from a single GraphQL schema.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph lets you generate a GraphQL API and a graph backend from a single
[GraphQL schema](/dgraph/graphql/schema/overview), no resolvers or custom
queries are needed. Dgraph automatically generates the GraphQL operations for
[queries](/dgraph/graphql/query/overview) and
[mutations](/dgraph/graphql/mutation/overview)

GraphQL developers can [get started](./quickstart) in minutes, and need not
concern themselves with the powerful graph database running in the background.

Dgraph extends the [GraphQL specifications](https://spec.graphql.org/) with
[directives](/dgraph/graphql/schema/directives/overview) and allows you to
customize the behavior of GraphQL operations using
[custom resolvers](/dgraph/graphql/schema/directives/custom) or to write you own
resolver logic with [Lambda resolvers](/dgraph/graphql/lambda/overview).

Dgraph also supports

* [GraphQL subscriptions](./subscriptions) with the `@withSubscription`
  directive: a client app can execute a subscription query and receive real-time
  updates when the subscription query result is updated.
* [Apollo federation](/dgraph/graphql/schema/federation) : you can create a
  gateway GraphQL service that includes the Dgraph GraphQL API and other GraphQL
  services.

Refer to the following pages for more details:


# Aggregate Queries
Source: https://docs.hypermode.com/dgraph/graphql/query/aggregate

Dgraph automatically generates aggregate queries for GraphQL schemas. These are compatible with the @auth directive

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph automatically generates aggregate queries for GraphQL schemas. Aggregate
queries fetch aggregate data, including the following:

* *Count queries* that let you count fields satisfying certain criteria
  specified using a filter.
* *Advanced aggregate queries* that let you calculate the maximum, minimum, sum
  and average of specified fields.

Aggregate queries are compatible with the `@auth` directive and follow the same
authorization rules as the `query` keyword. You can also use filters with
aggregate queries, as shown in some of the examples provided below.

## Count queries at root

For every `type` defined in a GraphQL schema, Dgraph generates an aggregate
query `aggregate<type name>`. This query includes a `count` field, as well as
[advanced aggregate query fields](#advanced-aggregate-queries-at-root).

Example: fetch the total number of `posts`.

```graphql
query {
  aggregatePost {
    count
  }
}
```

Example: fetch the number of `posts` whose titles contain `GraphQL`.

```graphql
query {
  aggregatePost(filter: { title: { anyofterms: "GraphQL" } }) {
    count
  }
}
```

## Count queries for child nodes

Dgraph also defines `<field name>Aggregate` fields for every field which is of
type `List[Type/Interface]` inside `query<type name>` queries, allowing you to
do a `count` on fields, or to use the
[advanced aggregate queries](#advanced-aggregate-queries-for-child-nodes).

Example: fetch the number of `posts` for all authors along with their `name`.

```graphql
query {
  queryAuthor {
    name
    postsAggregate {
      count
    }
  }
}
```

Example: fetch the number of `posts` with a `score` greater than `10` for all
authors, along with their `name`

```graphql
query {
  queryAuthor {
    name
    postsAggregate(filter: { score: { gt: 10 } }) {
      count
    }
  }
}
```

## Advanced aggregate queries at root

For every `type` defined in the GraphQL schema, Dgraph generates an aggregate
query `aggregate<type name>` that includes advanced aggregate query fields, and
also includes a `count` field (see
[Count queries at root](#count-queries-at-root)). Dgraph generates one or more
advanced aggregate query fields (`<field-name>Min`, `<field-name>Max`,
`<field-name>Sum` and `<field-name>Avg`) for fields in the schema that are typed
as `Int`, `Float`, `String` and `Datetime`.

<Note> Advanced aggregate query fields are generated according to
a field's type. Fields typed as `Int` and `Float` get the following query
fields: `<field name>Max`, `<field name>Min`, `<field name>Sum` and
`<field name>Avg`. Fields typed as `String` and `Datetime` only get the
`<field name>Max`, `<field name>Min` query fields. </Note>

Example: fetch the average number of `posts` written by authors:

```graphql
query {
  aggregateAuthor {
    numPostsAvg
  }
}
```

Example: fetch the total number of `posts` by all authors, and the maximum
number of `posts` by any single `Author`:

```graphql
query {
  aggregateAuthor {
    numPostsSum
    numPostsMax
  }
}
```

Example: fetch the average number of `posts` for authors with more than 20
`friends`:

```graphql
query {
  aggregateAuthor(filter: { friends: { gt: 20 } }) {
    numPostsAvg
  }
}
```

## Advanced aggregate queries for child nodes

Dgraph also defines aggregate `<field name>Aggregate` fields for child nodes
within `query<type name>` queries. This is done for each field of type
`List[Type/Interface]` inside `query<type name>` queries, letting you fetch
minimums, maximums, averages and sums for those fields.

<Note> Aggregate query fields are generated according to a
field's type. Fields typed as `Int` and `Float` get the following query
fields:`<field name>Max`, `<field name>Min`, `<field name>Sum` and
`<field name>Avg`. Fields typed as `String` and `Datetime` only get the
`<field name>Max`, `<field name>Min` query fields. </Note>

Example: fetch the minimum, maximum and average `score` of the `posts` for each
`Author`, along with each author's `name`.

```graphql
query {
  queryAuthor {
    name
    postsAggregate {
      scoreMin
      scoreMax
      scoreAvg
    }
  }
}
```

Example: fetch the date of the most recent post with a `score` greater than `10`
for all authors, along with the author's `name`.

```graphql
query {
  queryAuthor {
    name
    postsAggregate(filter: { score: { gt: 10 } }) {
      datePublishedMax
    }
  }
}
```

## Aggregate queries on null data

Aggregate queries against empty data return `null`. This is true for both the
`<field name>Aggregate` fields and `aggregate<type name>` queries generated by
Dgraph.

So, in these examples,the following is true:

* If there are no nodes of type `Author`, the `aggregateAuthor` query returns
  null.
* If an `Author` hasn't written any posts, the field `postsAggregate` is null
  for that `Author`.


# And, Or, and Not Operators in GraphQL
Source: https://docs.hypermode.com/dgraph/graphql/query/and-or-not

Every GraphQL search filter can use AND, OR, and NOT operators.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Every GraphQL search filter can use `and`, `or`, and `not` operators.

GraphQL syntax uses infix notation, so: "a and b" is `a, and: { b }`, "a or b or
c" is `a, or: { b, or: c }`, and "not" is a prefix (`not:`).

The following example queries demonstrate the use of `and`, `or`, and `not`
operators:

Example: posts that don't have "GraphQL" in the title

```graphql
queryPost(filter: { not: { title: { allofterms: "GraphQL"} } } ) { ... }
```

Example: *"Posts that have "GraphQL" or "Dgraph" in the title"*

```graphql
queryPost(filter: {
  title: { allofterms: "GraphQL"},
  or: { title: { allofterms: "Dgraph" } }
} ) { ... }
```

Example: *"Posts that have "GraphQL" and "Dgraph" in the title"*

```graphql
queryPost(filter: {
  title: { allofterms: "GraphQL"},
  and: { title: { allofterms: "Dgraph" } }
} ) { ... }
```

The `and` operator is implicit for a single filter object, if the fields don't
overlap. For example, the `and` is required because `title` is in both filters,
whereas in the query below `and` isn't required.

```graphql
queryPost(filter: {
  title: { allofterms: "GraphQL" },
  datePublished: { ge: "2020-06-15" }
} ) { ... }
```

Example: *"Posts that have "GraphQL" in the title, or have the tag "GraphQL" and
mention "Dgraph" in the title"*

```graphql
queryPost(filter: {
  title: { allofterms: "GraphQL"},
  or: { title: { allofterms: "Dgraph" }, tags: { eq: "GraphQL" } }
} ) { ... }
```

The `and` and `or` filter both accept a list of filters. Per the GraphQL
specification, non-list filters are coerced into a list. This provides
backwards-compatibility while allowing for more complex filters.

Example: *"Query for posts that have `GraphQL` in the title but that lack the
`GraphQL` tag, or that have `Dgraph` in the title but lack the `Dgraph` tag"*

```graphql
queryPost(filter: {
  or: [
    { and: [{ title: { allofterms: "GraphQL" } }, { not: { tags: { eq: "GraphQL" } } }] }
    { and: [{ title: { allofterms: "Dgraph" } }, { not: { tags: { eq: "Dgraph" } } }] }
  ]
} ) { ... }
```

### Nesting

Nested logic with the same `and`/`or` conjunction can be simplified into a
single list.

For example, the following complex query:

```graphql
queryPost(filter: {
  or: [
    { or: [ { foo: { eq: "A" } }, { bar: { eq: "B" } } ] },
    { or: [ { baz: { eq: "C" } }, { quz: { eq: "D" } } ] }
  ]
} ) { ... }
```

Can be simplified into the following simplified query syntax:

```graphql
queryPost(filter: {
  or: [
    { foo: { eq: "A" } },
    { bar: { eq: "B" } },
    { baz: { eq: "C" } },
    { quz: { eq: "D" } }
  ]
} ) { ... }
```


# Cached Results
Source: https://docs.hypermode.com/dgraph/graphql/query/cached-results

Cached results can serve read-heavy workloads with complex queries to improve performance. This refers to external caching at the browser/CDN level

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Cached results can be used to serve read-heavy workloads with complex queries to
improve performance. When cached results are enabled for a query, the stored
results are served if queried within the defined Time to Live (TTL) of the
cached query.

When using cached results, Dgraph adds the appropriate HTTP headers so the
caching can be done at the browser or content delivery network (CDN) level.

<Note>
  Caching refers to external caching at the browser/CDN level. Internal caching
  at the database layer isn't currently supported.
</Note>

### Enabling cached results

To enable the external result cache you need to add the
`@cacheControl(maxAge: int)` directive at the top of your query. This directive
adds the appropriate `Cache-Control` HTTP headers to the response, so that
browsers and CDNs can cache the results.

For example, the following query defines a cache with TTL of 15 seconds.

```graphql
query @cacheControl(maxAge: 15) {
  queryReview(filter: { comment: { alloftext: "Fantastic" } }) {
    comment
    by {
      username
    }
    about {
      name
    }
  }
}
```

Dgraph's returned HTTP headers:

```txt
Cache-Control: public,max-age=15
Vary: Accept-Encoding
```


# @cascade Directive
Source: https://docs.hypermode.com/dgraph/graphql/query/cascade

The @cascade directive can be applied to fields. With the @cascade directive, nodes that don’t have all fields specified in the query are removed

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The `@cascade` directive can be applied to fields. With the `@cascade`
directive, nodes that don’t have all fields specified in the query are removed.
This can be useful in cases where some filter was applied and some nodes might
not have all the listed fields.

For example, the query below only returns the authors which have both
`reputation` and `posts`, where posts have `text`. Note that `@cascade` trickles
down so if it's applied at the `queryAuthor` level, it is automatically applied
at the `posts` level too.

```graphql
{
  queryAuthor @cascade {
    reputation
    posts {
      text
    }
  }
}
```

### Pagination

Starting from v21.03, the `@cascade` directive supports pagination of query
results.

For example, to get to get the next 5 results after skipping the first 2 with
all the fields non-null:

```graphql
query {
  queryTask(first: 5, offset: 2) @cascade {
    name
    completed
  }
}
```

### Nested `@cascade`

`@cascade` can also be used at nested levels, so the query below would return
all authors but only those posts which have both `text` and `id`.

```graphql
{
  queryAuthor {
    reputation
    posts @cascade {
      id
      text
    }
  }
}
```

### Parameterized `@cascade`

The `@cascade` directive can optionally take a list of fields as an argument.
This changes the default behavior, considering only the supplied fields as
mandatory instead of all the fields for a type. Listed fields are automatically
cascaded as a required argument to nested selection sets.

In the example below, `name` is supplied in the `fields` argument. For an author
to be in the query response, it must have a `name`, and if it has a `country`
subfield, then that subfield must also have `name`.

```graphql
{
  queryAuthor @cascade(fields: ["name"]) {
    reputation
    name
    country {
      Id
      name
    }
  }
}
```

The query below only return those `posts` which have a non-null `text` field.

```graphql
{
  queryAuthor {
    reputation
    name
    posts @cascade(fields: ["text"]) {
      title
      text
    }
  }
}
```

#### Nesting

The cascading nature of field selection is overwritten by a nested `@cascade`.

For example, the query below ensures that an author has the `reputation` and
`name` fields, and, if it has a `posts` subfield, then that subfield must have a
`text` field.

```graphql
{
  queryAuthor @cascade(fields: ["reputation", "name"]) {
    reputation
    name
    dob
    posts @cascade(fields: ["text"]) {
      title
      text
    }
  }
}
```

#### Filtering

Filters can be used with the `@cascade` directive if they're placed before it:

```graphql
{
  queryAuthor(filter: { name: { anyofterms: "Alice Bob" } })
    @cascade(fields: ["reputation", "name"]) {
    reputation
    name
    dob
    posts @cascade(fields: ["text"]) {
      title
      text
    }
  }
}
```


# Order and Pagination
Source: https://docs.hypermode.com/dgraph/graphql/query/order-page

Every type with fields whose types can be ordered gets ordering built into the query and any list fields of that type.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Every type with fields whose types can be ordered (`Int`, `Float`, `String`,
`DateTime`) gets ordering built into the query and any list fields of that type.
Every query and list field gets pagination with `first` and `offset` and
ordering with `order` parameter.

The `order` parameter isn't required for pagination.

For example, find the most recent 5 posts.

```graphql
queryPost(order: { desc: datePublished }, first: 5) { ... }
```

Skip the first five recent posts and then get the next 10.

```graphql
queryPost(order: { desc: datePublished }, offset: 5, first: 10) { ... }
```

It is also possible to give multiple orders. For example, sort by date and
within each date order the posts by number of likes.

```graphql
queryPost(order: { desc: datePublished, then: { desc: numLikes } }, first: 5) { ... }
```


# Overview
Source: https://docs.hypermode.com/dgraph/graphql/query/overview

Dgraph automatically generates GraphQL queries for each type that you define in your schema. There are three types of queries generated for each type.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

How to use queries to fetch data from Dgraph.

Dgraph automatically generates GraphQL queries for each type that you define in
your schema. There are three types of queries generated for each type.

Example

```graphql
type Post {
  id: ID!
  title: String! @search
  text: String
  score: Float @search
  completed: Boolean @search
  datePublished: DateTime @search(by: [year])
  author: Author!
}

type Author {
  id: ID!
  name: String! @search
  posts: [Post!]
  friends: [Author]
}
```

With this schema, there would be three queries generated for Post and three for
Author. Here are the queries that are generated for the Post type:

```graphql
getPost(postID: ID!): Post
queryPost(filter: PostFilter, order: PostOrder, first: Int, offset: Int): [Post]
aggregatePost(filter: PostFilter): PostAggregateResult
```

The first query allows you to fetch a post and its related fields given an ID.
The second query allows you to fetch a list of posts based on some filters,
sorting and pagination parameters. The third query allows you to fetch aggregate
parameters like count of nodes based on filters.

Additionally, a `check<Type>Password` query is generated for types that have
been specified with a `@secret` directive.

You can look at all the queries that are generated by using any GraphQL client
such as Insomnia or GraphQL playground.


# Persistent Queries
Source: https://docs.hypermode.com/dgraph/graphql/query/persistent-queries

Persistent queries significantly improve the performance of an app as the smaller hash signature reduces bandwidth utilization.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph supports Persistent Queries. When a client uses persistent queries, the
client only sends the hash of a query to the server. The server has a list of
known hashes and uses the associated query accordingly.

Persistent queries significantly improve the performance and the security of an
app since the smaller hash signature reduces bandwidth utilization and speeds up
client loading times.

### Persisted query logic

The execution of Persistent Queries follows this logic:

* If the `extensions` key isn't provided in the `GET` request, Dgraph processes
  the request as usual
* If a `persistedQuery` exists under the `extensions` key, Dgraph tries to
  process a Persisted Query:
  * if no `sha256` hash is provided, process the query without persisting
  * if the `sha256` hash is provided, try to retrieve the persisted query

Example:

```json
{
  "persistedQuery": {
    "sha256Hash": "b952c19b894e1aa89dc05b7d53e15ab34ee0b3a3f11cdf3486acef4f0fe85c52"
  }
}
```

### Create

To create a Persistent Query, both `query` and `sha256` must be provided.

Dgraph verifies the hash and performs a lookup. If the query doesn't exist,
Dgraph stores the query, provided that the `sha256` of the query is correct.
Finally, Dgraph processes the query and returns the results.

Example:

```sh
curl -g 'http://localhost:8080/graphql/?query={sample_query}&extensions={"persistedQuery":{"sha256Hash":"b952c19b894e1aa89dc05b7d53e15ab34ee0b3a3f11cdf3486acef4f0fe85c52"}}'
```

### Lookup

If only a `sha256` is provided, Dgraph does a look-up, and processes the query
if found. Otherwise a `PersistedQueryNotFound` error is returned.

Example: curl -g
'[http://localhost:8080/graphql/?extensions=\{"persistedQuery":\{"sha256Hash":"b952c19b894e1aa89dc05b7d53e15ab34ee0b3a3f11cdf3486acef4f0fe85c52"}}](http://localhost:8080/graphql/?extensions=\{"persistedQuery":\{"sha256Hash":"b952c19b894e1aa89dc05b7d53e15ab34ee0b3a3f11cdf3486acef4f0fe85c52"}})'

### Usage with Apollo

You can create an [Apollo GraphQL](https://www.apollographql.com/) client with
persisted queries enabled. In the background, Apollo sends the same requests
like the ones previously shown.

For example:

```go
import { createPersistedQueryLink } from "apollo-link-persisted-queries";
import { createHttpLink } from "apollo-link-http";
import { InMemoryCache } from "apollo-cache-inmemory";
import ApolloClient from "apollo-client";
const link = createPersistedQueryLink().concat(createHttpLink({ uri: "/graphql" }));
const client = new ApolloClient({
  cache: new InMemoryCache(),
  link: link,
});
```


# Search and Filtering
Source: https://docs.hypermode.com/dgraph/graphql/query/search-filtering

Queries generated for a GraphQL type allow you to generate a single list of objects for a type. You can also query a list of objects using GraphQL.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Queries generated for a GraphQL type allow you to generate a single list of
objects for a type.

### Get a single object

Fetch the `title`, `text` and `datePublished` for a post with id `0x1`.

```graphql
query {
  getPost(id: "0x1") {
    title
    text
    datePublished
  }
}
```

Fetching nested linked objects, while using `get` queries is also easy. For
example, this is how you would fetch the authors for a post and their friends.

```graphql
query {
  getPost(id: "0x1") {
    id
    title
    text
    datePublished
    author {
      name
      friends {
        name
      }
    }
  }
}
```

While fetching nested linked objects, you can also apply a filter on them.

For example, the following query fetches the author with the `id` 0x1 and their
posts about `GraphQL`.

```graphql
query {
  getAuthor(id: "0x1") {
    name
    posts(filter: { title: { allofterms: "GraphQL" } }) {
      title
      text
      datePublished
    }
  }
}
```

If your type has a field with the `@id` directive applied to it, you can also
fetch objects using that.

For example, given the following schema, the query below fetches a user's `name`
and `age` by `userID` (which has the `@id` directive):

**Schema**:

```graphql
type User {
  userID: String! @id
  name: String!
  age: String
}
```

**Query**:

```graphql
query {
  getUser(userID: "0x2") {
    name
    age
  }
}
```

### Query a list of objects

You can query a list of objects using GraphQL. For example, the following query
fetches the `title`, `text` and `datePublished` for all posts:

```graphql
query {
  queryPost {
    id
    title
    text
    datePublished
  }
}
```

The following example query fetches a list of posts by their post `id`:

```graphql
query {
  queryPost(filter: { id: ["0x1", "0x2", "0x3", "0x4"] }) {
    id
    title
    text
    datePublished
  }
}
```

### Query that filters objects by predicate

Before filtering an object by a predicate, you need to add a `@search` directive
to the field that's used to filter the results.

For example, if you wanted to query events between two dates, or events that
fall within a certain radius of a point, you could have an `Event` schema, as
follows:

```graphql
type Event {
  id: ID!
  date: DateTime! @search
  location: Point @search
}
```

The search directive would let you filter events that fall within a date range,
as follows:

```graphql
query {
  queryEvent(
    filter: { date: { between: { min: "2020-01-01", max: "2020-02-01" } } }
  ) {
    id
  }
}
```

You can also filter events that have a location near a certain point with the
following query:

```graphql
query {
  queryEvent(
    filter: {
      location: {
        near: {
          coordinate: { latitude: 37.771935, longitude: -122.469829 }
          distance: 1000
        }
      }
    }
  ) {
    id
  }
}
```

You can also use connectors such as the `and` keyword to show results with
multiple filters applied. In the query below, we fetch posts that have `GraphQL`
in their title and have a `score > 100`.

This example assumes that the `Post` type has a `@search` directive applied to
the `title` field and the `score` field.

```graphql
query {
  queryPost(
    filter: { title: { anyofterms: "GraphQL" }, and: { score: { gt: 100 } } }
  ) {
    id
    title
    text
    datePublished
  }
}
```

### Filter a query for a list of objects

You can also filter nested objects while querying for a list of objects.

For example, the following query fetches all of the authors whose name contains
`Lee` and with their `completed` posts that have a score greater than `10`:

```graphql
query {
  queryAuthor(filter: { name: { anyofterms: "Lee" } }) {
    name
    posts(filter: { score: { gt: 10 }, and: { completed: true } }) {
      title
      text
      datePublished
    }
  }
}
```

### Filter a query for a range of objects with `between`

You can filter query results within an inclusive range of indexed and typed
scalar values using the `between` keyword.

<Tip>
  This keyword is also supported for DQL. To learn more, see [DQL Functions:
  `between`](/dgraph/dql/functions#between).
</Tip>

For example, you might start with the following example schema used to track
students at a school:

**Schema**:

```graphql
type Student {
  age: Int @search
  name: String @search(by: [exact])
}
```

Using the `between` filter, you could fetch records for students who are between
10 and 20 years of age:

**Query**:

```graphql
queryStudent(filter: {age: between: {min: 10, max: 20}}){
    age
    name
}
```

You could also use this filter to fetch records for students whose names fall
alphabetically between `ba` and `hz`:

**Query**:

```graphql
queryStudent(filter: {name: between: {min: "ba", max: "hz"}}){
    age
    name
}
```

### Filter to match specified field values with `in`

You can filter query results to find objects with one or more specified values
using the `in` keyword. This keyword can find matches for fields with the `@id`
directive applied. The `in` filter is supported for all data types such as
`string`, `enum`, `Int`, `Int64`, `Float`, and `DateTime`.

For example, let's say that your schema defines a `State` type that has the
`@id` directive applied to the `code` field:

```graphql
type State {
  code: String! @id
  name: String!
  capital: String
}
```

Using the `in` keyword, you can query for a list of states that have the postal
code **WA** or **VA** using the following query:

```graphql
query {
  queryState(filter: { code: { in: ["WA", "VA"] } }) {
    code
    name
  }
}
```

### Filter for objects with specified non-null fields using `has`

You can filter queries to find objects with a non-null value in a specified
field using the `has` keyword. The `has` keyword can only check whether a field
returns a non-null value, not for specific field values.

For example, your schema might define a `Student` type that has basic
information about each student such as their ID number, age, name, and email
address:

```graphql
type Student {
  tid: ID!
  age: Int!
  name: String
  email: String
}
```

To find those students who have a non-null `name`, run the following query:

```graphql
queryStudent(filter: { has : name } ){
   tid
   age
   name
}
```

You can also specify a list of fields, like the following:

```graphql
queryStudent(filter: { has : [name, email] } ){
   tid
   age
   name
   email
}
```

This would return `Student` objects where both `name` and `email` fields are
non-null.


# @skip and @include Directives
Source: https://docs.hypermode.com/dgraph/graphql/query/skip-include

@skip and @include directives can be applied to query fields. They let you skip or include a field based on the value of the if argument.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

`@skip` and `@include` directives can be applied to query fields. They allow you
to skip or include a field based on the value of the `if` argument passed to the
directive.

## `@skip`

In the query below, we fetch posts and decide whether to fetch the title for
them or not based on the `skipTitle` GraphQL variable.

GraphQL query

```graphql
query ($skipTitle: Boolean!) {
  queryPost {
    id
    title @skip(if: $skipTitle)
    text
  }
}
```

GraphQL variables

```json
{
  "skipTitle": true
}
```

## `@include`

Similarly, the `@include` directive can be used to include a field based on the
value of the `if` argument. The query below would only include the authors for a
post if `includeAuthor` GraphQL variable has value true.

GraphQL Query

```graphql
query ($includeAuthor: Boolean!) {
  queryPost {
    id
    title
    text
    author @include(if: $includeAuthor) {
      id
      name
    }
  }
}
```

GraphQL variables

```json
{
  "includeAuthor": false
}
```


# GraphQL Variables
Source: https://docs.hypermode.com/dgraph/graphql/query/variables



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Syntax Examples (using default values):

* `query title($name: string = "Bauman") { ... }`
* `query title($age: int = "95") { ... }`
* `query title($uids: string = "0x1") { ... }`
* `query title($uids: string = "[0x1, 0x2, 0x3]") { ... }`. The value of the
  variable is a quoted array.

`Variables` can be defined and used in queries which helps in query reuse and
avoids costly string building in clients at runtime by passing a separate
variable map. A variable starts with a `$` symbol. For **HTTP requests** with
GraphQL Variables, we must use `Content-Type: application/json` header and pass
data with a JSON object containing `query` and `variables`.

```sh
curl -H "Content-Type: application/json" localhost:8080/query -XPOST -d $'{
  "query": "query test($a: string) { test(func: eq(name, $a)) { \n uid \n name \n } }",
  "variables": { "$a": "Alice" }
}' | python -m json.tool | less
```

```json
$a: "5",
$b: "10",
$name: "Steven Spielberg"

query {
  test($a: int, $b: int, $name: string) {
    me(func: allofterms(name@en, $name)) {
      name@en
      director.film (first: $a, offset: $b) {
        name @en genre(first: $a) { name@en }
      }
    }
  }
}
```

* Variables can have default values. In the example below, `$a` has a default
  value of `2`. Since the value for `$a` isn't provided in the variable map,
  `$a` takes on the default value.
* Variables whose type is suffixed with a `!` can't have a default value but
  must have a value as part of the variables map.
* The value of the variable must be parsable to the given type, if not, an error
  is thrown.
* The variable types that are supported as of now are: `int`, `float`, `bool`
  and `string`.
* Any variable that's being used must be declared in the named query clause in
  the beginning.

```json
$b: "10",
$name: "Steven Spielberg"

query {
  test($a: int = 2, $b: int!, $name: string) {
    me(func: allofterms(name@en, $name)) {
      director.film (first: $a, offset: $b) { genre(first: $a) { name@en } }
    }
  }
}
```

You can also use array with GraphQL Variables.

```json
$b: "10",
$aName: "Steven Spielberg",
$bName: "Quentin Tarantino"

query {
  test($a: int = 2, $b: int!, $aName: string, $bName: string) {
    me(func: eq(name@en, [$aName, $bName])) {
      director.film (first: $a, offset: $b) { genre(first: $a) { name@en } }
    }
  }
}
```

We also support variable substitution in facets.

```json
$name: "Alice",
$IsClose: "true"

query {
  test($name: string = "Alice", $IsClose: string = "true") {
    data(func: eq(name, $name)) {
      friend @facets(eq(close, $IsClose)) { name }
      colleague : friend @facets(eq(close, false)) { name }
    }
  }
}
```

<Note>
  If you want to input a list of UIDs as a GraphQL variable value, you can have
  the variable as string type and have the value surrounded by square brackets
  like `["13", "14"]`.
</Note>


# Similarity Search
Source: https://docs.hypermode.com/dgraph/graphql/query/vector-similarity

Dgraph automatically generates GraphQL queries for each vector index that you define in your schema. There are two types of queries generated for each index.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph automatically generates two GraphQL similarity queries for each type that
have at least one [vector predicate](/dgraph/graphql/schema/types#vectors) with
`@search` directive.

For example

```graphql
type User {
  id: ID!
  name: String!
  name_v: [Float!]
    @embedding
    @search(by: ["hnsw(metric: euclidean, exponent: 4)"])
}
```

With this schema, the auto-generated `querySimilar<Object>ByEmbedding` query
allows us to run similarity search using the vector index specified in our
schema.

```graphql
getSimilar<Object>ByEmbedding(
    by: vector_predicate,
    topK: n,
    vector: searchVector): [User]
```

For example, to find top 3 users with names similar to a given user name
embedding the following query function can be used.

```graphql
querySimilarUserByEmbedding(by: name_v, topK: 3, vector: [0.1, 0.2, 0.3, 0.4, 0.5]) {
        id
        name
        vector_distance
     }
```

The results obtained for this query includes the 3 closest Users ordered by
`vector_distance`. The `vector_distance` is the Euclidean distance between the
`name_v` embedding vector and the input vector used in our query.

Note: you can omit `vector_distance` predicate in the query, the result is still
ordered by `vector_distance`.

The distance metric used is specified in the index creation.

Similarly, the auto-generated `querySimilar<Object>ById` query allows us to
search for similar objects to an existing object, given it’s Id. using the
function.

```graphql
getSimilar<Object>ById(
    by: vector_predicate,
    topK: n,
    id: userID):  [User]
```

For example the following query searches for top 3 users whose names are most
similar to the name of the user with id `0xef7`.

```graphql
querySimilarUserById(by: name_v, topK: 3, id: "0xef7") {
    id
    name
    vector_distance
}
```


# Quickstart
Source: https://docs.hypermode.com/dgraph/graphql/quickstart

Deploy a GraphQL API in one step. Define your schema and Dgraph does the rest.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

When you write an app that implements GraphQL over a REST endpoint or maybe over
a relational database, you know that GraphQL issues many queries to translate
the REST/relational data into something that looks like a graph. You also have
to be familiar with the GraphQL types, fields, and resolvers. However, with
Dgraph you can generate a running GraphQL API with the associated graph backend
just by deploying the GraphQL schema of your API. Dgraph does the rest!

## Step 1: Run Dgraph

The easiest way to get Dgraph up and running is on Hypermode.

## Step 2: Deploy a GraphQL Schema

1. In the [Schema](https://cloud.dgraph.io/_/schema) tab of the console, paste
   the following schema:

   ```graphql
   type Product {
     productID: ID!
     name: String @search(by: [term])
     reviews: [Review] @hasInverse(field: about)
   }

   type Customer {
     username: String! @id @search(by: [hash, regexp])
     reviews: [Review] @hasInverse(field: by)
   }

   type Review {
     id: ID!
     about: Product!
     by: Customer!
     comment: String @search(by: [fulltext])
     rating: Int @search
   }
   ```

2. Click `deploy`

   You now have a GraphQL API up and running and a graph database as a backend.

## Step 3: Test your GraphQL API

You can access the `GraphQL endpoint` with any GraphQL clients such as
[GraphQL Playground](https://github.com/prisma-labs/graphql-playground),
[Insomnia](https://insomnia.rest/),
[GraphiQL](https://github.com/graphql/graphiql),
[Altair](https://github.com/imolorhe/altair) or Postman.

If you want to use those clients, copy the `GraphQL endpoint` from the
[Cloud dashboard](https://cloud.dgraph.io/_/dashboard).

You may want to use the introspection capability of the client to explore the
schema, queries, and mutations that were generated by Dgraph.

### A first GraphQL mutation

To populate the database,

1. Open the [API Explorer](https://cloud.dgraph.io/_/explorer) tab

2. Paste the following code into the text area:

   ```graphql
   mutation {
     addProduct(
       input: [
         { name: "GraphQL on Dgraph" }
         { name: "Dgraph: The GraphQL Database" }
       ]
     ) {
       product {
         productID
         name
       }
     }
     addCustomer(input: [{ username: "Michael" }]) {
       customer {
         username
       }
     }
   }
   ```

3. Click **Execute Query** .

   The GraphQL server returns a JSON response similar to this:

   ```json
   {
     "data": {
       "addProduct": {
         "product": [
           {
             "productID": "0x2",
             "name": "GraphQL on Dgraph"
           },
           {
             "productID": "0x3",
             "name": "Dgraph: The GraphQL Database"
           }
         ]
       },
       "addCustomer": {
         "customer": [
           {
             "username": "Michael"
           }
         ]
       }
     },
     "extensions": {
       "requestID": "b155867e-4241-4cfb-a564-802f2d3808a6"
     }
   }
   ```

### A second GraphQL mutation

Because the schema defined Customer with the field `username: String! @id`, the
`username` field acts like an ID, so we can identify customers just with their
names.

Products, on the other hand, had `productID: ID!`, so they'll get an
auto-generated ID which are returned by the mutation.

1. Paste the following mutation in the text area of the
   [API Explorer](https://cloud.dgraph.io/_/explorer) tab.
2. Your ID for the product might be different than `0x2`. Make sure to replace
   the product ID with the ID from the response of the previous mutation.
3. Execute the mutation

   ```graphql
   mutation {
     addReview(
       input: [
         {
           by: { username: "Michael" }
           about: { productID: "0x2" }
           comment: "Fantastic, easy to install, worked great.  Best GraphQL server available"
           rating: 10
         }
       ]
     ) {
       review {
         comment
         rating
         by {
           username
         }
         about {
           name
         }
       }
     }
   }
   ```

   This time, the mutation result queries for the author making the review and
   the product being reviewed, so it's gone deeper into the graph to get the
   result than just the mutation data.

   ```json
   {
     "data": {
       "addReview": {
         "review": [
           {
             "comment": "Fantastic, easy to install, worked great.  Best GraphQL server available",
             "rating": 10,
             "by": {
               "username": "Michael"
             },
             "about": {
               "name": "GraphQL on Dgraph"
             }
           }
         ]
       }
     },
     "extensions": {
       "requestID": "11bc2841-8c19-45a6-bb31-7c37c9b027c9"
     }
   }
   ```

### GraphQL queries

With Dgraph, you get powerful graph search built into your GraphQL API. The
schema for search is generated from the schema document that we started with and
automatically added to the GraphQL API for you.

Remember the definition of a review.

```graphql
type Review {
    ...
    comment: String @search(by: [fulltext])
    ...
}
```

The directive `@search(by: [fulltext])` tells Dgraph we want to be able to
search for comments with full-text search.

Dgraph took that directive and the other information in the schema, and built
queries and search into the API.

Let's find all the products that were easy to install.

1. Paste the following query in the text area of the
   [API Explorer](https://cloud.dgraph.io/_/explorer) tab.
2. Execute the mutation

   ```graphql
   query {
     queryReview(filter: { comment: { alloftext: "easy to install" } }) {
       comment
       by {
         username
       }
       about {
         name
       }
     }
   }
   ```

What reviews did you get back? It'll depend on the data you added, but you'll at
least get the initial review we added.

Maybe you want to find reviews that describe best GraphQL products and give a
high rating.

```graphql
query {
  queryReview(
    filter: { comment: { alloftext: "best GraphQL" }, rating: { ge: 10 } }
  ) {
    comment
    by {
      username
    }
    about {
      name
    }
  }
}
```

How about we find the customers with names starting with "Mich" and the five
products that each of those liked the most.

```graphql
query {
  queryCustomer(filter: { username: { regexp: "/Mich.*/" } }) {
    username
    reviews(order: { asc: rating }, first: 5) {
      comment
      rating
      about {
        name
      }
    }
  }
}
```

## Conclusion

Dgraph allows you to have a fully functional GraphQL API in minutes with a
highly scalable graph backend to serve complex nested queries. Moreover, you can
update or change your schema freely and just re-deploy new versions. For GraphQL
in Dgraph, you just concentrate on defining the schema of your graph and how
you'd like to search that graph. Dgraph does the rest.

## What's next

Learn more about [GraphQL schema](./schema/overview) and Dgraph directives.


# Custom DQL
Source: https://docs.hypermode.com/dgraph/graphql/schema/custom-dql

Dgraph Query Language (DQL) includes support for custom logic. Specify the DQL query you want to execute and the Dgraph GraphQL API will execute it

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph Query Language ([DQL](/dgraph/dql/schema)) lets you build custom
resolvers logic that goes beyond what's possible with the GraphQL CRUD API.

To define a DQL custom query, use the notation:

```graphql
 @custom(dql: """
  ...
  """)
```

<Tip>
  Since v21.03, you can also [subscribe to custom
  DQL](/dgraph/graphql/subscriptions/#subscriptions-to-custom-dql) queries.
</Tip>

For example, lets say you had following schema:

```graphql
type Tweets {
  id: ID!
  text: String! @search(by: [fulltext])
  author: User
  timestamp: DateTime! @search
}
type User {
  screen_name: String! @id
  followers: Int @search
  tweets: [Tweets] @hasInverse(field: author)
}
```

and you wanted to query tweets containing some particular text sorted by the
number of followers their author has. Then, this isn't possible with the
automatically generated CRUD API. Similarly, let's say you have a table sort of
UI component in your app which displays only a user's name and the number of
tweets done by that user. Doing this with the auto-generated CRUD API would
require you to fetch unnecessary data at client side, and then employ client
side logic to find the count. Instead, all this could simply be achieved by
specifying a DQL query for such custom use-cases.

So, you would need to modify your schema like this:

```graphql
type Tweets {
  id: ID!
  text: String! @search(by: [fulltext])
  author: User
  timestamp: DateTime! @search
}
type User {
  screen_name: String! @id
  followers: Int @search
  tweets: [Tweets] @hasInverse(field: author)
}
type UserTweetCount @remote {
  screen_name: String
  tweetCount: Int
}

type Query {
  queryTweetsSortedByAuthorFollowers(search: String!): [Tweets]
    @custom(
      dql: """
      query q($search: string) {
        var(func: type(Tweets)) @filter(anyoftext(Tweets.text, $search)) {
          Tweets.author {
            followers as User.followers
          }
          authorFollowerCount as sum(val(followers))
        }
        queryTweetsSortedByAuthorFollowers(func: uid(authorFollowerCount), orderdesc: val(authorFollowerCount)) {
          id: uid
          text: Tweets.text
          author: Tweets.author {
              screen_name: User.screen_name
              followers: User.followers
          }
          timestamp: Tweets.timestamp
        }
      }
      """
    )

  queryUserTweetCounts: [UserTweetCount]
    @custom(
      dql: """
      query {
        queryUserTweetCounts(func: type(User)) {
          screen_name: User.screen_name
          tweetCount: count(User.tweets)
        }
      }
      """
    )
}
```

Now, if you run following query, it would fetch you the tweets containing
"GraphQL" in their text, sorted by the number of followers their author has:

```graphql
query {
  queryTweetsSortedByAuthorFollowers(search: "GraphQL") {
    text
  }
}
```

There are following points to note while specifying the DQL query for such
custom resolvers:

* The name of the DQL query that you want to map to the GraphQL response, should
  be same as the name of the GraphQL query.
* You must use proper aliases inside DQL queries to map them to the GraphQL
  response.
* If you are using variables in DQL queries, their names should be same as the
  name of the arguments for the GraphQL query.
* For variables, only scalar GraphQL arguments like `Boolean`, `Int`, `Float`,
  etc are allowed. Lists and Object types aren't allowed to be used as variables
  with DQL queries.
* You would be able to query only those many levels with GraphQL which you have
  mapped with the DQL query. For instance, in the first custom query in this
  example, we haven't mapped an author's tweets to GraphQL alias, so, we won't
  be able to fetch author's tweets using that query.
* If the custom GraphQL query returns an interface, and you want to use
  `__typename` in GraphQL query, then you should add `dgraph.type` as a field in
  DQL query without any alias. This isn't required for types, only for
  interfaces.
* to subscribe to a custom DQL query, use the `@withSubscription` directive. See
  the [Subscriptions article](/dgraph/graphql/subscriptions) for more
  information.

***


# Dgraph Schema Fragment
Source: https://docs.hypermode.com/dgraph/graphql/schema/dgraph-schema

While editing your schema, this GraphQL schema fragment can be useful. It sets up the definitions of the directives that you’ll use in your schema.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

While editing your schema, you might find it useful to include this GraphQL
schema fragment. It sets up the definitions of the directives, etc. (like
`@search`) that you'll use in your schema. If your editor is GraphQL aware, it
may give you errors if you don't have this available and context sensitive help
if you do.

Don't include it in your input schema to Dgraph - use your editing environment
to set it up as an import. The details depend on your setup.

```graphql
"""
The Int64 scalar type represents a signed 64‐bit numeric non‐fractional value.
Int64 can represent values in range [-(2^63),(2^63 - 1)].
"""
scalar Int64

"""
The DateTime scalar type represents date and time as a string in RFC3339 format.
For example: "1985-04-12T23:20:50.52Z" represents 20 minutes and 50.52 seconds after the 23rd hour of April 12th, 1985 in UTC.
"""
scalar DateTime

input IntRange {
  min: Int!
  max: Int!
}

input FloatRange {
  min: Float!
  max: Float!
}

input Int64Range {
  min: Int64!
  max: Int64!
}

input DateTimeRange {
  min: DateTime!
  max: DateTime!
}

input StringRange {
  min: String!
  max: String!
}

enum DgraphIndex {
  int
  int64
  float
  bool
  hash
  exact
  term
  fulltext
  trigram
  regexp
  year
  month
  day
  hour
  geo
}

input AuthRule {
  and: [AuthRule]
  or: [AuthRule]
  not: AuthRule
  rule: String
}

enum HTTPMethod {
  GET
  POST
  PUT
  PATCH
  DELETE
}

enum Mode {
  BATCH
  SINGLE
}

input CustomHTTP {
  url: String!
  method: HTTPMethod!
  body: String
  graphql: String
  mode: Mode
  forwardHeaders: [String!]
  secretHeaders: [String!]
  introspectionHeaders: [String!]
  skipIntrospection: Boolean
}

type Point {
  longitude: Float!
  latitude: Float!
}

input PointRef {
  longitude: Float!
  latitude: Float!
}

input NearFilter {
  distance: Float!
  coordinate: PointRef!
}

input PointGeoFilter {
  near: NearFilter
  within: WithinFilter
}

type PointList {
  points: [Point!]!
}

input PointListRef {
  points: [PointRef!]!
}

type Polygon {
  coordinates: [PointList!]!
}

input PolygonRef {
  coordinates: [PointListRef!]!
}

type MultiPolygon {
  polygons: [Polygon!]!
}

input MultiPolygonRef {
  polygons: [PolygonRef!]!
}

input WithinFilter {
  polygon: PolygonRef!
}

input ContainsFilter {
  point: PointRef
  polygon: PolygonRef
}

input IntersectsFilter {
  polygon: PolygonRef
  multiPolygon: MultiPolygonRef
}

input PolygonGeoFilter {
  near: NearFilter
  within: WithinFilter
  contains: ContainsFilter
  intersects: IntersectsFilter
}

input GenerateQueryParams {
  get: Boolean
  query: Boolean
  password: Boolean
  aggregate: Boolean
}

input GenerateMutationParams {
  add: Boolean
  update: Boolean
  delete: Boolean
}

directive @hasInverse(field: String!) on FIELD_DEFINITION
directive @search(by: [DgraphIndex!]) on FIELD_DEFINITION
directive @dgraph(
  type: String
  pred: String
) on OBJECT | INTERFACE | FIELD_DEFINITION
directive @id(interface: Boolean) on FIELD_DEFINITION
directive @withSubscription on OBJECT | INTERFACE | FIELD_DEFINITION
directive @secret(field: String!, pred: String) on OBJECT | INTERFACE
directive @auth(
  password: AuthRule
  query: AuthRule
  add: AuthRule
  update: AuthRule
  delete: AuthRule
) on OBJECT | INTERFACE
directive @custom(http: CustomHTTP, dql: String) on FIELD_DEFINITION
directive @remote on OBJECT | INTERFACE | UNION | INPUT_OBJECT | ENUM
directive @remoteResponse(name: String) on FIELD_DEFINITION
directive @cascade(fields: [String]) on FIELD
directive @lambda on FIELD_DEFINITION
directive @lambdaOnMutate(
  add: Boolean
  update: Boolean
  delete: Boolean
) on OBJECT | INTERFACE
directive @cacheControl(maxAge: Int!) on QUERY
directive @generate(
  query: GenerateQueryParams
  mutation: GenerateMutationParams
  subscription: Boolean
) on OBJECT | INTERFACE

input IntFilter {
  eq: Int
  in: [Int]
  le: Int
  lt: Int
  ge: Int
  gt: Int
  between: IntRange
}

input Int64Filter {
  eq: Int64
  in: [Int64]
  le: Int64
  lt: Int64
  ge: Int64
  gt: Int64
  between: Int64Range
}

input FloatFilter {
  eq: Float
  in: [Float]
  le: Float
  lt: Float
  ge: Float
  gt: Float
  between: FloatRange
}

input DateTimeFilter {
  eq: DateTime
  in: [DateTime]
  le: DateTime
  lt: DateTime
  ge: DateTime
  gt: DateTime
  between: DateTimeRange
}

input StringTermFilter {
  allofterms: String
  anyofterms: String
}

input StringRegExpFilter {
  regexp: String
}

input StringFullTextFilter {
  alloftext: String
  anyoftext: String
}

input StringExactFilter {
  eq: String
  in: [String]
  le: String
  lt: String
  ge: String
  gt: String
  between: StringRange
}

input StringHashFilter {
  eq: String
  in: [String]
}
```


# @auth
Source: https://docs.hypermode.com/dgraph/graphql/schema/directives/auth



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

`@auth` allows you to define how to apply authorization rules on the
queries/mutation for a type.

Refer to [graphql endpoint security](/dgraph/graphql/security/overview),
[Role-Based Access Control (RBAC) rules](/dgraph/graphql/security/rbac-rules)
and [Graph traversal rules](/dgraph/graphql/security/graphtraversal-rules) for
details.

`@auth` directive isn't supported on `union` and `@remote` types.


# Custom Resolvers Overview
Source: https://docs.hypermode.com/dgraph/graphql/schema/directives/custom

Dgraph creates a GraphQL API from nothing more than GraphQL types. To customize the behavior of your schema, you can implement custom resolvers.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph creates a GraphQL API from nothing more than GraphQL types. That's great,
and gets you moving fast from an idea to a running app. However, at some point,
as your app develops, you might want to customize the behavior of your schema.

In Dgraph, you do that with code (in any language you like) that implements
custom resolvers.

Dgraph doesn't execute your custom logic itself. It makes external HTTP
requests. That means, you can deploy your custom logic into the same Kubernetes
cluster as your Dgraph instance, deploy and call, for example, AWS Lambda
functions, or even make calls to existing HTTP and GraphQL endpoints.

## The `@custom` directive

There are three places you can use the `@custom` directive and thus tell Dgraph
where to apply custom logic.

1. You can add custom queries to the Query type

   ```graphql
   type Query {
     myCustomQuery(...): QueryResultType @custom(...)
   }
   ```

2. You can add custom mutations to the Mutation type

   ```graphql
   type Mutation {
       myCustomMutation(...): MutationResult @custom(...)
   }
   ```

3. You can add custom fields to your types

   ```graphql
   type MyType {
        ...
        customField: FieldType @custom(...)
        ...
   }
   ```

The `@custom` directive is used to define custom queries, mutations and fields.

In all cases, the result type (of the query, mutation, or field) can be either:

* a type that's stored in Dgraph (that's any type you've defined in your
  schema), or
* a type that's not stored in Dgraph and is marked with the `@remote` directive.

Because the result types can be local or remote, you can call other HTTP
endpoints, call remote GraphQL, or even call back to your Dgraph instance to add
extra logic on top of Dgraph's graph search or mutations.

Here's the GraphQL definition of the directives:

```graphql
directive @custom(http: CustomHTTP) on FIELD_DEFINITION
directive @remote on OBJECT | INTERFACE

input CustomHTTP {
  url: String!
  method: HTTPMethod!
  body: String
  graphql: String
  mode: Mode
  forwardHeaders: [String!]
  secretHeaders: [String!]
  introspectionHeaders: [String!]
  skipIntrospection: Boolean
}

enum HTTPMethod {
  GET
  POST
  PUT
  PATCH
  DELETE
}
enum Mode {
  SINGLE
  BATCH
}
```

Each definition of custom logic must include:

* the `url` where the custom logic is called. This can include a path and
  parameters that depend on query/mutation arguments or other fields.
* the HTTP `method` to use in the call. For example, when calling a REST
  endpoint with `GET`, `POST`, etc.

Optionally, the custom logic definition can also include:

* a `body` definition that can be used to construct a HTTP body from arguments
  or fields.
* a list of `forwardHeaders` to take from the incoming request and add to the
  outgoing HTTP call. Used, for example, if the incoming request contains an
  auth token that must be passed to the custom logic.
* a list of `secretHeaders` to take from the `Dgraph.Secret` defined in the
  schema file and add to the outgoing HTTP call. Used, for example, for a server
  side API key and other static value that must be passed to the custom logic.
* the `graphql` query/mutation to call if the custom logic is a GraphQL server
  and whether to introspect or not (`skipIntrospection`) the remote GraphQL
  endpoint.
* `mode` which is used for resolving fields by calling an external GraphQL
  query/mutation. It can either be `BATCH` or `SINGLE`.
* a list of `introspectionHeaders` to take from the `Dgraph.Secret`
  [object](#dgraphsecret) defined in the schema file. They're added to the
  introspection requests sent to the endpoint.

The result type of custom queries and mutations can be any object type in your
schema, including `@remote` types. For custom fields the type can be object
types or scalar types.

The `method` can be any of the HTTP methods: `GET`, `POST`, `PUT`, `PATCH`, or
`DELETE`, and `forwardHeaders` is a list of headers that should be passed from
the incoming request to the outgoing HTTP custom request. Let's look at each of
the other `http` arguments in detail.

## Dgraph.Secret

Sometimes you might want to forward some static headers to your custom API which
can't be exposed to the client. This could be an API key from a payment
processor or an auth token for your organization on GitHub. These secrets can be
specified as comments in the schema file and then can be used in `secretHeaders`
and `introspectionHeaders` while defining the custom directive for a
field/query.

```graphql
type Query {
  getTopUsers(id: ID!): [User]
    @custom(
      http: {
        url: "http://api.github.com/topUsers"
        method: "POST"
        introspectionHeaders: ["Github-Api-Token"]
        secretHeaders: ["Authorization:Github-Api-Token"]
        graphql: "..."
      }
    )
}

# Dgraph.Secret Github-Api-Token "long-token"
```

In the preceding request, `Github-Api-Token` would be sent as a header with
value `long-token` for the introspection request. For the actual `/graphql`
request, the `Authorization` header would be sent with the value `long-token`.

<Note>
  `Authorization:Github-Api-Token` syntax tells us to use the value for
  `Github-Api-Token` from `Dgraph.Secret` and forward it to the custom API with
  the header key as `Authorization`.
</Note>

## The URL and method

The URL can be as simple as a fixed URL string, or include details drawn from
the arguments or fields.

A simple string might look like:

```graphql
type Query {
  myCustomQuery: MyResult
    @custom(http: { url: "https://my.api.com/theQuery", method: GET })
}
```

While, in more complex cases, the arguments of the query/mutation can be used as
a pattern for the URL:

```graphql
type Query {
  myGetPerson(id: ID!): Person
    @custom(http: { url: "https://my.api.com/person/$id", method: GET })

  getPosts(authorID: ID!, numToFetch: Int!): [Post]
    @custom(
      http: {
        url: "https://my.api.com/person/$authorID/posts?limit=$numToFetch"
        method: GET
      }
    )
}
```

In this case, a query like

```graphql
query {
  getPosts(authorID: "auth123", numToFetch: 10) {
    title
  }
}
```

gets transformed to an outgoing HTTP GET request to the URL
`https://my.api.com/person/auth123/posts?limit=10`.

When using custom logic on fields, the URL can draw from other fields in the
type. For example:

```graphql
type User {
    username: String! @id
    ...
    posts: [Post] @custom(http: {
        url: "https://my.api.com/person/$username/posts",
        method: GET
    })
}
```

Note that:

* Fields or arguments used in the path of a URL, such as `username` or
  `authorID` in the preceding examples, must be marked as non-nullable (have `!`
  in their type); whereas, those used in parameters, such as `numToFetch`, can
  be nullable.
* Currently, only scalar fields or arguments are allowed to be used in URLs or
  bodies; though, see body below, this doesn't restrict the objects you can
  construct and pass to custom logic functions.
* Currently, the body can only contain alphanumeric characters in the key and
  other characters like `_` aren't yet supported.
* Currently, constant values are also not allowed in the body template. This
  would soon be supported.

## The body

Many HTTP requests, such as add and update operations on REST APIs, require a
JSON formatted body to supply the data. In a similar way to how `url` allows
specifying a url pattern to use in resolving the custom request, Dgraph allows a
`body` pattern that's used to build HTTP request bodies.

For example, this body can be structured JSON that relates a mutation's
arguments to the JSON structure required by the remote endpoint.

```graphql
type Mutation {
    newMovie(title: String!, desc: String, dir: ID, imdb: ID): Movie @custom(http: {
            url: "http://myapi.com/movies",
            method: "POST",
            body: "{ title: $title, imdbID: $imdb, storyLine: $desc, director: { id: $dir }}",
    })
```

A request with
`newMovie(title: "...", desc: "...", dir: "dir123", imdb: "tt0120316")` is
transformed into a `POST` request to `http://myapi.com/movies` with a JSON body
of:

```json
{
  "title": "...",
  "imdbID": "tt0120316",
  "storyLine": "...",
  "director": {
    "id": "dir123"
  }
}
```

`url` and `body` templates can be used together in a single custom definition.

For both `url` and `body` templates, any non-null arguments or fields must be
present to evaluate the custom logic. And the following rules are applied when
building the request from the template for nullable arguments or fields.

* If the value of a nullable argument is present, it's used in the template.
* If a nullable argument is present, but null, then in a body `null` is
  inserted, while in a url nothing is added. For example, if the `desc` argument
  above is null then `{ ..., storyLine: null, ...}` is constructed for the body.
  Whereas, in a URL pattern like `https://a.b.c/endpoint?arg=$gqlArg`, if
  `gqlArg` is present, but null, the generated URL is
  `https://a.b.c/endpoint?arg=`.
* If a nullable argument is not present, nothing is added to the URL/body. That
  would mean the constructed body would not contain `storyLine` if the `desc`
  argument is missing, and in `https://a.b.c/endpoint?arg=$gqlArg` the result
  would be `https://a.b.c/endpoint` if `gqlArg` weren't present in the request
  arguments.

## Calling GraphQL custom resolvers

Custom queries, mutations, and fields can be implemented by custom GraphQL
resolvers. In this case, use the `graphql` argument to specify which
query/mutation on the remote server to call. The syntax includes if the call is
a query or mutation, the arguments, and what query/mutation to use on the remote
endpoint.

For example, you can pass arguments to queries onward as arguments to remote
GraphQL endpoints:

```graphql
type Query {
  getPosts(authorID: ID!, numToFetch: Int!): [Post]
    @custom(
      http: {
        url: "https://my.api.com/graphql"
        method: POST
        graphql: "query($authorID: ID!, $numToFetch: Int!) { posts(auth: $authorID, first: $numToFetch) }"
      }
    )
}
```

You can also define your own inputs and pass those to the remote GraphQL
endpoint.

```graphql
input NewMovieInput { ... }

type Mutation {
    newMovie(input: NewMovieInput!): Movie @custom(http: {
        url: "http://movies.com/graphql",
        method: "POST",
        graphql: "mutation($input: NewMovieInput!) { addMovie(data: $input) }",
    })
```

When a schema is uploaded, Dgraph introspects the remote GraphQL endpoints on
any custom logic that uses the `graphql` argument. From the results of
introspection, it tries to match up arguments and input and object types to
ensure that the calls to and expected responses from the remote GraphQL make
sense.

If that introspection isn't possible, set `skipIntrospection: true` in the
custom definition and Dgraph won't perform GraphQL schema introspection for this
custom definition.

## Remote types

Any type annotated with the `@remote` directive isn't stored in Dgraph. This
allows your Dgraph GraphQL instance to serve an API that includes both data
stored locally and data stored or generated elsewhere. You can also use custom
fields, for example, to join data from disparate datasets.

Remote types can only be returned by custom resolvers and Dgraph won't generate
any search or CRUD operations for remote types.

The schema definition used to define your Dgraph GraphQL API must include
definitions of all the types used. If a custom logic call returns a type not
stored in Dgraph, then that type must be added to the Dgraph schema with the
`@remote` directive.

For example, your API might use custom logic to integrate with GitHub, using
either `https://api.github.com` or the GitHub GraphQL API
`https://api.github.com/graphql` and calling the `user` query. Either way, your
GraphQL schema needs to include the type you expect back from that remote call.
That could be linking a `User` as stored in your Dgraph instance with the
`Repository` data from GitHub. With `@remote` types, that's as simple as adding
the type and custom call to your schema.

```graphql
# GitHub's repository type
type Repository @remote { ... }

# Dgraph user type
type User {
    # local user name = GitHub id
    username: String! @id

    # ...
    # other data stored in Dgraph
    # ...

    # join local data with remote
    repositories: [Repository] @custom(http: {
        url:  "https://api.github.com/users/$username/repos",
        method: GET
    })
}
```

Just defining the connection is all it takes and then you can ask a single
GraphQL query that performs a local query and joins with (potentially many)
remote data sources.

### RemoteResponse directive

In combination with the `@remote` directive, in a GraphQL schema you can also
use the `@remoteResponse` directive. You can define the `@remoteResponse`
directive on the fields of a `@remote` type to map the JSON key response of a
custom query to a GraphQL field.

For example, in the given GraphQL schema there's a defined custom DQL query,
whose JSON response contains the results of the `groupby` clause in the
`@groupby` key. By using the `@remoteResponse` directive you'll map the
`groupby` field in `GroupUserMapQ` type to the `@groupby` key in the JSON
response:

```graphql
type User {
  screen_name: String! @id
  followers: Int @search
  tweets: [Tweets] @hasInverse(field: user)
}
type UserTweetCount @remote {
  screen_name: String
  tweetCount: Int
}
type UserMap @remote {
  followers: Int
  count: Int
}
type GroupUserMapQ @remote {
  groupby: [UserMap] @remoteResponse(name: "@groupby")
}
```

it's possible to define the following `@custom` DQL query:

```graphql
queryUserKeyMap: [GroupUserMapQ] @custom(dql: """
{
  queryUserKeyMap(func: type(User)) @groupby(followers: User.followers) {
    count(uid)
  }
}
""")
```

## How Dgraph processes custom results

Given types like

```graphql
type Post @remote {
    id: ID!
    title: String!
    datePublished: DateTime
    author: Author
}

type Author { ... }
```

and a custom query

```graphql
type Query {
  getCustomPost(id: ID!): Post
    @custom(http: { url: "https://my.api.com/post/$id", method: GET })

  getPosts(authorID: ID!, numToFetch: Int!): [Post]
    @custom(
      http: {
        url: "https://my.api.com/person/$authorID/posts?limit=$numToFetch"
        method: GET
      }
    )
}
```

Dgraph turns the `getCustomPost` query into a HTTP request to
`https://my.api.com/post/$id` and expects a single JSON object with fields `id`,
`title`, `datePublished` and `author` as result. Any additional fields are
ignored, while if non-nullable fields (like `id` and `title`) are missing,
GraphQL error propagation is triggered.

For `getPosts`, Dgraph expects the HTTP call to
`https://my.api.com/person/$authorID/posts?limit=$numToFetch` to return a JSON
array of JSON objects, with each object matching the `Post` type as described
above.

If the custom resolvers are GraphQL calls, like:

```graphql
type Query {
  getCustomPost(id: ID!): Post
    @custom(
      http: {
        url: "https://my.api.com/graphql"
        method: POST
        graphql: "query(id: ID) { post(postID: $id) }"
      }
    )

  getPosts(authorID: ID!, numToFetch: Int!): [Post]
    @custom(
      http: {
        url: "https://my.api.com/graphql"
        method: POST
        graphql: "query(id: ID) { postByAuthor(authorID: $id, first: $numToFetch) }"
      }
    )
}
```

then Dgraph expects a GraphQL call to `post` to return a valid GraphQL result
like `{ "data": { "post": {...} } }` and will use the JSON object that is the
value of `post` as the data resolved by the request.

Similarly, Dgraph expects `postByAuthor` to return data like
`{ "data": { "postByAuthor": [ {...}, ... ] } }` and will use the array value of
`postByAuthor` to build its array of posts result.

## How errors from custom endpoints are handled

When a query returns an error while resolving from a custom HTTP endpoint, the
error is added to the `errors` array and sent back to the user in the JSON
response.

When a field returns an error while resolving a custom HTTP endpoint, the
field's value becomes `null` and the error is added to the `errors` JSON array.
The rest of the fields are still resolved as required by the request.

For example, a query from a custom HTTP endpoint will return an error in the
following format:

```json
{
  "errors": [
    {
      "message": "Rest API returns Error for myFavoriteMovies query",
      "locations": [
        {
          "line": 5,
          "column": 4
        }
      ],
      "path": ["Movies", "name"]
    }
  ]
}
```

## How custom fields are resolved

When evaluating a request that includes custom fields, Dgraph might run multiple
resolution stages to resolve all the fields. Dgraph must also ensure it requests
enough data to forfull the custom fields. For example, given the `User` type
defined as:

```graphql
type User {
    username: String! @id
    ...
    posts: [Post] @custom(http: {
        url: "https://my.api.com/person/$username/posts",
        method: GET
    })
}
```

a query such as:

```graphql
query {
  queryUser {
    username
    posts
  }
}
```

is executed by first querying in Dgraph for `username` and then using the result
to resolve the custom field `posts` (which relies on `username`). For a request
like:

```graphql
query {
  queryUser {
    posts
  }
}
```

Dgraph works out that it must first get `username` so it can run the custom
field `posts`, even though `username` isn't part of the original query. So
Dgraph retrieves enough data to satisfy the custom request, even if that
involves data that isn't asked for in the query.

There are currently a few limitations on custom fields:

* each custom call must include either an `ID` or `@id` field
* arguments are not allowed (soon custom field arguments will be allowed and
  will be used in the `@custom` directive in the same manner as for custom
  queries and mutations), and
* a custom field can't depend on another custom field (longer term, we intend to
  lift this restriction).

## Restrictions / Roadmap

Our custom logic is still in beta and we are improving it quickly. Here's a few
points that we plan to work on soon:

* adding arguments to custom fields
* relaxing the restrictions on custom fields using id values
* iterative evaluation of `@custom` and `@remote` - in the current version you
  can't have `@custom` inside an `@remote` type once we add this, you'll be able
  to extend remote types with custom fields, and
* allowing fine tuning of the generated API, for example removing of customizing
  the generated CRUD mutations.

## Example: query

Let's say we want to integrate our app with an existing external REST API.
There's a few things we need to know:

* The URL of the API, the path and any parameters required
* The shape of the resulting JSON data
* The method (GET, POST, etc.), and
* What authorization we need to pass to the external endpoint

The custom query can take any number of scalar arguments and use those to
construct the path, parameters and body (we'll see an example of that in the
custom mutation section) of the request that gets sent to the remote endpoint.

In an app, you'd deploy an endpoint that does some custom work and returns data
that's used in your UI, or you'd wrap some logic or call around an existing
endpoint. So that we can walk through a whole example, let's use the Twitter
API.

To integrate a call that returns the data of Twitter user with our app, all we
need to do is add the expected result type `TwitterUser` and set up a custom
query:

```graphql
type TwitterUser @remote {
    id: ID!
    name: String
    screen_name: String
    location: String
    description: String
    followers_count: Int
    ...
}

type Query{
    getCustomTwitterUser(name: String!): TwitterUser @custom(http:{
        url: "https://api.twitter.com/1.1/users/show.json?screen_name=$name"
        method: "GET",
        forwardHeaders: ["Authorization"]
    })
}
```

Dgraph will then be able to accept a GraphQL query like

```graphql
query {
  getCustomTwitterUser(name: "dgraphlabs") {
    location
    description
    followers_count
  }
}
```

construct a HTTP GET request to
`https://api.twitter.com/1.1/users/show.json?screen_name=dgraphlabs`, attach
header `Authorization` from the incoming GraphQL request to the outgoing HTTP,
and make the call and return a GraphQL result.

The result JSON of the actual HTTP call will contain the whole object from the
REST endpoint (you can see how much is in the Twitter user object
[here](https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/user-object)).
But, the GraphQL query only asked for some of that, so Dgraph filters out any
returned values that weren't asked for in the GraphQL query and builds a valid
GraphQL response to the query and returns GraphQL.

```json
{
    "data": {
        "getCustomTwitterUser": { "location": ..., "description": ..., "followers_count": ... }
    }
}
```

Your version of the remote type doesn't have to be equal to the remote type. For
example, if you don't want to allow users to query the full Twitter user, you
include in the type definition only the fields that can be queried.

All the usual options for custom queries are allowed; for example, you can have
multiple queries in a single GraphQL request and a mix of custom and Dgraph
generated queries, you can get the result compressed by setting
`Accept-Encoding` to `gzip`, etc.

## Example: mutation

With custom mutations, you can use custom logic to define values for one or more
fields in a mutation.

Let's say we have an app about authors and posts. Logged in authors can add
posts, but we want to do some input validation and add extra value when a post
is added. The key types might be as follows.

```graphql
type Author { ... }

type Post {
    id: ID!
    title: String
    text: String
    datePublished: DateTime
    author: Author
    ...
}
```

Dgraph generates an `addPost` mutation from those types, but we want to do
something extra. We don't want the `author` field to come in with the mutation,
that should get filled in from the JWT of the logged in user. Also, the
`datePublished` shouldn't be in the input; it should be set as the current time
at point of mutation. Maybe we also have some community guidelines about what
might constitute an offensive `title` or `text` in a post. Maybe users can only
post if they have enough community credit.

We'll need custom code to do all that, so we can write a custom function that
takes in only the title and text of the new post. Internally, it can check that
the title and text satisfy the guidelines and that this user has enough credit
to make a post. If those checks pass, it then builds a full post object by
adding the current time as the `datePublished` and adding the `author` from the
JWT information it gets from the forward header. It can then call the `addPost`
mutation constructed by Dgraph to add the post into Dgraph and returns the
resulting post as its GraphQL output.

So as well as the types above, we need a custom mutation:

```graphql
type Mutation {
  newPost(title: String!, text: String): Post
    @custom(
      http: {
        url: "https://my.api.com/addPost"
        method: "POST"
        body: "{ postText: $text, postTitle: $title }"
        forwardHeaders: ["AuthHdr"]
      }
    )
}
```

Find out more about how to turn off generated mutations and protecting mutations
with authorization rules at:

* [Remote Types - Turning off Generated Mutations with `@remote` Directive](./overview#remote-types)
* [Securing Mutations with the `@auth` Directive](./auth)

## Example: field

Custom fields allow you to extend your types with custom logic as well as make
joins between your local data and remote data.

Let's say we are building an app for managing projects. Users will login with
their GitHub id and we want to connect some data about their work stored in
Dgraph with say their GitHub profile, issues, etc.

Our first version of our users might start out with just their GitHub username
and some data about what projects they are working on.

```graphql
type User {
  username: String! @id
  projects: [Project]
  tickets: [Ticket]
}
```

We can then add their GitHub repositories by just extending the definitions with
the types and custom field needed to make the remote call.

```graphql
# GitHub's repository type
type Repository @remote { ... }

# Dgraph user type
type User {
    # local user name = GitHub id
    username: String! @id

    # join local data with remote
    repositories: [Repository] @custom(http: {
        url:  "https://api.github.com/users/$username/repos",
        method: GET
    })
}
```

We could similarly join with say the GitHub user details, or open pull requests,
to further fill out the join between GitHub and our local data. Instead of the
REST API, let's use the GitHub GraphQL endpoint

```graphql
# GitHub's User type
type GitHubUser @remote { ... }

# Dgraph user type
type User {
    # local user name = GitHub id
    username: String! @id

    # join local data with remote
    gitDetails: GitHubUser @custom(http: {
        url:  "https://api.github.com/graphql",
        method: POST,
        graphql: "query(username: String!) { user(login: $username) }",
        skipIntrospection: true
    })
}
```

Perhaps our app has some measure of their velocity that's calculated by a custom
function that looks at both their GitHub commits and some other places where
work is added. Soon we'll have a schema where we can render a user's home page,
the projects they work on, their open tickets, their GitHub details, etc. in a
single request that queries across multiple sources and can mix Dgraph filtering
with external calls.

```graphql
query {
    getUser(id: "aUser") {
        username
        projects(order: { asc: lastUpdate }, first: 10) {
            projectName
        }
        tickets {
            connectedGitIssue { ... }
        }
        velocityMeasure
        gitDetails { ... }
        repositories { ... }
    }
}
```


# @deprecated
Source: https://docs.hypermode.com/dgraph/graphql/schema/directives/deprecated



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The `@deprecated` directive allows you to tag the schema definition of a field
or enum value as deprecated with an optional reason.

When you use the `@deprecated` directive, GraphQL users can deprecate their use
of the deprecated field or `enum` value. Most GraphQL tools and clients will
pick up this notification and give you a warning if you try to use a deprecated
field.

### Example

For example, to mark `oldField` in the schema as deprecated:

```graphql
type MyType {
  id: ID!
  oldField: String
    @deprecated(reason: "oldField is deprecated. Use newField instead.")
  newField: String
  deprecatedField: String @deprecated
}
```


# @dgraph
Source: https://docs.hypermode.com/dgraph/graphql/schema/directives/dgraph



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The `@dgraph` directive customizes the name of the types and predicates
generated in Dgraph when deploying a GraphQL Schema.

* `type <type> @dgraph(type: "TypeNameToUseInDgraph")` controls what Dgraph type
  is used for a GraphQL type.
* `field: SomeType @dgraph(pred: "DgraphPredicate")` controls what Dgraph
  predicate is mapped to a GraphQL field.

For example, if you have existing types that don't match GraphQL requirements,
you can create a schema like the following.

```graphql
type Person @dgraph(type: "Human-Person") {
  name: String @search(by: [hash]) @dgraph(pred: "name")
  age: Int
}

type Movie @dgraph(type: "film") {
  name: String @search(by: [term]) @dgraph(pred: "film.name")
}
```

Which maps to the Dgraph schema:

```graphql
type Human-Person {
    name
    Person.age
}

type film {
    film.name
}

name string @index(hash) .
Person.age: int .
film.name string @index(term) .
```

You might also have the situation where you have used `name` for both movie
names and people's names. In this case you can map fields in two different
GraphQL types to the one Dgraph predicate.

```graphql
type Person {
    name: String @dgraph(pred: "name")
    ...
}

type Movie {
    name: String @dgraph(pred: "name")
    ...
}
```

<Note>
  In Dgraph's current GraphQL implementation, if two fields are mapped to the
  same Dgraph predicate, both should have the same `@search` directive.
</Note>


# @embedding
Source: https://docs.hypermode.com/dgraph/graphql/schema/directives/embedding



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

A Float array can be used as a vector using `@embedding` directive. It denotes a
vector of floating point numbers, i.e an ordered array of float32.

The embeddings can be defined on one or more predicates of a type and they're
generated using suitable machine learning models.

This directive is used in conjunction with `@search` directive to declare the
Hierarchical Navigable Small World (HNSW) index. For more information see:
[@search](/dgraph/graphql/schema/directives/search#vector-embedding) directive
for vector embeddings.


# @generate
Source: https://docs.hypermode.com/dgraph/graphql/schema/directives/generate

The @generate directive specifies which GraphQL APIs are generated for a given type. Without it, all queries & mutations are generated except subscription.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The `@generate` directive is used to specify which GraphQL APIs are generated
for a given type.

Here's the GraphQL definition of the directive

```graphql
input GenerateQueryParams {
  get: Boolean
  query: Boolean
  password: Boolean
  aggregate: Boolean
}

input GenerateMutationParams {
  add: Boolean
  update: Boolean
  delete: Boolean
}
directive @generate(
  query: GenerateQueryParams
  mutation: GenerateMutationParams
  subscription: Boolean
) on OBJECT | INTERFACE
```

The corresponding APIs are generated by setting the `Boolean` variables inside
the `@generate` directive to `true`. Passing `false` forbids the generation of
the corresponding APIs.

The default value of the `subscription` variable is `false` while the default
value of all other variables is `true`. Therefore, if no `@generate` directive
is specified for a type, all queries and mutations except `subscription` are
generated.

## Example of @generate directive

```graphql
type Person
  @generate(
    query: { get: false, query: true, aggregate: false }
    mutation: { add: true, delete: false }
    subscription: false
  ) {
  id: ID!
  name: String!
}
```

The GraphQL schema above will generate a `queryPerson` query and `addPerson`,
`updatePerson` mutations. It won't generate `getPerson`, `aggregatePerson`
queries nor a `deletePerson` mutation as these have been marked as `false` using
the `@generate` directive. Note that the `updatePerson` mutation is generated
because the default value of the `update` variable is `true`.


# @id
Source: https://docs.hypermode.com/dgraph/graphql/schema/directives/ids

Dgraph database provides two types of identifiers: the ID scalar type and the @id directive.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph provides two types of built-in identifiers: the `ID` scalar type and the
`@id` directive.

* The `ID` scalar type is used when you don't need to set an identifier outside
  of Dgraph.
* The `@id` directive is used for external identifiers, such as email addresses.

## The `@id` directive

For some types, you'll need a unique identifier set from outside Dgraph. A
common example is a username.

The `@id` directive tells Dgraph to keep that field's values unique and use them
as identifiers.

For example, you might set the following type in a schema:

```graphql
type User {
    username: String! @id
    ...
}
```

Dgraph requires a unique username when creating a new user. It generates the
input type for `addUser` with `username: String!`, so you can't make an add
mutation without setting a username; and when processing the mutation, Dgraph
will ensure that the username isn't already set for another node of the `User`
type.

In a single-page app, you could render the page for `http://.../user/Erik` when
a user clicks to view the author bio page for that user. Your app can then use a
`getUser(username: "Erik") { ... }` GraphQL query to fetch the data and generate
the page.

Identities created with `@id` are reusable. If you delete an existing user, you
can reuse the username.

Fields with the `@id` directive must have the type `String!`.

As with `ID` types, Dgraph generates queries and mutations so you can query,
update, and delete data in nodes, using the fields with the `@id` directive as
references.

It is possible to use the `@id` directive on more than one field in a type. For
example, you can define a type like the following:

```graphql
type Book {
    name: String! @id
    isbn: String! @id
    genre: String!
    ...
}
```

You can then use multiple `@id` fields in arguments to `get` queries, and while
searching, these fields will be combined with the `AND` operator, resulting in a
Boolean `AND` operation. For example, for the above schema, you can send a
`getBook` query like the following:

```graphql
query {
  getBook(name: "The Metamorphosis", isbn: "9871165072") {
    name
    genre
    ...
  }
}
```

This will yield a positive response if both the `name` **and** `isbn` match any
data in the database.

### `@id` and interfaces

By default, if used in an interface, the `@id` directive will ensure field
uniqueness for each implementing type separately. In this case, the `@id` field
in the interface won't be unique for the interface but for each of its
implementing types. This allows two different types implementing the same
interface to have the same value for the inherited `@id` field.

There are scenarios where this behavior might not be desired, and you may want
to constrain the `@id` field to be unique across all the implementing types. In
that case, you can set the `interface` argument of the `@id` directive to
`true`, and Dgraph will ensure that the field has unique values across all the
implementing types of an interface.

For example:

```graphql
interface Item {
  refID: Int! @id(interface: true) # if there is a Book with refID = 1, then there can't be a chair with that refID.
  itemID: Int! @id # If there is a Book with itemID = 1, there can still be a Chair with the same itemID.
}

type Book implements Item { ... }
type Chair implements Item { ... }
```

In the above example, `itemID` won't be present as an argument to the `getItem`
query as it might return more than one `Item`.

<Note>
  `get` queries generated for an interface will have only the `@id(interface:
        true)` fields as arguments.
</Note>

## Combining `ID` and `@id`

You can use both the `ID` type and the `@id` directive on another field
definition to have both a unique identifier and a generated identifier.

For example, you might define the following type in a schema:

```graphql
type User {
    id: ID!
    username: String! @id
    ...
}
```

With this schema, Dgraph requires a unique `username` when creating a new user.
This schema provides the benefits of both of the previous examples above. Your
app can then use the `getUser(...) { ... }` query to provide either the
Dgraph-generated `id` or the externally-generated `username`.

<Note>
  If in a type there are multiple `@id` fields, then in a `get` query these
  arguments will be optional. If in a type there's only one field defined with
  either `@id` or `ID`, then that will be a required field in the `get` query's
  arguments.
</Note>


# Directives
Source: https://docs.hypermode.com/dgraph/graphql/schema/directives/overview



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The list of all directives supported by Dgraph.

### @auth

`@auth` allows you to define how to apply authorization rules on the
queries/mutation for a type.

Reference: [auth directive](./auth)

### @cascade

`@cascade` allows you to filter out certain nodes within a query.

Reference: [cascade](/dgraph/graphql/query/cascade)

### @custom

`@custom` directive is used to define custom queries, mutations, and fields.

Reference: [custom directive](./custom)

### @deprecated

The `@deprecated` directive lets you mark the schema definition of a field or
`enum` value as deprecated, and also lets you provide an optional reason for the
deprecation.

Reference: [deprecation](./deprecated)

### @dgraph

`@dgraph` directive tells us how to map fields within a type to existing
predicates inside Dgraph.

Reference: [@dgraph directive](./dgraph)

### @embedding

`@embedding` directive designates one or more fields as vector embeddings.

Reference: [@embedding directive](./search#vector-embedding)

### @generate

The `@generate` directive is used to specify which GraphQL APIs are generated
for a type.

Reference: [generate directive](./generate)

### @hasInverse

`@hasInverse` is used to setup up two way edges such that adding a edge in one
direction automatically adds the one in the inverse direction.

Reference: [linking nodes in the graph](/dgraph/graphql/schema/relationships)

### @id

`@id` directive is used to annotate a field which represents a unique identifier
coming from outside of Dgraph.

Reference: \[Identity]\((./ids)

### @include

The `@include` directive can be used to include a field based on the value of an
`if` argument.

Reference: [include directive](/dgraph/graphql/query/skip-include)

### @lambda

The `@lambda` directive allows you to call custom JavaScript resolvers. The
`@lambda` queries, mutations, and fields are resolved through the lambda
functions implemented on a given lambda server.

Reference: [lambda directive](/dgraph/graphql/lambda/overview)

### @remote

`@remote` directive is used to annotate types for which data isn't stored in
Dgraph. These types are typically used with custom queries and mutations.

### @remoteResponse

The `@remoteResponse` directive allows you to annotate the fields of a `@remote`
type to map a custom query's JSON key response to a GraphQL field.

### @search

`@search` allows you to perform filtering on a field while querying for nodes.

Reference: [search](./search)

### @secret

`@secret` directive is used to store secret information, it gets encrypted and
then stored in Dgraph.

Reference: [password type](/dgraph/graphql/schema/types#password-type)

### @skip

The `@skip` directive can be used to fetch a field based on the value of a
user-defined GraphQL variable.

Reference: [skip directive](/dgraph/graphql/query/skip-include)

### @withSubscription

`@withSubscription` directive when applied on a type, generates subscription
queries for it.

Reference: [subscriptions](/dgraph/graphql/subscriptions)

### @lambdaOnMutate

The `@lambdaOnMutate` directive allows you to listen to mutation
events(`add`/`update`/`delete`). Depending on the defined events and the
occurrence of a mutation event, `@lambdaOnMutate` triggers the appropriate
lambda function implemented on a given lambda server.

Reference: [lambdaOnMutate directive](/dgraph/graphql/lambda/webhook)


# Search and Filtering
Source: https://docs.hypermode.com/dgraph/graphql/schema/directives/search

What search can you build into your GraphQL API? Dgraph builds search into the fields of each type, so searching is available at deep levels in a query

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The `@search` directive tells Dgraph what search to build into your GraphQL API.

When a type contains an `@search` directive, Dgraph constructs a search input
type and a query in the GraphQL `Query` type. For example, if the schema
contains

```graphql
type Post {
    ...
}
```

then Dgraph constructs a `queryPost` GraphQL query for querying posts. The
`@search` directives in the `Post` type control how Dgraph builds indexes and
what kinds of search it builds into `queryPost`. If the type contains

```graphql
type Post {
    ...
    datePublished: DateTime @search
}
```

then it's possible to filter posts with a date-time search like:

```graphql
query {
    queryPost(filter: { datePublished: { ge: "2020-06-15" }}) {
        ...
    }
}
```

If the type tells Dgraph to build search capability based on a term (word) index
for the `title` field

```graphql
type Post {
    ...
    title: String @search(by: [term])
}
```

then, the generated GraphQL API will allow search by terms in the title.

```graphql
query {
    queryPost(filter: { title: { anyofterms: "GraphQL" }}) {
        ...
    }
}
```

Dgraph also builds search into the fields of each type, so searching is
available at deep levels in a query. For example, if the schema contained these
types

```graphql
type Post {
    ...
    title: String @search(by: [term])
}

type Author {
    name: String @search(by: [hash])
    posts: [Post]
}
```

then Dgraph builds GraphQL search such that a query can, for example, find an
author by name (from the hash search on `name`) and return only their posts that
contain the term "GraphQL".

```graphql
queryAuthor(filter: { name: { eq: "Diggy" } } ) {
    posts(filter: { title: { anyofterms: "GraphQL" }}) {
        title
    }
}
```

Dgraph can build search types with the ability to search between a range. For
example with the above Post type with datePublished field, a query can find
publish dates within a range

```graphql
query {
    queryPost(filter: { datePublished: { between: { min: "2020-06-15", max: "2020-06-16" }}}) {
        ...
    }
}
```

Dgraph can also build GraphQL search ability to find match a value from a list.
For example with the above Author type with the name field, a query can return
the Authors that match a list

```graphql
queryAuthor(filter: { name: { in: ["Diggy", "Jarvis"] } } ) {
    ...
}
```

There's different search possible for each type as explained below.

### Int, Float and DateTime

| argument | constructed filter                                |
| -------- | ------------------------------------------------- |
| none     | `lt`, `le`, `eq`, `in`, `between`, `ge`, and `gt` |

Search for fields of types `Int`, `Float` and `DateTime` is enabled by adding
`@search` to the field with no arguments. For example, if a schema contains:

```graphql
type Post {
    ...
    numLikes: Int @search
}
```

Dgraph generates search into the API for `numLikes` in two ways: a query for
posts and field search on any post list.

A field `queryPost` is added to the `Query` type of the schema.

```graphql
type Query {
    ...
    queryPost(filter: PostFilter, order: PostOrder, first: Int, offset: Int): [Post]
}
```

`PostFilter` will contain less than `lt`, less than or equal to `le`, equal
`eq`, in list `in`, between range `between`, greater than or equal to `ge`, and
greater than `gt` search on `numLikes`. Allowing for example:

```graphql
query {
    queryPost(filter: { numLikes: { gt: 50 }}) {
        ...
    }
}
```

Also, any field with a type of list of posts has search options added to it. For
example, if the input schema also contained:

```graphql
type Author {
    ...
    posts: [Post]
}
```

Dgraph would insert search into `posts`, with

```graphql
type Author {
    ...
    posts(filter: PostFilter, order: PostOrder, first: Int, offset: Int): [Post]
}
```

That allows search within the GraphQL query. For example, to find Diggy's posts
with more than 50 likes.

```graphql
queryAuthor(filter: { name: { eq: "Diggy" } } ) {
    ...
    posts(filter: { numLikes: { gt: 50 }}) {
        title
        text
    }
}
```

### DateTime

| argument                          | constructed filters                               |
| --------------------------------- | ------------------------------------------------- |
| `year`, `month`, `day`, or `hour` | `lt`, `le`, `eq`, `in`, `between`, `ge`, and `gt` |

As well as `@search` with no arguments, `DateTime` also allows specifying how
the search index should be built: by year, month, day or hour. `@search`
defaults to year, but once you understand your data and query patterns, you
might want to changes that like `@search(by: [day])`.

### Boolean

| argument | constructed filter |
| -------- | ------------------ |
| none     | `true` and `false` |

Booleans can only be tested for true or false. If `isPublished: Boolean @search`
is in the schema, then the search allows

```graphql
filter: { isPublished: true }
```

and

```graphql
filter: { isPublished: false }
```

### String

Strings allow a wider variety of search options than other types. For strings,
you have the following options as arguments to `@search`.

| argument   | constructed searches                                                  |
| ---------- | --------------------------------------------------------------------- |
| `hash`     | `eq` and `in`                                                         |
| `exact`    | `lt`, `le`, `eq`, `in`, `between`, `ge`, and `gt` (lexicographically) |
| `regexp`   | `regexp` (regular expressions)                                        |
| `term`     | `allofterms` and `anyofterms`                                         |
| `fulltext` | `alloftext` and `anyoftext`                                           |

* *Schema rule*: `hash` and `exact` can't be used together.

#### String exact and hash search

Exact and hash search has the standard lexicographic meaning.

```graphql
query {
    queryAuthor(filter: { name: { eq: "Diggy" } }) { ... }
}
```

And for exact search

```graphql
query {
    queryAuthor(filter: { name: { gt: "Diggy" } }) { ... }
}
```

to find users with names lexicographically after "Diggy".

#### String regular expression search

Search by regular expression requires bracketing the expression with `/` and
`/`. For example, query for "Diggy" and anyone else with "iggy" in their name:

```graphql
query {
    queryAuthor(filter: { name: { regexp: "/.*iggy.*/" } }) { ... }
}
```

#### String term and fulltext search

If the schema has

```graphql
type Post {
    title: String @search(by: [term])
    text: String @search(by: [fulltext])
    ...
}
```

then

```graphql
query {
    queryPost(filter: { title: { `allofterms: "GraphQL tutorial"` } } ) { ... }
}
```

will match all posts with both "GraphQL and "tutorial" in the title, while
`anyofterms: "GraphQL tutorial"` would match posts with either "GraphQL" or
"tutorial".

`fulltext` search is Google-stye text search with stop words, stemming. etc. So
`alloftext: "run woman"` would match "run" as well as "running", etc. For
example, to find posts that talk about fantastic GraphQL tutorials:

```graphql
query {
    queryPost(filter: { title: { `alloftext: "fantastic GraphQL tutorials"` } } ) { ... }
}
```

#### Strings with multiple searches

It is possible to add multiple string indexes to a field. For example to search
for authors by `eq` and regular expressions, add both options to the type
definition, as follows.

```graphql
type Author {
    ...
    name: String! @search(by: [hash, regexp])
}
```

### Enums

| argument | constructed searches                                                  |
| -------- | --------------------------------------------------------------------- |
| none     | `eq` and `in`                                                         |
| `hash`   | `eq` and `in`                                                         |
| `exact`  | `lt`, `le`, `eq`, `in`, `between`, `ge`, and `gt` (lexicographically) |
| `regexp` | `regexp` (regular expressions)                                        |

Enums are serialized in Dgraph as strings. `@search` with no arguments is the
same as `@search(by: [hash])` and provides `eq` and `in` searches. Also
available for enums are `exact` and `regexp`. For hash and exact search on
enums, the literal enum value, without quotes `"..."`, is used, for regexp,
strings are required. For example:

```graphql
enum Tag {
    GraphQL
    Database
    Question
    ...
}

type Post {
    ...
    tags: [Tag!]! @search
}
```

would allow

```graphql
query {
    queryPost(filter: { tags: { eq: GraphQL } } ) { ... }
}
```

Which would find any post with the `GraphQL` tag.

While `@search(by: [exact, regexp]` would also admit `lt` etc. and

```graphql
query {
    queryPost(filter: { tags: { regexp: "/.*aph.*/" } } ) { ... }
}
```

which is helpful for example if the enums are something like product codes where
regular expressions can match a number of values.

### Geolocation

There are 3 Geolocation types: `Point`, `Polygon` and `MultiPolygon`. All of
them are searchable.

The following table lists the generated filters for each type when you include
`@search` on the corresponding field:

| type           | constructed searches                       |
| -------------- | ------------------------------------------ |
| `Point`        | `near`, `within`                           |
| `Polygon`      | `near`, `within`, `contains`, `intersects` |
| `MultiPolygon` | `near`, `within`, `contains`, `intersects` |

#### Example

Take for example a `Hotel` type that has a `location` and an `area`:

```graphql
type Hotel {
  id: ID!
  name: String!
  location: Point @search
  area: Polygon @search
}
```

#### near

The `near` filter matches all entities where the location given by a field is
within a distance `meters` from a coordinate.

```graphql
queryHotel(filter: {
    location: {
        near: {
            coordinate: {
                latitude: 37.771935,
                longitude: -122.469829
            },
            distance: 1000
        }
    }
}) {
  name
}
```

#### within

The `within` filter matches all entities where the location given by a field is
within a defined `polygon`.

```graphql
queryHotel(filter: {
    location: {
        within: {
            polygon: {
                coordinates: [{
                    points: [{
                        latitude: 11.11,
                        longitude: 22.22
                    }, {
                        latitude: 15.15,
                        longitude: 16.16
                    }, {
                        latitude: 20.20,
                        longitude: 21.21
                    }, {
                        latitude: 11.11,
                        longitude: 22.22
                    }]
                }],
            }
        }
    }
}) {
  name
}
```

#### contains

The `contains` filter matches all entities where the `Polygon` or `MultiPolygon`
field contains another given `point` or `polygon`.

<Tip>
  Only one `point` or `polygon` can be taken inside the `ContainsFilter` at a
  time.
</Tip>

A `contains` example using `point`:

```graphql
queryHotel(filter: {
    area: {
        contains: {
            point: {
              latitude: 0.5,
              longitude: 2.5
            }
        }
    }
}) {
  name
}
```

A `contains` example using `polygon`:

```graphql
 queryHotel(filter: {
    area: {
        contains: {
            polygon: {
                coordinates: [{
                  points:[{
                    latitude: 37.771935,
                    longitude: -122.469829
                  }]
                }],
            }
        }
    }
}) {
  name
}
```

#### intersects

The `intersects` filter matches all entities where the `Polygon` or
`MultiPolygon` field intersects another given `polygon` or `multiPolygon`.

<Tip>
  Only one `polygon` or `multiPolygon` can be given inside the
  `IntersectsFilter` at a time.
</Tip>

```graphql
  queryHotel(filter: {
    area: {
      intersects: {
        multiPolygon: {
          polygons: [{
            coordinates: [{
              points: [{
                latitude: 11.11,
                longitude: 22.22
              }, {
                latitude: 15.15,
                longitude: 16.16
              }, {
                latitude: 20.20,
                longitude: 21.21
              }, {
                latitude: 11.11,
                longitude: 22.22
              }]
            }, {
              points: [{
                latitude: 11.18,
                longitude: 22.28
              }, {
                latitude: 15.18,
                longitude: 16.18
              }, {
                latitude: 20.28,
                longitude: 21.28
              }, {
                latitude: 11.18,
                longitude: 22.28
              }]
            }]
          }, {
            coordinates: [{
              points: [{
                latitude: 91.11,
                longitude: 92.22
              }, {
                latitude: 15.15,
                longitude: 16.16
              }, {
                latitude: 20.20,
                longitude: 21.21
              }, {
                latitude: 91.11,
                longitude: 92.22
              }]
            }, {
              points: [{
                latitude: 11.18,
                longitude: 22.28
              }, {
                latitude: 15.18,
                longitude: 16.18
              }, {
                latitude: 20.28,
                longitude: 21.28
              }, {
                latitude: 11.18,
                longitude: 22.28
              }]
            }]
          }]
        }
      }
    }
  }) {
    name
  }
```

### Union

Unions can be queried only as a field of a type. Union queries can't be ordered,
but you can filter and paginate them.

<Note>
  Union queries do not support the `order` argument. The results will be ordered
  by the `uid` of each node in ascending order.
</Note>

For example, the following schema will enable to query the `members` union field
in the `Home` type with filters and pagination.

```graphql
union HomeMember = Dog | Parrot | Human

type Home {
  id: ID!
  address: String

  members(filter: HomeMemberFilter, first: Int, offset: Int): [HomeMember]
}

# Not specifying a field in the filter input will be considered as a null value for that field.
input HomeMemberFilter {
  # `homeMemberTypes` is used to specify which types to report back.
  homeMemberTypes: [HomeMemberType]

  # specifying a null value for this field means query all dogs
  dogFilter: DogFilter

  # specifying a null value for this field means query all parrots
  parrotFilter: ParrotFilter
  # note that there is no HumanFilter because the Human type wasn't filterable
}

enum HomeMemberType {
  dog
  parrot
  human
}

input DogFilter {
  id: [ID!]
  category: Category_hash
  breed: StringTermFilter
  and: DogFilter
  or: DogFilter
  not: DogFilter
}

input ParrotFilter {
  id: [ID!]
  category: Category_hash
  and: ParrotFilter
  or: ParrotFilter
  not: ParrotFilter
}
```

<Tip>
  Not specifying any filter at all or specifying any of the `null` values for a
  filter will query all members.
</Tip>

The same example, but this time with filter and pagination arguments:

```graphql
query {
  queryHome {
    address
    members(
      filter: {
        homeMemberTypes: [dog, parrot] # means we don't want to query humans
        dogFilter: {
          # means in Dogs, we only want to query "German Shepherd" breed
          breed: { allofterms: "German Shepherd" }
        }
        # not specifying any filter for parrots means we want to query all parrots
      }
      first: 5
      offset: 10
    ) {
      ... on Animal {
        category
      }
      ... on Dog {
        breed
      }
      ... on Parrot {
        repeatsWords
      }
      ... on HomeMember {
        name
      }
    }
  }
}
```

### Vector embedding

The `@search` directive is used in conjunction with `@embedding` directive to
define the HNSW index on vector embeddings. These vector embeddings are obtained
from external Machine Learning models.

```graphql
type User {
  userID: ID!
  name: String!
  name_v: [Float!]
    @embedding
    @search(by: ["hnsw(metric: euclidean, exponent: 4)"])
}
```

In this schema, the field `name_v` is an embedding on which the HNSW algorithm
is used to create a vector search index.

The metric used to compute the distance between vectors (in this example) is
Euclidean distance. Other possible metrics are `cosine` and `dotproduct`.

The directive, `@embedding`, designates one or more fields as vector embeddings.

The `exponent` value is used to set reasonable defaults for HNSW internal tuning
parameters. It is an integer representing an approximate number for the vectors
expected in the index, in terms of power of 10. Default is “4” (10^4 vectors).


# @withSubscription
Source: https://docs.hypermode.com/dgraph/graphql/schema/directives/withsubscription



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The `@withSubscription` directive enables **subscription** operation on a
GraphQL type.

A subscription notifies your client with changes to back-end data using the
WebSocket protocol. Subscriptions are useful to get low-latency, real-time
updates.

To enable subscriptions on any type add the `@withSubscription` directive to the
schema as part of the type definition, as in the following example:

```graphql
type Todo @withSubscription {
  id: ID!
  title: String!
  description: String!
  completed: Boolean!
}
```

Refer to [GraphQL Subscriptions](/dgraph/graphql/subscriptions) to learn how to
use subscriptions in you client app.


# Documentation and Comments
Source: https://docs.hypermode.com/dgraph/graphql/schema/documentation

Dgraph accepts GraphQL documentation comments, which get passed through to the generated API and shown as documentation in GraphQL tools.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## Schema Documentation Processed by Generated API

Dgraph accepts GraphQL documentation comments (e.g.
`""" This is a graphql comment """`), which get passed through to the generated
API and thus shown as documentation in GraphQL tools like GraphiQL, GraphQL
Playground, Insomnia etc.

## Schema Documentation Ignored by Generated API

You can also add `# ...` comments where ever you like. These comments are not
passed via the generated API and are not visible in the API docs.

## Reserved Namespace in Dgraph

Any comment starting with `# Dgraph.` is **reserved** and **should not be used**
to document your input schema.

## An Example

An example that adds comments to a type as well as fields within the type would
be as below.

```graphql
"""
Author of questions and answers in a website
"""
type Author {
  # ... username is the author name , this is an example of a dropped comment
  username: String! @id
  """
  The questions submitted by this author
  """
  questions: [Question] @hasInverse(field: author)
  """
  The answers submitted by this author
  """
  answers: [Answer] @hasInverse(field: author)
}
```

It is also possible to add comments for queries or mutations that have been
added via the custom directive.

```graphql
type Query {
  """
  This query involves a custom directive, and gets top authors.
  """
  getTopAuthors(id: ID!): [Author]
    @custom(
      http: {
        url: "http://api.github.com/topAuthors"
        method: "POST"
        introspectionHeaders: ["Github-Api-Token"]
        secretHeaders: ["Authorization:Github-Api-Token"]
      }
    )
}
```

The screenshots below shows how the documentation appear in a GraphQL API
explorer.

![Schema Documentation On Types](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/graphql/authors1.png)

Schema Documentation on Types

![Schema Documentation On Custom Directive](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/graphql/CustomDirectiveDocumentation.png)


# Graph Federation
Source: https://docs.hypermode.com/dgraph/graphql/schema/federation

Dgraph supports Apollo federation so that you can create a gateway GraphQL service that includes the Dgraph GraphQL API and other GraphQL services

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph supports
[Apollo federation](https://www.apollographql.com/docs/federation/) starting in
release version 21.03. This lets you create a gateway GraphQL service that
includes the Dgraph GraphQL API and other GraphQL services.

## Support for Apollo federation directives

The current implementation supports the following five directives: `@key`,
`@extends`, `@external`, `@provides`, and `@requires`.

### `@key` directive

This directive takes one field argument inside it: the `@key` field. There are
few limitations on how to use `@key` directives:

* Users can define the `@key` directive only once for a type
* Support for multiple key fields isn't currently available.
* Since the `@key` field acts as a foreign key to resolve entities from the
  service where it's extended, the field provided as an argument inside the
  `@key` directive should be of `ID` type or have the `@id` directive on it.

For example -

```graphql
type User @key(fields: "id") {
  id: ID!
  name: String
}
```

### `@extends` directive

This directive provides support for extended definitions. For example, if the
`User` type is defined in some other service, you can extend it in Dgraph's
GraphQL service by using the `@extends` directive, as follows:

```graphql
type User @key(fields: "id") @extends {
  id: String! @id @external
  products: [Product]
}
```

You can also achieve this with the `extend` keyword. Either syntax to extend a
type into your Dgraph GraphQL service works: `extend type User ...` or
`type User @extends ...`.

### `@external` directive

You use this directive when the given field isn't stored in this service. It can
only be used on extended type definitions. For instance, it's used in this
example on the `id` field of the `User` type.

### `@provides` directive

You use this directive on a field that tells the gateway to return a specific
set of fields from the base type while fetching the field.

For example -

```graphql
type Review @key(fields: "id") {
  product: Product @provides(fields: "name price")
}

extend type Product @key(fields: "upc") {
  upc: String @external
  name: String @external
  price: Int @external
}
```

While fetching `Review.product` from the `review` service, and if the `name` or
`price` is also queried, the gateway fetches these from the `review` service
itself. So, the `review` service also resolves these fields, even though both
fields are `@external`.

### `@requires` directive

You use this directive on a field to annotate the fields of the base type. You
can use it to develop a query plan where the required fields may not be needed
by the client, but the service may need additional information from other
services.

For example -

```graphql
extend type User @key(fields: "id") {
  id: ID! @external
  email: String @external
  reviews: [Review] @requires(fields: "email")
}
```

When the gateway fetches `user.reviews` from the `review` service, the gateway
gets `user.email` from the `User` service and provides it as an argument to the
`_entities` query.

Using `@requires` alone on a field doesn't make much sense. In cases where you
need to use `@requires`, you should also add some custom logic on that field.
You can add such logic using the `@lambda` or `@custom(http: {...})` directives.

Here's an example -

1. Schema:

   ```graphql
   extend type User @key(fields: "id") {
     id: ID! @external
     email: String @external
     reviews: [Review] @requires(fields: "email") @lambda
   }
   ```

2. Lambda script:

   ```js
   // returns a list of reviews for a user
   async function userReviews({ parent, graphql }) {
     let reviews = []
     // find the reviews for a user using the email and return them.
     // Even though the email has been declared `@external`, it will be available as `parent.email` as it's mentioned in `@requires`.
     return reviews
   }
   self.addGraphQLResolvers({
     "User.reviews": userReviews,
   })
   ```

## Generated queries and mutations

In this section, you'll see what queries and mutations are available to
individual service and to the Apollo gateway.

Let's take the below schema as an example:

```graphql
type Mission @key(fields: "id") {
  id: ID!
  crew: [Astronaut]
  designation: String!
  startDate: String
  endDate: String
}

type Astronaut @key(fields: "id") @extends {
  id: ID! @external
  missions: [Mission]
}
```

The queries and mutations which are exposed to the gateway are:

```graphql
type Query {
  getMission(id: ID!): Mission
  queryMission(
    filter: MissionFilter
    order: MissionOrder
    first: Int
    offset: Int
  ): [Mission]
  aggregateMission(filter: MissionFilter): MissionAggregateResult
}

type Mutation {
  addMission(input: [AddMissionInput!]!): AddMissionPayload
  updateMission(input: UpdateMissionInput!): UpdateMissionPayload
  deleteMission(filter: MissionFilter!): DeleteMissionPayload
  addAstronaut(input: [AddAstronautInput!]!): AddAstronautPayload
  updateAstronaut(input: UpdateAstronautInput!): UpdateAstronautPayload
  deleteAstronaut(filter: AstronautFilter!): DeleteAstronautPayload
}
```

The queries for `Astronaut` aren't exposed to the gateway because they resolve
through the `_entities` resolver. However, these queries are available on the
Dgraph GraphQL API endpoint.

## Mutation for `extended` types

If you want to add an object of `Astronaut` type which is extended in this
service. The mutation `addAstronaut` takes `AddAstronautInput`, which is
generated as follows:

```graphql
input AddAstronautInput {
  id: ID!
  missions: [MissionRef]
}
```

The `id` field is of `ID` type, which is usually generated internally by Dgraph.
But, In this case, it's provided as an input. The user should provide the same
`id` value present in the GraphQL service where the type `Astronaut` is defined.

For example, let's assume that the type `Astronaut` is defined in some other
service, `AstronautService`, as follows:

```graphql
type Astronaut @key(fields: "id") {
  id: ID!
  name: String!
}
```

When adding an object of type `Astronaut`, you should first add it to the
`AstronautService` service. Then, you can call the `addAstronaut` mutation with
the value of `id` provided as an argument that must be equal to the value in
`AstronautService` service.


# GraphQL and DQL schemas
Source: https://docs.hypermode.com/dgraph/graphql/schema/graphql-dql



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The first step in mastering DQL in the context of GraphQL API is probably to
understand the fundamental difference between GraphQL schema and DQL schema.

### In GraphQL, the schema is a central notion

GraphQL is a strongly typed language. Contrary to REST which is organized in
terms of endpoints, GraphQL APIs are organized in terms of types and fields. The
type system is used to define the schema, which is a contract between client and
server. GraphQL uses types to ensure Apps only ask for what’s possible and
provide clear and helpful errors.

In the [GraphQL Quick start](/dgraph/graphql/quickstart), we used a schema to
generate a GraphQL API:

```graphql
type Product {
  productID: ID!
  name: String @search(by: [term])
  reviews: [Review] @hasInverse(field: about)
}

type Customer {
  username: String! @id @search(by: [hash, regexp])
  reviews: [Review] @hasInverse(field: by)
}

type Review {
  id: ID!
  about: Product!
  by: Customer!
  comment: String @search(by: [fulltext])
  rating: Int @search
}
```

The API and the engine logic are generated from the schema defining the types of
objects we're dealing with, the fields, and the relationships in the form of
fields referencing other types.

### In DQL, the schema described the predicates

Dgraph maintains a list of all predicates names with their type and indexes in
the [Dgraph types schema](./types).

### Schema mapping

When deploying a GraphQL Schema, Dgraph generates DQL predicates and types for
the graph backend. In order to distinguish a field `name` from a type `Person`
from the field `name` of different type (they may have different indexes),
Dgraph is using a dotted notation for the DQL schema.

For example, deploying the following GraphQL Schema

```graphql
type Person {
  id: ID
  name: String!
  friends: [Person]
}
```

leads to the declaration of 3 predicates in the DQL Schema:

* `Person.id default`
* `Person.name string`
* `Person.friends [uid]`

and one DQL type

```dql
type Person {
   Person.name
   Person.friends
}
```

Once again, the DQL type is just a declaration of the list of predicates that
one can expect to be present in a node of having `dgraph.type` equal `Person`.

The default mapping can be customized by using the
[@dgraph directive](/dgraph/graphql/schema/directives/dgraph).

#### GraphQL ID type and Dgraph `uid`

Person.id isn't part of the Person DQL type: internally Dgraph is using `uid`
predicate as unique identifier for every node in the graph. Dgraph returns the
value of `uid` when a GraphQL field of type ID is requested.

#### Search directive and predicate indexes

`@search` directive tells Dgraph what search to build into your GraphQL API.

```graphql
type Person {
    name: String @search(by: [hash])
    ...
```

Is simply translated into a predicate index specification in the Dgraph schema:

```dql
Person.name: string @index(hash) .
```

#### Constraints

DQL doesn't have 'non nullable' constraint `!` nor 'unique' constraint.
Constraints on the graph are handled by correctly using `upsert` operation in
DQL.

#### DQL queries

You can use DQL to query the data generated by the GraphQL API operations. For
example the GraphQL Query

```graphql
query {
  queryPerson {
    id
    name
    friends {
      id
      name
    }
  }
}
```

can be executed in DQL

```graphql
{
  queryPerson(func: type(Person)) {
    id: uid
    name: Person.name
    friends: Person.friends {
      id: uid
      name: Person.name
    }
  }
}
```

Note that in this query, we use `aliases` such as `name: Person.name` to name
the predicates in the JSON response,as they're declared in the GraphQL schema.

#### GraphQL Interface

DQL doesn't have the concept of interfaces.

Considering the following GraphQL schema :

```graphql
interface Location {
  id: ID!
  geoloc: Point
}

type Property implements Location {
  price: Float
}
```

The predicates and types generated for a `Property` are:

```graphql
Location.geoloc: geo .
Location.name: string .
Property.price: float .
type Property {
  Location.name
  Location.geoloc
  Property.price
}
```

### Consequences

The fact that the GraphQL API backend is a graph in Dgraph, implies that you can
use Dgraph DQL on the data that's also served by the GraphQL API operations.

In particular, you can

* use Dgraph DQL mutations but also Dgraph's
  [import tools](/dgraph/admin/import) to populate the graph after you have
  deployed a GraphQL Schema
* use DQL to query the graph in the context of authorization rules and custom
  resolvers.
* add knowledge to your graph such as metadata, score, and annotations, but also
  relationships or relationships attributes (facets) that could be the result of
  similarity computation, threat detection a.s.o. The added data could be hidden
  from your GraphQL API clients but be available to logic written with DQL
  clients.
* break things using DQL: DQL is powerful and is bypassing constraints expressed
  in the GraphQL schema. You can for example delete a node predicate that's
  mandatory in the GraphQL API! Hopefully there are ways to secure who can
  read/write/delete predicates. ( see the
  [ACL](/dgraph/enterprise/access-control-lists)) section.
* fix things using DQL: this is especially useful when doing GraphQL Schema
  updates which require some [data migrations](./migration).


# Schema Migration
Source: https://docs.hypermode.com/dgraph/graphql/schema/migration

This document describes all the things that you need to take care while doing a schema update or migration.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

In every app's development lifecycle, there's a point where the underlying
schema doesn't fit the requirements and must be changed for good. That requires
a migration for both schema and the underlying data. This article guides you
through common migration scenarios you can encounter with Dgraph and help you
avoid any pitfalls around them.

These are the most common scenarios that can occur:

* Renaming a type
* Renaming a field
* Changing a field's type
* Adding `@id` to an existing field

<Note>
  As long as you can avoid migration, avoid it. Because there can be scenarios
  where you might need to update downstream clients, which can be hard. So, its
  always best to try out things first, once you are confident enough, then only
  push them to production.
</Note>

### Renaming a type

Let's say you had the following schema:

```graphql
type User {
  id: ID!
  name: String
}
```

and you had your app working fine with it. Now, you feel that the name `AppUser`
would be more sensible than the name `User` because `User` seems a bit generic
to you. Then you are in a situation where you need migration.

This can be handled in a couple of ways:

1. Migrate all the data for type `User` to use the new name `AppUser`. OR,
2. Just use the [`@dgraph(type: ...)`](./directives/dgraph) directive to
   maintain backward compatibility with the existing data.

Depending on your use-case, you might find option 1 or 2 better for you. For
example, if you have accumulated very little data for the `User` type till now,
then you might want to go with option #1. But, if you have an active app with a
very large dataset then updating the node of each user may not be a thing you
might want to commit to, as that can require some maintenance downtime. So,
option #2 could be a better choice in such conditions.

Option #2 makes your new schema compatible with your existing data. Here's an
example:

```graphql
type AppUser @dgraph(type: "User") {
  id: ID!
  name: String
}
```

So, no downtime required. Migration is done by just updating your schema. Fast,
easy, and simple.

Note that, irrespective of what option you choose for migration on Dgraph side,
you still need to migrate your GraphQL clients to use the new name in
queries/mutations. For example, the query `getUser` would now be renamed to
`getAppUser`. So, your downstream clients need to update that bit in the code.

### Renaming a field

Just like renaming a type, let's say you had the following working schema:

```graphql
type User {
  id: ID!
  name: String
  phone: String
}
```

and now you figured that it would be better to call `phone` as `tel`. You need
migration.

You have the same two choices as before:

1. Migrate all the data for the field `phone` to use the new name `tel`. OR,
2. Just use the [`@dgraph(pred: ...)`](./directives/dgraph) directive to
   maintain backward compatibility with the existing data.

Here's an example if you want to go with option #2:

```graphql
type User {
  id: ID!
  name: String
  tel: String @dgraph(pred: "User.phone")
}
```

Again, note that, irrespective of what option you choose for migration on Dgraph
side, you still need to migrate your GraphQL clients to use the new name in
queries/mutations. For example, the following query:

```graphql
query {
  getUser(id: "0x05") {
    name
    phone
  }
}
```

would now have to be changed to:

```graphql
query {
  getUser(id: "0x05") {
    name
    tel
  }
}
```

So, your downstream clients need to update that bit in the code.

### Changing a field's type

There can be multiple scenarios in this category:

* List -> Single item
* `String` -> `Int`
* Any other combination you can imagine

It is strictly advisable that you figure out a solid schema before going in
production, so that you don't have to deal with such cases later. Nevertheless,
if you ended up in such a situation, you have to migrate your data to fit the
new schema. There is no easy way around here.

An example scenario is, if you initially had this schema:

```graphql
type Todo {
  id: ID!
  task: String
  owner: Owner
}

type Owner {
  name: String! @id
  todo: [Todo] @hasInverse(field: "owner")
}
```

and later you decided that you want an owner to have only one to do at a time.
So, you want to make your schema look like this:

```graphql
type Todo {
  id: ID!
  task: String
  owner: Owner
}

type Owner {
  name: String! @id
  todo: Todo @hasInverse(field: "owner")
}
```

If you try updating your schema, you may end up getting an error like this:

```txt
resolving updateGQLSchema failed because succeeded in saving GraphQL schema but failed to
alter Dgraph schema - GraphQL layer may exhibit unexpected behavior, reapplying the old
GraphQL schema may prevent any issues: Schema change not allowed from [uid] => uid without
deleting pred: owner.todo
```

This is a red flag. As the error message says, you should revert to the old
schema to make your clients work correctly. In such cases, you should have
migrated your data to fit the new schema *before* applying the new schema. The
steps for such a data migration varies from case to case, and so can't all be
listed down here, but you need to migrate your data first, is all you need to
keep in mind while making such changes.

### Adding `@id` to an existing field

Let's say you had the following schema:

```graphql
type User {
  id: ID!
  username: String
}
```

and now you think that `username` must be unique for every user. So, you change
the schema to this:

```graphql
type User {
  id: ID!
  username: String! @id
}
```

Now, here's the catch: with the old schema, it was possible that there could
have existed multiple users with the username `Alice`. If that was true, then
the queries would break in such cases. Like, if you run this query after the
schema change:

```graphql
query {
  getUser(username: "Alice") {
    id
  }
}
```

Then it might error out saying:

```txt
A list was returned, but GraphQL was expecting just one item. This indicates an internal error - probably a mismatch between the GraphQL and Dgraph/remote schemas. The value was resolved as null (which may trigger GraphQL error propagation) and as much other data as possible returned.
```

So, while making such a schema change, you need to make sure that the underlying
data really honors the uniqueness constraint on the username field. If not, you
need to do a data migration to honor such constraints.

### Unused fields

For example, let's assume that you have deployed the following schema:

```graphql
type TestDataMigration {
  id: ID!
  someInfo: String!
  someOtherInfo: String
}
```

Then you create a `TestDataMigration` with `someOtherInfo` value.

Then you update the Schema and remove the field.

```graphql
type TestDataMigration {
  id: ID!
  someInfo: String!
}
```

The data you have previously created is still in the graph database !

Moreover if you delete the `TestDataMigration` object using its `id`, the
GraphQL API delete operation is successful.

If you followed the [GraphQL - DQL Schema mapping](./graphql-dql), you
understand that Dgraph has used the list the known list of predicates (`id`,
`someInfo`) and removed them. In fact, Dgraph also removed the
`dgraph.type`predicate and so this`TestDataMigration` node isn't visible anymore
to the GraphQL API.

The point is that a node with this `uid` exists and has a predicate
`someOtherInfo`. This is because this data has been created initially and
nothing in the process of deploying a new version and then using a delete
operation by ID instructed Dgraph to delete this predicate.

You end up with a node without type (i.e without a `dgraph.type` predicate) and
with an old predicate value which is 'invisible' to your GraphQL API!

When doing a GraphQL schema deployment, you must take care of the data cleaning
and data migration. The good news is that DQL offers you the tools to identify
(search) potential issues and to correct the data (mutations).

In the previous case, you can alter the database and completely delete the
predicate or you can write an 'upsert' DQL query that searches the nodes of
interest and delete the unused predicate for those nodes.

### New non-nullable field

Another obvious example appears if you deploy a new version containing a new
non-nullable field for an existing type. The existing 'nodes' of the same type
in the graph don't have this predicate. A GraphQL query reaching those nodes
returns a list of errors. You can easily write an 'upsert' DQL mutation to find
all node of this type not having the new predicate and update them with a
default value.


# Schema
Source: https://docs.hypermode.com/dgraph/graphql/schema/overview



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

This section describes all the things you can include in your input GraphQL
schema, and what gets generated from that.

The process for serving GraphQL with Dgraph is to add a set of GraphQL type
definitions using the `/admin` endpoint. Dgraph takes those definitions,
generates queries and mutations, and serves the generated GraphQL schema.

The input schema may contain interfaces, types, and enums that follow the usual
GraphQL syntax and validation rules.

If you want to make your schema editing experience nicer, you should use an
editor that does syntax highlighting for GraphQL. With that, you may also want
to include the definitions [here](./dgraph-schema) as an import.


# Relationships
Source: https://docs.hypermode.com/dgraph/graphql/schema/relationships

All the data in your app form a GraphQL data graph. That graph has nodes of particular types and relationships between the nodes to form the data graph.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

All the data in your app form a GraphQL data graph. That graph has nodes of
particular types and relationships between the nodes to form the data graph.

Dgraph uses the types and fields in the schema to work out how to link that
graph, what to accept for mutations and what shape responses should take.

Relationships in that graph are directed: either pointing in one direction or
two. You use the `@hasInverse` directive to tell Dgraph how to handle two-way
relationship.

### One-way relationship

If you only ever need to traverse the graph between nodes in a particular
direction, then your schema can simply contain the types and the relationship.

In this schema, posts have an author - each post in the graph is linked to its
author - but that relationship is one-way.

```graphql
type Author {
    ...
}

type Post {
    ...
    author: Author
}
```

You'll be able to traverse the graph from a Post to its author, but not able to
traverse from an author to all their posts. Sometimes that's the right choice,
but mostly, you'll want two way relationships.

<Note>
  Dgraph won't store the reverse direction, so if you change your schema to
  include a `@hasInverse`, you'll need to migrate the data to add the reverse
  edges.
</Note>

### Two-way relationship

In Dgraph, the directive `@hasInverse` is used to create a two-way relationship.

```graphql
type Author {
    ...
    posts: [Post] @hasInverse(field: author)
}

type Post {
    ...
    author: Author
}
```

With that, `posts` and `author` are just two directions of the same link in the
graph. For example, adding a new post with

```graphql
mutation {
    addPost(input: [
        { ..., author: { username: "Alice" }}
    ]) {
        ...
    }
}
```

automatically adds it to Alice's list of `posts`. Deleting the post removes it
from Alice's `posts`. Similarly, using an update mutation on an author to insert
a new post automatically adds Alice as the author.

```graphql
mutation {
    updateAuthor(input: {
        filter: { username: { eq: "Alice" }},
        set: { posts: [ {... new post ...}]}
    }) {
        ...
    }
}
```

### Many edges

It isn't possible to determine what a schema designer meant for two-way edges.
There's not even a single possible relationship between two types. Consider, for
example, if an app recorded the posts an `Author` had recently liked (so it can
suggest interesting material) and just a tally of all likes on a post.

```graphql
type Author {
    ...
    posts: [Post]
    recentlyLiked: [Post]
}

type Post {
    ...
    author: Author
    numLikes: Int
}
```

It isn't possible to detect what's meant here as a one-way edge, or which edges
are linked as a two-way connection. That's why `@hasInverse` is needed - so you
can enforce the semantics your app needs.

```graphql
type Author {
    ...
    posts: [Post] @hasInverse(field: author)
    recentlyLiked: [Post]
}

type Post {
    ...
    author: Author
    numLikes: Int
}
```

Now, Dgraph manages the connection between posts and authors and you can get on
with concentrating on your app.


# Reserved Names
Source: https://docs.hypermode.com/dgraph/graphql/schema/reserved

This document provides the full list of names that are reserved and can’t be used to define any other identifiers.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The following names are reserved and can't be used to define any other
identifiers:

* `Int`
* `Float`
* `Boolean`
* `String`
* `DateTime`
* `ID`
* `uid`
* `Subscription`
* `as` (case-insensitive)
* `Query`
* `Mutation`
* `Point`
* `PointList`
* `Polygon`
* `MultiPolygon`
* `Aggregate` (as a suffix of any identifier name)

For each type, Dgraph generates a number of GraphQL types needed to operate the
GraphQL API, these generated type names also can't be present in the input
schema. For example, for a type `Author`, Dgraph generates:

* `AuthorFilter`
* `AuthorOrderable`
* `AuthorOrder`
* `AuthorRef`
* `AddAuthorInput`
* `UpdateAuthorInput`
* `AuthorPatch`
* `AddAuthorPayload`
* `DeleteAuthorPayload`
* `UpdateAuthorPayload`
* `AuthorAggregateResult`

**Mutations**

* `addAuthor`
* `updateAuthor`
* `deleteAuthor`

**Queries**

* `getAuthor`
* `queryAuthor`
* `aggregateAuthor`

Thus if `Author` is present in the input schema, all of those become reserved
type names.


# Types
Source: https://docs.hypermode.com/dgraph/graphql/schema/types

How to use GraphQL types to set a GraphQL schema for the Dgraph database. Includes scalars, enums, types, interfaces, union, password, & geolocation types.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

This page describes how to use GraphQL types to set the a GraphQL schema for
Dgraph database.

### Scalars

Dgraph's GraphQL implementation comes with the standard GraphQL scalar types:
`Int`, `Float`, `String`, `Boolean` and `ID`. There's also an `Int64` scalar,
and a `DateTime` scalar types represented as a string in RFC3339 format.

Scalar types, including `Int`, `Int64`, `Float`, `String` and `DateTime`, can be
used in lists. Lists behave like an unordered set in Dgraph. For example:
`["e1", "e1", "e2"]` may get stored as `["e2", "e1"]`, so duplicate values are
not stored and order might not be preserved. All scalars may be nullable or
non-nullable.

<Note>
  The `Int64` type introduced in release v20.11 represents a signed integer
  ranging between `-(2^63)` and `(2^63 -1)`. Signed `Int64` values in this range
  are parsed correctly by Dgraph as long as the client can serialize the number
  correctly in JSON. For example, a JavaScript client might need to use a
  serialization library such as
  [`json-bigint`](https://www.npmjs.com/package/json-bigint) to correctly write
  an `Int64` value in JSON.
</Note>

The `ID` type is special. IDs are auto-generated, immutable, and can be treated
as strings. Fields of type `ID` can be listed as nullable in a schema, but
Dgraph never returns null.

* Schema rule: `ID` lists aren't allowed. For example, `tags: [String]` is
  valid, but `ids: [ID]` isn't valid.
* Schema rule: Each type you define can have at most one field with type `ID`.
  That includes IDs implemented through interfaces.

It isn't possible to define further scalars - you'll receive an error if the
input schema contains the definition of a new scalar.

For example, the following GraphQL type uses all of the available scalars.

```graphql
type User {
  userID: ID!
  name: String!
  lastSignIn: DateTime
  recentScores: [Float]
  reputation: Int
  active: Boolean
}
```

Scalar lists in Dgraph act more like sets, so `tags: [String]` would always
contain unique tags. Similarly, `recentScores: [Float]` could never contain
duplicate scores.

### Vectors

A Float array can be used as a vector using `@embedding` directive. It denotes a
vector of floating point numbers, i.e an ordered array of float32. A type can
contain more than one vector predicate.

Vectors are normally used to store embeddings obtained from a language model.

When a Float vector is indexed, the GraphQL `querySimilar<type name>ByEmbedding`
and `querySimilar<type name>ById` functions can be used for
[similarity search](/dgraph/graphql/query/vector-similarity).

A simple example of adding a vector embedding on `name` to `User` type is shown
below.

```graphql
type User {
  userID: ID!
  name: String!
  name_v: [Float!]
    @embedding
    @search(by: ["hnsw(metric: euclidean, exponent: 4)"])
}
```

In this schema, the field `name_v` is an embedding on which the
[@search](/dgraph/graphql/schema/directives/search#vector-embedding) directive
for vector embeddings is used.

### The `ID` type

In Dgraph, every node has a unique 64-bit identifier that you can expose in
GraphQL using the `ID` type. An `ID` is auto-generated, immutable and never
reused. Each type can have at most one `ID` field.

The `ID` type works great when you need to use an identifier on nodes and don't
need to set that identifier externally (for example, posts and comments).

For example, you might set the following type in a schema:

```graphql
type Post {
    id: ID!
    ...
}
```

In a single-page app, you could generate the page for `http://.../posts/0x123`
when a user clicks to view the post with `ID` 0x123. Your app can then use a
`getPost(id: "0x123") { ... }` GraphQL query to fetch the data used to generate
the page.

For input and output, `ID`s are treated as strings.

You can also update and delete posts by `ID`.

### Enums

You can define enums in your input schema. For example:

```graphql
enum Tag {
    GraphQL
    Database
    Question
    ...
}

type Post {
    ...
    tags: [Tag!]!
}
```

### Types

From the built-in scalars and the enums you add, you can generate types in the
usual way for GraphQL. For example:

```graphql
enum Tag {
  GraphQL
  Database
  Dgraph
}

type Post {
  id: ID!
  title: String!
  text: String
  datePublished: DateTime
  tags: [Tag!]!
  author: Author!
}

type Author {
  id: ID!
  name: String!
  posts: [Post!]
  friends: [Author]
}
```

* Schema rule: Lists of lists aren't accepted. For example:
  `multiTags: [[Tag!]]` isn't valid.
* Schema rule: Fields with arguments aren't accepted in the input schema unless
  the field is implemented using the `@custom` directive.

### Interfaces

GraphQL interfaces allow you to define a generic pattern that multiple types
follow. When a type implements an interface, that means it has all fields of the
interface and some extras.

According to GraphQL specifications, you can have the same fields in
implementing types as the interface. In such cases, the GraphQL layer generates
the correct Dgraph schema without duplicate fields.

If you repeat a field name in a type, it must be of the same type (including
list or scalar types), and it must have the same nullable condition as the
interface's field. Note that if the interface's field has a directive like
`@search` then it is inherited by the implementing type's field.

For example:

```graphql
interface Fruit {
  id: ID!
  price: Int!
}

type Apple implements Fruit {
  id: ID!
  price: Int!
  color: String!
}

type Banana implements Fruit {
  id: ID!
  price: Int!
}
```

<Tip>
  GraphQL generates the correct Dgraph schema where fields occur only once.
</Tip>

The following example defines the schema for posts with comment threads. As
mentioned, Dgraph fills in the `Question` and `Comment` types to make the full
GraphQL types.

```graphql
interface Post {
  id: ID!
  text: String
  datePublished: DateTime
}

type Question implements Post {
  title: String!
}
type Comment implements Post {
  commentsOn: Post!
}
```

The generated schema contains the full types, for example, `Question` and
`Comment` get expanded as:

```graphql
type Question implements Post {
  id: ID!
  text: String
  datePublished: DateTime
  title: String!
}

type Comment implements Post {
  id: ID!
  text: String
  datePublished: DateTime
  commentsOn: Post!
}
```

<Note>
  If you have a type that implements two interfaces, Dgraph won't allow a field
  of the same name in both interfaces, except for the `ID` field.
</Note>

Dgraph currently allows this behavior for `ID` type fields since the `ID` type
field isn't a predicate. Note that in both interfaces and the implementing type,
the nullable condition and type (list or scalar) for the `ID` field should be
the same. For example:

```graphql
interface Shape {
  id: ID!
  shape: String!
}

interface Color {
  id: ID!
  color: String!
}

type Figure implements Shape & Color {
  id: ID!
  shape: String!
  color: String!
  size: Int!
}
```

### Union type

GraphQL Unions represent an object that could be one of a list of GraphQL Object
types, but provides for no guaranteed fields between those types. So no fields
may be queried on this type without the use of type refining fragments or inline
fragments.

Union types have the potential to be invalid if incorrectly defined:

* A `Union` type must include one or more unique member types.
* The member types of a `Union` type must all be Object base types;
  [Scalar](#scalars), [Interface](#interfaces) and `Union` types must not be
  member types of a Union. Similarly, wrapping types must not be member types of
  a Union.

For example, the following defines the `HomeMember` union type:

```graphql
enum Category {
  Fish
  Amphibian
  Reptile
  Bird
  Mammal
  InVertebrate
}

interface Animal {
  id: ID!
  category: Category @search
}

type Dog implements Animal {
  breed: String @search
}

type Parrot implements Animal {
  repeatsWords: [String]
}

type Cheetah implements Animal {
  speed: Float
}

type Human {
  name: String!
  pets: [Animal!]!
}

union HomeMember = Dog | Parrot | Human

type Zoo {
  id: ID!
  animals: [Animal]
  city: String
}

type Home {
  id: ID!
  address: String
  members: [HomeMember]
}
```

So, when you want to query members in a `Home`, you can submit a GraphQL query
like this:

```graphql
query {
  queryHome {
    address
    members {
      ... on Animal {
        category
      }
      ... on Dog {
        breed
      }
      ... on Parrot {
        repeatsWords
      }
      ... on Human {
        name
      }
    }
  }
}
```

And the results of the GraphQL query looks like the following:

```json
{
  "data": {
    "queryHome": {
      "address": "Earth",
      "members": [
        {
          "category": "Mammal",
          "breed": "German Shepherd"
        },
        {
          "category": "Bird",
          "repeatsWords": ["Good Morning!", "I am a GraphQL parrot"]
        },
        {
          "name": "Alice"
        }
      ]
    }
  }
}
```

### Password type

A password for an entity is set with setting the schema for the node type with
`@secret` directive. Passwords can't be queried directly, only checked for a
match using the `checkTypePassword` function where `Type` is the node type. The
passwords are encrypted using [`Bcrypt`](https://en.wikipedia.org/wiki/Bcrypt).

<Note>
  For security reasons, Dgraph enforces a minimum password length of 6
  characters on `@secret` fields.
</Note>

For example, to set a password, first set schema:

1. Cut-and-paste the following schema into a file called `schema.graphql`

   ```graphql
   type Author @secret(field: "pwd") {
     name: String! @id
   }
   ```

2. Run the following curl request:

   ```sh
   curl -X POST localhost:8080/admin/schema --data-binary '@schema.graphql'
   ```

3. Set the password by pointing to the `graphql` endpoint
   ([http://localhost:8080/graphql](http://localhost:8080/graphql)):

   ```graphql
   mutation {
     addAuthor(input: [{ name: "myname", pwd: "mypassword" }]) {
       author {
         name
       }
     }
   }
   ```

The output should look like:

```json
{
  "data": {
    "addAuthor": {
      "author": [
        {
          "name": "myname"
        }
      ]
    }
  }
}
```

You can check a password:

```graphql
query {
  checkAuthorPassword(name: "myname", pwd: "mypassword") {
    name
  }
}
```

output:

```json
{
  "data": {
    "checkAuthorPassword": {
      "name": "myname"
    }
  }
}
```

If the password is wrong you receive the following response:

```json
{
  "data": {
    "checkAuthorPassword": null
  }
}
```

### Geolocation types

Dgraph GraphQL comes with built-in types to store Geolocation data. Currently,
it supports `Point`, `Polygon` and `MultiPolygon`. These types are useful in
scenarios like storing a location's GPS coordinates, representing a city on the
map, etc.

For example:

```graphql
type Hotel {
  id: ID!
  name: String!
  location: Point
  area: Polygon
}
```

#### Point

```graphql
type Point {
  longitude: Float!
  latitude: Float!
}
```

#### PointList

```graphql
type PointList {
  points: [Point!]!
}
```

#### Polygon

```graphql
type Polygon {
  coordinates: [PointList!]!
}
```

#### MultiPolygon

```graphql
type MultiPolygon {
  polygons: [Polygon!]!
}
```


# Authorization tips
Source: https://docs.hypermode.com/dgraph/graphql/security/auth-tips

Given an authentication mechanism and a signed JSON Web Token (JWT), the `@auth` directive tells Dgraph how to apply authorization.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## Public data

Many apps have data that can be accessed by anyone, logged in or not. That also
works nicely with Dgraph auth rules.

For example, in Twitter, StackOverflow, etc. you can see authors and posts
without being signed it - but you'd need to be signed in to add a post. With
Dgraph auth rules, if a type doesn't have, for example, a `query` auth rule or
the auth rule doesn't depend on a JWT value, then the data can be accessed
without a signed JWT.

For example, the to do app might allow anyone, logged in or not, to view any
author, but not make any mutations unless logged in as the author or an admin.
That would be achieved by rules like the following.

```graphql
type User @auth(
    # no query rule
    add: { rule:  "{$ROLE: { eq: \"ADMIN\" } }" },
    update: ...
    delete: ...
) {
    username: String! @id
    todos: [Todo]
}
```

Maybe some to dos can be marked as public and users you aren't logged in can see
those.

```graphql
type Todo @auth(
    query: { or: [
        # you are the author
        { rule: ... },
        # or, the todo is marked as public
        { rule: """query {
            queryTodo(filter: { isPublic: { eq: true } } ) {
                id
            }
        }"""}
    ]}
) {
    ...
    isPublic: Boolean
}

```

Because the rule doesn't depend on a JWT value, it can be successfully evaluated
for users who aren't logged in.

Ensuring that requests are from an authenticated JWT, and no further
restrictions, can be done by arranging the JWT to contain a value like
`"isAuthenticated": "true"`. For example,

```graphql
type User @auth(query: { rule: "{$isAuthenticated: { eq: \"true\" } }" }) {
  username: String! @id
  todos: [Todo]
}
```

specifies that only authenticated users can query other users.

### Blocking an operation for everyone

If the `ROLE` claim isn't present in a JWT, any rule that relies on `ROLE`
simply evaluates to false.

You can also simply disallow some queries and mutations by using a condition on
a non-existing claim:

If you know that your JWTs never contain the claim `DENIED`, then a rule such as

```graphql
type User @auth(
    delete: { rule:  "{$DENIED: { eq: \"DENIED\" } }"}
) {
    ...
}
```

block the delete operation for everyone.


# Restrict origins
Source: https://docs.hypermode.com/dgraph/graphql/security/cors



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

To restrict origins of HTTP requests add lines starting with
`# Dgraph.Allow-Origin` at the end of your GraphQL schema specifying the origins
allowed.

For example, the following restricts all origins except the ones specified.

```sh
# Dgraph.Allow-Origin "https://example.com"
# Dgraph.Allow-Origin "https://www.example.com"
```

<Note>CORS restrictions only apply to browsers.</Note>

<Note>
  By default, the `/graphql` endpoint doesn't limit the request origin
  (`Access-Control-Allow-Origin: *`).
</Note>


# ABAC Rules
Source: https://docs.hypermode.com/dgraph/graphql/security/graphtraversal-rules

Attribute Based Access Control (ABAC) on GraphQL API operations

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph support Attribute Based Access Control (ABAC) on GraphQL API operations:
you can specify which data a user can query, add, update or delete for each type
of your GraphQL schema based on JWT claims, using the `@auth` directive and
graph traversal queries.

To implement graph traversal rule on GraphQL API operations :

1. Ensure your have configured the GraphQL schema to [Handle JWT tokens](./jwt)
   using `# Dgraph.Authorization` This step is important to be able to use the
   [JWT claims](./overview#jwt-claims)
2. Annotate the Types in the GraphQL schema with the `@auth` directive and
   specify conditions to be met for `query`, `add`, `update` or `delete`
   operations.

A graph traversal rule is expressed as GraphQL query on the type on which the
@auth directive applies.

For example, a rule on `Contact` type can only use a `queryContact` query :

```graphql
type Contact @auth(
  query: { rule: "query { queryContact(filter: { isPublic: true }) { id } }" },
  add: ...
  update: ...
  delete: ...
) {
  <type definition>
  ...
}
```

You can use triple quotation marks. In that case the query can be defined on
multiple lines.

The following schema is also valid:

```graphql
type Contact @auth(
  query: { rule: """query {
    queryContact(filter: { isPublic: true }) {
        id
    }
    } """
}) {
  <type definition>
  ...
}
```

The rules are expressed as GraphQL queries, so they can also have a name and
parameters:

```graphql
type Todo
  @auth(
    query: {
      rule: """
      query ($USER: String!) {
          queryTodo(filter: { owner: { eq: $USER } } ) {
              id
          }
      }
      """
    }
  ) {
  id: ID!
  text: String! @search(by: [term])
  owner: String! @search(by: [hash])
}
```

The parameters are replaced at runtime by the corresponding `claims` found in
the JWT token. In this case, the query is executed with the value of the `USER`
claim.

When a user sends a request on `/graphql` endpoint for a `get<Type>` or
`query<Type>` operation, Dgraph executes the query specified in the @auth
directive of the `Type` to build a list of "authorized" UIDs. Dgraph returns
only the data matching both the requested data and the "authorized" list. That
means that the client can apply any filter condition, the result is the
intersection of the data matching the filter and the "authorized" data.

The same logic applies for `update<Type>` and `delete<Type>`: only the data
matching the @auth query are affected.

```graphql
type Todo
  @auth(
    delete: {
      or: [
        {
          rule: """
          query ($USER: String!) {
            queryTodo(filter: { owner: { eq: $USER } } ) {
                __typename
            }
          }
          """
        } # you are the author graph query
        { rule: "{$ROLE: { eq: \"ADMIN\" } }" }
      ]
    }
  )
```

In the context of @auth directive, Dgraph executes the @auth query differently
that a normal query : if the query has nested blocks, all levels must match
existing data. Dgraph internally applies a `@cascade` directive, making the
directive more like a **pattern matching** condition.

For example, in the cases of `To do`, the access depends not on a value in the
to do, but on checking which owner it's linked to. This means our auth rule must
make a step further into the graph to check who the owner is :

```graphql
type User {
  username: String! @id
  todos: [Todo]
}

type Todo
  @auth(
    query: {
      rule: """
      query ($USER: String!) {
          queryTodo {
              owner(filter: { username: { eq: $USER } } ) {
                  __typename
              }
          }
      }
      """
    }
  ) {
  id: ID!
  text: String!
  owner: User
}
```

The @auth query rule only returns to dos having an owner matching the condition:
the owner `username` must be equal to the JWT claim `USER`.

All blocks must return some data for the query to succeed. You may want to use
the field `__typename` in the most inner block to ensure a data match at this
level.

### Rules combination

Rules can be combined with the logical connectives `and`, `or` and `not`. A
permission can be a mixture of graph traversals and role based rules.

### `@auth` on interfaces

The rules provided inside the `@auth` directive on an interface are applied as
an `AND` rule to those on the implementing types.

A type inherits the `@auth` rules of all the implemented interfaces. The final
authorization rule is an `AND` of the type's `@auth` rule and of all the
implemented interfaces.

### Claims

Rules may use claims from the namespace specified by the
[`Dgraph.Authorization`](./jwt) or claims present at the root level of the JWT
payload.

### Error handling

When deploying the schema, Dgraph tests if you are using valid queries in your
@auth directive.

For example, using `queryFilm` for a rule on a type `Actor` results in an error:

```sh
resolving updateGQLSchema failed because Type Actor: @auth: expected only queryActor rules,but found queryFilm
```


# Handle JWT Token
Source: https://docs.hypermode.com/dgraph/graphql/security/jwt



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

When deploying a GraphQL schema, the admin user can set a
`# Dgraph.Authorization` line at the bottom of the schema to specify how JWT
tokens present in the HTTP header requests are extracted, validated and used.

This line must start with the exact string `# Dgraph.Authorization` and be at
the bottom of the schema file.

## Configure JWT token handling

To configure how Dgraph should handle JWT token for `/graphql` endpoint add a
line starting with `# Dgraph.Authorization` and with the following parameters at
the very end of your GraphQL schema.

The `Dgraph.Authorization` object uses the following syntax:

```graphql
# Dgraph.Authorization {"VerificationKey":"<verification-key-here>","Header":"X-My-App-Auth","Namespace":"https://my.app.io/jwt/claims","Algo":"HS256","Audience":["aud1"],"ClosedByDefault":true}
```

`Dgraph.Authorization` object contains the following parameters:

* `Header` name of the header field used by the client to send the token.

  <Note>
    Don't use `Dg-Auth`, `X-Auth-Token` or `Authorization` headers which are
    used by Dgraph for other purposes.
  </Note>

* `Namespace` is the key inside the JWT that contains the claims relevant to
  Dgraph authorization.

* `Algo` is the JWT verification algorithm which can be either `HS256` or
  `RS256`.

* `VerificationKey` is the string value of the key, with newlines replaced with
  `\n` and the key string wrapped in `""`:
  * **For asymmetric encryption**: `VerificationKey` contains the public key
    string.
  * **For symmetric (secret-based) encryption**: `VerificationKey` is the secret
    key.

* `JWKURL`/`JWKURLs` is the URL for the JSON Web Key sets. If you want to pass
  multiple URLs, use `JWKURLs` as an array of multiple JWK URLs for the JSON Web
  Key sets. You can only use one authentication connection method, either JWT
  (`Header`), a single JWK URL, or multiple JWK URLs.

* `Audience` is used to verify the `aud` field of a JWT, which is used by
  certain providers to indicate the intended audience for the JWT. When doing
  authentication with `JWKURL`, this field is mandatory.

* `ClosedByDefault`, if set to `true`, requires authorization for all requests
  even if the GraphQL type doesn't specify rules. If omitted, the default
  setting is `false`.

When the `# Dgraph.Authorization` line is present in the GraphQL schema, Dgraph
uses the settings in that line to

* read the specified header in each HTTP request sent on the /graphql endpoint,
* decode that header as a JWT token using the specified algorithm
* validate the token signature and the audience
* extract the JWT claims present in the specified namespace and at the root
  level

These claims are accessible to any @auth schema directives (a GraphQL schema
directive specific to Dgraph) that are associated with GraphQL types in the
schema file.

See the [RBAC rules](./rbac-rules) and \[Graph traversal
rules]\(./graphtraversal-rules for details on how to restrict data access using
the @auth directive on a per-type basis.

### Require JWT token

To not only accept but to require the JWT token regardless of @auth directives
in your GraphQL schema, set option "ClosedByDefault" to true in the
`# Dgraph.Authorization` line.

## Working with authentication providers

`Dgraph.Authorization` is fully configurable to work with various authentication
providers. Authentication providers have options to configure how to generate
JWT tokens.

Here are some configuration examples.

### Clerk.com

In your clerk dashboard, Access `JWT Templates` and create a template for
Dgraph.

Your template must have an `aud` (audience), this is mandatory for Dgraph when
the token is verified using JWKURL.

Decide on a claim namespace and add the information you want to use in your RBAC
rules.

This example uses the `https://dgraph.io/jwt/claims` namespace and is retrieving
the user current organization, role (Clerk has currently two roles `admin` and
`basic_member`) and email.

This is our JWT Template in Clerk:

```json
{
  "aud": "dgraph",
  "https://dgraph.io/jwt/claims": {
    "org": "{{org.name}}",
    "role": "{{org.role}}",
    "userid": "{{user.primary_email_address}}"
  }
}
```

In the same configuration panel

* set the **token lifetime**
* copy the **JWKS Endpoint**

Configure your Dgraph GraphQL schema with the following authorization

```graphql
# Dgraph.Authorization {"header":"X-Dgraph-AuthToken","namespace":"https://dgraph.io/jwt/claims","jwkurl":"https://<>.clerk.accounts.dev/.well-known/jwks.json","audience":["dgraph"],"closedbydefault":true}
```

Note that

* **namespace** matches the namespace used in the JWT Template
* **audience** is an array and contains the **aud** used in the JWT token
* **jwkurl** is the **JWKS Endpoint** from Clerk

You can select the header to receive the JWT token from your client app,
`X-Dgraph-AuthToken` is a header authorized by default by Dgraph GraphQL API to
pass CORS requirements.

## Other Dgraph.Authorization Examples

To use a single JWK URL:

```graphql
# Dgraph.Authorization {"VerificationKey":"","Header":"X-My-App-Auth", "jwkurl":"https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com", "Namespace":"https://xyz.io/jwt/claims","Algo":"","Audience":["fir-project1-259e7", "HhaXkQVRBn5e0K3DmMp2zbjI8i1wcv2e"]}
```

To use multiple JWK URL:

```graphql
# Dgraph.Authorization {"VerificationKey":"","Header":"X-My-App-Auth","jwkurls":["https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com","https://dev-hr2kugfp.us.auth0.com/.well-known/jwks.json"], "Namespace":"https://xyz.io/jwt/claims","Algo":"","Audience":["fir-project1-259e7", "HhaXkQVRBn5e0K3DmMp2zbjI8i1wcv2e"]}
```

Using HMAC-SHA256 token in `X-My-App-Auth` header and authorization claims in
`https://my.app.io/jwt/claims` namespace:

```graphql
# Dgraph.Authorization {"VerificationKey":"secretkey","Header":"X-My-App-Auth","Namespace":"https://my.app.io/jwt/claims","Algo":"HS256"}
```

Using HMAC-SHA256 token in `X-My-App-Auth` header and authorization claims in
`https://my.app.io/jwt/claims` namespace:

```graphql
# Dgraph.Authorization {"VerificationKey":"-----BEGIN PUBLIC KEY-----\n...\n-----END PUBLIC KEY-----","Header":"X-My-App-Auth","Namespace":"https://my.app.io/jwt/claims","Algo":"RS256"}
```

### JWT format

The value of the JWT `header` is expected to be in one of the following forms:

* Bare token.\
  For example:

  ```txt
  eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyLCJodHRwczovL215LmFwcC5pby9qd3QvY2xhaW1zIjp7fX0.Pjlxpf-3FhH61EtHBRo2g1amQPRi0pNwoLUooGbxIho
  ```

* A Bearer token, e.g., a JWT prepended with `Bearer ` prefix (including
  space).\
  For example:
  ```txt
  Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyLCJodHRwczovL215LmFwcC5pby9qd3QvY2xhaW1zIjp7fX0.Pjlxpf-3FhH61EtHBRo2g1amQPRi0pNwoLUooGbxIho
  ```

### Error handling

If ClosedByDefault is set to true, and the JWT is not present or if the JWT
token does not include the proper audience information, or is not properly
encoded, or is expired, Dgraph replies to requests on `/graphql` endpoint with
an error message rejecting the operation similar to:

```graphql
{
   "errors": [
       {
           "message": "couldn't rewrite query queryContact because a valid JWT is required but was not provided",
           "path": [
               "queryContact"
           ]
       }
   ],
   "data": {
       "queryContact": []
   },...
```

**Error messages**

* "couldn't rewrite query queryContact because a valid JWT is required but was
  not provided"
* "couldn't rewrite query queryMessage because unable to parse jwt token:token
  is expired by 5h49m46.236018623s"
* "couldn't rewrite query queryMessage because JWT `aud` value doesn't match
  with the audience"
* "couldn't rewrite query queryMessage because unable to parse jwt token:token
  signature is invalid"


# Mutations and GraphQL Authorization
Source: https://docs.hypermode.com/dgraph/graphql/security/mutations

Learn how to use GraphQL Authorization with Mutations to protect your data and control access in Dgraph.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Mutations with authorization work like queries. But because mutations involve a
state change in the database, it's important to understand when the
authorization rules are applied and what they mean.

## Add

Rules for `add` authorization state that the rule must hold of nodes created by
the mutation data once committed to the database.

For example, a rule such as the following:

```graphql
type Todo
  @auth(
    add: {
      rule: """
      query ($USER: String!) {
          queryTodo {
              owner(filter: { username: { eq: $USER } } ) {
                  username
              }
          }
      }
      """
    }
  ) {
  id: ID!
  text: String!
  owner: User
}
type User {
  username: String! @id
  todos: [Todo]
}
```

... states that if you add a new to-do list item, then that new to-do must
satisfy the `add` rule, in this case saying that you can only add to-do list
items with yourself as the author.

## Delete

Delete rules filter the nodes that can be deleted. A user can only ever delete a
subset of the nodes that the `delete` rules allow.

For example, the following rule states that a user can delete a to-do list item
if they own it, or they have the `ADMIN` role:

```graphql
type Todo
  @auth(
    delete: {
      or: [
        {
          rule: """
          query ($USER: String!) {
              queryTodo {
                  owner(filter: { username: { eq: $USER } } ) {
                      username
                  }
              }
          }
          """
        }
        { rule: "{$ROLE: { eq: \"ADMIN\" } }" }
      ]
    }
  ) {
  id: ID!
  text: String! @search(by: [term])
  owner: User
}

type User {
  username: String! @id
  todos: [Todo]
}
```

When using these types of rules, a mutation such as the one shown below will
behave differently. depending on which user is running it:

* For most users, the following mutation deletes the posts that contain the term
  "graphql" and are owned by the user who runs the mutation, but doesn't affect
  any other user's to-do list items
* For an admin user, the following mutation deletes any posts that contain the
  term "graphql", regardless of which user owns these posts

```graphql
mutation {
  deleteTodo(filter: { text: { anyofterms: "graphql" } }) {
    numUids
  }
}
```

When adding data, what matters is the resulting state of the database, when
deleting, what matters is the state before the delete occurs.

## Update

Updates have both a before and after state that can be important for
authorization.

For example, consider a rule stating that you can only update your own to-do
list items. If evaluated in the database before the mutation (like the delete
rules) it would prevent you from updating anyone elses to-do list items, but it
does not stop you from updating your own to-do items to have a different
`owner`. If evaluated in the database after the mutation occurs, like for add
rules, it would prevent setting the `owner` to another user, but would not
prevent editing other's posts.

Currently, Dgraph evaluates `update` rules *before* the mutation.

## Update and add mutations

Update mutations can also insert new data. For example, you might allow a
mutation that runs an update mutation to add a new to-do list item:

```graphql
mutation {
    updateUser(input: {
        filter: { username: { eq: "aUser" }},
        set: { todos: [ { text: "do this new todo"} ] }
    }) {
        ...
    }
}
```

Because a mutation updates a user's to-do list by inserting a new to-do list
item, it would have to satisfy the rules to update the author *and* the rules to
add a to-do list item. If either fail, the mutation has no effect.

***


# Security
Source: https://docs.hypermode.com/dgraph/graphql/security/overview

Built-in authorization with various authentication methods, so you can annotate your schema with rules that determine who can access or mutate the data

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

When you deploy a GraphQL schema, Dgraph automatically generates the query and
mutation operations for each type and exposes them as a GraphQL API on the
`/graphql` endpoint.

Dgraph's GraphQL authorization features let you specify :

* if the client requires an API key or not if **anonymous access** is allowed to
  invoke a specific operation of the API.
* if a client must present an identity in the form of a **JWT token** to use the
  API.
* **RBAC rules** (Role Based Access Control) at operation level based on the
  claims included in the client JWT token.
* **ABAC rules** (Attribute Based Access Control) at data level using graph
  traversal queries.

<Note>
  By default all operations are accessible to anonymous clients, no JWT token is
  required and no authorization rules are applied. It is your responsibility to
  correctly configure the authorization for the `/graphql` endpoint.
</Note>

Refer to the following documentation to set your `/graphql` endpoint security :

* [Handle JWT token](./jwt)

* [RBAC rules](./rbac-rules)

* [ABAC rules](./graphtraversal-rules)

### `/graphql` security flow

In summary, the Dgraph security flow on `/graphql` endpoint is as follow:

![graphql endpoint security](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/graphql/RBAC.jpeg)

### CORS

Additionally, you can [restrict the origins](./cors) that `/graphql` endpoint
responds to.

This is a best practice to prevent XSS exploits.

## Authentication

Dgraph's GraphQL authorization relies on the presence of a valid JWT token in
the request.

Dgraph supports both symmetric (HS256) and asymmetric (RS256) encryption and
accepts JSON Web Key (JWK) URL or signed JSON Web Token (JWT).

You can use any authentication method that is capable of generating such JWT
token (Auth0, Cognito, Firebase, etc...) including Dgraph login mechanism.

### ACL

Note that another token may be needed to access the system if ACL security is
also enabled. See the [ACLs](/dgraph/enterprise/access-control-lists) section
for details. The ACLs are a separate security mechanism.

### JWT Claims

In JSON web tokens (JWTs) ([https://www.rfc-editor.org/rfc/rfc7519](https://www.rfc-editor.org/rfc/rfc7519)) , a claim
appears as a name/value pair.

When we talk about a claim in the context of a JWT, we are referring to the name
(or key). For example, the following JSON object contains three claims `sub`,
`name` and `admin`:

```json
{
  "sub": "1234567890",
  "name": "John Doe",
  "admin": true
}
```

So that different organizations can specify different claims without
conflicting, claims typically have a namespace, and it's a good practice to
specify the namespace of your claims. put specific claims in a nested structure
called a namespace.

```json
{
  "https://mycompany.org/jwt/claims": {
    "username": "auth0|63fe77f32cef38f4fa3dab34",
    "role": "Admin"
  },
  "name": "raph@hypermode.com",
  "email": "raph@@hypermode.com",
  "email_verified": false,
  "iss": "https://dev-5q3n8cc7nckhu5w8.us.auth0.com/",
  "aud": "aqk1CSVtliyoXUfLaaLKSKUtkaIel6Vd",
  "iat": 1677705681,
  "exp": 1677741681
}
```

This JSON is a JWT token payload containing a namespace
`https://mycompany.org/jwt/claims` having a `username` claim and a `role` claim.


# RBAC Rules
Source: https://docs.hypermode.com/dgraph/graphql/security/rbac-rules

Dgraph support Role Based Access Control (RBAC) on GraphQL API operations.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph support Role Based Access Control (RBAC) on GraphQL API operations: you
can specify who can invoke query, add, update and delete operations on each type
of your GraphQL schema based on JWT claims, using the `@auth` directive.

To implement Role Based Access Control on GraphQL API operations :

1. Ensure your have configured the GraphQL schema to [Handle JWT tokens](./jwt)
   using `# Dgraph.Authorization` This step is important to be able to use the
   [JWT claims](./overview#jwt-claims)
2. Annotate the Types in the GraphQL schema with the `@auth` directive and
   specify conditions to be met for `query`, `add`, `update` or `delete`
   operations.

The generic format of RBAC rule is as follow

```graphql
type User @auth(
    query: { rule:  "{$<claim>: { eq: \"<value>\" } }" },
    add: { rule:  "{$<claim>: { in: [\"<value1>\",...] } }" },
    update: ...
    delete: ...
)
```

RBAC rule supports `eq` or `in` functions to test the value of a
[JWT claim](./overview#jwt-claims) from the JWT token payload.

The claim value may be a string or array of strings.

For example the following schema has a @auth directive specifying that a delete
operation on a User object can only be done if the connected user has a 'ROLE'
claim in the JWT token with the value "admin" :

```graphql
type User @auth(delete: { rule: "{$ROLE: { eq: \"admin\" } }" }) {
  username: String! @id
  todos: [Todo]
}
```

The following JWT token payload will pass the test (provided that
Dgraph.Authorization is configured correctly with the right namespace)

```json
{
  "aud": "dgraph",
  "exp": 1695359621,
  "https://dgraph.io/jwt/claims": {
    "ROLE": "admin",
    "USERID": "testuser@dgraph.io"
  },
  "iat": 1695359591,
  ...
}
```

The rule is also working with an array of roles in the JWT token:

```json
{
  "aud": "dgraph",
  "exp": 1695359621,
  "https://dgraph.io/jwt/claims": {
    "ROLE": ["admin","user"]
    "USERID": "testuser@dgraph.io"
  },
  "iat": 1695359591,
  ...
}
```

In the case of an array used with the "in" function, the rule is valid is at
least one of the claim value is "in" the provided list.

For example, with the following rule, the previous token will be valid because
one of the ROLE is in the authorized roles.

```graphql
type User
  @auth(delete: { rule: "{$ROLE: { in: [\"admin\",\"superadmin\"] } }" }) {
  username: String! @id
  todos: [Todo]
}
```

## rules combination

Rules can be combined with the logical connectives `and`, `or` and `not`. A
permission can be a mixture of graph traversals and role based rules.

In the todo app, you can express, for example, that you can delete a `Todo` if
you are the author, or are the site admin.

```graphql
type Todo
  @auth(
    delete: {
      or: [
        { rule: "query ($USER: String!) { ... }" } # you are the author graph query
        { rule: "{$ROLE: { eq: \"ADMIN\" } }" }
      ]
    }
  )
```

## claims

Rules may use claims from the namespace specified by the
[# Dgraph.Authorization](./jwt) or claims present at the root level of the JWT
payload.

For example, given the following JWT payload

```json
{
   "https://xyz.io/jwt/claims": [
      "ROLE": "ADMIN"
   ],
  "email": "random@example.com"
}
```

If `https://xyz.io/jwt/claims` is declared as the namespace to use, the
authorization rules can use `$ROLE` but also `$email`.

In cases where the same claim is present in the namespace and at the root level,
the claim value in the namespace takes precedence.

## `@auth` on Interfaces

The rules provided inside the `@auth` directive on an interface will be applied
as an `AND` rule to those on the implementing types.

A type inherits the `@auth` rules of all the implemented interfaces. The final
authorization rule is an `AND` of the type's `@auth` rule and of all the
implemented interfaces.


# Subscriptions
Source: https://docs.hypermode.com/dgraph/graphql/subscriptions

Subscriptions allow clients to listen to real-time messages from the server. In GraphQL, it's straightforward to enable subscriptions on any type.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Subscriptions allow clients to listen to real-time messages from the server. The
client connects to the server with a bi-directional communication channel using
the WebSocket protocol and sends a subscription query that specifies which event
it's interested in. When an event is triggered, the server executes the stored
GraphQL query, and the result is sent back to the client using the same
communication channel.

The client can unsubscribe by sending a message to the server. The server can
also unsubscribe at any time due to errors or timeouts. A significant difference
between queries or mutations and subscriptions is that subscriptions are
stateful and require maintaining the GraphQL document, variables, and context
over the lifetime of the subscription.

![Subscription](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/graphql/subscription_flow.png "Subscription in GraphQL")

## Enable subscriptions in GraphQL

In GraphQL, it's straightforward to enable subscriptions on any type. You can
add the `@withSubscription` directive to the schema as part of the type
definition, as in the following example:

```graphql
type Todo @withSubscription {
  id: ID!
  title: String!
  description: String!
  completed: Boolean!
}
```

## `@withSubscription` with `@auth`

You can use [@auth](/dgraph/graphql/schema/directives/auth) access control rules
in conjunction with `@withSubscription`.

Consider following Schema that has both the `@withSubscription` and `@auth`
directives defined on type `Todo`.

```graphql
type Todo
  @withSubscription
  @auth(
    query: {
      rule: """
      query ($USER: String!) {
        queryTodo(filter: { owner: { eq: $USER } } ) {
          __typename
        }
      }
      """
    }
  ) {
  id: ID!
  text: String! @search(by: [term])
  owner: String! @search(by: [hash])
}
# Dgraph.Authorization {"Header":"X-Dgraph-AuthToken","Namespace":"https://dgraph.io/jwt/claims","jwkurl":"https://xyz.clerk.accounts.dev/.well-known/jwks.json","audience":["dgraph"],"ClosedByDefault":true}
```

The generated GraphQL API expects a JWT token in the `X-Dgraph-AuthToken` header
and uses the `USER` claim to apply a Role-based Access Control (RBAC). The
authorization rule enforces that only to-do tasks owned by `$USER` are returned.

## WebSocket client

Dgraph uses the WebSocket protocol `subscription-transport-ws`.

Clients must be instantiated using the WebSocket URL of the GraphQL API which is
your
[Dgraph GraphQL endpoint](dgraph/graphql/connecting#getting-your-graphql-endpoint)
with `https` replaced by `wss`.

If your Dgraph endpoint is `https://<path>` the WebSocket URL is `wss://<path>`

If your GraphQL API is configured to expect a JWT token in a header, you must
configure the WebSocket client to pass the token. Additionally, the subscription
terminates when the JWT expires.

Here are some examples of frontend clients setup.

### Urql client setup in a React app

In this scenario, we're using the
[urql client](https://formidable.com/open-source/urql/) and
`subscriptions-transport-ws` modules.

In order to use a GraphQL subscription query in a component, you need to

* instantiate a `subscriptionClient`
* instantiate a urql client with a `subscriptionExchange` using the
  `ubscriptionClient`

```js
import {
  Client,
  Provider,
  cacheExchange,
  fetchExchange,
  subscriptionExchange,
} from "urql"
import { SubscriptionClient } from "subscriptions-transport-ws"

const subscriptionClient = new SubscriptionClient(
  process.env.REACT_APP_DGRAPH_WSS,
  { reconnect: true, connectionParams: { "X-Dgraph-AuthToken": props.token } },
)

const client = new Client({
  url: process.env.REACT_APP_DGRAPH_ENDPOINT,
  fetchOptions: { headers: { "X-Dgraph-AuthToken": `Bearer ${props.token}` } },
  exchanges: [
    cacheExchange,
    fetchExchange,
    subscriptionExchange({
      forwardSubscription: (request) => subscriptionClient.request(request),
    }),
  ],
})
```

In this example,

* `process.env.REACT_APP_DGRAPH_ENDPOINT` is your
  [Dgraph GraphQL endpoint](dgraph/graphql/connecting#getting-your-graphql-endpoint)
* `process.env.REACT_APP_DGRAPH_WSS` is the WebSocket URL
* `props.token` is the JWT token of the logged-in user.

Note that we pass the JWT token in the GraphQL client using `fetchOptions` and
in the WebSocket client using `connectionParams`.

Assuming we use graphql-codegen, we can define a subscription query:

```js
import { graphql } from "../gql"

export const TodoFragment = graphql(`
  fragment TodoItem on Todo {
    id
    text
  }
`)

export const TodoSubscription = graphql(`
  subscription myTodo {
    queryTodo(first: 100) {
      ...TodoItem
    }
  }
`)
```

and use it in a React component

```js
import { useQuery, useSubscription } from "urql";
...
const [messages] = useSubscription({ query: MyMessagesDocument});

```

That's it, the react component is able to use `messages.data.queryTodo` to
display the updated list of to dos.

### Apollo client setup

To learn about using subscriptions with Apollo client, see a blog post on
[GraphQL Subscriptions with Apollo client](https://hypermode.com/blog/post/how-does-graphql-subscription/).

To pass the user JWT token in the Apollo client,use `connectionParams`, as
follows.

```javascript
const wsLink = new WebSocketLink({
  uri: `wss://${ENDPOINT}`,
  options: {
    reconnect: true,
    connectionParams: {  "<header>": "<token>", },});
```

Use the header expected by the `Dgraph.Authorization` configuration of your
GraphQL schema.

## Subscriptions to custom DQL

You can also apply `@withSubscription` directive to custom DQL queries by
specifying `@withSubscription` on individual DQL queries in `type Query`, and
those queries are added to `type subscription`.

For example, see the custom DQL query `queryUserTweetCounts` below:

```graphql
type Query {
  queryUserTweetCounts: [UserTweetCount]
    @withSubscription
    @custom(
      dql: """
      query {
        queryUserTweetCounts(func: type(User)) {
          screen_name: User.screen_name
          tweetCount: count(User.tweets)
        }
      }
      """
    )
}
```

`queryUserTweetCounts` is added to the `subscription` type, allowing users to
subscribe to this query.

<Note>
  Currently, Dgraph only supports subscriptions on custom **DQL queries**. You
  can't subscribe to custom **HTTP queries**.
</Note>

<Note>
  Starting in release v21.03, Dgraph supports compression for subscriptions.
  Dgraph uses `permessage-deflate` compression if the GraphQL client's
  `Sec-Websocket-Extensions` request header includes `permessage-deflate`, as
  follows: `Sec-WebSocket-Extensions: permessage-deflate`.
</Note>


# Guides
Source: https://docs.hypermode.com/dgraph/guides



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

* [Get Started with Dgraph](/dgraph/guides/get-started-with-dgraph/introduction)
* [Graph Data Models 101](/dgraph/guides/graph-data-models-101)
* [Build a To-Do list app](/dgraph/guides/to-do-app/introduction)
* [Build a Message Board app](/dgraph/guides/message-board-app/introduction)


# Get Started with Dgraph - Advanced Text Search
Source: https://docs.hypermode.com/dgraph/guides/get-started-with-dgraph/advanced-text-search



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

**Welcome to the sixth tutorial of getting started with Dgraph.**

In the [previous tutorial](./string-indicies), we learned about building social
graphs in Dgraph, by modeling tweets as an example. We queried the tweets using
the `hash` and `exact` indices, and implemented a keyword-based search to find
your favorite tweets using the `term` index and its functions.

In this tutorial, we'll continue from where we left off and learn about advanced
text search features in Dgraph.

Specifically, we'll focus on two advanced feature:

* Searching for tweets using Full-text search.
* Searching for hashtags using the regular expression search.

The accompanying video of the tutorial will be out shortly, so stay tuned to
[our YouTube channel](https://www.youtube.com/channel/UCghE41LR8nkKFlR3IFTRO4w).

***

Before we dive in, let's do a quick recap of how to model the tweets in Dgraph.

![tweet model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-graph-model.jpg)

In the previous tutorial, we took three real tweets as a sample dataset and
stored them in Dgraph using the above graph as a model.

In case you haven't stored the tweets from the
[previous tutorial](./string-indicies) into Dgraph, here's the sample dataset
again.

Copy the mutation below, go to the mutation tab and click Run.

```json
{
  "set": [
    {
      "user_handle": "hackintoshrao",
      "user_name": "Karthic Rao",
      "uid": "_:hackintoshrao",
      "authored": [
        {
          "tweet": "Test tweet for the fifth episode of getting started series with @dgraphlabs. Wait for the video of the fourth one by @francesc the coming Wednesday!\n#GraphDB #GraphQL",
          "tagged_with": [
            {
              "uid": "_:graphql",
              "hashtag": "GraphQL"
            },
            {
              "uid": "_:graphdb",
              "hashtag": "GraphDB"
            }
          ],
          "mentioned": [
            {
              "uid": "_:francesc"
            },
            {
              "uid": "_:dgraphlabs"
            }
          ]
        }
      ]
    },
    {
      "user_handle": "francesc",
      "user_name": "Francesc Campoy",
      "uid": "_:francesc",
      "authored": [
        {
          "tweet": "So many good talks at #graphqlconf, next year I'll make sure to be *at least* in the audience!\nAlso huge thanks to the live tweeting by @dgraphlabs for alleviating the FOMO😊\n#GraphDB ♥️ #GraphQL",
          "tagged_with": [
            {
              "uid": "_:graphql"
            },
            {
              "uid": "_:graphdb"
            },
            {
              "hashtag": "graphqlconf"
            }
          ],
          "mentioned": [
            {
              "uid": "_:dgraphlabs"
            }
          ]
        }
      ]
    },
    {
      "user_handle": "dgraphlabs",
      "user_name": "Dgraph Labs",
      "uid": "_:dgraphlabs",
      "authored": [
        {
          "tweet": "Let's Go and catch @francesc at @Gopherpalooza today, as he scans into Go source code by building its Graph in Dgraph!\nBe there, as he Goes through analyzing Go source code, using a Go program, that stores data in the GraphDB built in Go!\n#golang #GraphDB #Databases #Dgraph ",
          "tagged_with": [
            {
              "hashtag": "golang"
            },
            {
              "uid": "_:graphdb"
            },
            {
              "hashtag": "Databases"
            },
            {
              "hashtag": "Dgraph"
            }
          ],
          "mentioned": [
            {
              "uid": "_:francesc"
            },
            {
              "uid": "_:dgraphlabs"
            }
          ]
        },
        {
          "uid": "_:gopherpalooza",
          "user_handle": "gopherpalooza",
          "user_name": "Gopherpalooza"
        }
      ]
    }
  ]
}
```

*Note: If you're new to Dgraph, and this is the first time you're running a
mutation, we highly recommend reading the
[first tutorial of the series before proceeding.](./introduction)*

Voilà! Now you have a graph with `tweets`, `users`, and `hashtags`. It is ready
for us to explore.

![tweet graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/x-all-tweets.png)

*Note: If you're curious to know how we modeled the tweets in Dgraph, refer to
[the previous tutorial.](./string-indicies)*

Let's start by finding your favorite tweets using the full-text search feature
first.

## Full text search

Before we learn how to use the Full-text search feature, it's important to
understand when to use it.

The length and the number of words in a string predicate value vary based on
what the predicates represent.

Some string predicate values have only a few terms (words) in them. Predicates
representing `names`, `hashtags`, `twitter handle`, `city names` are a few good
examples. These predicates are easy to query using their exact values.

For instance, here is an example query.

*Give me all the tweets where the user name is equal to `John Campbell`*.

You can easily compose queries like these after adding either the `hash` or an
`exact` index to the string predicates.

But, some of the string predicates store sentences. Sometimes even one or more
paragraphs of text data in them. Predicates representing a tweet, a bio, a blog
post, a product description, or a movie review are just some examples. It is
relatively hard to query these predicates.

It is not practical to query such predicates using the `hash` or `exact` string
indices. A keyword-based search using the `term` index is a good starting point
to query such predicates. We used it in our
[previous tutorial](./string-indicies) to find the tweets with an exact match
for keywords like `GraphQL`, `Graphs`, and `Go`.

But, for some of the use cases, just the keyword-based search may not be
sufficient. You might need a more powerful search capability, and that's when
you should consider using Full-text search.

Let's write some queries and understand Dgraph's Full-text search capability in
detail.

To be able to do a Full-text search, you need to first set a `fulltext` index on
the `tweet` predicate.

Creating a `fulltext` index on any string predicate is similar to creating any
other string indices.

![full text](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-set-index.png)

*Note: Refer to the [previous tutorial](./string-indicies) if you're not sure
about creating an index on a string predicate.*

Now, let's do a Full-text search query to find tweets related to the following
topic: `graph data and analyzing it in graphdb`.

You can do so by using either of `alloftext` or `anyoftext` in-built functions.
Both functions take two arguments. The first argument is the predicate to
search. The second argument is the space-separated string values to search for,
and we call these as the `search strings`.

```sh
- alloftext(predicate, "space-separated search strings")
- anyoftext(predicate, "space-separated search strings")
```

We'll look at the difference between these two functions later. For now, let's
use the `alloftext` function.

Go to the query tab, paste the query below, and click Run. Here is our search
string: `graph data and analyze it in graphdb`.

```graphql
{
  search_tweet(func: alloftext(tweet, "graph data and analyze it in graphdb")) {
    tweet
  }
}
```

![tweet graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/b-full-text-query-1.png)

Here's the matched tweet, which made it to the result.

```Let's Go and catch @francesc at @Gopherpalooza today, as he scans into Go source code by building its Graph in Dgraph!

Be there, as he Goes through analyzing Go source code, using a Go program, that stores data in the GraphDB built in Go!#golang #GraphDB #Databases #Dgraph pic.twitter.com/sK90DJ6rLs

— Dgraph Labs (@dgraphlabs) November 8, 2019
```

If you observe, you can see some of the words from the search strings are not
present in the matched tweet, but the tweet has still made it to the result.

To be able to use the Full-text search capability effectively, we must
understand how it works.

Let's understand it in detail.

Once you set a `fulltext` index on the tweets, internally, the tweets are
processed, and `fulltext` tokens are generated. These `fulltext` tokens are then
indexed.

The search string also goes through the same processing pipeline, and `fulltext`
tokens generated them too.

Here are the steps to generate the `fulltext` tokens:

* Split the tweets into chunks of words called tokens (tokenizing).
* Convert these tokens to lowercase.
* [Unicode-normalize](http://unicode.org/reports/tr15/#Norm_Forms) the tokens.
* Reduce the tokens to their root form, this is called
  [stemming](https://en.wikipedia.org/wiki/Stemming) (running to run, faster to
  fast and so on).
* Remove the [stop words](https://en.wikipedia.org/wiki/Stop_words).

You would have seen in [the fourth tutorial](./multi-language-strings) that
Dgraph allows you to build multi-lingual apps.

The stemming and stop words removal are not supported for all the languages.
Here is [the link to the docs](/dgraph/dql/functions#full-text-search) that
contains the list of languages and their support for stemming and stop words
removal.

Here is the table with the matched tweet and its search string in the first
column. The second column contains their corresponding `fulltext` tokens
generated by Dgraph.

| Actual text data                                                                                                                                                                                                                                                                     | fulltext tokens generated by Dgraph                                                                                                       |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |
| Let's Go and catch @francesc at @Gopherpalooza today, as he scans into Go source code by building its Graph in Dgraph!\nBe there, as he Goes through analyzing Go source code, using a Go program, that stores data in the GraphDB built in Go!\n#golang #GraphDB #Databases #Dgraph | \[analyz build built catch code data databas dgraph francesc go goe golang gopherpalooza graph graphdb program scan sourc store todai us] |
| graph data and analyze it in graphdb                                                                                                                                                                                                                                                 | \[analyz data graph graphdb]                                                                                                              |

From the table above, you can see that the tweets are reduced to an array of
strings or tokens.

Dgraph internally uses [Bleve package](https://github.com/blevesearch/bleve) to
do the stemming.

Here are the `fulltext` tokens generated for our search string: \[`analyz`,
`data`, `graph`, `graphdb`].

As you can see from the table above, all of the `fulltext` tokens generated for
the search string exist in the matched tweet. Hence, the `alloftext` function
returns a positive match for the tweet. It would not have returned a positive
match even if one of the tokens in the search string is missing for the tweet.
But, the `anyoftext` function would've returned a positive match as long as the
tweets and the search string have at least one of the tokens in common.

If you're interested to see Dgraph's `fulltext` tokenizer in action,
[here is the gist](https://gist.github.com/hackintoshrao/0e8d715d8739b12c67a804c7249146a3)
containing the instructions to use it.

Dgraph generates the same `fulltext` tokens even if the words in a search string
is differently ordered. Hence, using the same search string with different order
would not impact the query result.

As you can see, all three queries below are the same for Dgraph.

```graphql
{
  search_tweet(func: alloftext(tweet, "graph analyze and it in graphdb data")) {
    tweet
  }
}
```

```graphql
{
  search_tweet(func: alloftext(tweet, "data and data analyze it graphdb in")) {
    tweet
  }
}
```

```graphql
{
  search_tweet(func: alloftext(tweet, "analyze data and it in graph graphdb")) {
    tweet
  }
}
```

Now, let's move onto the next advanced text search feature of Dgraph: regular
expression based queries.

Let's use them to find all the hashtags containing the following substring:
`graph`.

## Regular expression search

[Regular expressions](https://www.geeksforgeeks.org/write-regular-expressions/)
are powerful ways of expressing search patterns. Dgraph allows you to search for
string predicates based on regular expressions. You need to set the `trigram`
index on the string predicate to be able to perform regex-based queries.

Using regular expression based search, let's match all the hashtags that have
this particular pattern:
`Starts and ends with any characters of indefinite length, but with the substring graph in it`.

Here is the regex expression we can use: `^.*graph.*$`

Check out
[this tutorial](https://www.geeksforgeeks.org/write-regular-expressions/) if
you're not familiar with writing a regular expression.

Let's first find all the hashtags in the database using the `has()` function.

```graphql
{
  hash_tags(func: has(hashtag)) {
    hashtag
  }
}
```

![The hashtags](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/has-hashtag.png)

*If you're not familiar with using the `has()` function, refer to
[the first tutorial](./introduction) of the series.*

You can see that we have six hashtags in total, and four of them have the
substring `graph` in them: `Dgraph`, `GraphQL`, `graphqlconf`, `graphDB`.

We should use the built-in function `regexp` to be able to use regular
expressions to search for predicates. This function takes two arguments, the
first is the name of the predicate, and the second one is the regular
expression.

Here is the syntax of the `regexp` function:
`regexp(predicate, /regular-expression/)`

Let's execute the following query to find the hashtags that have the substring
`graph`.

Go to the query tab, type in the query, and click Run.

```graphql
{
  reg_search(func: regexp(hashtag, /^.*graph.*$/)) {
    hashtag
  }
}
```

Oops! We have an error! It looks like we forgot to set the `trigram` index on
the `hashtag` predicate.

![The hashtags](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/trigram-error.png)

Again, setting a `trigram` index is similar to setting any other string index,
let's do that for the `hashtag` predicate.

![The hashtags](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/set-trigram.png)

*Note: Refer to the [previous tutorial](./string-indicies) if you're not sure
about creating an index on a string predicate.*

Now, let's re-run the `regexp` query.

![regex-1](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/regex-query-1.png)

*Note: Refer to [the first tutorial](./introduction) if you're not familiar with
the query structure in general* Success!

But we only have the following hashtags in the result: `Dgraph` and
`graphqlconf`.

That's because `regexp` function is case-sensitive by default.

Add the character `i` at the end of the second argument of the `regexp` function
to make it case insensitive: `regexp(predicate, /regular-expression/i)`

![regex-2](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/regex-query-2.png)

Now we have the four hashtags with substring `graph` in them.

Let's modify the regular expression to match only the `hashtags` which have a
prefix called `graph`.

```graphql
{
  reg_search(func: regexp(hashtag, /^graph.*$/i)) {
    hashtag
  }
}
```

![regex-3](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/regex-query-3.png)

## Summary

In this tutorial, we learned about Full-text search and regular expression based
search capabilities in Dgraph.

Did you know that Dgraph also offers fuzzy search capabilities, which can be
used to power features like `product` search in an e-commerce store?

Let's learn about the fuzzy search in our next tutorial.

Sounds interesting?

Check out our next tutorial of the getting started series
[here](./fuzzy-search).

## Need Help

* Please use [discuss.hypermode.com](https://discuss.hypermode.com) for
  questions, feature requests, bugs, and discussions.


# Get Started with Dgraph - Basic Operations
Source: https://docs.hypermode.com/dgraph/guides/get-started-with-dgraph/basic-operations



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

**Welcome to the second tutorial of getting started with Dgraph.**

In the [previous tutorial](./introduction) of getting started, we learned some
of the basics of Dgraph. Including how to run the database, add new nodes and
predicates, and query them back.

![Graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/graph-1.jpg)

In this tutorial, we'll build the above Graph and learn more about operations
using the UID (Universal Identifier) of the nodes. Specifically, we'll learn
about:

* Querying and updating nodes, deleting predicates using their UIDs.
* Adding an edge between existing nodes.
* Adding a new predicate to an existing node.
* Traversing the Graph.

You can see the accompanying video below.

<iframe width="560" height="315" src="https://www.youtube.com/embed/8TKD-FFBVgE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen />

***

First, let's create our Graph.

Go to Ratel's mutate tab, paste the mutation below in the text area, and click
Run.

```json
{
  "set": [
    {
      "name": "Michael",
      "age": 40,
      "follows": {
        "name": "Pawan",
        "age": 28,
        "follows": {
          "name": "Leyla",
          "age": 31
        }
      }
    }
  ]
}
```

![mutation-1](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-add-data.gif)

## Query using UIDs

The UID of the nodes can be used to query them back. The built-in function `uid`
takes a list of UIDs as an argument, so you can pass one (`uid(0x1)`) or as many
as you need (`uid(0x1, 0x2)`).

It returns the same UIDs that were passed as input, no matter whether they exist
in the database or not. But the predicates requested are returned only if both
the UIDs and their predicates exist.

Let's see the `uid` function in action.

First, let's copy the UID of the node created for `Michael`.

Go to the query tab, type in the query below, and click Run.

```graphql
{
  people(func: has(name)) {
    uid
    name
    age
  }
}
```

Now, from the result, copy the UID of Michael's node.

![Get UID](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/b-get-uid-1.png)

In the query below, replace the placeholder `MICHAELS_UID` with the UID you just
copied, and run the query.

```graphql
{
    find_using_uid(func: uid(MICHAELS_UID)){
        uid
        name
        age
    }
}
```

![Get node from UID](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/c-query-uid.png)

*Note: `MICHAELS_UID` appears as `0x8` in the images. The UID you get on your
machine might have a different value.*

You can see that the `uid` function returns the node matching the UID for
Michael's node.

Refer to the [previous tutorial](./introduction) if you have questions related
to the structure of the query in general.

## Updating predicates

You can also update one or more predicates of a node using its UID.

Michael recently celebrated his birthday. Let's update his age to 41.

Go to the mutate tab and execute the mutation. Again, don't forget to replace
the placeholder `MICHAELS_UID` with the actual UID of the node for `Michael`.

```json
{
  "set": [
    {
      "uid": "MICHAELS_UID",
      "age": 41
    }
  ]
}
```

We had earlier used `set` to create new nodes. But on using the UID of an
existing node, it updates its predicates, instead of creating a new node.

You can see that Michael's age is updated to 41.

```graphql
{
    find_using_uid(func: uid(MICHAELS_UID)){
        name
        age
    }
}
```

![update check](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/d-update-check.png)

Similarly, you can also add new predicates to an existing node. Since the
predicate `country` doesn't exist for the node for `Michael`, it creates a new
one.

```json
{
  "set": [
    {
      "uid": "MICHAELS_UID",
      "country": "Australia"
    }
  ]
}
```

## Adding an edge between existing nodes

You can also add an edge between existing nodes using their UIDs.

Let's say, `Leyla` starts to follow `Michael`.

We know that this relationship between them has to represented by creating the
`follows` edge between them.

![Graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/graph-2.jpg)

First, let's copy the UIDs of nodes for `Leyla` and `Michael` from Ratel.

Now, replace the placeholders `LEYLAS_UID` and `MICHAELS_UID` with the ones you
copied, and execute the mutation.

```json
{
  "set": [
    {
      "uid": "LEYLAS_UID",
      "follows": {
        "uid": "MICHAELS_UID"
      }
    }
  ]
}
```

## Traversing the edges

Graph databases offer many distinct capabilities. `Traversals` are among them.

Traversals answer questions or queries related to the relationship between the
nodes. Hence, queries like, `who does Michael follow?` are answered by
traversing the `follows` relationship.

Let's run a traversal query and then understand it in detail.

```graphql
{
    find_follower(func: uid(MICHAELS_UID)){
        name
        age
        follows {
          name
          age
        }
    }
}
```

Here's the result.

![traversal-result](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/e-traversal.png)

The query has three parts:

* **Selecting the root nodes.**

First, you need to select one or more nodes as the starting point for
traversals. These are called the root nodes. In the preceding query, we use the
`uid()` function to select the node created for `Michael` as the root node.

* **Choosing the edge to be traversed**

You need to specify the edge to be traversed, starting from the selected root
nodes. And then, the traversal, travels along these edges, from one end to the
nodes at the other end.

In our query, we chose to traverse the `follows` edge starting from the node for
`Michael`. The traversal returns all the nodes connected to the node for
`Michael` via the `follows` edge.

* **Specify the predicates to get back**

Since Michael follows only one person, the traversal returns just one node.
These are `level-2` nodes. The root nodes constitute the nodes for `level-1`.
Again, we need to specify which predicates you want to get back from `level-2`
nodes.

![Get node from UID](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/j-explain.JPG)

You can extend the query to make use of `level-2` nodes and traverse the Graph
further and deeper. Let's explore that in the next section.

### Multi-level traversals

The first level of traversal returns people followed by Michael. The next level
of traversal further returns the people they in-turn follow.

This pattern can be repeated multiple times to achieve multi-level traversals.
The depth of the query increases by one as we traverse each level of the Graph.
That's when we say that the query is deep!

```graphql
{
  find_follower(func: uid(MICHAELS_UID)) {
    name
    age
    follows {
      name
      age
      follows {
        name
        age
      }
    }
  }
}
```

![level-3-query](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/f-level-3-traverse.png)

Here is one more example from the extension of the last query.

```graphql
{
  find_follower(func: uid(MICHAELS_UID)) {
    name
    age
    follows {
      name
      age
      follows {
        name
        age
        follows {
          name
          age
        }
      }
    }
  }
}
```

![level 3](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/g-level-4-traversal.png)

This query is really long! The query is four levels deep. In other words, the
depth of the query is four. If you ask, isn't there an in-built function that
makes multi-level deep queries or traversals easy?

The answer is Yes! That's what the `recurse()` function does. Let's explore that
in our next section.

#### Recursive traversals

Recursive queries makes it easier to perform multi-level deep traversals. They
let you easily traverse a subset of the Graph.

With the following recursive query, we achieve the same result as our last
query. But, with a much better querying experience.

```graphql
{
  find_follower(func: uid(MICHAELS_UID)) @recurse(depth: 4) {
    name
    age
    follows
  }
}
```

In the query, the `recurse` function traverses the graph starting from the node
for `Michael`. You can choose any other node to be the starting point. The depth
parameter specifies the maximum depth the traversal query should consider.

Let's run the recursive traversal query after replacing the placeholder with the
UID of node for Michael.

![recurse](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/h-recursive-traversal.png)

[Check out the docs](/dgraph/dql/recurse#recurse) for detailed instructions on
using the `recurse` directive.

#### Edges have directions

Edges in Dgraph have directions.

For instance, the `follows` edge emerging from the node for `Michael`, points at
the node for `Pawan`. They have a notion of direction.

Traversing along the direction of an edge is natural to Dgraph. We'll learn
about traversing edges in reverse direction in our next tutorial.

## Deleting a predicate

Predicates of a node can be deleted using the `delete` mutation. Here's the
syntax of the delete mutation to delete any predicate of a node,

```graphql
{
    delete {
        <UID> <predicate_name> * .
    }
}
```

Using the mutation syntax, let's compose a delete mutation. Let's delete the
`age` predicate of the node for `Michael`.

```graphql
{
  delete {
    <MICHAELS_UID> <age> * .
  }
}
```

![recurse](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/i-delete.png)

## Wrapping up

In this tutorial, we learned about the CRUD operations using UIDs. We also
learned about `recurse()` function.

Before we wrap, here's a sneak peek into our next tutorial.

Did you know that you could search predicates based on their value?

Sounds interesting?

Check out our next tutorial of the getting started series
[here](./types-and-operations).

## Need help

* Please use [discuss.hypermode.com](https://discuss.hypermode.com) for
  questions, feature requests, bugs, and discussions.


# Get Started with Dgraph - Fuzzy Search
Source: https://docs.hypermode.com/dgraph/guides/get-started-with-dgraph/fuzzy-search



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

**Welcome to the seventh tutorial of getting started with Dgraph.**

In the [previous tutorial](./advanced-text-search), we learned about building
advanced text searches on social graphs in Dgraph, by modeling tweets as an
example. We queried the tweets using the `fulltext` and `trigram` indices and
implemented full-text and regular expression search on the tweets.

In this tutorial, we'll continue exploring Dgraph's string querying capabilities
using the twitter model from [the fifth](./string-indicies) and
[the sixth](./advanced-text-search) tutorials. In particular, we'll implement a
`twitter username` search feature using the Dgraph's fuzzy search function.

The accompanying video of the tutorial will be out shortly, so stay tuned to
[our YouTube channel](https://www.youtube.com/channel/UCghE41LR8nkKFlR3IFTRO4w).

***

Before we dive in, let's review of how we modeled the tweets in the previous two
tutorials:

![tweet model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-graph-model.jpg)

We used three real-life example tweets as a sample dataset and stored them in
Dgraph using the above graph as a model.

Here is the sample dataset again if you skipped the previous tutorials. Copy the
mutation below, go to the mutation tab and click Run.

```json
{
  "set": [
    {
      "user_handle": "hackintoshrao",
      "user_name": "Karthic Rao",
      "uid": "_:hackintoshrao",
      "authored": [
        {
          "tweet": "Test tweet for the fifth episode of getting started series with @dgraphlabs. Wait for the video of the fourth one by @francesc the coming Wednesday!\n#GraphDB #GraphQL",
          "tagged_with": [
            {
              "uid": "_:graphql",
              "hashtag": "GraphQL"
            },
            {
              "uid": "_:graphdb",
              "hashtag": "GraphDB"
            }
          ],
          "mentioned": [
            {
              "uid": "_:francesc"
            },
            {
              "uid": "_:dgraphlabs"
            }
          ]
        }
      ]
    },
    {
      "user_handle": "francesc",
      "user_name": "Francesc Campoy",
      "uid": "_:francesc",
      "authored": [
        {
          "tweet": "So many good talks at #graphqlconf, next year I'll make sure to be *at least* in the audience!\nAlso huge thanks to the live tweeting by @dgraphlabs for alleviating the FOMO😊\n#GraphDB ♥️ #GraphQL",
          "tagged_with": [
            {
              "uid": "_:graphql"
            },
            {
              "uid": "_:graphdb"
            },
            {
              "hashtag": "graphqlconf"
            }
          ],
          "mentioned": [
            {
              "uid": "_:dgraphlabs"
            }
          ]
        }
      ]
    },
    {
      "user_handle": "dgraphlabs",
      "user_name": "Dgraph Labs",
      "uid": "_:dgraphlabs",
      "authored": [
        {
          "tweet": "Let's Go and catch @francesc at @Gopherpalooza today, as he scans into Go source code by building its Graph in Dgraph!\nBe there, as he Goes through analyzing Go source code, using a Go program, that stores data in the GraphDB built in Go!\n#golang #GraphDB #Databases #Dgraph ",
          "tagged_with": [
            {
              "hashtag": "golang"
            },
            {
              "uid": "_:graphdb"
            },
            {
              "hashtag": "Databases"
            },
            {
              "hashtag": "Dgraph"
            }
          ],
          "mentioned": [
            {
              "uid": "_:francesc"
            },
            {
              "uid": "_:dgraphlabs"
            }
          ]
        },
        {
          "uid": "_:gopherpalooza",
          "user_handle": "gopherpalooza",
          "user_name": "Gopherpalooza"
        }
      ]
    }
  ]
}
```

*Note: If you're new to Dgraph, and this is the first time you're running a
mutation, we highly recommend reading the
[first tutorial of the series before proceeding](./introduction).*

Now you should have a graph with tweets, users, and hashtags, and it's ready for
us to explore.

![tweet graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/x-all-tweets.png)

*Note: If you're curious to know how we modeled the tweets in Dgraph, refer to
[the fifth tutorial](./string-indicies).*

Before we show you the fuzzy search in action, let's first understand what it's
and how does it work.

## Fuzzy search

Providing search capabilities on products or usernames requires searching for
the closest match to a string, if a full match doesn't exist. This feature helps
you get relevant results even if there's a typo or the user doesn't search based
on the exact name it's stored. This is exactly what the fuzzy search does: it
compares the string values and returns the nearest matches. Hence, it's ideal
for our use case of implementing search on the `twitter usernames`.

The functioning of the fuzzy search is based on the `Levenshtein distance`
between the value of the user name stored in Dgraph and the search string.

[`Levenshtein distance`](https://en.wikipedia.org/wiki/Levenshtein_distance) is
a metric that defines the closeness of two strings. `Levenshtein distance`
between two words is the minimum number of single-character edits (insertions,
deletions or substitutions) required to change one word into the other.

For instance, the `Levenshtein Distance` between the strings `book` and `back`
is 2. The value of 2 is justified because by changing two characters, we changed
the word `book` to `back`.

Now you've understood what the fuzzy search is and what it can do. Next, let's
learn how to use it on string predicates in Dgraph.

## Implement Fuzzy Search in Dgraph

To use the fuzzy search on a string predicate in Dgraph, you first set the
`trigram` index.

Go to the Schema tab and set the `trigram` index on the `user_name` predicate.

After setting the `trigram` index on the `user_name` predicate, you can use
Dgraph's built-in function `match` to run a fuzzy search query.

Here is the syntax of the `match` function:
`match(predicate, search string, distance)`

The [match function](/dgraph/dql/functions#fuzzy-matching) takes in three
parameters:

1. The name of the string predicate used for querying.
2. The search string provided by the user
3. An integer that represents the maximum `Levenshtein Distance` between the
   first two parameters. This value should be greater than 0. For example, when
   having an integer of 8 returns predicates with a distance value of less than
   or equal to 8.

Using a greater value for the `distance` parameter can potentially match more
string predicates, but it also yields less accurate results.

Before we use the `match` function, let's first get the list of user names
stored in the database.

```graphql
{
    names(func: has(user_name)) {
        user_name
    }
}
```

![tweet graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/e-names.png)

As you can see from the result, we have four user names: `Gopherpalooza`,
`Karthic Rao`, `Francesc Campoy`, and `Dgraph Labs`.

First, we set the `Levenshtein Distance` parameter to 3. We expect to see Dgraph
returns all the `username` predicates with three or fewer distances from the
provided searching string.

Then, we set the second parameter, the search string provided by the user, as
`graphLabs`.

Go to the query tab, paste the query below and click Run.

```graphql
{
    user_names_Search(func: match(user_name, "graphLabs", 3)) {
        user_name
    }
}
```

![first query](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/h-one.png)

We got a positive match! Because the search string `graphLabs` is at a distance
of two from the predicate value of `Dgraph Labs`, so we see it in the search
result.

If you are interested in learning more about how to find the Levenshtein
Distance between two strings,
[here is a useful site](https://planetcalc.com/1721/).

Let's run the above query again, but this time we will use the search string
`graphLab` instead. Go to the query tab, paste the query below and click Run.

```graphql
{
    user_names_Search(func: match(user_name, "graphLab", 3)) {
        user_name
    }
}
```

![first query](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/i-two.png)

We still got a positive match with the `user_name` predicate with the value
`Dgraph Labs`! That's because the search string `graphLab` is at a distance of
three from the predicate value of `Dgraph Labs`, so we see it in the search
result.

In this case, the `Levenshtein Distance` between the search string `graphLab`
and the predicate `Dgraph Labs` is 3, hence the match.

For the last run of the query, let's change the search string to `Dgraph` but
keep the Levenshtein Distance at 3.

```graphql
{
    user_names_Search(func: match(user_name, "Dgraph", 3)) {
        user_name
    }
}
```

![first query](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/j-three.png)

Now you no longer see Dgraph Labs appears in the search result because the
distance between the word `Dgraph` and `Dgraph Labs` is larger than 3. But based
on normal human rationales, you would naturally expect Dgraph Labs appears in
the search result while using Dgraph as the search string.

This is one of the downsides of the fuzzy search based on the
`Levenshtein Distance` algorithm. The effectiveness of the fuzzy search reduces
as the value of the distance parameter decreases, and it also reduces with an
increase in the number of words included in the string predicate.

Therefore it's not recommended to use the fuzzy search on the string predicates
which could contain many words, for instance, predicates which store the values
for `blog posts`, `bio`, `product description` and so on. Hence, the ideal
candidates to use fuzzy search are predicates like `names`, `zipcodes`,
`places`, where the number of words in the string predicate would generally
between 1-3.

Also, based on the use case, tuning the `distance` parameter is crucial for the
effectiveness of fuzzy search.

## Fuzzy search scoring because you asked for it

At Dgraph, we're committed to improving the all-round capabilities of the
distributed Graph database. As part of one of our recent efforts to improve the
database features, we've taken note of the
[request on Github](https://github.com/hypermodeinc/dgraph/issues/3211) by one
of our community members to integrate a `tf-idf` score based text search. This
integration will further enhance the search capabilities of Dgraph.

We've prioritized the resolve of the issue in our product roadmap. We would like
to take this opportunity to say thank you to our community of users for helping
us make the product better.

## Summary

Fuzzy search is a simple and yet effective search technique for a wide range of
use cases. Along with the existing features to query and search string
predicates, the addition of `tf-idf` based search will further improve Dgraph's
capabilities.

This marks the end of our three tutorial streak exploring string indices and
their queries using the graph model of tweets.

Check out our next tutorial of the getting started series [here](./geolocation).

Remember to click the “Join our community” button below and subscribe to our
newsletter to get the latest tutorial right to your inbox.

## Need Help

* Please use [discuss.hypermode.com](https://discuss.hypermode.com) for
  questions, feature requests, bugs, and discussions.


# Get Started with Dgraph - Geolocation
Source: https://docs.hypermode.com/dgraph/guides/get-started-with-dgraph/geolocation



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

**Welcome to the eight tutorial of getting started with Dgraph.**

In the [previous tutorial](./fuzzy-search), we learned about building a
twitter-like user-search feature using
[Dgraph's fuzzy search](dgraph/dql/functions#fuzzy-matching).

In this tutorial, we'll build a graph of tourist locations around San Francisco
and help our Zoologist friend, Mary, and her team in their mission to conserve
birds using Dgraph's geolocation capabilities.

You might have used Google to find the restaurants near you or to find the
shopping centers within a mile of your current location. Apps like these make
use of your geolocation data.

Geolocation has become an integral part of mobile apps, especially with the
advent of smartphones in the last decade, the list of apps which revolves around
users location to power app features has grown beyond imagination.

Let's take Uber, for instance, the location data of the driver and passenger is
pivotal for the app. We're gathering more GPS data than ever before, being able
to store and query the location data efficiently can give you an edge over your
competitors.

Real-world data is interconnected and they are not sparse. This is even more
relevant when it comes to location data. The natural representation of railway
networks, maps, routes are graphs.

The good news is that Dgraph, the world's most advanced graph database, comes
with functionalities to efficiently store and perform useful queries on graphs
containing location data. If you want to run queries like
`find hotels near Golden Gate Bridge`, or
`find all the tourist location around Golden Gate Park`, Dgraph has your back.

First, let's learn how to represent Geolocation data in Dgraph.

## Representing geolocation data

You can represent location data in Dgraph using two ways:

* **Point location**

Point location contains the geo-coordinate tuple (latitude, longitude) of your
location of interest.

The following image has the point location with the latitude and longitude for
the Eiffel Tower in Paris. Point locations are useful for representing a precise
location. For instance, your location when booking a cab or your delivery
address.

![model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/b-paris.png)

* **Polygonal location**

It isn't possible to represent geographical entities which are spread across
multiple geo-coordinates just using a point location. To represent geo entities
like a city, a lake, or a national park, you should use a polygonal location.

Here is an example:

![model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/c-delhi.jpg)

The polygonal fence above represents the city of Delhi, India. This polygonal
fence or the geo-fence is formed by connecting multiple straight-line
boundaries, and they're collectively represented using an array of location
tuples of format `[(latitude, longitude), (latitude, longitude), ...]`. Each
tuple pair `(2 tuples and 4 coordinates)` represents a straight line boundary of
the geo-fence, and a polygonal fence can contain any number of lines.

Let's start with building a simple San Francisco tourist graph, here's the graph
model.

![model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-graph.jpg)

The above graph has three entities represented by the nodes:

* **City**

A `city node` represents the tourist city. Our dataset only contains the city of
`San Francisco`, and a node in the graph represents it.

* **Location**

A location node, along with the name of the location, it contains the point or
polygonal location of the place of interest.

* **Location Type**

A location type consists of the type of location. There are four types of
location in our dataset: `zoo`, `museum`, `hotel` or a `tourist attraction`.

The `location nodes` with geo-coordinates of a `hotel` also contains their
pricing information.

There are different ways to model the same graph. For instance, the
`location type` could just be a property or a predicate of the `location node`,
rather than being a node of its own.

The queries you want to perform or the relationships you like to explore mostly
influence the modeling decisions. The goal of the tutorial isn't to arrive at
the ideal graph model, but to use a simple dataset to demonstrate the
geolocation capabilities of Dgraph.

For the rest of the tutorial, let's call the node representing a `City` as a
`city` node, and the node representing a `Location` as a `location` node, and
the node representing the `Location Type` as a `location type` node.

Here's the relationship between these nodes:

* Every `city node` is connected to a `location node` via the `has_location`
  edge.
* Every `location node` is connected to its node representing a `location type`
  via the `has_type` edge.

<Note>
  Dgraph allows you to associate one or more types for the nodes using its type
  system feature, for now, we're using nodes without types, we'll learn about
  type system for nodes in a future tutorial. Read more on the [DQL
  schema](/dgraph/dql/schema) to explore type system feature for nodes
</Note>

Here is our sample dataset. Open Ratel, go to the mutate tab, paste the
mutation, and click Run.

```json
{
  "set": [
    {
      "city": "San Francisco",
      "uid": "_:SFO",
      "has_location": [
        {
          "name": "USS Pampanito",
          "location": {
            "type": "Polygon",
            "coordinates": [
              [
                [-122.4160088, 37.8096674],
                [-122.4161147, 37.8097628],
                [-122.4162064, 37.8098357],
                [-122.4163467, 37.8099312],
                [-122.416527, 37.8100471],
                [-122.4167504, 37.8101792],
                [-122.4168272, 37.8102137],
                [-122.4167719, 37.8101612],
                [-122.4165683, 37.8100108],
                [-122.4163888, 37.8098923],
                [-122.4162492, 37.8097986],
                [-122.4161469, 37.8097352],
                [-122.4160088, 37.8096674]
              ]
            ]
          },
          "has_type": [
            {
              "uid": "_:museum",
              "loc_type": "Museum"
            }
          ]
        },
        {
          "name": "Alameda Naval Air Museum",
          "location": {
            "type": "Polygon",
            "coordinates": [
              [
                [-122.2995054, 37.7813924],
                [-122.2988538, 37.7813582],
                [-122.2988421, 37.7814972],
                [-122.2994937, 37.7815314],
                [-122.2995054, 37.7813924]
              ]
            ]
          },
          "street": "Ferry Point Road",
          "has_type": [
            {
              "uid": "_:museum"
            }
          ]
        },
        {
          "name": "Burlingame Museum of PEZ Memorabilia",
          "location": {
            "type": "Polygon",
            "coordinates": [
              [
                [-122.3441509, 37.5792003],
                [-122.3438207, 37.5794257],
                [-122.3438987, 37.5794587],
                [-122.3442289, 37.5792333],
                [-122.3441509, 37.5792003]
              ]
            ]
          },
          "street": "California Drive",
          "has_type": [
            {
              "uid": "_:museum"
            }
          ]
        },
        {
          "name": "Carriage Inn",
          "location": {
            "type": "Polygon",
            "coordinates": [
              [
                [-122.3441509, 37.5792003],
                [-122.3438207, 37.5794257],
                [-122.3438987, 37.5794587],
                [-122.3442289, 37.5792333],
                [-122.3441509, 37.5792003]
              ]
            ]
          },
          "street": "7th street",
          "price_per_night": 350.0,
          "has_type": [
            {
              "uid": "_:hotel",
              "loc_type": "Hotel"
            }
          ]
        },
        {
          "name": "Lombard Motor In",
          "location": {
            "type": "Polygon",
            "coordinates": [
              [
                [-122.4260484, 37.8009811],
                [-122.4260137, 37.8007969],
                [-122.4259083, 37.80081],
                [-122.4258724, 37.8008144],
                [-122.4257962, 37.8008239],
                [-122.4256354, 37.8008438],
                [-122.4256729, 37.8010277],
                [-122.4260484, 37.8009811]
              ]
            ]
          },
          "street": "Lombard Street",
          "price_per_night": 400.0,
          "has_type": [
            {
              "uid": "_:hotel"
            }
          ]
        },
        {
          "name": "Holiday Inn San Francisco Golden Gateway",
          "location": {
            "type": "Polygon",
            "coordinates": [
              [
                [-122.4214895, 37.7896108],
                [-122.4215628, 37.7899798],
                [-122.4215712, 37.790022],
                [-122.4215987, 37.7901606],
                [-122.4221004, 37.7900985],
                [-122.4221044, 37.790098],
                [-122.4219952, 37.7895481],
                [-122.4218207, 37.78957],
                [-122.4216158, 37.7895961],
                [-122.4214895, 37.7896108]
              ]
            ]
          },
          "street": "Van Ness Avenue",
          "price_per_night": 250.0,
          "has_type": [
            {
              "uid": "_:hotel"
            }
          ]
        },
        {
          "name": "Golden Gate Bridge",
          "location": {
            "type": "Polygon",
            "coordinates": [
              [
                [-122.479784, 37.8288329],
                [-122.4775646, 37.8096291],
                [-122.4775538, 37.8095165],
                [-122.4775465, 37.8093304],
                [-122.4775823, 37.8093296],
                [-122.4775387, 37.8089749],
                [-122.4773545, 37.8089887],
                [-122.4773402, 37.8089575],
                [-122.4772752, 37.8088285],
                [-122.4772084, 37.8087099],
                [-122.4771322, 37.8085903],
                [-122.4770518, 37.8084793],
                [-122.4769647, 37.8083687],
                [-122.4766802, 37.8080091],
                [-122.4766629, 37.8080195],
                [-122.4765701, 37.8080751],
                [-122.476475, 37.8081322],
                [-122.4764106, 37.8081708],
                [-122.476396, 37.8081795],
                [-122.4764936, 37.8082814],
                [-122.476591, 37.8083823],
                [-122.4766888, 37.8084949],
                [-122.47677, 37.808598],
                [-122.4768444, 37.8087008],
                [-122.4769144, 37.8088105],
                [-122.4769763, 37.8089206],
                [-122.4770373, 37.8090416],
                [-122.477086, 37.809151],
                [-122.4771219, 37.8092501],
                [-122.4771529, 37.809347],
                [-122.477179, 37.8094517],
                [-122.4772003, 37.809556],
                [-122.4772159, 37.8096583],
                [-122.4794624, 37.8288561],
                [-122.4794098, 37.82886],
                [-122.4794817, 37.8294742],
                [-122.4794505, 37.8294765],
                [-122.4794585, 37.8295453],
                [-122.4795423, 37.8295391],
                [-122.4796312, 37.8302987],
                [-122.4796495, 37.8304478],
                [-122.4796698, 37.8306078],
                [-122.4796903, 37.830746],
                [-122.4797182, 37.8308784],
                [-122.4797544, 37.83102],
                [-122.479799, 37.8311522],
                [-122.4798502, 37.8312845],
                [-122.4799025, 37.8314139],
                [-122.4799654, 37.8315458],
                [-122.4800346, 37.8316718],
                [-122.4801231, 37.8318137],
                [-122.4802112, 37.8319368],
                [-122.4803028, 37.8320547],
                [-122.4804046, 37.8321657],
                [-122.4805121, 37.8322792],
                [-122.4805883, 37.8323459],
                [-122.4805934, 37.8323502],
                [-122.4807146, 37.8323294],
                [-122.4808917, 37.832299],
                [-122.4809526, 37.8322548],
                [-122.4809672, 37.8322442],
                [-122.4808396, 37.8321298],
                [-122.4807166, 37.8320077],
                [-122.4806215, 37.8319052],
                [-122.4805254, 37.8317908],
                [-122.4804447, 37.8316857],
                [-122.4803548, 37.8315539],
                [-122.4802858, 37.8314395],
                [-122.4802227, 37.8313237],
                [-122.4801667, 37.8312051],
                [-122.4801133, 37.8310812],
                [-122.4800723, 37.8309602],
                [-122.4800376, 37.8308265],
                [-122.4800087, 37.8307005],
                [-122.4799884, 37.8305759],
                [-122.4799682, 37.8304181],
                [-122.4799501, 37.8302699],
                [-122.4798628, 37.8295146],
                [-122.4799157, 37.8295107],
                [-122.4798451, 37.8289002],
                [-122.4798369, 37.828829],
                [-122.479784, 37.8288329]
              ]
            ]
          },
          "street": "Golden Gate Bridge",
          "has_type": [
            {
              "uid": "_:attraction",
              "loc_type": "Tourist Attraction"
            }
          ]
        },
        {
          "name": "Carriage Inn",
          "location": {
            "type": "Polygon",
            "coordinates": [
              [
                [-122.3441509, 37.5792003],
                [-122.3438207, 37.5794257],
                [-122.3438987, 37.5794587],
                [-122.3442289, 37.5792333],
                [-122.3441509, 37.5792003]
              ]
            ]
          },
          "street": "7th street",
          "has_type": [
            {
              "uid": "_:attraction"
            }
          ]
        },
        {
          "name": "San Francisco Zoo",
          "location": {
            "type": "Polygon",
            "coordinates": [
              [
                [-122.5036126, 37.7308562],
                [-122.5028991, 37.7305879],
                [-122.5028274, 37.7305622],
                [-122.5027812, 37.7305477],
                [-122.5026992, 37.7305269],
                [-122.5026211, 37.7305141],
                [-122.5025342, 37.7305081],
                [-122.5024478, 37.7305103],
                [-122.5023667, 37.7305221],
                [-122.5022769, 37.7305423],
                [-122.5017546, 37.7307008],
                [-122.5006917, 37.7311277],
                [-122.4992484, 37.7317075],
                [-122.4991414, 37.7317614],
                [-122.4990379, 37.7318177],
                [-122.4989369, 37.7318762],
                [-122.4988408, 37.731938],
                [-122.4987386, 37.7320142],
                [-122.4986377, 37.732092],
                [-122.4978359, 37.7328712],
                [-122.4979122, 37.7333232],
                [-122.4979485, 37.7333909],
                [-122.4980162, 37.7334494],
                [-122.4980945, 37.7334801],
                [-122.4989553, 37.7337384],
                [-122.4990551, 37.7337743],
                [-122.4991479, 37.7338184],
                [-122.4992482, 37.7338769],
                [-122.4993518, 37.7339426],
                [-122.4997605, 37.7342142],
                [-122.4997578, 37.7343433],
                [-122.5001258, 37.7345486],
                [-122.5003425, 37.7346621],
                [-122.5005576, 37.7347566],
                [-122.5007622, 37.7348353],
                [-122.500956, 37.7349063],
                [-122.5011438, 37.7349706],
                [-122.5011677, 37.7349215],
                [-122.5013556, 37.7349785],
                [-122.5013329, 37.7350294],
                [-122.5015181, 37.7350801],
                [-122.5017265, 37.7351269],
                [-122.5019229, 37.735164],
                [-122.5021252, 37.7351953],
                [-122.5023116, 37.7352187],
                [-122.50246, 37.7352327],
                [-122.5026074, 37.7352433],
                [-122.5027534, 37.7352501],
                [-122.5029253, 37.7352536],
                [-122.5029246, 37.735286],
                [-122.5033453, 37.7352858],
                [-122.5038376, 37.7352855],
                [-122.5038374, 37.7352516],
                [-122.5054006, 37.7352553],
                [-122.5056182, 37.7352867],
                [-122.5061792, 37.7352946],
                [-122.5061848, 37.7352696],
                [-122.5063093, 37.7352671],
                [-122.5063297, 37.7352886],
                [-122.5064719, 37.7352881],
                [-122.5064722, 37.735256],
                [-122.506505, 37.7352268],
                [-122.5065452, 37.7352287],
                [-122.5065508, 37.7351214],
                [-122.5065135, 37.7350885],
                [-122.5065011, 37.7351479],
                [-122.5062471, 37.7351127],
                [-122.5059669, 37.7349341],
                [-122.5060092, 37.7348205],
                [-122.5060405, 37.7347219],
                [-122.5060611, 37.734624],
                [-122.5060726, 37.7345101],
                [-122.5060758, 37.73439],
                [-122.5060658, 37.73427],
                [-122.5065549, 37.7342676],
                [-122.5067262, 37.7340364],
                [-122.506795, 37.7340317],
                [-122.5068355, 37.733827],
                [-122.5068791, 37.7335407],
                [-122.5068869, 37.7334106],
                [-122.5068877, 37.733281],
                [-122.5068713, 37.7329795],
                [-122.5068598, 37.7328652],
                [-122.506808, 37.7325954],
                [-122.5067837, 37.732482],
                [-122.5067561, 37.7323727],
                [-122.5066387, 37.7319688],
                [-122.5066273, 37.731939],
                [-122.5066106, 37.7319109],
                [-122.506581, 37.7318869],
                [-122.5065404, 37.731872],
                [-122.5064982, 37.7318679],
                [-122.5064615, 37.731878],
                [-122.5064297, 37.7318936],
                [-122.5063553, 37.7317985],
                [-122.5063872, 37.7317679],
                [-122.5064106, 37.7317374],
                [-122.5064136, 37.7317109],
                [-122.5063998, 37.7316828],
                [-122.5063753, 37.7316581],
                [-122.5061296, 37.7314636],
                [-122.5061417, 37.731453],
                [-122.5060145, 37.7313791],
                [-122.5057839, 37.7312678],
                [-122.5054352, 37.7311479],
                [-122.5043701, 37.7310447],
                [-122.5042805, 37.7310343],
                [-122.5041861, 37.7310189],
                [-122.5041155, 37.7310037],
                [-122.5036126, 37.7308562]
              ]
            ]
          },
          "street": "San Francisco Zoo",
          "has_type": [
            {
              "uid": "_:zoo",
              "loc_type": "Zoo"
            }
          ]
        },
        {
          "name": "Flamingo Park",
          "location": {
            "type": "Polygon",
            "coordinates": [
              [
                [-122.5033039, 37.7334601],
                [-122.5032811, 37.7334601],
                [-122.503261, 37.7334601],
                [-122.5032208, 37.7334495],
                [-122.5031846, 37.7334357],
                [-122.5031806, 37.7334718],
                [-122.5031685, 37.7334962],
                [-122.5031336, 37.7335078],
                [-122.503128, 37.7335189],
                [-122.5031222, 37.7335205],
                [-122.5030954, 37.7335269],
                [-122.5030692, 37.7335444],
                [-122.5030699, 37.7335677],
                [-122.5030813, 37.7335868],
                [-122.5031034, 37.7335948],
                [-122.5031511, 37.73359],
                [-122.5031933, 37.7335916],
                [-122.5032228, 37.7336022],
                [-122.5032697, 37.7335937],
                [-122.5033194, 37.7335874],
                [-122.5033515, 37.7335693],
                [-122.5033723, 37.7335518],
                [-122.503369, 37.7335068],
                [-122.5033603, 37.7334702],
                [-122.5033462, 37.7334474],
                [-122.5033073, 37.733449],
                [-122.5033039, 37.7334601]
              ]
            ]
          },
          "street": "San Francisco Zoo",
          "has_type": [
            {
              "uid": "_:zoo"
            }
          ]
        },
        {
          "name": "Peace Lantern",
          "location": {
            "type": "Point",
            "coordinates": [-122.4705776, 37.7701084]
          },
          "street": "Golden Gate Park",
          "has_type": [
            {
              "uid": "_:attraction"
            }
          ]
        },
        {
          "name": "Buddha",
          "location": {
            "type": "Point",
            "coordinates": [-122.469942, 37.7703183]
          },
          "street": "Golden Gate Park",
          "has_type": [
            {
              "uid": "_:attraction"
            }
          ]
        },
        {
          "name": "Japanese Tea Garden",
          "location": {
            "type": "Polygon",
            "coordinates": [
              [
                [-122.4692131, 37.7705116],
                [-122.4698998, 37.7710069],
                [-122.4702431, 37.7710137],
                [-122.4707248, 37.7708919],
                [-122.4708911, 37.7701541],
                [-122.4708428, 37.7700354],
                [-122.4703492, 37.7695011],
                [-122.4699255, 37.7693989],
                [-122.4692131, 37.7705116]
              ]
            ]
          },
          "street": "Golden Gate Park",
          "has_type": [
            {
              "uid": "_:attraction"
            }
          ]
        }
      ]
    }
  ]
}
```

*Note: If this mutation syntax is new to you, refer to the
[first tutorial](/introduction) to learn the basics of mutations in Dgraph.*

Run the query below to fetch the entire graph:

```graphql
{
  entire_graph(func: has(city)) {
    city
    has_location {
    name
    has_type {
      loc_type
      }
    }
  }
}
```

*Note: Check the [second tutorial](./basic-operations) if you want to learn more
about traversal queries like the above one.*

Here's our graph!

![full graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/d-full-graph.png)

Our graph has:

* One blue `city node`. We just have one node which represents the city of
  `San Francisco`.
* The green ones are the `location` nodes. We have a total of 13 locations.
* The pink nodes represent the `location types`. We have four kinds of locations
  in our dataset: `museum`, `zoo`, `hotel`, and `tourist attractions`.

You can also see that Dgraph has auto-detected the data types of the predicates
from the schema tab, and the location predicate has been auto-assigned `geo`
type.

![type detection](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/e-schema.png)

*Note: Check out the [previous tutorial](./types-and-operations) to know more
about data types in Dgraph.*

Before we start, please say Hello to `Mary`, a zoologist who has dedicated her
research for the cause of conserving various bird species.

For the rest of the tutorial, let's help Mary and her team of zoologists in
their mission to conserving birds.

## Enter San Francisco: Hotel booking

Several research projects done by Mary suggested that Flamingos thrive better
when there are abundant water bodies for their habitat.

Her team got approval for expanding the water source for the Flamingos in the
San Francisco Zoo, and her team is ready for a trip to San Francisco with Mary
remotely monitoring the progress of the team.

Her teammates wish to stay close to the `Golden Gate Bridge` so that they could
cycle around the Golden gate, enjoy the breeze, and the sunrise every morning.

Let's help them find a hotel which is within a reasonable distance from the
`Golden Gate Bridge`, and we'll do so using Dgraph's geolocation functions.

Dgraph provides a variety of functions to query geolocation data. To use them,
you have to set the `geo` index first.

Go to the Schema tab and set the index on the `location` predicate.

![geo-index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/f-index.png)

After setting the `geo` index on the `location` predicate, you can use Dgraph's
built-in function `near` to find the hotels near the Golden gate bridge.

Here is the syntax of the `near` function:
`near(geo-predicate, [long, lat], distance)`.

The [`near` function](/dgraph/graphql/schema/directives/search#near) matches and
returns all the geo-predicates stored in the database which are within
`distance meters` of geojson coordinate `[long, lat]` provided by the user.

Let's search for hotels within 7KM of from a point on the Golden Gate bridge.

Go to the query tab, paste the query below and click Run.

```graphql
{
  find_hotel(func: near(location, [-122.479784,37.82883295],7000) )  {
    name
    has_type {
      loc_type
    }
  }
}
```

![geo-index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/g-near-1.png)

Wait! The search returns not just the hotels, but also all other locations
within 7 Km from the point coordinate on the `Golden Gate Bridge`.

Let's use the `@filter` function to filter for search results containing only
the hotels. You can visit our [third tutorial](./types-and-operations) of the
series to refresh our previous discussions around using the `@filter` directive.

```graphql
{
  find_hotel(func: near(location, [-122.479784,37.82883295],7000)) {
    name
    has_type @filter(eq(loc_type, "Hotel")){
      loc_type
    }
  }
}
```

Oops, we forgot to add an index while using the `eq` comparator in the filter.

![geo-index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/h-near-2.png)

Let's add a `hash` index to the `loc_type` and re-run the query.

![geo-index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/i-near-3.png)

![geo-index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/j-near-4.png)

*Note: Refer to the [third tutorial](./types-and-operations) of the series to
learn more about hash index and comparator functions in Dgraph.*

The search result still contains nodes representing locations which are not
hotels. That's because the root query first finds all the location nodes which
are within 7KM from the specified point location, and then it applies the filter
while selectively traversing to the `location type nodes`.

Only the predicates in the location nodes can be filtered at the root level, and
you cannot filter the `location types` without traversing to the
`location type nodes`.

We have the filter to select only the `hotels` while we traverse the
`location type nodes`. Can we cascade or bubble up the filter to the root level,
so that, we only have `hotels` in the final result?

Yes you can! You can do by using the `@cascade` directive.

The `@cascade` directive helps you `cascade` or `bubble up` the filters applied
to your inner query traversals to the root level nodes, by doing so, we get only
the locations of `hotels` in our result.

```graphql
{
  find_hotel(func: near(location, [-122.479784,37.82883295],7000)) @cascade {
   name
   price_per_night
   has_type @filter(eq(loc_type,"Hotel")){
     loc_type
    }
  }
}
```

![geo-index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/k-near-5.png)

Voila! You can see in the result that, after adding the `@cascade` directive in
the query, only the locations with type `hotel` appear in the result.

We have two hotels in the result, and one of them is over their budget of 300$per night. Let's add another filter to search for Hotels priced below$300 per
night.

The price information of every hotel is stored in the `location nodes` along
with their coordinates, hence the filter on the pricing should be at the root
level of the query, not at the level we traverse the location type nodes.

Before you jump onto run the query, don't forget to add an index on the
`price_per_night` predicate.

![geo-index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/l-float-index.png)

```graphql
{
  find_hotel(func: near(location, [-122.479784,37.82883295],7000)) @cascade @filter(le(price_per_night, 300)){
    name
    price_per_night
    has_type @filter(eq(loc_type,"Hotel")){
      loc_type
    }
  }
}

```

![geo-index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/m-final-result.png)

Now we have a hotel well within the budget, and also close to the Golden Gate
Bridge!

## Summary

In this tutorial, we learned about geolocation capabilities in Dgraph, and
helped Mary's team book a hotel near Golden bridge.

In the next tutorial, we'll showcase more geolocation functionalities in Dgraph
and assist Mary's team in their quest for conserving Flamingo's.

See you all in the next tutorial. Till then, happy Graphing!

Remember to click the "Join our community" button below and subscribe to our
newsletter to get the latest tutorial right into your inbox.

## What's next?

* Go to [Clients](/dgraph/sdks/overview) to see how to communicate with Dgraph
  from your app.
* A wider range of queries can also be found in the
  [Query Language](/dgraph/dql/query) reference.
* See [Deploy](/dgraph/self-managed/overview) if you wish to run Dgraph in a
  cluster.

## Need help

* Please use [discuss.hypermode.com](https://discuss.hypermode.com) for
  questions, feature requests, bugs, and discussions.


# Get Started with Dgraph
Source: https://docs.hypermode.com/dgraph/guides/get-started-with-dgraph/introduction



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

**Welcome to getting started with Dgraph.**

[Dgraph](https://github.com/hypermodeinc/dgraph) is an open source,
transactional, distributed, native Graph Database. Here is the first tutorial of
the get started series on using Dgraph.

In this tutorial, we'll learn about:

* Running Dgraph using the `dgraph/standalone` docker image.
* Running the following basic operations using Dgraph's UI Ratel,
* Creating a node.
* Creating an edge between two nodes.
* Querying for the nodes.

Our use case represents a person named Ann, age 28, who follows on social media,
a person named Ben, age 31.

You can see the accompanying video below.

<iframe width="560" height="315" src="https://www.youtube.com/embed/u73ovhDCPQQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen />

***

## Running Dgraph

Running the `dgraph/standalone` docker image is the quickest way to get started
with Dgraph. This standalone image is meant for quickstart purposes only. It is
not recommended for production environments.

Ensure that [Docker](https://docs.docker.com/install/) is installed and running
on your machine.

Now, it's just a matter of running the following command, and you have Dgraph up
and running.

```sh
docker run --rm -it -p 8080:8080 -p 9080:9080 dgraph/standalone:latest
```

### Nodes and relationships

The mental picture of the use case may be a graph with 2 nodes representing the
2 persons and an relationship representing the fact that "Ann" follows "Ben" :

![A simple graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/gs-1.png)

Dgraph is using those very same concepts, making it simple to store and
manipulate your data.

We then create two nodes, one representing the information we know about `Ann`
and one holding the information about `Ben`.

What we know is the `name` and the `age` of those persons.

We also know that Ann follows Jessica. This is also stored as a relationship
between the two nodes.

### Using Ratel

Launch Ratel image

```sh
docker run --rm -d -p 8000:8000 dgraph/ratel:latest
```

Visit [http://localhost:8000](http://localhost:8000) from your browser, and you
are able to access it.

![Ratel](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/gs-2.png)

We'll be using the Console tab of Ratel.

![Ratel console tab](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/gs-3.png)

### Mutations using Ratel

The create, update, and delete operations in Dgraph are called mutations.

In Ratel console, select the `Mutate` tab and paste the following mutation into
the text area.

```json
{
  "set": [
    {
      "name": "Ann",
      "age": 28,
      "follows": {
        "name": "Ben",
        "age": 31
      }
    }
  ]
}
```

The query creates an entity and saves the predicates `name` and `age` with the
corresponding values.

It also creates a predicate 'follows' for that entity but the value isn't a
literal (string, int, float, boolean).

So Dgraph also creates a second entity that's the object of this predicate. This
second entity has itself some predicates (`name` and `age`).

Let's execute this mutation. Click Run!

![Query-gif](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/mutate-example.gif)

You can see in the response that two UIDs (Universal IDentifiers) have been
created. The two values in the `"uids"` field of the response correspond to the
two entities created for Ann and Ben.

### Querying using the has function

Now, let's run a query to visualize the graph which we just created. We'll be
using Dgraph's `has` function. The expression `has(name)` returns all the
entities with a predicate `name` associated with them.

```sh
{
  people(func: has(name)) {
    name
    age
  }
}
```

Go to the `Query` tab this time and type in the query. Then, click `Run` on the
top right of the screen.

![query-1](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/query-1.png)

Ratel renders a graph visualization of the result.

Just click any of them, notice that the nodes are assigned UIDs, matching the
ones, we saw in the mutation's response.

You can also view the JSON results in the JSON tab on the right.

![query-2](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/query-2.png)

#### Understanding the query

![Illustration with explanation](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/explain-query-2.JPG)

The first part of the query is the user-defined function name. In our query, we
have named it as `people`. However, you could use any other name.

The `func` parameter has to be associated with a built-in function of Dgraph.
Dgraph offers a variety of built-in functions. The `has` function is one of
them. Check out the [query language guide](/dgraph/dql/schema) to know more
about other built-in functions in Dgraph.

The inner fields of the query are similar to the column names in a SQL select
statement or to a GraphQL query!

You can easily specify which predicates you want to get back.

```graphql
{
  people(func: has(name)) {
    name
  }
}
```

Similarly, you can use the `has` function to find all entities with the `age`
predicate.

```graphql
{
  people(func: has(age)) {
    name
  }
}
```

### Flexible schema

Dgraph doesn't enforce a structure or a schema. Instead, you can start entering
your data immediately and add constraints as needed.

Let's look at this mutation.

```json
{
  "set": [
    {
      "name": "Balaji",
      "age": 23,
      "country": "India"
    },
    {
      "name": "Daniel",
      "age": 25,
      "city": "San Diego"
    }
  ]
}
```

We're creating two entities, while the first entity has predicates `name`,
`age`, and `country`, the second one has `name`, `age`, and `city`.

Schemas aren't needed initially. Dgraph creates new predicates as they appear in
your mutations. This flexibility can be beneficial, but if you prefer to force
your mutations to follow a given schema there are options available that we'll
explore in the next tutorial.

## Wrapping up

In this tutorial, we learned the basics of Dgraph, including how to run the
database, add new entities and predicates, and query them back.

Check out our next tutorial of the getting started series
[here](./basic-operations).

## Need help

* Please use [discuss.hypermode.com](https://discuss.hypermode.com) for
  questions, feature requests, bugs, and discussions.


# Get Started with Dgraph -  Multi-language strings
Source: https://docs.hypermode.com/dgraph/guides/get-started-with-dgraph/multi-language-strings



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

**Welcome to the fourth tutorial of getting started with Dgraph.**

In the [previous tutorial](./types-and-operations), we learned about Datatypes,
Indexing, Filtering, and Reverse traversals in Dgraph.

In this tutorial, we'll learn about using multi-language strings and operations
on them using the language tags.

You can see the accompanying video below.

<iframe width="560" height="315" src="https://www.youtube.com/embed/_lDE9QXHZC0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen />

***

## Strings and languages

Strings values in Dgraph are of UTF-8 format. Dgraph also supports values for
string predicate types in multiple languages. The multi-lingual capability is
particularly useful to build features, which requires you to store the same
information in multiple languages.

Let's learn more about them!

Let's start with building a simple food review Graph. Here's the Graph model.

![model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-graph-model.jpg)

The above Graph has three entities: Food, Comment, and Country.

The nodes in the Graph represent these entities.

For the rest of the tutorial, let's call the node representing a food item as a
`food` node. The node representing a review comment as a `review` node, and the
node representing the country of origin as a `country` node.

Here's the relationship between them:

* Every food item is connected to its reviews via the `review` edge.
* Every food item is connected to its country of origin via the `origin` edge.

Let's add some reviews for some fantastic dishes!

How about spicing it up a bit before we do that?

Let's add the reviews for these dishes in the native language of their country
of origin.

Let's go, amigos!

```json
{
  "set": [
    {
      "food_name": "Hamburger",
      "review": [
        {
          "comment": "Tastes very good"
        }
      ],
      "origin": [
        {
          "country": "United states of America"
        }
      ]
    },
    {
      "food_name": "Carrillada",
      "review": [
        {
          "comment": "Sabe muy sabroso"
        }
      ],
      "origin": [
        {
          "country": "Spain"
        }
      ]
    },
    {
      "food_name": "Pav Bhaji",
      "review": [
        {
          "comment": "स्वाद बहुत अच्छा है"
        }
      ],
      "origin": [
        {
          "country": "India"
        }
      ]
    },
    {
      "food_name": "Borscht",
      "review": [
        {
          "comment": "очень вкусно"
        }
      ],
      "origin": [
        {
          "country": "Russia"
        }
      ]
    },
    {
      "food_name": "mapo tofu",
      "review": [
        {
          "comment": "真好吃"
        }
      ],
      "origin": [
        {
          "country": "China"
        }
      ]
    }
  ]
}
```

*Note: If this mutation syntax is new to you, refer to the
[first tutorial](/introduction) to learn basics of mutation in Dgraph.*

Here's our Graph!

![full graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-full-graph.png)

Our Graph has:

* Five blue food nodes.
* The green nodes represent the country of origin of these food items.
* The reviews of the food items are in pink.

You can also see that Dgraph has auto-detected the data types of the predicates.
You can check that out from the schema tab.

![full graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/c-schema.png)

*Note: Check out the [previous tutorial](./types-and-operations) to know more
about data types in Dgraph.*

Let's write a query to fetch all the food items, their reviews, and their
country of origin.

Go to the query tab, paste the query, and click Run.

```graphql
{
  food_review(func: has(food_name)) {
    food_name
      review {
        comment
      }
      origin {
        country
      }
  }
}
```

*Note: Check the [second tutorial](./basic-operations) if you want to learn more
about traversal queries like the above one*

Now, Let's fetch only the food items and their reviews,

```graphql
{
  food_review(func: has(food_name)) {
    food_name
      review {
        comment
      }
  }
}
```

As expected, these comments are in different languages.

![full graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/b-comments.png)

But can we fetch the reviews based on their language? Can we write a query which
says: *Hey Dgraph, can you give me only the reviews written in Chinese?*

That's possible, but only if you provide additional information about the
language of the string data. You can do so by using language tags. While adding
the string data using mutations, you can use the language tags to specify the
language of the string predicates.

Let's see the language tags in action!

I've heard that Sushi is yummy! Let's add a review for `Sushi` in more than one
language. We'll be writing the review in three different languages: English,
Japanese, and Russian.

Here's the mutation to do so.

```json
{
  "set": [
    {
      "food_name": "Sushi",
      "review": [
        {
          "comment": "Tastes very good",
          "comment@jp": "とても美味しい",
          "comment@ru": "очень вкусно"
        }
      ],
      "origin": [
        {
          "country": "Japan"
        }
      ]
    }
  ]
}
```

Let's take a closer look at how we assigned values for the `comment` predicate
in different languages.

We used the language tags (@ru, @jp) as a suffix for the `comment` predicate.

In the above mutation:

* We used the `@ru` language tag to add the comment in Russian:
  `"comment@ru": "очень вкусно"`.

* We used the `@jp` language tag to add the comment in Japanese:
  `"comment@jp": "とても美味しい"`.

* The comment in `English` is untagged: `"comment": "Tastes very good"`.

In the mutation above, Dgraph creates a new node for the reviews, and stores
`comment`, `comment@ru`, and `comment@jp` in different predicates inside the
same node.

*Note: If you're not clear about basic terminology like `predicates`, do read
the [first tutorial](./introduction).*

Let's run the above mutation.

Go to the mutate tab, paste the mutation, and click Run.

![lang error](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/d-lang-error.png)

We got an error! Using the language tag requires you to add the `@lang`
directive to the schema.

Follow the instructions below to add the `@lang` directive to the `comment`
predicate.

* Go to the Schema tab.
* Click on the `comment` predicate.
* Tick mark the `lang` directive.
* Click on the `Update` button.

![lang error](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/e-update-lang.png)

Let's re-run the mutation.

![lang error](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/f-mutation-success.png)

Success!

Again, remember that using the above mutation, we have added only one review for
Sushi, not three different reviews!

But, if you want to add three different reviews, here's how you do it.

Adding the review in the format below creates three nodes, one for each of the
comments. But, do it only when you're adding a new review, not to represent the
same review in different languages.

```json
"review": [
  {
    "comment": "Tastes very good"
  },
  {
    "comment@jp": "とても美味しい"
  },
  {
    "comment@ru": "очень вкусно"
  }
]
```

Dgraph allows any strings to be used as language tags. But, it's highly
recommended only to use the ISO standard code for language tags.

By following the standard, you eliminate the need to communicate the tags to
your team or to document it somewhere.
[Click here](https://www.w3schools.com/tags/ref_language_codes.asp) to see the
list of ISO standard codes for language tags.

In our next section, let's make use of the language tags in our queries.

## Querying using language tags

Let's obtain the review comments only for `Sushi`.

In the [previous article](./types-and-operations), we learned about using the
`eq` operator and the `hash` index to query for string predicate values.

Using that knowledge, let's first add the `hash` index for the `food_name`
predicate.

![hash index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/g-hash.png)

Now, go to the query tab, paste the query in the text area, and click Run.

```graphql
{
  food_review(func: eq(food_name,"Sushi")) {
    food_name
      review {
        comment
      }
  }
}
```

![hash index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/h-comment.png)

By default, the query only returns the untagged comment.

But you can use the language tag to query specifically for a review comment in a
given language.

Let's query for a review for `Sushi` in Japanese.

```graphql
{
  food_review(func: eq(food_name,"Sushi")) {
    food_name
    review {
      comment@jp
    }
  }
}
```

![Japanese](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/i-japanese.png)

Now, let's query for a review for `Sushi` in Russian.

```graphql
{
  food_review(func: eq(food_name,"Sushi")) {
    food_name
    review {
      comment@ru
    }
  }
}
```

![Russian](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/j-russian.png)

You can also fetch all the comments for `Sushi` written in any language.

```graphql
{
  food_review(func: eq(food_name,"Sushi")) {
    food_name
    review {
      comment@*
    }
  }
}
```

![Russian](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/k-star.png)

Here is the table with the syntax for various ways of making use of language
tags while querying.

| Syntax           | Result                                                                                                                                                        |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| comment          | Look for an untagged string; return nothing if no untagged review exists.                                                                                     |
| comment\@.       | Look for an untagged string, if not found, then return review in any language. But, this returns only a single value.                                         |
| comment\@jp      | Look for comment tagged `@jp`. If not found, the query returns nothing.                                                                                       |
| comment\@ru      | Look for comment tagged `@ru`. If not found, the query returns nothing.                                                                                       |
| comment\@jp:.    | Look for comment tagged `@jp` first. If not found, then find the untagged comment. If that's not found too, return anyone comment in other languages.         |
| comment\@jp:ru   | Look for comment tagged `@jp`, then `@ru`. If neither is found, it returns nothing.                                                                           |
| comment\@jp:ru:. | Look for comment tagged `@jp`, then `@ru`. If both not found, then find the untagged comment. If that's not found too, return any other comment if it exists. |
| comment@\*       | Return all the language tags, including the untagged.                                                                                                         |

If you remember, we had initially added a Russian dish `Borscht` with its review
in `Russian`.

![Russian](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/l-russian.png)

If you notice, we haven't used the language tag `@ru` for the review written in
Russian.

Hence, if we query for all the reviews written in `Russian`, the review for
`Borscht` doesn't make it to the list.

Only the review for `Sushi,` written in `Russian`, makes it to the list.

![Russian](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/m-sushi.png)

So, here's the lesson of the day!

> If you are representing the same information in different languages, don't
> forget to add your language tags!

## Summary

In this tutorial, we learned about using multi-language string and operations on
them using the language tags.

The usage of tags is not just restricted to multi-lingual strings. Language tags
are just a use case of Dgraph's capability to tag data.

In the next tutorial, we'll continue our quest into the string types in Dgraph.
We'll explore the string type indices in detail.

Sounds interesting?

Check out our next tutorial of the getting started series
[here](./string-indicies).

## Need Help

* Please use [discuss.hypermode.com](https://discuss.hypermode.com) for
  questions, feature requests, bugs, and discussions.


# Get Started with Dgraph - String Indices
Source: https://docs.hypermode.com/dgraph/guides/get-started-with-dgraph/string-indicies



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

**Welcome to the fifth tutorial of getting started with Dgraph.**

In the [previous tutorial](./multi-language-strings), we learned about using
multi-language strings and operations on them using
[language tags](https://www.w3schools.com/tags/ref_language_codes.asp).

In this tutorial, we'll model tweets in Dgraph and, using it, we'll learn more
about string indices in Dgraph.

We'll specifically learn about:

* Modeling tweets in Dgraph.
* Using String indices in Dgraph
  * Querying twitter users using the `hash` index.
  * Comparing strings using the `exact` index.
  * Searching for tweets based on keywords using the `term` index.

Here's the complimentary video for this blog post. It'll walk you through the
steps of this getting started episode.

<iframe width="560" height="315" src="https://www.youtube.com/embed/Ww5cwixwkHo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen />

Let's start analyzing the anatomy of a real tweet and figure out how to model it
in Dgraph.

The accompanying video of the tutorial will be out shortly, so stay tuned to
[our YouTube channel](https://www.youtube.com/channel/UCghE41LR8nkKFlR3IFTRO4w).

## Modeling a tweet in Dgraph

Here's a sample tweet.

```Test tweet for the fifth episode of getting started series with @dgraphlabs.
Wait for the video of the fourth one by @francesc the coming Wednesday! #GraphDB #GraphQL

— Karthic Rao | karthic.eth (@hackintoshrao) November 13, 2019
```

Let's dissect the tweet above. Here are the components of the tweet:

* **The Author**

  The author of the tweet is the user `@hackintoshrao`.

* **The Body**

  This component is the content of the tweet.

  > Test tweet for the fifth episode of getting started series with @dgraphlabs.
  > Wait for the video of the fourth one by @francesc the coming Wednesday!
  > \#GraphDB #GraphQL

* **The Hashtags**

  Here are the hashtags in the tweet: `#GraphQL` and `#GraphDB`.

* **The Mentions**

  A tweet can mention other twitter users.

  Here are the mentions in the tweet above: `@dgraphlabs` and `@francesc`.

Before we model tweets in Dgraph using these components, let's recap the design
principles of a graph model:

> `Nodes` and `Edges` are the building blocks of a graph model. May it be a
> sale, a tweet, user info, any concept or an entity is represented as a node.
> If any two nodes are related, represent that by creating an edge between them.

With the above design principles in mind, let's go through components of a tweet
and see how we could fit them into Dgraph.

**The Author**

The Author of a tweet is a twitter user. We should use a node to represent this.

**The Body**

We should represent every tweet as a node.

**The Hashtags**

It is advantageous to represent a hashtag as a node of its own. It gives us
better flexibility while querying.

Though you can search for hashtags from the body of a tweet, it's not efficient
to do so. Creating unique nodes to represent a hashtag, allows you to write
performant queries like the following: *Hey Dgraph, give me all the tweets with
hashtag #graphql*

**The Mentions**

A mention represents a twitter user, and we've already modeled a user as a node.
Therefore, we represent a mention as an edge between a tweet and the users
mentioned.

### The Relationships

We have three types of nodes: `User`, `Tweet,` and `Hashtag`.

![graph nodes](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-nodes.jpg)

Let's look at how these nodes might be related to each other and model their
relationship as an edge between them.

**The User and Tweet nodes**

There's a two-way relationship between a `Tweet` and a `User` node.

* Every tweet is authored by a user, and a user can author many tweets.

Let's name the edge representing this relationship as `authored` .

An `authored` edge points from a `User` node to a `Tweet` node.

* A tweet can mention many users, and users can be mentioned in many tweets.

Let's name the edge which represents this relationship as `mentioned`.

A `mentioned` edge points from a `Tweet` node to a `User` node. These users are
the ones who are mentioned in the tweet.

![graph nodes](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-tweet-user.jpg)

**The tweet and the hashtag nodes**

A tweet can have one or more hashtags. Let's name the edge, which represents
this relationship as `tagged_with`.

A `tagged_with` edge points from a `Tweet` node to a `Hashtag` node. These
hashtag nodes correspond to the hashtags in the tweets.

![graph nodes](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-tagged.jpg)

**The Author and hashtag nodes**

There's no direct relationship between an author and a hashtag node. Hence, we
don't need a direct edge between them.

Our graph model of a tweet is ready! Here's it's.

![tweet model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-graph-model-2.jpg)

Here is the graph of our sample tweet.

![tweet model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/c-tweet-model.jpg)

Let's add a couple of tweets to the list.

```
So many good talks at #graphqlconf, next year I'll make sure to be *at least* in the audience!

Also huge thanks to the live tweeting by @dgraphlabs for alleviating the FOMO 😊#GraphDB ♥️ #GraphQL https://t.co/5uDpbswFZi

— francesc (@francesc) June 21, 2019
Let's Go and catch @francesc at @Gopherpalooza today, as he scans into Go source code by building its Graph in Dgraph!

Be there, as he Goes through analyzing Go source code, using a Go program, that stores data in the GraphDB built in Go!#golang #GraphDB #Databases #Dgraph pic.twitter.com/sK90DJ6rLs

— Dgraph Labs (@dgraphlabs) November 8, 2019
```

We'll be using these two tweets and the sample tweet, which we used in the
beginning as our dataset. Open Ratel, go to the mutate tab, paste the mutation,
and click Run.

```json
{
  "set": [
    {
      "user_handle": "hackintoshrao",
      "user_name": "Karthic Rao",
      "uid": "_:hackintoshrao",
      "authored": [
        {
          "tweet": "Test tweet for the fifth episode of getting started series with @dgraphlabs. Wait for the video of the fourth one by @francesc the coming Wednesday!\n#GraphDB #GraphQL",
          "tagged_with": [
            {
              "uid": "_:graphql",
              "hashtag": "GraphQL"
            },
            {
              "uid": "_:graphdb",
              "hashtag": "GraphDB"
            }
          ],
          "mentioned": [
            {
              "uid": "_:francesc"
            },
            {
              "uid": "_:dgraphlabs"
            }
          ]
        }
      ]
    },
    {
      "user_handle": "francesc",
      "user_name": "Francesc Campoy",
      "uid": "_:francesc",
      "authored": [
        {
          "tweet": "So many good talks at #graphqlconf, next year I'll make sure to be *at least* in the audience!\nAlso huge thanks to the live tweeting by @dgraphlabs for alleviating the FOMO😊\n#GraphDB ♥️ #GraphQL",
          "tagged_with": [
            {
              "uid": "_:graphql"
            },
            {
              "uid": "_:graphdb"
            },
            {
              "hashtag": "graphqlconf"
            }
          ],
          "mentioned": [
            {
              "uid": "_:dgraphlabs"
            }
          ]
        }
      ]
    },
    {
      "user_handle": "dgraphlabs",
      "user_name": "Dgraph Labs",
      "uid": "_:dgraphlabs",
      "authored": [
        {
          "tweet": "Let's Go and catch @francesc at @Gopherpalooza today, as he scans into Go source code by building its Graph in Dgraph!\nBe there, as he Goes through analyzing Go source code, using a Go program, that stores data in the GraphDB built in Go!\n#golang #GraphDB #Databases #Dgraph ",
          "tagged_with": [
            {
              "hashtag": "golang"
            },
            {
              "uid": "_:graphdb"
            },
            {
              "hashtag": "Databases"
            },
            {
              "hashtag": "Dgraph"
            }
          ],
          "mentioned": [
            {
              "uid": "_:francesc"
            },
            {
              "uid": "_:dgraphlabs"
            }
          ]
        },
        {
          "uid": "_:gopherpalooza",
          "user_handle": "gopherpalooza",
          "user_name": "Gopherpalooza"
        }
      ]
    }
  ]
}
```

<Note>
  {" "}

  If you're new to Dgraph, and yet to figure out how to run the database and use
  Ratel, we highly recommend reading the [first article of the
  series](/introduction)
</Note>

Here is the graph we built.

![tweet graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/x-all-tweets.png)

Our graph has:

* Five blue twitter user nodes.
* The green nodes are the tweets.
* The blue ones are the hashtags.

Let's start our tweet exploration by querying for the twitter users in the
database.

```
{
  tweet_graph(func: has(user_handle)) {
     user_handle
  }
}
```

![tweet model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/j-users.png)

*Note: If the query syntax above looks not so familiar to you, check out the
[first tutorial](./introduction).*

We have four twitter users: `@hackintoshrao`, `@francesc`, `@dgraphlabs`, and
`@gopherpalooza`.

Now, let's find their tweets and hashtags too.

```graphql
{
  tweet_graph(func: has(user_handle)) {
     user_name
     authored {
      tweet
      tagged_with {
        hashtag
      }
    }
  }
}
```

![tweet model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/y-author-tweet.png)

*Note: If the traversal query syntax in the above query is not familiar to you,
[check out the third tutorial](./types-and-operations) of the series.*

Before we start querying our graph, let's learn a bit about database indices
using a simple analogy.

### What are indices?

Indexing is a way to optimize the performance of a database by minimizing the
number of disk accesses required when a query is processed.

Consider a "Book" of 600 pages, divided into 30 sections. Let's say each section
has a different number of pages in it.

Now, without an index page, to find a particular section that starts with the
letter "F", you have no other option than scanning through the entire book. i.e:
600 pages.

But with an index page at the beginning makes it easier to access the intended
information. You just need to look over the index page, after finding the
matching index, you can efficiently jump to the section by skipping other
sections.

But remember that the index page also takes disk space! Use them only when
necessary.

In our next section,let's learn some interesting queries on our twitter graph.

## String indices and querying

### Hash index

Let's compose a query which says: *Hey Dgraph, find me the tweets of user with
twitter handle equals to `hackintoshrao`.*

Before we do so, we need first to add an index has to the `user_handle`
predicate. We know that there are 5 types of string indices: `hash`, `exact`,
`term`, `full-text`, and `trigram`.

The type of string index to be used depends on the kind of queries you want to
run on the string predicate.

In this case, we want to search for a node based on the exact string value of a
predicate. For a use case like this one, the `hash` index is recommended.

Let's first add the `hash` index to the `user_handle` predicate.

![tweet model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/k-hash.png)

Now, let's use the `eq` comparator to find all the tweets of `hackintoshrao`.

Go to the query tab, type in the query, and click Run.

```graphql
 {
  tweet_graph(func: eq(user_handle, "hackintoshrao")) {
     user_name
     authored {
    tweet
    }
  }
}
```

![tweet model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/z-exact.png)

*Note: Refer to [the third tutorial](./types-and-operations), if you want to
know about comparator functions like `eq` in detail.*

Let's extend the last query also to fetch the hashtags and the mentions.

```graphql
{
  tweet_graph(func: eq(user_handle, "hackintoshrao")) {
     user_name
     authored {
      tweet
      tagged_with {
        hashtag
      }
      mentioned {
        user_name
      }
    }
  }
}
```

![tweet model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/l-hash-query.png)

*Note: If the traversal query syntax in the above query is not familiar to you,
[check out the third tutorial](./types-and-operations) of the series.*

Did you know that string values in Dgraph can also be compared using comparators
like greater-than or less-than?

In our next section, let's see how to run the comparison functions other than
`equals to (eq)` on the string predicates.

### Exact Index

We discussed in the [third tutorial](./types-and-operations) that there five
comparator functions in Dgraph.

Here's a quick recap:

| comparator function name | Full form                |
| ------------------------ | ------------------------ |
| eq                       | equals to                |
| lt                       | less than                |
| le                       | less than or equal to    |
| gt                       | greater than             |
| ge                       | greater than or equal to |

All five comparator functions can be applied to the string predicates.

We have already used the `eq` operator. The other four are useful for
operations, which depend on the alphabetical ordering of the strings.

Let's learn about it with a simple example.

Let's find the twitter accounts which come after `dgraphlabs` in alphabetically
sorted order.

```graphql
{
  using_greater_than(func: gt(user_handle, "dgraphlabs")) {
    user_handle
  }
}
```

![tweet model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/n-exact-error.png)

Oops, we have an error!

You can see from the error that the current `hash` index on the `user_handle`
predicate doesn't support the `gt` function.

To be able to do string comparison operations like the one above, you need first
set the `exact` index on the string predicate.

The `exact` index is the only string index that allows you to use the `ge`,
`gt`, `le`, `lt` comparators on the string predicates.

Remind you that the `exact` index also allows you to use `equals to (eq)`
comparator. But, if you want to just use the `equals to (eq)` comparator on
string predicates, using the `exact` index would be an overkill. The `hash`
index would be a better option, as it's, in general, much more space-efficient.

Let's see the `exact` index in action.

![set exact](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/o-exact-conflict.png)

We again have an error!

Though a string predicate can have more than one index, some of them are not
compatible with each other. One such example is the combination of the `hash`
and the `exact` indices.

The `user_handle` predicate already has the `hash` index, so trying to set the
`exact` index gives you an error.

Let's uncheck the `hash` index for the `user_handle` predicate, select the
`exact` index, and click update.

![set exact](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/p-set-exact.png)

Though Dgraph allows you to change the index type of a predicate, do it only if
it's necessary. When the indices are changed, the data needs to be re-indexed,
and this takes some computing, so it could take a bit of time. While the
re-indexing operation is running, all mutations will be put on hold.

Now, let's re-run the query.

![tweet model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/q-exact-gt.png)

The result contains three twitter handles: `francesc`, `gopherpalooza`, and
`hackintoshrao`.

In the alphabetically sorted order, these twitter handles are greater than
`dgraphlabs`.

Some tweets appeal to us better than others. For instance, I love `Graphs` and
`Go`. Hence, I would surely enjoy tweets that are related to these topics. A
keyword-based search is a useful way to find relevant information.

Can we search for tweets based on one or more keywords related to your
interests?

Yes, we can! Let's do that in our next section.

### The Term index

The `term` index lets you search string predicates based on one or more
keywords. These keywords are called terms.

To be able to search tweets with specific keywords or terms, we need to first
set the `term` index on the tweets.

Adding the `term` index is similar to adding any other string index.

![term set](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/r-term-set.png)

Dgraph provides two built-in functions specifically to search for terms:
`allofterms` and `anyofterms`.

Apart from these two functions, the `term` index only supports the `eq`
comparator. This means any other query functions (like lt, gt, le...) fails when
run on string predicates with the `term` index.

We'll soon take a look at the table containing the string indices and their
supporting query functions. But first, let's learn how to use `anyofterms` and
`allofterms` query functions. Let's write a query to find all tweets with terms
or keywords `Go` or `Graph` in them.

Go the query tab, paste the query, and click Run.

```graphql
{
  find_tweets(func: anyofterms(tweet, "Go Graph")) {
    tweet
  }
}
```

Here's the matched tweet from the query response:

```json
{
  "tweet": "Let's Go and catch @francesc at @Gopherpalooza today, as he scans into Go source code by building its Graph in Dgraph!\nBe there, as he Goes through analyzing Go source code, using a Go program, that stores data in the GraphDB built in Go!\n#golang #GraphDB #Databases #Dgraph "
}
```

![go graph set](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/s-go-graph.png)

*Note: Check out [the first tutorial](./introduction) if the query syntax, in
general, is not familiar to you*

The `anyofterms` function returns tweets which have either of `Go` or `Graph`
keyword.

In this case, we've used only two terms to search for (`Go` and `Graph`), but
you can extend for any number of terms to be searched or matched.

The result has one of the three tweets in the database. The other two tweets
don't make it to the result since they don't have either of the terms `Go` or
`Graph`.

It is also important to notice that the term search functions (`anyofterms` and
`allofterms`) are insensitive to case and special characters.

This means, if you search for the term `GraphQL`, the query returns a positive
match for all of the following terms found in the tweets: `graphql`, `graphQL`,
`#graphql`, `#GraphQL`.

Now, let's find tweets that have either of the terms `Go` or `GraphQL` in them.

```graphql
{
  find_tweets(func: anyofterms(tweet, "Go GraphQL")) {
    tweet
  }
}
```

![Go Graphql](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/t-go-graphql-all.png)

Oh wow, we have all the three tweets in the result. This means, all of the three
tweets have either of the terms `Go` or `GraphQL`.

Now, how about finding tweets that contain both the terms `Go` and `GraphQL` in
them. We can do it by using the `allofterms` function.

```graphql
{
  find_tweets(func: allofterms(tweet, "Go GraphQL")) {
    tweet
  }
}
```

![Go Graphql](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/u-allofterms.png)

We have an empty result. None of the tweets have both the terms `Go` and
`GraphQL` in them.

Besides `Go` and `Graph`, I'm also a big fan of `GraphQL` and `GraphDB`.

Let's find out tweets that contain both the keywords `GraphQL` and `GraphDB` in
them.

![Graphdb-GraphQL](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/v-graphdb-graphql.png)

We have two tweets in a result which has both the terms `GraphQL` and `GraphDB`.

```
{
  "tweet": "Test tweet for the fifth episode of getting started series with @dgraphlabs. Wait for the video of the fourth one by @francesc the coming Wednesday!\n#GraphDB #GraphQL"
},
{
  "tweet": "So many good talks at #graphqlconf, next year I'll make sure to be *at least* in the audience!\nAlso huge thanks to the live tweeting by @dgraphlabs for alleviating the FOMO😊\n#GraphDB ♥️ #GraphQL"
}
```

Before we wrap up, here's the table containing the three string indices we
learned about, and their compatible built-in functions.

| Index | Valid query functions      |
| ----- | -------------------------- |
| hash  | eq                         |
| exact | eq, lt, gt, le, ge         |
| term  | eq, allofterms, anyofterms |

## Summary

In this tutorial, we modeled a series of tweets and set up the exact, term, and
hash indices in order to query them.

Did you know that Dgraph also offers more powerful search capabilities like
full-text search and regular expressions based search?

In the next tutorial, we'll explore these features and learn about more powerful
ways of searching for your favorite tweets!

Sounds interesting? Then see you all soon in the next tutorial. Till then, happy
Graphing!

Check out our next tutorial of the getting started series
[here](./advanced-text-search).

## Need Help

* Please use [discuss.hypermode.com](https://discuss.hypermode.com) for
  questions, feature requests, bugs, and discussions.


# Get Started with Dgraph - Types and Operations
Source: https://docs.hypermode.com/dgraph/guides/get-started-with-dgraph/types-and-operations



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

**Welcome to the third tutorial of getting started with Dgraph.**

In the [previous tutorial](./basic-operations), we learned about CRUD operations
using UIDs. We also learned about traversals and recursive traversals.

In this tutorial, we'll learn about Dgraph's basic types and how to query for
them. Specifically, we'll learn about:

* Basic data types in Dgraph.
* Querying for predicate values.
* Indexing.
* Filtering nodes.
* Reverse traversing.

Check out the accompanying video:

<iframe width="560" height="315" src="https://www.youtube.com/embed/f401or0hg5E" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen />

Let's start by building the graph of a simple blog app. Here's the Graph model
of our app:

![main graph model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-main-graph.JPG)

This graph has three entities, Author, Blog posts, and Tags. The nodes in the
graph represent these entities. For the rest of the tutorial, we'll call the
nodes representing a blog as a `blog post` node and the node presenting a `tag`
as a `tag node`, and so on.

You can see from the graph model that these entities are related:

* Every Author has one or more blog posts.

The `published` edge relates the blogs to their authors. These edges start from
an `author node` and point to a `blog post` node.

* Every Blog post has one or more tags.

The `tagged` edge relates the blog posts to their tags. These edges emerge from
a `blog post node` and point to a `tag node`.

Let's build our graph.

Go to Ratel, click the mutate tab, paste the following mutation, and click Run.

```json
{
  "set": [
    {
      "author_name": "John Campbell",
      "rating": 4.1,
      "published": [
        {
          "title": "Dgraph's recap of GraphQL Conf - Berlin 2019",
          "url": "https://hypermode.com/blog/graphql-conf-19/",
          "content": "We took part in the recently held GraphQL conference in Berlin. The experience was fascinating, and we were amazed by the high voltage enthusiasm in the GraphQL community. Now, we couldn’t help ourselves from sharing this with Dgraph’s community! This is the story of the GraphQL conference in Berlin.",
          "likes": 100,
          "dislikes": 4,
          "publish_time": "2018-06-25T02:30:00",
          "tagged": [
            {
              "uid": "_:graphql",
              "tag_name": "graphql"
            },
            {
              "uid": "_:devrel",
              "tag_name": "devrel"
            }
          ]
        },
        {
          "title": "Dgraph Labs wants you!",
          "url": "https://hypermode.com/blog/hiring-19/",
          "content": "We recently announced our successful Series A fundraise and, since then, many people have shown interest to join our team. We are very grateful to have so many people interested in joining our team! We also realized that the job openings were neither really up to date nor covered all of the roles that we are looking for. This is why we decided to spend some time rewriting them and the result is these six new job openings!.",
          "likes": 60,
          "dislikes": 2,
          "publish_time": "2018-08-25T03:45:00",
          "tagged": [
            {
              "uid": "_:hiring",
              "tag_name": "hiring"
            },
            {
              "uid": "_:careers",
              "tag_name": "careers"
            }
          ]
        }
      ]
    },
    {
      "author_name": "John Travis",
      "rating": 4.5,
      "published": [
        {
          "title": "How Dgraph Labs Raised Series A",
          "url": "https://hypermode.com/blog/how-dgraph-labs-raised-series-a/",
          "content": "I’m really excited to announce that Dgraph has raised $11.5M in Series A funding. This round is led by Redpoint Ventures, with investment from our previous lead, Bain Capital Ventures, and participation from all our existing investors – Blackbird, Grok and AirTree. With this round, Satish Dharmaraj joins Dgraph’s board of directors, which includes Salil Deshpande from Bain and myself. Their guidance is exactly what we need as we transition from building a product to bringing it to market. So, thanks to all our investors!.",
          "likes": 139,
          "dislikes": 6,
          "publish_time": "2019-07-11T01:45:00",
          "tagged": [
            {
              "uid": "_:announcement",
              "tag_name": "announcement"
            },
            {
              "uid": "_:funding",
              "tag_name": "funding"
            }
          ]
        },
        {
          "title": "Celebrating 10,000 GitHub Stars",
          "url": "https://hypermode.com/blog/10k-github-stars/",
          "content": "Dgraph is celebrating the milestone of reaching 10,000 GitHub stars 🎉. This wouldn’t have happened without all of you, so we want to thank the awesome community for being with us all the way along. This milestone comes at an exciting time for Dgraph.",
          "likes": 33,
          "dislikes": 12,
          "publish_time": "2017-03-11T01:45:00",
          "tagged": [
            {
              "uid": "_:devrel"
            },
            {
              "uid": "_:announcement"
            }
          ]
        }
      ]
    },
    {
      "author_name": "Katie Perry",
      "rating": 3.9,
      "published": [
        {
          "title": "Migrating data from SQL to Dgraph!",
          "url": "https://hypermode.com/blog/migrating-from-sql-to-dgraph/",
          "content": "Dgraph is rapidly gaining reputation as an easy to use database to build apps upon. Many new users of Dgraph have existing relational databases that they want to migrate from. In particular, we get asked a lot about how to migrate data from MySQL to Dgraph. In this article, we present a tool that makes this migration really easy: all a user needs to do is write a small 3 lines configuration file and type in 2 commands. In essence, this tool bridges one of the best technologies of the 20th century with one of the best ones of the 21st (if you ask us).",
          "likes": 20,
          "dislikes": 1,
          "publish_time": "2018-08-25T01:44:00",
          "tagged": [
            {
              "uid": "_:tutorial",
              "tag_name": "tutorial"
            }
          ]
        },
        {
          "title": "Building a To-Do List React App with Dgraph",
          "url": "https://hypermode.com/blog/building-todo-list-react-dgraph/",
          "content": "In this tutorial we will build a To-Do List app using React JavaScript library and Dgraph as a backend database. We will use dgraph-js-http — a library designed to greatly simplify the life of JavaScript developers when accessing Dgraph databases.",
          "likes": 97,
          "dislikes": 5,
          "publish_time": "2019-02-11T03:33:00",
          "tagged": [
            {
              "uid": "_:tutorial"
            },
            {
              "uid": "_:devrel"
            },
            {
              "uid": "_:javascript",
              "tag_name": "javascript"
            }
          ]
        }
      ]
    }
  ]
}
```

Our Graph is ready!

![rating-blog-rating](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/l-fullgraph-2.png)

Our Graph has:

* Three blue author nodes.
* Each author has two blog posts each - six in total - which are represented by
  the green nodes.
* The tags of the blog posts are in pink. You can see that there are 8 unique
  tags, and some of the blogs share a common tag.

## Data types for predicates

Dgraph automatically detects the data type of its predicates. You can see the
auto-detected data types using the Ratel UI.

Click on the schema tab on the left and then check the `Type` column. You'll see
the predicate names and their corresponding data types.

![rating-blog-rating](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-initial.png)

These data types include `string`, `float`, and `int`, and `uid`. Besides them,
Dgraph also offers three more basic data types: `geo`, `dateTime`, and `bool`.

The `uid` types represent predicates between two nodes. In other words, they
represent edges connecting two nodes.

You might have noticed that the `published` and `tagged` predicates are of type
`uid` array (`[uid]`). UID arrays represent a collection of UIDs. This is used
to represent one to many relationships.

For instance, we know that an author can publish more than one blog. Hence,
there could be more than one `published` edge emerging from a given `author`
node, each pointing to a different blog post of the author.

Dgraph's [v1.1 release](https://hypermode.com/blog/release-v1.1.0/) introduced
the type system feature. This feature made it possible to create custom data
types by grouping one or more predicates. But in this tutorial, we'll only focus
on the basic data types.

Also, notice that there are no entries in the indexes column. We'll talk about
indexes in detail shortly.

## Querying for predicate values

First, let's query for all the Authors and their ratings:

```dql
{
  authors_and_ratings(func: has(author_name)) {
    uid
    author_name
    rating
  }
}
```

![authors](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-find-rating-2.png)

Refer to the [first episode](./introduction) if you have any questions related
to the structure of the query in general.

There are three authors in total in our dataset. Now, let's find the best
authors. Let's query for authors whose rating is 4.0 or more.

In order to achieve our goal, we need a way to select nodes that meets certain
criteria (for example, rating > 4.0). You can do this by using Dgraph's built-in
comparator functions. Here's the list of comparator functions available in
Dgraph:

| comparator function name | Full form                |
| ------------------------ | ------------------------ |
| `eq`                     | equals to                |
| `lt`                     | less than                |
| `le`                     | less than or equal to    |
| `gt`                     | greater than             |
| `ge`                     | greater than or equal to |

There are a total of five comparator functions in Dgraph. You can use any of
them alongside the `func` keyword in your queries.

The comparator function takes two arguments. One is the predicate name and the
other is its comparable value. Here are a few examples.

| Example usage          | Description                                                                  |
| ---------------------- | ---------------------------------------------------------------------------- |
| func: eq(age, 60)      | Return nodes with `age` predicate equal to 60.                               |
| func: gt(likes, 100)   | Return nodes with a value of `likes` predicate greater than 100.             |
| func: le(dislikes, 10) | Return nodes with a value of `dislikes` predicates less than or equal to 10. |

Now, guess the comparator function we should use to select `author nodes` with a
rating of 4.0 or more.

If you think it should be the `greater than or equal to(ge)` function, then
you're right!

Let's try it out.

```graphql
{
  best_authors(func: ge(rating, 4.0)) {
    uid
    author_name
    rating
  }
}
```

![index missing](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/b-index-missing.png)

We got an error! The index for the `rating` predicate is missing. You can't
query for the value of a predicate unless you've added an index for it.

Let's learn more about indexes in Dgraph and also how to add them.

## Indexing in Dgraph

Indexes are used to speed up your queries on predicates. They have to be
explicitly added to a predicate when required (only when you need to query for
the value of a predicate).

Also, there's no need to anticipate the indexes to be added right at the
beginning. You can add them as you go along.

Dgraph offers different types of indexes. The choice of index depends on the
data type of the predicate.

Here is the table containing data types and the set of indexes that can be
applied to them.

| Data type  | Available index types                          |
| ---------- | ---------------------------------------------- |
| `int`      | `int`                                          |
| `float`    | `float`                                        |
| `string`   | `hash`, `exact`, `term`, `fulltext`, `trigram` |
| `bool`     | `bool`                                         |
| `geo`      | `geo`                                          |
| `dateTime` | `year`, `month`, `day`, `hour`                 |

Only `string` and `dateTime` data types have an option for more than one index
type.

Let's create an index on the rating predicate. Ratel UI makes it super simple to
add an index.

Here's the sequence of steps:

* Go to the schema tab on the left.
* Click on the `rating` predicate from the list.
* Tick the index option in the Properties UI on the right.

![Add schema](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/c-add-schema.png)

We successfully added the index for `rating` predicate! Let's rerun our previous
query.

![rating](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/d-rating-query.png)

We successfully queried for Author nodes with a rating of 4.0 or more. How about
we also fetch the Blog posts of these authors?

We already know that the `published` edge points from an `author` node to a
`blog post` node. So fetching the blog posts of the `author` nodes is simple. We
need to traverse the `published` edge starting from the `author` nodes.

```graphql
{
  authors_and_ratings(func: ge(rating, 4.0)) {
    uid
    author_name
    rating
    published {
      title
      content
      dislikes
    }
  }
}
```

![rating-blog-rating](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/e-rating-blog.png)

*Check out our [previous tutorial](./basic-operations) if you have questions
around graph traversal queries.*

Similarly, let's extend our previous query to fetch the tags of these blog
posts.

```graphql
{
  authors_and_ratings(func: ge(rating, 4.0)) {
    uid
    author_name
    rating
    published {
      title
      content
      dislikes
      tagged {
        tag_name
      }
    }
  }
}
```

![rating-blog-rating](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/m-four-blogs.png)

Note: author nodes are in blue, blogs posts in green, and tags in pink.

There are two authors, four blog posts, and their tags in the result. If you
take a closer look at the result, there's a blog post with 12 dislikes.

![Dislikes](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/i-dislikes-2.png)

Let's filter and fetch only the popular blog posts. Let's query for only those
blog posts with fewer than 10 dislikes.

To achieve that, we need to express the following statement as a query to
Dgraph:

*Hey, traverse the `published` edge, but only return those blogs with fewer than
10 dislikes*

Can we also filter the nodes during traversals? Yes, we can! Let's learn how to
do that in our next section.

## Filtering traversals

We can filter the result of traversals by using the `@filter` directive. You can
use any of the Dgraph's comparator functions with the `@filter` directive. You
should use the `lt` comparator to filter for only those blog posts with fewer
than 10 dislikes.

Here's the query.

```graphql
{
  authors_and_ratings(func: ge(rating, 4.0)) {
    author_name
    rating

    published @filter(lt(dislikes, 10)) {
      title
      likes
      dislikes
      tagged {
        tag_name
      }
    }
  }
}
```

The query returns:

![rating-blog-rating](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/n-three-blogs.png)

Now, we only have three blogs in the result. The blog with 12 dislikes is
filtered out.

Notice that the blog posts are associated with a series of tags.

Let's run the following query and find all the tags in the database.

```sh
{
  all_tags(func: has(tag_name)) {
    tag_name
  }
}
```

![tags](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/o-tags.png)

We got all the tags in the database.

In our next section, let's find all the blog posts which are tagged `devrel`.

## Querying string predicates

The `tag_name` predicate represents the name of a tag. It is of type `string`.
Here are the steps to fetch all blog posts which are tagged `devrel`.

* Find the root node with the value of `tag_name` predicate set to `devrel`. We
  can use the `eq` comparator function to do so.
* Don't forget to add an index to the `tag_name` predicate before you run the
  query.
* Traverse starting from the node for `devrel` tag along the `tagged` edge.

Let's start by adding an index to the `tag_name` predicate. Go to Ratel, click
`tag_name` predicate from the list.

![string index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/p-string-index-2.png)

You can see that there are five choices for indexes that can be applied to any
`string` predicate. The `fulltext`, `term`, and `trigram` are advanced string
indexes. We'll discuss them in detail in our next episode.

There are a few constraints around the use of string type indexes and the
comparator functions.

For example, only the `exact` index is compatible with the `le`, `ge`,`lt`, and
`gt` built-in functions. If you set a string predicate with any other index and
run the these comparators, the query fails.

Although, any of the five string type indexes are compatible with the `eq`
function, the `hash` index used with the `eq` comparator would normally yield
the best performance.

Let's add the `hash` index to the `tag_name` predicate.

![string index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/m-hash.png)

Let's use the `eq` comparator and fetch the root node with `tag_name` set to
`devrel`.

```graphql
{
  devrel_tag(func: eq(tag_name,"devrel")) {
    tag_name
  }
}
```

![string index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/q-devrel-2.png)

We finally have the node we wanted!

We know that the `blog post` nodes are connected to their `tag nodes` via the
`tagged` edges. Do you think that a traversal from the node for `devrel` tag
should give us the blog posts? Let's try it out!

```graphql
{
  devrel_tag(func: eq(tag_name,"devrel")) {
    tag_name
      tagged {
        title
        content
    }
  }
}
```

Looks like the query didn't work! It didn't return us the blog posts! Don't be
surprised as this is expected.

Let's observe our Graph model again.

![main graph model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/a-main-graph.JPG)

We know that the edges in Dgraph have directions. You can see that the `tagged`
edge points from a `blog post` node to a `tag` node.

Traversing along the direction of an edge is natural to Dgraph. Hence, you can
traverse from any `blog post node` to its `tag node` via the `tagged` edge.

But to traverse the other way around requires you to move opposite to the
direction of the edge. You can still do so by adding a tilde(~~) sign in your
query. The tilde(~~) has to be added at the beginning of the name of the edge to
be traversed.

Let's add the `tilde (~)` at the beginning of the `tagged` edge and initiate a
reverse edge traversal.

```graphql
{
  devrel_tag(func: eq(tag_name,"devrel")) {
    tag_name

    ~tagged {
      title
      content
    }
  }
}
```

![string index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/r-reverse-2.png)

We got an error!

Reverse traversals require an index on their predicate.

Let's go to Ratel and add the `reverse` index to the edge.

![string index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/r-reverse-1.png)

Let's re-run the reverse edge traversal.

```graphql
{
  devrel_tag(func: eq(tag_name, "devrel")) {
    tag_name

    ~tagged {
      title
      content
    }
  }
}
```

![UID index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/s-devrel-blogs.png)

![UID index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/s-devrel-blogs-2.png)

Phew! Now we got all the blog posts that are tagged `devrel`.

Similarly, you can extend the query to also find the authors of these blog
posts. It requires you to reverse traverse the `published` predicate.

Let's add the reverse index to the `published` edge.

![UID index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/t-reverse-published.png)

Now, let's run the following query.

```graphql
{
  devrel_tag(func: eq(tag_name,"devrel")) {
    tag_name

    ~tagged {
      title
      content

      ~published {
        author_name
      }
    }
  }
}
```

![UID index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/u-author-reverse-1.png)

![UID index](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/get-started-with-dgraph/u-author-reverse-2.png)

With our previous query, we traversed the entire graph in reverse order.
Starting from the tag nodes, we traversed up to the author nodes.

## Summary

In this tutorial, we learned about basic types, indexes, filtering, and reverse
edge traversals.

Before we wrap up, here’s a sneak peek into our next tutorial.

Did you know that Dgraph offers advanced text search capabilities? How about the
geo-location querying capabilities?

Sounds interesting?

Check out our next tutorial of the getting started series
[here](./multi-language-strings).

## Need help

* Please use [discuss.hypermode.com](https://discuss.hypermode.com) for
  questions, feature requests, bugs, and discussions.


# Graph Data Models 101
Source: https://docs.hypermode.com/dgraph/guides/graph-data-models-101

Graphs provide an alternative to tabular data structures, allowing for a more natural way to store and retrieve data

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

When building an app, you might wonder which database is the best choice. A
traditional relational database that you can query using SQL is a familiar
choice, but does a relational database really provide a natural fit to your data
model, and the performance that you need if your app goes viral and needs to
scale up rapidly?

This tutorial takes a deeper look at data modeling using relational databases
compared to graph databases like Dgraph, to give you a better understanding of
the advantages of using a graph database to power your app. If you aren't
familiar with graph data models or graph databases, this tutorial was written
for you.

## Learning goals

In this tutorial, you'll learn about graphs, and how a graph database is
different from a database built on a relational data model. This tutorial
doesn't include any code or syntax, but rather a comparison of graphs and
relational data models. By the end of this tutorial, you should be able to
answer the following questions:

* What's a graph?
* How are graphs different from relational models?
* How's data modeled in a graph?
* How's data queried from a graph?

Along the way, you might find that a graph is the right fit for the data model
used by your app. Any data model that tracks lots of different relationships (or
*edges*) between various data types is a good candidate for a graph model.

Whether this is the first time you are learning about graphs or looking to
deepen your understanding of graphs with some concrete examples, this tutorial
should help you along your journey.

If you are already familiar with graphs, you can jump right into our coding
example for [React](/dgraph/guides/message-board-app/introduction).

## Graphs and natural data modeling

Graphs provide an alternative to tabular data structures, allowing for a more
natural way to store and retrieve data.

For example, you could imagine that we're modeling a conversation within a
family:

* A `father`, who starts a conversation about going to get ice cream.
* A `mother`, who comments that she would also like ice cream.
* A `child`, who likes the idea of the family going to get ice cream.

This conversation could easily occur in the context of a modern social media or
messaging app, so you can imagine the data model for such an app as follows:

![A graph diagram for a social media app's data model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-3.png)

For the remainder of this guide, we use this as our example app: a basic social
media or messaging app, with a data model that includes `people`, `posts`,
`comments`, and `reactions`.

A graph data model is different from a relational model. A graph focuses on the
relationships between information, whereas a relational model focuses on storing
similar information in a list. The graph model received its name because it
resembles a graph when illustrated.

* Data objects are called *nodes* and are illustrated with a circle.
* Properties of nodes are called *predicates* and are illustrated as a panel on
  the node.
* Relationships between nodes are called *edges* and are illustrated as
  connecting lines. Edges are named to describe the relationship between two
  nodes. A `reaction` is an example of an edge, in which a person reacts to a
  post.

Some illustrations omit the predicates panel and show only the nodes and edges.

Referring back to the example app, the `father`, `mother`, `child`, `post`, and
`comment` are nodes. The name of the people, the post's title, and text of the
comment are the predicates. The natural relationships between the authors of the
posts, authors of the comments, and the comments' topics are edges.

As you can see, a graph models data in a natural way that shows the
relationships (edges) between the entities (nodes) that contain predicates.

## Relational Data Modeling

This section considers the example social media app introduced in the previous
section and discusses how it could be modeled with a traditional relational data
model, such as those used by SQL databases.

With relational data models, you create lists of each type of data in tables,
and then add columns in those tables to track the attributes of that table's
data. Looking back on our data, we remember that there are three main types,
`People`, `Posts`, and `Comments`

![Three tables](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-4.png)

To define relationships between records in two tables, a relational data model
uses numeric identifiers called *foreign keys*, that take the form of table
columns. Foreign keys can only model one-to-many relationship types, such as the
following:

* The relationship from `Posts` to `People`, to track contributors (authors,
  editors, etc.) of a `Post`
* The relationship from `Comments` to `People`, to track the author of the
  comment
* The relationship from `Comments` to `Posts`, to track on which post comments
  were made
* The relationship between rows in the `Comments` table, to track comments made
  in reply to other comments (a self-reference relationship)

![Relationships between rows in tables](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-5.png)

The limitations of foreign keys become apparent when your app requires you to
model many-to-many relationships. In our example app, a person can like many
posts or comments, and posts and comments can be liked by many people. The only
way to model this relationship in a relational database is to create a new
table. This so-called *pivot table* usually doesn't store any information
itself, it just stores links between two other tables.

In our example app, we decided to limit the number of tables by having a single
“Likes” table instead of having `people_like_posts` and `people_like_comments`
tables. None of these solutions is perfect, though, and there is a trade-off
between having a lower table count or having more empty fields in our tables
(also known as "sparse data").

![An illustration of sparse data when creating a Likes table](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-6.png)

Because foreign keys can't be added in reference to entities that don't exist,
adding new posts and authors requires additional work. To add a new post and a
new author at the same time (in the Posts and People tables), we must first add
a row to the `People` table and then retrieve their primary key and associate it
with the new row in the `Posts` table.

![Adding a post and an author at the same time](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-7.png)

By now, you might ask yourself: how does a relational model expand to handle new
data, new types of data, and new data relationships?

When new data is added to the model, the model changes to accept the data. The
simplest type of change is when you add a new row to a table. The new row adopts
all of the columns from the table. When you add a new property to a table, the
model changes and adds the new property as a column on every existing and future
row for the table. And when you add a new data type to the database, you create
a new table with its own pre-defined columns. This new data type might link to
existing tables or need more pivot tables for a new many-to-many relationship.
So, with each data type added to your relational data model, the need to add
foreign keys and pivot tables increases, making support for querying every
potential data relationship increasingly unwieldy.

![Expanding a relational data model means more pivot tables](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-8.png)

Properties are stored as new columns and relationships require new columns and
sometimes new pivot tables. Changing the schema in a relational model directly
effects the data that's held by the model, and can impact database query
performance.

## Graph Data Modeling

In this section we take our example social media app and see how it could be
modeled in a graph.

The concept of modeling data in a graph starts by placing dots, which represent
nodes. Nodes can have one or more predicates (properties). A `person` may have
predicates for their name, age, and gender. A `post` might have a predicate
value showing when it was posted, and a value containing the contents of the
post. A `comment` would most likely have a predicate containing the comment
string. However, any one node could have other predicates that aren't contained
on any other node. Each node represents an individual item, hence the singular
naming structure.

![Nodes used in the example social media app](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-9.png)

As graphs naturally resemble the data you are modeling, the individual nodes can
be moved around this conceptual space to clearly show the relationships between
these data nodes. Relationships are formed in graphs by creating an edge between
them. In our app, a post has an author, a post can have comments, a comment has
an author, and a comment can have a reply.

For sake of illustration we also show the family tree information. The `father`
and the `mother` are linked together with a `spouse` edge, and both parents are
related to the child along a `child` edge.

![Illustration of relationships as edges](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-10.png)

With a graph, you can also name the inverse relations. From here we can quickly
see the inverse relationships. A `Post` has an `Author` and a `Person` has
`Posts`. A `Post` has `Comments` and a `Comment` is on a `Post`. A `Comment` has
an `Author`, and a `Person` has `Comments`. A `Parent` has a `Child`, and a
`Child` has a `Parent`.

You create many-to-many relationships in the same way that you make one-to-many
relationships, with an edge between nodes.

Adding groups of related data occurs naturally within a graph. The data is sent
as a complete object instead of separate pieces of information that needs to be
connected afterwards. Adding a new person and a new post to our graph is a
one-step process. New data coming in doesn't have to be related to any existing
data. You can insert this whole data object with 3 people, a post, and a comment
all in one step.

When new data is added to the model, the model changes to accept the data. Every
change to a graph model is received naturally. When you add a new node with a
data type, you are simply creating a new dot in space and applying a type to it.
The new node doesn't include any predicates or relationships other than what you
define for it. When you want to add a new predicate onto an existing data type,
the model changes and adds the new property onto the items that you define.
Other items not specifically given the new property type aren't changed. When
you add a new data type to the database, a new node is created, ready to receive
new edges and predicates.

![Illustration of expanding a graph data model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-11.png)

The key to remember when modeling a graph is to focus on the relationships
between nodes. In a graph you can change the model without affecting the
underlying data. Because the graph is stored as individual nodes, you can adjust
predicates of individual nodes, create edges between sets of nodes, and add new
node types without affecting any of the other nodes.

## Query Data in a Relational Model

Storing our data is great, but the best data model would be useless without the
ability to query the data our app requires. So, how does information get
retrieved in a relational model compared to a graph model?

In a relational model, tables are stored in files. To support the sample social
media app described in this tutorial, you would need four files: `People`,
`Posts`, `Comments`, and `Likes`.

![Visualization of four files](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-12.png)

When you request data from a file, one of two things happens: either a table
scan takes place or an index is invoked. To find this data, the whole file must
be read until the data is found or the end of the file is reached. In our
example app there is a post titled, “Ice Cream?”. If the title isn't indexed,
every post in the file would need to be read until the database finds the post
entitled, “Ice Cream?”. This method would be like reading the entire dictionary
to find the definition of a single word: very time-consuming. This process could
be optimized by creating an index on the post’s title column. Using an index
speeds up searches for data, but it can still be time-consuming.

### What's an index?

An index is an algorithm used to find the location of data. Instead of scanning
an entire file looking for a piece of data, an index is used to aggregate the
data into "chunks" and then create a decision tree pointing to the individual
chunks of data. Such a decision tree could look like the following:

![Image showing a tree to lookup the term graph from an index. The tree should be in a “graph” type format with circles instead of squares.](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-13.png)

Relational data models rely heavily on indexes to quickly find the requested
data. Because the data required to answer a single question usually lives in
multiple tables, you must use multiple indexes each time that related data is
joined together. And because you can't index every column, some types of queries
won't benefit from indexing.

### How data is joined in a relational model

In a relational model, the request's response must be returned as a single table
consisting of columns and rows. To form this single table response, data from
multiple tables must be joined together. In our app example, we found the post
entitled “Ice Cream?” and also found the comments, “Yes!”, “When?”, and “After
Lunch”. Each of these comments also has a corresponding author: `Mother`,
`Child`, and `Father`. Because there is only one post as the root of the join,
the post is duplicated to join to each comment.

![Joins in a relational model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-15.png)

Flattening query results can lead to many duplicate rows. Consider the case
where you also want to query which people liked the comments on this example
post. This query requires mapping a many-to-many relationship, which invokes two
additional index searches to get the list of likes by `person`.

![Flattening in a relational model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-16.png)

Joining all of this together would form a single table containing many
duplicates: duplicate `posts` and duplicate `comments`. Another side effect of
this response approach is that it's likely that empty data exists in the
response.

In the next section, you'll see that querying a graph data model avoids the
issues that you would face when querying a relational data model.

## Query Data in a Graph Model

As you'll see in this section, the data model we use determines the ease with
which we can query for different types of data. The more your app relies on
queries about the relationships between different types of data, the more you
benefit from querying data using a graph data model.

In a graph data model, each record (a `person`, `post` or `comment`) is stored
as a data *object* (sometimes also called a *node*). In the example social media
app described in this tutorial, there are objects for individual people, posts,
and comments.

![Image of many objects of people, posts, and comments(not showing the relationships for clarity of the objects themselves)](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-18.png)

When data is requested from the graph, a root function determines which nodes
are selected for the starting points. This root function uses indexes to
determine which nodes match quickly. In our app example, we want to start with
the root being the post with the title “Ice Cream?”. This type of lookup evokes
an index on the post's title, much like indexes work in a relational model. The
indexes at the root of the graph use the full index tree to find the data.

Connecting edges together to form a connected graph is called *traversal*. After
arriving at our `post`, “Ice Cream?”, we traverse the graph to arrive at the
post's `comments`. To find the post's `author`, we traverse the next step to
arrive at the people who authored the comment. This process follows the natural
progression of related data, and graph data models allow us to query our data to
follow this progression efficiently.

What do we mean by efficiently? A graph data model lets you traverse from one
node to a distantly related node without the need for anything like pivot
tables. This means that queries based on edges can be updated easily, with no
need to change the schema to support new many-to-many relationships. And, with
no need to build tables specifically for query optimization, you can adjust your
schema quickly to accommodate new types of data without adversely impacting
existing queries.

![Image of post with connected comments and author](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-19.png)

A feature of a graph model is that related edges can be filtered anywhere within
the graph's traversing. When you want to know the most recent `comment` on your
post or the last `person` to like the comment, filters can be applied to the
edge.

![Image of filters along an edge](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/data-model-101/evolution-21.png)

When filters get applied along an edge, only the nodes that match the edge are
filtered - not all of the nodes in the graph. Applying this logic reduces the
size of the graph and makes index trees smaller. The smaller an index tree is,
the faster that it can be resolved.

In a graph model, data is returned in an object-oriented format. Any related
data is joined to its parent within the object in a nested structure.

```json
{
  "title": "IceCream?",
  "comments": [
    {
      "title": "Yes!",
      "author": {
        "name": "Mother"
      }
    },
    {
      "title": "When?",
      "author": {
        "name": "Child"
      }
    },
    {
      "title": "After Lunch",
      "author": {
        "name": "Father"
      }
    }
  ]
}
```

This object-oriented structure allows data to be joined without duplication.

## Conclusion

Congratulations on finishing the Dgraph learn course **Graph Data Models 101**!

Now that you have an overview and understanding of

* what a graph is
* how a graph differs from a relational model
* how to model a graph
* and how to query a graph

you are ready to jump into using Dgraph, the only truly native distributed graph
database.


# Conclusion
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/conclusion

This is the end of the Dgraph Learn course - Build a Message Board App in React. But there's still more to learn.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Congratulations on finishing the Dgraph Learn course: **Build a Message Board
App in React**!

You’ve now learned how to add much of the core functionality to your React app.
In the advanced course (coming soon), you’ll add login and authorization,
subscriptions, and custom logic to this app.

Playing with your app and taking your code to the next level? Be sure to share
your creations on our [discussion boards](https://discuss.hypermode.com). We
love to see what you’re working on, and feel free to ask any questions - our
team of developers typically respond within 30 minutes!

We hope you’ve enjoyed playing with [Dgraph Cloud](https://dgraph.io/cloud) for
this course. A fully managed GraphQL backend database service, Dgraph Cloud lets
you focus on building apps, not managing infrastructure.

Want to learn more? Check out [another guide](/dgraph/guides), or explore the
[docs](/dgraph/overview).

And if you have been reading along but haven't tried it yet,
[try Dgraph Cloud](https://cloud.dgraph.io/) to launch your first hosted GraphQL
database.


# Design a Schema for the App
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/graphql/design-app-schema

Build a Message Board App in React with Dgraph Learn. Step 2: GraphQL schema design - how graph schemas and graph queries work.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

In this section, you'll start designing the schema of the message board app and
look at how graph schemas and graph queries work.

To design the schema, you won't think in terms of tables or joins or documents,
you'll think in terms of entities in your app and how they're linked to make a
graph. Any requirements or design analysis needs iteration and thinking from a
number of perspectives, so you'll work through some of that process and sketch
out where you are going.

Graphs tend to model domains like your app really nicely because they naturally
model things like the subgraph of a `user`, their `posts`, and the `comments` on
those posts, or the network of friends of a user, or the kinds of posts a user
tends to like.

## UI requirements

Most apps are more than what you can see on the screen, but UI is what you are
focusing on here, and thinking about the UI you want to kick-off your design
process. So, let's at start by looking at what you would like to build for your
app's UI

Although a single GraphQL query can save you lots of calls and return you a
subgraph of data, a complete page might be built up of blocks that have
different data requirements. For example, in a sketch of your app's UI you can
already see these data requirements forming.

![App UI requirements](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/UI-components.gif)

You can start to see the building blocks of the UI and some of the entities
(users, categories, and posts) that will form the data in your app.

## Thinking in graphs

Designing a graph schema is about designing the things, or entities, that form
nodes in the graph, and designing the shape of the graph, or what links those
entities have to other entities.

There's really two concepts in play here. One is the data itself, often called
the app data graph. The other is the schema, which is itself graph shaped but
really forms the pattern for the data graph. You can think of the difference as
somewhat similar to objects (or data structure definitions) versus instances in
a program, or a relational database schema versus rows of actual data.

Already you can start to tease out what some of the types of data and
relationships in your graph are. There's users who post posts, so you know
there's a relationship between users and the posts they've made. You know the
posts are going to be assigned to some set of categories and that each post
might have a list of comments posted by users.

So your schema is going to have these kinds of entities and relationships
between them.
![Graph schema sketch](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/schema-inital-sketch.png)

I've borrowed some notation from other data modeling patterns here. That's
pretty much the modeling capability GraphQL allows, so let's start sketching
with it for now.

A `user` is going to have some number of `posts` and a `post` can have exactly
one `author`. A `post` can be in only a single `category`, which, in turn, can
contain many `posts`.

How does that translate into the app data graph? Let's sketch out some examples.

Let's start with a single user who's posted three posts into a couple of
different categories. Your graph might start looking like this.

![first-posts-in-graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/first-posts-in-graph.png)

Then another user joins and makes some posts. Your graph gets a bit bigger and
more interesting, but the types of things in the graph and the links they can
have follow what the schema sets out as the pattern --- for example you aren't
linking users to categories.
![more users and posts](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/user2-posts-in-graph.png)

Next the users read some posts and start making and replying to comments.
![users, posts and comments](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/comments-in-graph.png)

Each node in the graph has the data (a bit like a document) that the schema says
it can have, such as a username for users and title, text and date published for
posts, and the links to other nodes (the shape of the graph).

While you are still sketching things out here, let's take a look at how queries
work.

## How graph queries work

Graph queries in GraphQL are really about entry points and traversals. A query
picks certain nodes as a starting point and then selects data from the nodes or
follows edges to traverse to other nodes.

For example, to render a user's information, you might need only to find the
user. So your use of the graph might be like in the following sketch --- you'll
find the user as an entry point into the graph, perhaps from searching users by
username, query some of their data, but not traverse any further.
![query a user](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/user1-search-in-graph.png)

Often, though, even in just presenting a user's information, you need to present
information like most recent activity or sum up interest in recent posts. So
it's more likely that you'll start by finding the user as an entry point and
then traversing some edges in the graph to explore a subgraph of interesting
data. That might look like this traversal, starting at the user and then
following edges to their posts.

![query a user and their posts](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/user1-post-search-in-graph.png)

You can really start to see that traversal when it comes to rendering an
individual post. You'll need to find the post, probably by its id when a user
navigates to a url like `/post/0x2`, then you'll follow edges to the post's
author and category, but you'll also need to follow the edges to all the
comments, and from there to the authors of the comments. That'll be a multi-step
traversal like the following sketch.
![query a post and follow edges](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/post2-search-in-graph.png)

Graphs make these kinds of data traversals really clear, as compared to table
joins or navigating your way through a RESTful API. It can also really help to
jot down a quick sketch.

It is also possible for a query to have multiple entry points and traversals
from all of those entry points. Imagine, for example, the query that renders the
post list on the main page. That's a query that finds multiple posts, maybe
ordered by date or from particular categories, and then, for each, traverses to
the author, category, etc.

You can now begin to see the GraphQL queries needed to fill out the UI. For
example, in the sketch at the top, there is a query starting at the logged in
user to find their details, a query finding all the category nodes to fill out
the category dropdown, and a more complex query that finds a number of posts and
make traversals to find the posts' authors and categories.

## Schema

Now that you have investigated and considered what you are going to show for
posts and users, you can start to flesh out your schema some more.

Posts, for example, are going to need a title and some text for the post, both
string valued. Posts also need some sort of date to record when they were
uploaded. They'll also need links to the author, category, and a list of
comments.

The next iteration of your schema might look like this sketch.
![Graph schema sketch with data](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/schema-sketch.png)

That's your first cut at a schema --- the pattern your app data graph follows.

You'll keep iterating on this as you work through the tutorial, that's what
you'd do in building an app, no use pretending like you have all the answers at
the start. Eventually, you'll want to add likes and dislikes on the posts, maybe
also tags, and you'll also layer in a permissions system so some categories
require permissions to view. But, those topics are for later sections in the
tutorial. This is enough to start building with.

## What's next

Next you'll make your design concrete, by writing it down as a
[GraphQL schema](./graphql-schema), and upload that to Dgraph. That'll give you
a running GraphQL API and you'll look at the queries and mutations that form the
data of your app.


# GraphQL Operations
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/graphql/graphql-operations

Using your schema, Dgraph Cloud generated ways to interact with the graph. In GraphQL, the API can be inspected with introspection queries.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The schema that you developed and deployed to Dgraph Cloud in the previous
sections was about the types in our domain and the shape of the app data graph.
From that, Dgraph Cloud generated some ways to interact with the graph. GraphQL
supports the following *operations*, which provide different ways to interact
with a graph:

* **queries**: used to find a starting point and traverse a subgraph
* **mutations**: used to change the graph and return a result
* **subscriptions**: used to listen for changes in the graph

In GraphQL, the API can be inspected with special queries called *introspection
queries*. Introspection queries are a type of GraphQL query that provides the
best way to find out what operations you can perform with a GraphQL API.

## Introspection

Many GraphQL tools support introspection and generate documentation to help you
explore an API. There are several tools in the GraphQL ecosystem you can use to
explore an API, including
[GraphQL Playground](https://github.com/prisma-labs/graphql-playground),
[Insomnia](https://insomnia.rest/),
[GraphiQL](https://github.com/graphql/graphiql),
[Postman](https://www.postman.com/graphql/), and
[Altair](https://github.com/imolorhe/altair).

You can also explore your GraphQL API using the API explorer that's included in
the Dgraph Cloud web UI. Navigate to the **GraphQL** tab where you can access
the introspected schema from the "Documentation Explorer" in the right menu.

*![Dgraph Cloud Schema Explorer](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/dgraph-cloud-schema-explorer.png)*

From there, you can click through to the queries and mutations and check out the
API. For example, this API includes mutations to add, update and delete users,
posts and comments.

Next, you'll
[learn more about the API that Dgraph Cloud created from the schema](./react-graphql-mutations)
by trying out the same kind of queries and mutations you'll use to build the
message board app.


# GraphQL Schema
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/graphql/graphql-schema

How to Build a Message Board App in React. Step 2: GraphQL schema - translate the schema design to the GraphQL SDL (Schema Definition Language).

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

In this section, you'll learn about how to translate the schema design to the
GraphQL SDL (Schema Definition Language).

## App Schema

In the schema design section, you saw the following sketch of a graph schema for
the example message board app:
![data model sketch](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/schema-sketch.png)

Using the GraphQL SDL, Dgraph Cloud generates a running GraphQL API from the
description of a schema as GraphQL types. There are two different aspects of a
GraphQL schema:

* **Type Definitions**: these define the things included in a graph and the
  shape of the graph. In this tutorial, you will derive the type definitions
  from the sketch shown above.
* **Operations**: these define what you can do in the graph using the API, like
  the search and traversal examples in the previous section. Initially, Dgraph
  Cloud will generate create, read, update and destroy (CRUD) operations for
  your API. Later in this the tutorial, you'll learn how to define other
  operations for your schema.

You'll start by learning about the GraphQL SDL and then translate the app schema
sketch into GraphQL SDL.

## GraphQL Schema

The input schema to Dgraph Cloud is a GraphQL schema fragment that contains type
definitions. Dgraph Cloud builds a GraphQL API from those definitions.

This input schema can contain types, search directives, IDs, and relationships.

### Types

Dgraph Cloud supports pre-defined scalar types (including `Int`, `String`,
`Float` and `DateTime`) and a schema can define any number of other types. For
example, you can start to define the `Post` type in the GraphQL SDL by
translating the following from the app schema sketch shown above:

```graphql
type Post {
  title: String
  text: String
  datePublished: DateTime
  author: User
  ...
}
```

A `type TypeName { ... }` definition defines a kind of node in your graph. In
this case, `Post` nodes. It also gives those nodes what GraphQL calls *fields*,
which define a node's data values. Those fields can be scalar values: in this
case a `title`, `text` and `datePublished`. They can also be links to other
nodes: in this case the `author` edge must link to a node of type `User`.

Edges in the graph can be either singular or multiple. If a field is a name and
a type, like `author: User`, then a post can have a single `author` edge. If a
field uses the list notation with square brackets (for example
`comments: [Comment]`), then a post can have multiple `comments` edges.

GraphQL allows the schema to mark some fields as required. For example, you
might decide that all users must have a username, but that users aren't required
to set a preferred display name. If the display name is null, your app can
choose to display the username instead. In GraphQL, required fields are marked
using an exclamation mark (`!`) annotation after the field's type.

So, to guarantee that `username` will never be null, but allow `displayName` to
be null, you would define the `User` type as follows in your schema:

```graphql
type User {
  username: String!
  displayName: String
  ...
}
```

This annotation carries over to lists, so `comments: [Comment]` would allow both
a null list and a list with some nulls in it, while `comments: [Comment!]!` will
never allow either a null comments list, nor will it allow a list with that
contains any null values. The `!` notation lets your UI code make some
simplifying assumptions about the data that the API returns, reducing the need
for client-side error handling.

### Search

The GraphQL SDL syntax shown above describes your types and the shape of your
app data graph, and you can start to make a pretty faithful translation of the
types in your schema design. However, there's a bit more that you'll need in the
API for this app.

As well as the shape of the graph, you can use GraphQL directives to tell Dgraph
Cloud some more about how to interpret the graph and what features you'd like in
the GraphQL API. Dgraph Cloud uses this information to specialize the GraphQL
API to fit the requirements of your app.

For example, with just the type definition, Dgraph Cloud doesn't know what kinds
of search you need your API to support. Adding the `@search` directive to the
schema tells Dgraph Cloud about the search needed. The following schema example
shows two ways to add search directives.

```graphql
type Post {
  ...
  title: String! @search(by: [term])
  text: String! @search(by: [fulltext])
  ...
}
```

These search directives tell Dgraph Cloud that you want your API support
searching posts by title using terms, and searching post text using full-text
search. This syntax supports searches like "all the posts with GraphQL in the
title" and broader search-engine style searches like "all the posts about
developing GraphQL apps".

### IDs

Dgraph Cloud supports two types of identifiers: an `ID` type that gives
auto-generated 64-bit IDs, and an `@id` directive that allows external IDs to be
used for IDs.

`ID` and `@id` have different purposes, as illustrated by their use in this app:

* `ID` is best for things like posts that need a uniquely-generated ID.
* `@id` is best for types, like `User`, where the ID (their username) is
  supplied by the user.

```graphql
type Post {
  id: ID!
  ...
}

type User {
  username: String! @id
  ...
}
```

A post's `id: ID` gives each post an auto-generated ID. For users, you'll need a
bit more. The `username` field should be unique; in fact, it should be the id
for a user. Adding the `@id` directive like `username: String! @id` tells Dgraph
Cloud GraphQL that `username` should be a unique ID for the `User` type. Dgraph
Cloud GraphQL will then generate the GraphQL API such that `username` is treated
as an ID, and ensure that usernames are unique.

### Relationships

A critical part of understanding GraphQL is learning how it handles
relationships. A GraphQL schema based around types like those in the following
example schema types specifies that an author has some posts and each post has
an author, but the schema doesn't connect them as a two-way edge in the graph.
So in this case, your app can't assume that the posts it can reach from a
particular author all have that author as the value of their `author` edge.

```graphql
type User {
  ...
  posts: [Post!]!
}

type Post {
  ...
  author: User!
}
```

GraphQL schemas are always under-specified in this way. It is left up to the
documentation and implementation to make a two-way connection, if it exists.
There might be multiple connections between two types; for example, an author
might also be linked to the posts they have commented on. So, it makes sense
that you need something other than just the types as defined above to specify
two-way edges.

With Dgraph Cloud you can specify two-way edges by adding the `@hasInverse`
directive. Two-way edges help your app to untangle situations where types have
multiple edges. For example, you might need to make sure that the relationship
between the posts that a user has authored and the ones they've liked are linked
correctly.

```graphql
type User {
  ...
  posts: [Post!]!
  liked: [Post!]!
}

type Post {
  ...
  author: User! @hasInverse(field: posts)
  likedBy: [User!]! @hasInverse(field: liked)
}
```

The `@hasInverse` directive is only needed on one end of a two-way edge, but you
can add it at both ends if that adds clarity to your documentation and makes
your schema more "human-readable".

## Final schema

Working through the four types in the schema sketch, and then adding `@search`
and `@hasInverse` directives, yields the following schema for your app.

```graphql
type User {
  username: String! @id
  displayName: String
  avatarImg: String
  posts: [Post!]
  comments: [Comment!]
}

type Post {
  id: ID!
  title: String! @search(by: [term])
  text: String! @search(by: [fulltext])
  tags: String @search(by: [term])
  datePublished: DateTime
  author: User! @hasInverse(field: posts)
  category: Category! @hasInverse(field: posts)
  comments: [Comment!]
}

type Comment {
  id: ID!
  text: String!
  commentsOn: Post! @hasInverse(field: comments)
  author: User! @hasInverse(field: comments)
}

type Category {
  id: ID!
  name: String! @search(by: [term])
  posts: [Post!]
}
```

Dgraph Cloud is built to allow for iteration of your schema. I'm sure you've
picked up things that could be added to enhance this example app, i.e., the
ability to add up and down votes, or to add "likes" to posts. In this tutorial,
we discuss adding new features using an iterative approach. This approach is the
same one that you take when working on your own project: start by building a
minimal working version, and then iterate from there.

Some iterations, such as adding likes, will just require a schema change; Dgraph
Cloud GraphQL will update very rapidly to adjust to this change. Some
iterations, such as adding a `@search` directive to comments, can be done by
extending the schema. This will cause Dgraph Cloud to index the new data and
then update the API. Very large iterations, such as extending the model to
include a history of edits on a post, might require a data migration.

Now, [deploy your schema](./load-schema-to-dgraph-cloud).


# Working in GraphQL
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/graphql/index

Developing a Message Board App in React with Dgraph Learn. Step 2: GraphQL schema design and loading, queries, and mutations.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

To build an app with Dgraph Cloud, you design your app in GraphQL. You design a
set of GraphQL types that describes the app's data requirements. Dgraph Cloud
GraphQL takes those types, prepares graph storage for them and generates a
GraphQL API with queries and mutations.

In this section of the tutorial, you'll walk through the process of designing a
schema for a message board app, loading the GraphQL schema into Dgraph Cloud,
and then working through the queries and mutations that Dgraph Cloud makes
available from that schema.


# Deploy the Schema
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/graphql/load-schema-to-dgraph-cloud

Building a Message Board App in React. Step 2: With the schema defined, it’s just one step to get a running GraphQL backend for the app.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

With the schema defined, it's just one step to get a running GraphQL backend for
the app.

Copy the schema, navigate to the **Schema** tab in Dgraph Cloud, paste the
schema in, and press **Deploy**.

![Deploy Dgraph Cloud schema](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/dgraph-cloud-deploy-schema-success.png)

As soon as the schema is added, Dgraph Cloud generates and deploys a GraphQL API
for the app.

Next you'll [learn about GraphQL operations](./graphql-operations) like queries,
mutations and subscriptions.


# GraphQL Mutations
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/graphql/react-graphql-mutations

Before you can query, you need to add data. Use GraphQL Mutations to add a user, category, posts, and sample data.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

It'd be great to start by writing some queries to explore the graph of the app,
like you did in the sketches exploring graph ideas, but there's no data yet, so
queries won't be much use. So instead, you'll start by adding data.

## Add a User

GraphQL mutations have the useful property of returning a result. That result
can help your UI, for example, to re-render a page without further queries.
Dgraph Cloud lets the result returned from a mutation be as expressive as any
graph traversal.

Let's start by adding a single test user. The user type has the following
fields: `username`, `displayName`, `avatarImg`, `posts` and `comments`, as shown
below:

```graphql
type User {
  username: String! @id
  displayName: String
  avatarImg: String
  posts: [Post!]
  comments: [Comment!]
}
```

In this example, the only required field (marked with `!`) is the username. So,
to generate a new user node in the graph, we only need to supply a username.

The sample app's GraphQL API includes an `addUser` mutation, which can be used
to add multiple users and their data in a single operation. You can add one user
and their `username` with the following mutation:

```graphql
mutation {
  addUser(input: [{ username: "User1" }]) {
    user {
      username
      displayName
    }
  }
}
```

The `mutation` keyword tells a GraphQL server that it's running a mutation. The
mutation (in this case, `addUser`) takes the provided arguments and adds that
data to the graph.

The mutation shown above adds a single user, `User1`, and returns the newly
created user's `username` and `displayName`. The `displayName` will be `null`
because you didn't provide that data. This user also has no `posts` or
`avatarImg`, but we aren't asking for those in the result. Here's how it looks
when run in the Dgraph Cloud API Explorer.

![run a mutation in Dgraph Cloud](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/dgraph-cloud-add-a-user.png)

## Add a category

The graph now has a single node. Next, you'll add a category using the
`addCategory` mutation. Categories are a little different than users because the
id is auto-generated by Dgraph Cloud. The following mutation creates the
category and returns its `name` and the `id` Dgraph Cloud gave it.

```graphql
mutation {
  addCategory(input: [{ name: "Category1" }]) {
    category {
      id
      name
    }
  }
}
```

When run in Dgraph Cloud's API Explorer, the mutation looks as shown below. Note
that the category's `id` is auto generated and will be different on any
execution of the mutation.

![run a mutation with auto generated id in Dgraph Cloud](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/dgraph-cloud-add-a-category.png)

## Add Some Posts

Dgraph Cloud can do more than add single graph nodes at a time. The mutations
can add whole subgraphs and link into the existing graph. To show this, let's do
a few things at once. Remember our first sketch of some graph data?

![sub graph with first posts](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/first-posts-in-graph.png)

At the moment we only have the `User1` and `Category1` nodes. It is not much of
a graph, so let's flesh out the rest of the graph in a single mutation. We'll
use the `addPost` mutation to add the three posts, link all the posts to
`User1`, link posts 2 and 3 to the existing category, and then create
`Category2`. And, you'll do all of this in a single operation using the
following mutation:

```graphql
mutation {
  addPost(
    input: [
      {
        title: "Post1"
        text: "Post1"
        author: { username: "User1" }
        category: { name: "Category2" }
      }
      {
        title: "Post2"
        text: "Post2"
        author: { username: "User1" }
        category: { id: "0xfffd8d6ab6e7890a" }
      }
      {
        title: "Post3"
        text: "Post3"
        author: { username: "User1" }
        category: { id: "0xfffd8d6ab6e7890a" }
      }
    ]
  ) {
    post {
      id
      title
      author {
        username
      }
      category {
        name
      }
    }
  }
}
```

Because categories are referenced by an auto-generated `ID`, when you run such a
mutation, you'll need to make sure that you use the right id value for
`Category1` --- in my run that was `0xfffd8d6ab6e7890a`, but yours might differ.
In the Dgraph Cloud API explorer, that mutation looked like this:

![dgraph-cloud-deep-mutations](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/dgraph-cloud-deep-mutations.png)

A real app probably wouldn't add multiple posts in that way, but this example
shows the what you can do with mutations in Dgraph Cloud. For example, you could
create a shopping cart and add the first items to that cart in a single
mutation.

The input format to Dgraph Cloud also shows you another important property that
helps you when you are building an app: serialization of data. In general, you
can serialize your data structures, send them to Dgraph Cloud and it mutates the
graph. So, you don't need to programmatically add single objects from the client
or work out which bits are in the graph and which aren't --- just serialize the
data and Dgraph Cloud works it out. Dgraph Cloud uses the id's in the data to
work out how to connect the new data into the existing graph.

## Add sample data

You can run some more mutations, add more users or posts, or add comments to the
posts. To get you started, here's a mutation that adds some more data that we
can use to explore GraphQL queries in the following section.

```graphql
mutation {
  addPost(
    input: [
      {
        title: "A Post about Dgraph Cloud"
        text: "Develop a GraphQL app"
        author: { username: "User1" }
        category: { id: "0xfffd8d6ab6e7890a" }
      }
      {
        title: "A Post about Dgraph"
        text: "It is a GraphQL database"
        author: { username: "User1" }
        category: { id: "0xfffd8d6ab6e7890a" }
      }
      {
        title: "A Post about GraphQL"
        text: "Nice technology for an app"
        author: { username: "User1" }
        category: { id: "0xfffd8d6ab6e7890a" }
      }
    ]
  ) {
    post {
      id
      title
    }
  }
}
```

## GraphQL Variables

A mutation that takes data in its arguments is great to try out in a UI tool,
but an app needs to connect the data in its internal data structures to the API
without building complex query strings. GraphQL *Query Variables* let a query or
mutation depend on input values that are resolved at run-time.

For example, the following `addOnePost` mutation requires an input `$post` (of
type `post`) that it then passes as the `input` argument to the `addPost`
mutation:

```graphql
mutation addOnePost($post: AddPostInput!) {
  addPost(input: [$post]) {
    post {
      id
      title
      text
    }
  }
}
```

Running this mutation requires a packet of JSON that supplies a value for the
needed variable, as follows:

```json
{
  "post": {
    "title": "GraphQL Variables",
    "text": "This post uses variables to input data",
    "author": { "username": "User1" },
    "category": { "id": "0xfffd8d6ab6e7890a" }
  }
}
```

In Dgraph Cloud's UI, there's a **Query Variables** tab that you can use to
enter the variables.

![Mutation with GraphQL Variables](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/graphql-variables.png)

GraphQL variables let an app depend on a fixed mutation string and simply inject
the actual data into the mutation when it's executed, meaning same mutation can
be used over and over with different data.

## Mutations used in the App

The app always uses GraphQL variables so that there's a small set of mutations
and the data can be supplied by serializing client-side data structures.

The app will need a mutation to add users:

```graphql
mutation ($username: String!) {
  addUser(input: [{ username: $username }]) {
    user {
      username
    }
  }
}
```

It will also need a mutation to add posts:

```graphql
mutation addPost($post: AddPostInput!) {
  addPost(input: [$post]) {
    post {
      id
      # ... and other post data
    }
  }
}
```

It will need a mutation to add comments:

```graphql
mutation addComment($comment: AddCommentInput!) {
  addComment(input: [$comment]) {
    comment {
      id
      # ... and other comment data
    }
  }
}
```

And finally, it will need a mutation to update posts:

```graphql
mutation updatePost($id: ID!, $post: PostPatch) {
  updatePost(input: { filter: { id: [$id] }, set: $post }) {
    post {
      id
      # ... and other post data
    }
  }
}
```

The `updatePost` mutation combines a search and mutation into one. The mutation
first finds the posts to update (the `filter`) and then sets new values for the
post's fields with the `set` argument. To learn how the `filter` works, let's
[look at how Dgraph Cloud handles queries](./react-graphql-queries).


# GraphQL Queries
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/graphql/react-graphql-queries

GraphQL queries are about starting points and traversals. From simple queries to deep filters, dive into the queries use in the message board app.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

As we learned earlier, GraphQL queries are about starting points and traversals.
For example, a query can start by finding a post, and then traversing edges from
that post to find the author, category, comments and authors of all the
comments.
![query a post and follow relationships](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/post2-search-in-graph.png)

## Dgraph Cloud Query

In the API that Dgraph Cloud built from the schema, queries are named for the
types that they let you query: `queryPost`, `queryUser`, etc. A query starts
with, for example, `queryPost` or by filtering to some subset of posts like
`queryPost(filter: ...)`. This defines a starting set of nodes in the graph.
From there, your query traverses into the graph and returns the subgraph it
finds. You can try this out with some example queries in the next section.

## Simple Queries

The simplest queries find some nodes and only return data about those nodes,
without traversing further into the graph. The query `queryUser` finds all
users. From those nodes, we can query the usernames as follows:

```graphql
query {
  queryUser {
    username
  }
}
```

The result will depend on how many users you have added. If it's just the
`User1` sample, then you'll get a result like the following:

```json
{
  "data": {
    "queryUser": [
      {
        "username": "User1"
      }
    ]
  }
}
```

That says that the `data` returned is about the `queryUser` query that was
executed and here's an array of JSON about those users.

## Query by identifier

Because `username` is an identifier, there's also a query that finds users by
ID. To grab the data for a single user if you already know their ID, use the
following query:

```graphql
query {
  getUser(username: "User1") {
    username
  }
}
```

This time the query returns a single object, instead of an array.

```json
{
  "data": {
    "getUser": {
      "username": "User1"
    }
  }
}
```

## Query with traversal

Let's do a bit more traversal into the graph. In the example app's UI you can
display the homepage of a user. You might need to find a user's data and some of
their posts.

![Graph schema sketch](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/user1-post-search-in-graph.png)

Using GraphQL, you can get the same data using the following query:

```graphql
query {
  getUser(username: "User1") {
    username
    displayName
    posts {
      title
    }
  }
}
```

This query finds `User1` as the starting point, grabs the `username` and
`displayName`, and then traverses into the graph following the `posts` edges to
get the titles of all the user's posts.

A query could step further into the graph, finding the category of every post,
like this:

```graphql
query {
  getUser(username: "User1") {
    username
    displayName
    posts {
      title
      category {
        name
      }
    }
  }
}
```

Or, a query could traverse even deeper to get the comments on every post and the
authors of those comments, as follows:

```graphql
query {
  getUser(username: "User1") {
    username
    displayName
    posts {
      title
      category {
        name
      }
      comments {
        text
        author {
          username
        }
      }
    }
  }
}
```

## Querying with filters

To render the app's home screen, the app need to find a list of posts. Knowing
how to find starting points in the graph and traverse with a query means we can
use the following query to grab enough data to display a post list for the home
screen:

```graphql
query {
  queryPost {
    id
    title
    author {
      username
    }
    category {
      name
    }
  }
}
```

We'll also want to limit the number of posts displayed, and order them. For
example, we probably want to limit the number of posts displayed (at least until
the user scrolls) and maybe order them from newest to oldest.

This can be accomplished by passing arguments to `queryPost` that specify how we
want the result sorted and paginated.

```graphql
query {
  queryPost(order: { desc: datePublished }, first: 10) {
    id
    title
    author {
      username
    }
    category {
      name
    }
  }
}
```

The UI for your app also lets users search for posts. To support this, you added
`@search(by: [term])` to your schema so that Dgraph Cloud would build an API for
searching posts. The nodes found as the starting points in `queryPost` can be
filtered down to match only a subset of posts that have the term "graphql" in
the title by adding `filter: { title: { anyofterms: "graphql" }}` to the query,
as follows:

```graphql
query {
  queryPost(
    filter: { title: { anyofterms: "graphql" } }
    order: { desc: datePublished }
    first: 10
  ) {
    id
    title
    author {
      username
    }
    category {
      name
    }
  }
}
```

## Querying with deep filters

The same filtering works during a traversal. For example, we can combine the
queries we have seen so far to find `User1`, and then traverse to their posts,
but only return those posts that have "graphql" in the title.

```graphql
query {
  getUser(username: "User1") {
    username
    displayName
    posts(filter: { title: { anyofterms: "graphql" } }) {
      title
      category {
        name
      }
    }
  }
}
```

Dgraph Cloud builds filters and ordering into the GraphQL API depending on the
types and the placement of the `@search` directive in the schema. Those filters
are then available at any depth in a query, or in returning results from
mutations.

## Queries used in the message board app

The message board app used in this tutorial uses a variety of queries, some of
which are described and shown below:

The following query gets a user's information:

```graphql
query getUser($username: String!) {
  getUser(username: $username) {
    username
    displayName
    avatarImg
  }
}
```

The following query gets all categories. It is used to render the categories
selector on the main page, and to allow a user to select categories when adding
new posts:

```graphql
query {
  queryCategory {
    id
    name
  }
}
```

The followings gets an individual post's data when a user navigates to the
post's URL:

```graphql
query getPost($id: ID!) {
  getPost(id: $id) {
    id
    title
    text
    datePublished
    author {
      username
      displayName
      avatarImg
    }
    comments {
      text
      author {
        username
        displayName
        avatarImg
      }
    }
  }
}
```

Next, you'll [learn how to build your app's React UI](../react-ui/index)


# Build a Message Board App in React
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/introduction

Learn to deploy a GraphQL Backend, design a schema, and implement a React UI. This 2-hour course walks you through it.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

This tutorial walks you through building a reasonably complete message board
app. We selected this app because it's familiar and easy enough to grasp the
schema at a glance, but also extensible enough to add things like integrated
authorization and real-time updates with subscriptions.

## The App

This app is designed to manage lists of posts in different categories. A home
page lets each user view a feed of posts, as follows:

![message board app main page](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/main-screenshot.png)

This app will use Dgraph Cloud's built-in authorization to allow public posts
that anyone can see (even without logging in) but restrict posting messages to
users who are logged-in. We'll also make some categories private, hiding them
from any users who haven't been granted viewer permissions. Users who are
logged-in can create new posts, and each post can have a stream of comments from
other users. A post is rendered on its own page, as follows:

![App post page](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/post-screenshot.png)

This app will be completely serverless app:

* Dgraph Cloud provides the "backend": a Dgraph database in a fully-managed
  environment
* Auth0 provides serverless authentication
* Netlify is used to deploy the app UI (the "frontend")

## Why use GraphQL?

You can build an app using any number of technologies, so why is GraphQL a good
choice for this app?

GraphQL is a good choice in many situations, but particularly where the app data
is inherently a *graph* (a network of data nodes) and where GraphQL queries let
us reduce the complexity of the UI code.

In this case, both are true. The data for the app is itself a graph; it's about
`users`, `posts` and `comments`, and the links between them. You'll naturally
want to explore that data graph as you work with the app, so GraphQL makes a
great choice. Also, in rendering the UI, GraphQL removes some complexity for
you.

If you built this app with REST APIs, for example, our clients (i.e., Web and
mobile) will have to programmatically manage getting all the data to render a
page. So, to render a post using a REST API, you will probably need to access
the `/post/{id}` endpoint to get the post itself, then the
`/comment?postid={id}` endpoint to get the comments, and then (iteratively for
each comment) access the `/author/{id}` endpoint. You would have to collect the
data from those endpoints, discard extra data, and then build a data structure
to render the UI. This approach requires different code in each version of the
app, and increases the engineering effort required and the opportunity for bugs
to occur in our app.

With GraphQL, rendering a page is much simpler. You can run a single GraphQL
query that gets all of the data for a post (its comments and the authors of
those comments) and then simply lay out the page from the returned JSON. GraphQL
gives you a query language to explore the app's data graph, instead of having to
write code to build the data structure that you need from a series of REST API
calls.

## Why Dgraph Cloud?

Dgraph Cloud lets you build a GraphQL API for your app with just a GraphQL
schema, and it gets you to a running GraphQL API faster than any other tool.

Often, a hybrid model is used where a GraphQL API is layered over a REST API or
over a document or relational database. So, in those cases, the GraphQL layer
sits over other data sources and issues many queries to translate the REST or
relational data into something that looks like a graph. There's a cognitive jump
there, because your app is about a graph, but you need to design a relational
schema and work out how that translates into a graph. So, you'll think about the
app in terms of the graph data model, but always have to mentally translate back
and forth between the relational and graph models. This translation presents
engineering challenges, as well as an impact to query efficiency.

You don't have any of these engineering challenges with Dgraph Cloud.

Dgraph Cloud provides a fully-managed Dgraph database that stores all data
natively as a graph; it's a database of nodes and edges, with no relational
database running in the background. Compared to a hybrid model, Dgraph lets you
efficiently store, query and traverse data as a graph. Your data will get stored
just like you design it in the schema, and app queries are a single graph query
that fetches data in a format that can be readily consumed by your app.

With Dgraph Cloud, you design your app in GraphQL. You design a set of GraphQL
types that describes your requirements. Dgraph Cloud takes those types, prepares
graph storage for them, and generates a GraphQL API with queries and mutations.

So, you can design a graph, store a graph and query a graph. You think and
design in terms of the graph that your app needs.

## What's next

First, you will deploy a running Dgraph Cloud backend that will host our GraphQL
API. This gives you a working backend that you can use to build out your app.

Then you will move on to the design process - it's graph-first, in fact, it's
GraphQL-first. After you design the GraphQL types that our app is based around,
Dgraph Cloud provides a GraphQL API for those types; then, you can move straight
to building your app around your GraphQL APIs.

Start by [provisioning your backend](./provision-backend).


# Provision a Dgraph Cloud backend
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/provision-backend

Dgraph Learn - Build a Message Board App in React. Deploy a backend for each app you build with Dgraph Cloud

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

In Dgraph Cloud, an app is served by a GraphQL backend powered by Dgraph
database. You should deploy a backend for each app you build, and potentially
backends for test and development environments as well.

For this tutorial, you'll deploy one backend for development.

The URL listed in "GraphQL Endpoint" is the URL at which Dgraph Cloud serves
data to your app. You'll need that for later, so note it down --- though you'll
always be able to access it from the dashboard. There's nothing at that URL yet,
first you need to design the GraphQL schema for the app.

## Move on to schema design

Let's now move on to the design process - it's graph-first, in fact, it's
GraphQL-first. You'll design the GraphQL types that your app is based around,
learn about how graphs work and then look at some example queries and mutations.

Next, [design your schema](./graphql/design-app-schema).


# Connect to Dgraph Cloud
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/react-ui/connect-to-dgraph-cloud

Apollo client provides a connection to the GraphQL endpoint & a GraphQL cache that lets you manipulate the visual state of the app from the internal cache

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The GraphQL and React state management library you'll be using in the app is
[Apollo Client 3.x](https://www.apollographql.com/docs/react/).

## Apollo client

For the purpose of this app, Apollo client provides a connection to a GraphQL
endpoint and a GraphQL cache that lets you manipulate the visual state of the
app from the internal cache. This helps to keep various components of the UI
that rely on the same data consistent.

Add Apollo client to the project with the following command:

```
yarn add graphql @apollo/client
```

## Create an Apollo client

After Apollo client is added to the app's dependencies, create an Apollo client
instance that is connected to your Dgraph Cloud endpoint. Edit `index.tsx` to
add a function to create the Apollo client, as follows:

```js
const createApolloClient = () => {
  const httpLink = createHttpLink({
    uri: "<<Dgraph Cloud-GraphQL-URL>>",
  })

  return new ApolloClient({
    link: httpLink,
    cache: new InMemoryCache(),
  })
}
```

Make sure to replace `<<Dgraph Cloud-GraphQL-URL>>` with the URL of your Dgraph
Cloud endpoint.

If you didn't note the URL when you created the Dgraph Cloud backend, don't
worry, you can always access it from the Dgraph Cloud dashboard in the Overview
tab.

## Add Apollo client to the React component hierarchy

With an Apollo client created, you then need to pass that client into the React
component hierarchy. Other components in the hierarchy can then use the Apollo
client's React hooks to make GraphQL queries and mutations.

Set up the component hierarchy with the `ApolloProvider` component as the root
component. It takes a `client` argument, which is the remainder of the app.
Change the root of the app in `index.tsx` to use the Apollo component as
follows.

```js
ReactDOM.render(
  <ApolloProvider client={createApolloClient()}>
    <React.StrictMode>
      <App />
    </React.StrictMode>
  </ApolloProvider>,
  document.getElementById("root"),
)
```

## This step in GitHub

This step is also available in the
[tutorial GitHub repo](https://github.com/dgraph-io/discuss-tutorial) with the
[connect-to-slash-graphql tag](https://github.com/dgraph-io/discuss-tutorial/releases/tag/connect-to-slash-graphql)
and is
[this code diff](https://github.com/dgraph-io/discuss-tutorial/commit/56e86302d0d7e77d3861708b77124dab9aeeca61).

There won't be any visible changes from this step.

Now, let's review [routing in React](./react-routing).


# Build a React UI
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/react-ui/index

With the GraphQL backend deployed and serving the schema, you can move directly to building a UI using React and Typescript with the GraphQL Code Generator.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

With the GraphQL backend deployed and serving the schema, you can move directly
to building a UI.

This section of the tutorial walks you through building a UI using React and
Typescript with the
[GraphQL Code Generator](https://graphql-code-generator.com/).

All of the code for building the React app for this tutorial is available on
GitHub in the
[Message Board App Tutorial repo](https://github.com/dgraph-io/discuss-tutorial).

Let's [review the tech stack](./tech-stack).


# React App Boiler Plate
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/react-ui/react-app-boiler-plate

Jump right in thanks to the Message Board App Tutorial repo on GitHub. Get started with your Message Board App in React with GraphQL.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## GitHub repo

All of the code for building the example React app shown in this tutorial is
available in GitHub in the
[Message Board App Tutorial repo](https://github.com/dgraph-io/discuss-tutorial).

Each step in the tutorial is recorded as a tag on the `learn-tutorial` branch in
that repo. That means you can complete each step in the tutorial and also look
at the git diff if you're comparing what's described to the corresponding code
changes.

## Boilerplate

You'd start an app like this with `npx create-react-app ...` and then
`yarn add ...` to add the dependencies listed on the previous page (i.e.,
Tailwind CSS, Semantic UI React, etc.)

This tutorial starts with the minimal boilerplate already complete. To read
through the setup process that was used to build this tutorial, see this
[blog about setting up a Dgraph Cloud app](https://hypermode.com/blog/slash-graphql-app-setup/).

For this tutorial, you can start with the boilerplate React app and CSS by
checking out the setup from GitHub. To do this, see the
[tutorial-boilerplate tag](https://github.com/dgraph-io/discuss-tutorial/releases/tag/tutorial-boilerplate).

You can do this using the `git` CLI.

```sh
git clone https://github.com/dgraph-io/discuss-tutorial
cd discuss-tutorial
git fetch --all --tags
git checkout tags/tutorial-boilerplate -b learn-tutorial
```

Alternatively, you can visit [https://github.com/dgraph-io/discuss-tutorial/tags](https://github.com/dgraph-io/discuss-tutorial/tags)
and download the archive (**.zip** or **.tar.gz**) for the
`tutorial-boilerplate` tag.

## Running app boilerplate

After you have the boilerplate code on your machine, you can start the app using
the following `yarn` command:

```sh
yarn install
yarn start
```

This command builds the source and serves the app UI in development mode. The
app UI is usually served at `http://localhost:3000`, but the exact port may vary
depending on what else is running on your machine. Yarn will report the URL as
soon as it has the server up and running.

Navigate to the provided URL, and you'll see the boilerplate app running, as
seen below:

![running boiler plate app](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/app-boilerplate.png)

At this point, you have just the CSS styling and minimal React setup. Next,
you'll [connect to your graph database](./connect-to-dgraph-cloud).


# Routing in React
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/react-ui/react-routing

Use React Router to build a message board app. A routing library in the UI interprets the URL path and renders an appropriate page for that path.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

In a single-page app like this, a routing library in the UI interprets the URL
path and renders an appropriate page for that path.

## React Router

The routing library you'll be using in the app is
[React Router](https://reactrouter.com/web/guides/quick-start). It provides a
way to create routes and navigate between them. For example, the app's home URL
at `/` will render a list of posts, while `/post/0x123` will render the React
component for the post with id `0x123`.

Add dependencies to the project using the following commands:

```
yarn add react-router-dom
yarn add -D @types/react-router-dom
```

The `-D` option adds the TypeScript types `@types/react-router-dom` to the
project as a development dependency. Types are part of the development
environment of the project to help you build the app; but, in the build these
types are compiled away.

## Add components

You'll need components for the app to route to the various URLs. Create a
`src/components` directory, and then components for the home page
(`components/home.tsx`) and posts (`components/post.tsx`), with the following
file content:

```js
// components/home.tsx
import React from "react"

export function Home() {
  return <div>Home</div>
}
```

```js
// components/post.tsx
import React from "react"

export function Post() {
  return <div>Post</div>
}
```

You can leave those as boilerplate for now and fill them in when you add GraphQL
queries in the next step of this tutorial.

## Add routing

With the boilerplate components in place, you are now ready to add the routing
logic to your app. Edit `App.tsx` to add routes for the `home` and `post` views,
as shown below.

Note that the base URL points to the `home` component and `/post/:id` to the
`post` component. In the post component, `id` is used to get the data for the
right post:

```js
...
import { Home } from "./components/home";
import { Post } from "./components/post";
import { BrowserRouter, Switch, Route } from "react-router-dom";

export function App() {
  return (
    <>
      <div className="app-banner">
        ...
      <div className="App">
        <div className="mt-4 mx-8">
          <p>
            Learn about building GraphQL apps with Dgraph Cloud at https://dgraph.io/learn
          </p>
          <BrowserRouter>
            <Switch>
              <Route exact path="/post/:id" component={Post} />
              <Route exact path="/" component={Home} />
            </Switch>
          </BrowserRouter>
        </div>
      </div>
    </>
  );
}
```

## This Step in GitHub

This step is also available in the
[tutorial GitHub repo](https://github.com/dgraph-io/discuss-tutorial) with the
[routing-in-react tag](https://github.com/dgraph-io/discuss-tutorial/releases/tag/routing-in-react)
and is
[this code diff](https://github.com/dgraph-io/discuss-tutorial/commit/8d488e8c9bbccaa96c88fc49860021c493f1afca).

You can run the app using the `yarn start` command, and then you can navigate to
`http://localhost:3000` and `http://localhost:3000/post/0x123` to see the
various pages rendered.

Next, let's [wire up the queries](./react-ui-graphql-queries).


# GraphQL Mutations
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/react-ui/react-ui-graphql-mutations

With a working UI for querying sample data you added, you now need a UI to add new posts using GraphQL Mutations in Apollo React.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Working through the tutorial to this point gives you a working UI that you can
use to query the sample data that you added, but doesn't give you a UI to add
new posts.

To add new posts, you'll need to generate and use GraphQL Code Generator hooks
for adding posts and layout the UI components so a user can enter the data.

## GraphQL fragments

In this part of the tutorial, you'll add the ability to add a post. That's an
`addPost` mutation, and a GraphQL mutation can return data, just like a query.
In this case, it makes sense to have the `addPost` mutation return the same data
as the `allPosts` query, because the UI should adjust to insert the new post
into the home page's post list. GraphQL has a nice mechanism called *fragments*
to allow this type of reuse.

In the previous section, you added the `allPosts` query like this:

```graphql
query allPosts {
  queryPost(order: { desc: datePublished }) {
    id
    title
    tags
    datePublished
    category {
      id
      name
    }
    author {
      username
      displayName
      avatarImg
    }
    commentsAggregate {
      count
    }
  }
}
```

This can be easily changed to use a fragment by defining the body of the query
as a fragment and then using that in the query. You can do this by updating the
definition of `allPosts` in the `src/components/operations.graphql` file as
follows:

```graphql
fragment postData on Post {
  id
  title
  text
  tags
  datePublished
  category {
    id
    name
  }
  author {
    username
    displayName
    avatarImg
  }
  commentsAggregate {
    count
  }
}

query allPosts {
  queryPost(order: { desc: datePublished }) {
    ...postData
  }
}
```

The syntax `...postData` says "take the `postData` fragment and use it here".

## GraphQL mutations

With a fragment setup for the return data, the mutation to add a post can use
exactly the same result data.

Add the following definition to `src/components/operations.graphql` to add the
mutation that lets users add a post:

```graphql
mutation addPost($post: AddPostInput!) {
  addPost(input: [$post]) {
    post {
      ...postData
    }
  }
}
```

This mutation expects input data in the shape of the `AddPostInput` input type.
TypeScript, and GraphQL Code Generator will make sure you provide an input of
the correct type. This mutation returns data of the same shape as the `allPosts`
query; you'll see why that's important when using the Apollo cache.

Run the following command to tell the GraphQL Code Generator to generate a React
hook, `useAddPostMutation`, that extracts the component logic of this mutation
into a reusable function:

```sh
yarn run generate-types
```

The boilerplate to use a query is to use the query as part of loading the
component, as in the following example:

```js
const { data, loading, error } = useAllPostsQuery()

if (loading) {
  /* render loading indicator */
}

if (error) {
  /* handle error */
}

// layout using 'data'
```

However, mutations work differently. To use a mutation, you use the hook to
create a function that actually runs the mutation and configure that with
callback functions that execute after the mutation completes. Accordingly, the
boilerplate for a mutation is as follows:

```js
const [addPost] = useAddPostMutation({
  /* what happens after the mutation is executed */
})
```

With this syntax, calling `addPost({ variables: ... })` executes the mutation
with the passed-in post data, and after the GraphQL mutation returns, the
callback functions are executed.

## Apollo cache

As well as GraphQL support, the Apollo Client library also provides state
management, using the Apollo Cache.

You can follow the flow of adding a new post, as follows: The user is on the
home (post list) page. There, they press a button to create a post, which brings
up a modal UI component (sometimes called a *modal dialog*) to enter the post
data. The user fills in the details of the post, and then the mutation is
submitted when they press *Submit*. This results in a new post, but how does
that new post get into the list of posts? One option is to force a reload of the
whole page, but that'll force all components to reload and probably won't be a
great user experience. Another option is to just force reloading of the
`allPosts` query, as follows:

```js
const [addPost] = useAddPostMutation({
    refetchQueries: [ { query: /* ... allPosts ... */ } ],
})
```

This would work, but still requires two round-trips from the UI to the server to
complete:

1. Clicking *Submit* on the new post sends data to the server, and the UI waits
   for that to complete (one round trip)
2. This then triggers execution of the `allPosts` query to execute (a second
   round trip)

When the `allPosts` query is re-executed, it changes the `data` value of
`const { data, loading, error } = useAllPostsQuery()` in the post list
component, and React re-renders that component.

Again, this works, but it could be more efficient: The UI actually already has
all of the data it needs to render the updated UI after the first round trip,
because the new post on the server is only going to be the post that was added
by the mutation. So, to avoid a trip to the server, you can manually update
Apollo's view of the result of the `allPosts` query and force the re-render,
without round-tripping to the server. That's done by editing the cached value,
as follows:

```js
const [addPost] = useAddPostMutation({
  update(cache, { data }) {
    const existing =
      cache.readQuery <
      AllPostsQuery >
      {
        query: AllPostsDocument,
      }

    cache.writeQuery({
      query: AllPostsDocument,
      data: {
        queryPost: [
          ...(data?.addPost?.post ?? []),
          ...(existing?.queryPost ?? []),
        ],
      },
    })
  },
})
```

That sets up the `addPost` function to run the `addPost` mutation, and on
completion inserts the new post into the cache.

## Layout for the mutation

All the logic for adding a post will be in the app header:
`src/component/header.tsx`. This logic adds a button that shows a modal to add
the post. The visibility of the modal is controlled by React state, set up
through the `useState` hook, as follows:

```js
const [createPost, setCreatePost] = useState(false)
...
<Button className="dgraph-btn mr-1" onClick={() => setCreatePost(true)}>
  Create Post
</Button>
```

The state for the new post data is again controlled by React state. The modal
gives the user input options to update that data, as follows:

```js
  const [title, setTitle] = useState("")
  const [category, setCategory]: any = useState("")
  const [text, setText]: any = useState("")
  const [tags, setTags]: any = useState("")
```

Then, clicking submit in the modal closes it and calls a function that collects
together the state and calls the `addPost` function, as follows:

```js
const submitPost = () => {
  setCreatePost(false)
  const post = {
    text: text,
    title: title,
    tags: tags,
    category: { id: category },
    author: { username: "TestUser" },
    datePublished: new Date().toISOString(),
    comments: [],
  }
  addPost({ variables: { post: post } })
}
```

The modal is now set up with a list of possible categories for the post by first
querying to find the existing categories and populating a dropdown from that.
With all of these changes, the `src/component/header.tsx` file looks as follows:

```js
import React, { useState } from "react"
import {
  Image,
  Modal,
  Form,
  Button,
  Dropdown,
  Loader,
  TextArea,
} from "semantic-ui-react"
import { Link } from "react-router-dom"
import {
  useAddPostMutation,
  AllPostsQuery,
  useCategoriesQuery,
  AllPostsDocument,
} from "./types/operations"

export function AppHeader() {
  const [createPost, setCreatePost] = useState(false)
  const [title, setTitle] = useState("")
  const [category, setCategory]: any = useState("")
  const [text, setText]: any = useState("")
  const [tags, setTags]: any = useState("")

  const {
    data: categoriesData,
    loading: categoriesLoading,
    error: categoriesError,
  } = useCategoriesQuery()

  const addPostButton = () => {
    if (categoriesLoading) {
      return <Loader active />
    } else if (categoriesError) {
      return <div>`Error! ${categoriesError.message}`</div>
    } else {
      return (
        <Button className="dgraph-btn mr-1" onClick={() => setCreatePost(true)}>
          Create Post
        </Button>
      )
    }
  }

  const categoriesOptions = categoriesData?.queryCategory?.map((category) => {
    return { key: category?.id, text: category?.name, value: category?.id }
  })

  const [addPost] = useAddPostMutation({
    update(cache, { data }) {
      const existing = cache.readQuery<AllPostsQuery>({
        query: AllPostsDocument,
      })

      cache.writeQuery({
        query: AllPostsDocument,
        data: {
          queryPost: [
            ...(data?.addPost?.post ?? []),
            ...(existing?.queryPost ?? []),
          ],
        },
      })
    },
  })

  const submitPost = () => {
    setCreatePost(false)
    const post = {
      text: text,
      title: title,
      tags: tags,
      category: { id: category },
      author: { username: "TestUser" },
      datePublished: new Date().toISOString(),
      comments: [],
    }
    addPost({ variables: { post: post } })
  }

  const showCreatePost = (
    <Modal
      onClose={() => setCreatePost(false)}
      onOpen={() => setCreatePost(true)}
      open={createPost}
    >
      <Modal.Header>Create Post</Modal.Header>
      <Modal.Content>
        <Modal.Description>
          <Form>
            <Form.Field>
              <label>Title</label>
              <input
                placeholder="Type title..."
                onChange={(e) => setTitle(e.target.value)}
              />
            </Form.Field>
            <Form.Field>
              <label>Category</label>
              <Dropdown
                placeholder="You must select a category to continue..."
                fluid
                search
                selection
                options={categoriesOptions}
                onChange={(e, data) => setCategory(data.value)}
              />
            </Form.Field>
            <Form.Field>
              <label>Tags (optional)</label>
              <input
                placeholder="Enter space separated tags..."
                onChange={(e) => setTags(e.target.value)}
              />
            </Form.Field>
            <Form.Field>
              <label>Your Message</label>
              <TextArea
                rows="3"
                placholder="Enter your message..."
                onChange={(e, data) => setText(data.value)}
              />
            </Form.Field>
          </Form>
        </Modal.Description>
      </Modal.Content>
      <Modal.Actions>
        <Button color="black" onClick={() => setCreatePost(false)}>
          Cancel
        </Button>
        <Button
          content="Submit"
          labelPosition="right"
          icon="checkmark"
          onClick={submitPost}
          positive
        />
      </Modal.Actions>
    </Modal>
  )

  return (
    <>
      {showCreatePost}
      <div className="ui clearing segment header-seg">
        <h3 className="ui right floated header header-seg-right">
          <span>{addPostButton()}</span>
        </h3>
        <h3 className="ui left floated header header-seg-left">
          <Link to="/">
            <div className="flex">
              <span>
                <Image size="tiny" src="/diggy.png" className="mr-5" />
              </span>
              <div>
                <p className="header-text">Dgraph</p>
                <p className="t-size">DISCUSS</p>
              </div>
            </div>
          </Link>
        </h3>
      </div>
    </>
  )
}
```

All of this adds a **Create Post** button to the header, along with supporting
logic:

![create post button](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/create-post-button.png)

When clicked, this button brings up the modal to create the new post:

![new post modal](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/new-post-modal.png)

## This Step in GitHub

This step is also available in the
[tutorial GitHub repo](https://github.com/dgraph-io/discuss-tutorial) with the
[graphql-mutations tag](https://github.com/dgraph-io/discuss-tutorial/releases/tag/graphql-mutations)
and is
[this code diff](https://github.com/dgraph-io/discuss-tutorial/commit/42d2c810cf2168cde630becf693466ae6acbdf50).

You can run the app using the `yarn start` command, and then navigate to
`http://localhost:3000` to see the post list on the home screen. Then, you can
click **Create Post** to add a new post to the backend GraphQL database. After
submitting the post, you'll see it in the post list.

The user the post is added for is hard-coded in this step (to "TestUser").

We're just about there. Let's [wrap up what we've learned](../conclusion).


# GraphQL Queries
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/react-ui/react-ui-graphql-queries

In this step, you can move on to the GraphQL queries that get the data to render the main pages, thanks to Apollo Client, Dgraph Cloud, and React routing.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

With Apollo Client set up and connected to your Dgraph Cloud backend, and React
routing set up, you can move on to the GraphQL queries that get the data to
render the main pages.

You'll use [GraphQL Code Generator](https://graphql-code-generator.com/) to
generate typed React hooks that help contact the GraphQL endpoint, and then use
those hooks in the React components to get the data.

## GraphQL Code Generator

The Apollo Client libraries give you generic React hooks to contact GraphQL
backends, but [GraphQL Code Generator](https://graphql-code-generator.com/)
takes that to the next level by using GraphQL introspection to generate types
with hooks specific to the API you are using. That means all your GraphQL calls
are typed and if anything ever changes, you'll know at development time.

Firstly, add all the GraphQL Code Generator dependencies as development
dependencies to the project with:

```sh
yarn add -D @graphql-codegen/cli @graphql-codegen/typescript @graphql-codegen/typescript-operations @graphql-codegen/typescript-react-apollo @graphql-codegen/add @graphql-codegen/near-operation-file-preset @graphql-codegen/named-operations-object
```

You can then run the following command to set up GraphQL Code Generator for the
project:

```sh
yarn graphql-codegen init
> ... answer questions ...
```

However, you can skip the setup steps and jump straight to using it by adding a
file, `codegen.yml`, in the top-level project directory. The following is the
configuration needed for this project. Remember to replace
`<<Dgraph Cloud-GraphQL-URL>>` with the URL or your Dgraph Cloud endpoint.

```yaml
overwrite: true
schema: "<<Dgraph Cloud-GraphQL-URL>>"
documents:
  - "src/**/*.graphql"
generates:
  src/types/graphql.ts:
    plugins:
      - typescript
  src/:
    preset: near-operation-file
    presetConfig:
      baseTypesPath: types/graphql
      folder: types
      extension: .ts
    plugins:
      - typescript-operations
      - typescript-react-apollo
      - named-operations-object
    config:
      reactApolloVersion: 3
      withHOC: false
      withHooks: true
      withComponent: false
```

That configuration tells GraphQL Code Generator to introspect the schema of your
GraphQL API, generate using the `typescript` plugin and place the generated code
`near-operation-file` (we'll see what that means just below).

Then, add `"generate-types": "graphql-codegen --config codegen.yml"` to the
scripts key in package.json, so it now looks like:

```json
"scripts": {
  "start": "react-scripts start",
  "build": "react-scripts build",
  "test": "react-scripts test",
  "eject": "react-scripts eject",
  "generate-types": "graphql-codegen --config codegen.yml"
}
```

Now, whenever the schema of your GraphQL database changes, you can regenerate
the project's types with:

```sh
yarn run generate-types
```

Running that now, won't do anything though, because you have to start your
GraphQL development first.

## GraphQL operations

You can layout the source of a Dgraph Cloud project however you wish. For this
tutorial you'll use the following project structure.

```
public
scripts
src
  components
    component1.tsx
    component2.tsx
    operations.graphql
    types
      operations.ts
  ...
  types
    graphql.ts
```

You'll write GraphQL queries and mutations in the `operations.graphql` file.
Then, run GraphQL Code Generator and it generates the `src/types/graphql.ts`
file with global types for the things that make sense globally and
`src/components/types/operations.ts` for things that are local to the
components.

Having `operations.graphql` file in the directory for the components that it
applies to makes it really easy to find the GraphQL (rather than it being split
as strings in a number of javascript files) while still making it clear what
components the GraphQL applies to. If your project gets larger, you might end up
with more project structure and more operations files, but the general process
still works.

Start by creating the `scr/components/operations.graphql` file and add a query
to find the data for home page's list of posts.

```graphql
query allPosts {
  queryPost(order: { desc: datePublished }) {
    id
    title
    tags
    datePublished
    category {
      id
      name
    }
    author {
      username
      displayName
      avatarImg
    }
    commentsAggregate {
      count
    }
  }
}
```

Then run:

```sh
yarn run generate-types
```

...and GraphQL Code Generator will create the `src/types/graphql.ts` and
`src/components/types/operations.ts` files. If your interested in what was
generated, open up those files and you'll see how much the code generator did.
If you want to use that to build a UI, read on.

## GraphQL React hooks

Of the things that GraphQL Code Generator built after introspecting your GraphQL
endpoint, it's the React hooks you'll use most in building a UI.

From the `allPosts` query in the `operations.graphql` file, GraphQL Code
Generator built a hook `useAllPostsQuery` with everything you need to make that
GraphQL query.

In general, you'll use it like this

```js
const { data, loading, error } = useAllPostsQuery()

if (loading) {
  /* render loading indicator */
}

if (error) {
  /* handle error */
}

// layout using 'data'
```

The `data` result will have exactly the same structure as the `allPosts`
operation, and it's typed, so you can layout with confidence by for example
using `map` on the post list returned by `queryPost` and then indexing into each
post.

```
data?.queryPost?.map((post) => {
  ...
  post?.author.displayName
  ...
}
```

Because of the types, you really can't go wrong.

## Layout with GraphQL - post list

Now that you have GraphQL to help write queries and get data and GraphQL Code
Generator to turn that into typed Javascript, you can now layout your data and
be sure you won't make a mistake because GraphQL and types will catch you.

Let's make a `PostFeed` component that uses the `useAllPostsQuery` and renders
the data into a Semantic React UI `Table`.

```js
import React from "react"
import {
  Header,
  Label,
  Loader,
  Image,
  Table,
  Container,
} from "semantic-ui-react"
import { useAllPostsQuery } from "./types/operations"
import { Link } from "react-router-dom"
import { avatar } from "./avatar"

export function PostFeed() {
  const { data, loading, error } = useAllPostsQuery()
  if (loading) return <Loader active />
  if (error) {
    return (
      <Container text className="mt-24">
        <Header as="h1">Ouch! That page didn't load</Header>
        <p>Here's why : {error.message}</p>
      </Container>
    )
  }

  const items = data?.queryPost?.map((post) => {
    const likes = Math.floor(Math.random() * 10)
    const replies = post?.commentsAggregate?.count
    const tagsArray = post?.tags?.trim().split(/\s+/) || []

    return (
      <Table.Row key={post?.id}>
        <Table.Cell>
          <Link
            to={{
              pathname: "/post/" + post?.id,
            }}
          >
            <Header as="h4" image>
              <Image src={avatar(post?.author.avatarImg)} rounded size="mini" />
              <Header.Content>
                {post?.title}
                <Header.Subheader>{post?.author.displayName}</Header.Subheader>
              </Header.Content>
            </Header>
          </Link>
        </Table.Cell>
        <Table.Cell>
          <span className="ui red empty mini circular label"></span>
          {" " + post?.category.name}
        </Table.Cell>
        <Table.Cell>
          {tagsArray.map((tag) => {
            if (tag !== "") {
              return (
                <Label as="a" basic color="grey" key={tag}>
                  {tag}
                </Label>
              )
            }
            return " "
          })}
        </Table.Cell>
        <Table.Cell>
          <p>
            <i className="heart outline icon"></i> {likes} Like
            {likes === 1 ? "" : "s"}
          </p>
          <p>
            <i className="comment outline icon"></i> {replies}
            {replies === 1 ? "Reply" : "Replies"}
          </p>
        </Table.Cell>
      </Table.Row>
    )
  })

  return (
    <>
      <Table basic="very">
        <Table.Header>
          <Table.Row>
            <Table.HeaderCell>Posts</Table.HeaderCell>
            <Table.HeaderCell>Category</Table.HeaderCell>
            <Table.HeaderCell>Tags</Table.HeaderCell>
            <Table.HeaderCell>Responses</Table.HeaderCell>
          </Table.Row>
        </Table.Header>

        <Table.Body>{items}</Table.Body>
      </Table>
    </>
  )
}
```

There's some layout and CSS styling in there, but the actual data layout is just
indexing into the queried data with `post?.title`, `post?.author.displayName`,
etc. Note that the title of the post is made into a link with the following:

```js
<Link to={{ pathname: "/post/" + post?.id }}> ... </Link>
```

When clicked, this link will go through the React router to render the post
component.

You can add whatever avatar links you like into the data, and you'll do that
later in the tutorial after you add authorization and logins; but for now, make
a file `src/components/avatar.ts` and fill it with this function that uses
random avatars we've supplied with the app boilerplate, as follows:

```js
export function avatar(img: string | null | undefined) {
  return img ?? "/" + Math.floor(Math.random() * (9 - 1) + 1) + ".svg"
}
```

Then, update the `src/components/home.tsx` component to render the post list, as
follows:

```js
import React from "react"
import { PostFeed } from "./posts"

export function Home() {
  return <div className="layout-margin">{PostFeed()}</div>
}
```

With this much in place, you will see a home screen (start the app with
`yarn start` if you haven't already) with a post list of the sample data you
have added to your Dgraph Cloud database.

![post list component](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/post-list-component.png)

Each post title in the post list is a link to `/post/0x...` for the id of the
post. At the moment, those like won't work because there's not component to
render the post. Let's add that component now.

## Layout of a post with GraphQL

Adding a new component that relies on different data is a matter of adding the
right query to `src/components/operations.graphql`, regenerating with GraphQL
Code Generator, and then using the generated hook to layout a component.

Add a GraphQL query that gets a particular post by it's id to
`src/components/operations.graphql` with this GraphQL query.

```graphql
query getPost($id: ID!) {
  getPost(id: $id) {
    id
    title
    text
    tags
    datePublished
    category {
      id
      name
    }
    author {
      username
      displayName
      avatarImg
    }
    comments {
      id
      text
      commentsOn {
        comments {
          id
          text
          author {
            username
            displayName
            avatarImg
          }
        }
      }
      author {
        username
        displayName
        avatarImg
      }
    }
  }
}
```

Then, regenerate with:

```sh
yarn run generate-types
```

...and you'll be able to use the `useGetPostQuery` hook in a component. The
difference with the previous hook is that `useGetPostQuery` relies on a variable
`id` to query for a particular post. You'll use React router's `useParams` to
get the id passed to the route and then pass that to `useGetPostQuery` like
this:

```js
const { id } = useParams<PostParams>()

const { data, loading, error } = useGetPostQuery({
  variables: { id: id },
})
```

Laying out the post component is then a matter of using the `data` from the hook
to layout an interesting UI. Edit the `src/components/post.tsx` component, so it
lays out the post's data like this:

```js
import React from "react"
import { useParams } from "react-router-dom"
import {
  Container,
  Header,
  Loader,
  Image,
  Label,
  Comment,
} from "semantic-ui-react"
import { useGetPostQuery } from "./types/operations"
import { DateTime } from "luxon"
import { avatar } from "./avatar"

interface PostParams {
  id: string
}

export function Post() {
  const { id } = useParams<PostParams>()

  const { data, loading, error } = useGetPostQuery({
    variables: { id: id },
  })
  if (loading) return <Loader active />
  if (error) {
    return (
      <Container text className="mt-24">
        <Header as="h1">Ouch! That page didn't load</Header>
        <p>Here's why : {error.message}</p>
      </Container>
    )
  }
  if (!data?.getPost) {
    return (
      <Container text className="mt-24">
        <Header as="h1">This is not a post</Header>
        <p>You've navigated to a post that doesn't exist.</p>
        <p>That most likely means that the id {id} isn't the id of post.</p>
      </Container>
    )
  }

  let dateStr = "at some unknown time"
  if (data.getPost.datePublished) {
    dateStr =
      DateTime.fromISO(data.getPost.datePublished).toRelative() ?? dateStr
  }

  const paras = data.getPost.text.split("\n").map((str) => (
    <p key={str}>
      {str}
      <br />
    </p>
  ))

  const comments = (
    <div className="mt-3">
      {data.getPost.comments?.map((comment) => {
        return (
          <Comment.Group key={comment.id}>
            <Comment>
              <Comment.Avatar
                src={avatar(comment.author.avatarImg)}
                size="mini"
              />
              <Comment.Content>
                <Comment.Author as="a">
                  {comment.author.displayName}
                </Comment.Author>
                <Comment.Text>{comment.text}</Comment.Text>
              </Comment.Content>
            </Comment>
          </Comment.Group>
        )
      })}
    </div>
  )

  return (
    <div className="layout-margin">
      <div>
        <Header as="h1">{data.getPost.title} </Header>
        <span className="ui red empty mini circular label"></span>
        {" " + data.getPost?.category.name + "  "}
        {data.getPost?.tags
          ?.trim()
          .split(/\s+/)
          .map((tag) => {
            if (tag !== "") {
              return (
                <Label as="a" basic color="grey" key={tag}>
                  {tag}
                </Label>
              )
            }
          })}
      </div>
      <Header as="h4" image>
        <Image
          src={avatar(data.getPost?.author.avatarImg)}
          rounded
          size="mini"
        />
        <Header.Content>
          {data.getPost?.author.displayName}
          <Header.Subheader>{dateStr}</Header.Subheader>
        </Header.Content>
      </Header>
      {paras}
      {comments}
    </div>
  )
}
```

Now you can click on a post from the home screen and navigate to its page.

![post component](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/message-board-app/post-component.png)

## This Step in GitHub

This step is also available in the
[tutorial GitHub repo](https://github.com/dgraph-io/discuss-tutorial) with the
[graphql-queries tag](https://github.com/dgraph-io/discuss-tutorial/releases/tag/graphql-queries)
and is
[this code diff](https://github.com/dgraph-io/discuss-tutorial/commit/db0fb435060d7369e11148054743b73fa62813f5).

If you have the app running (`yarn start`) you can navigate to
`http://localhost:3000` to see the post list on the home screen and click on a
post's title to navigate to the post's page. In the diff, we've added a little
extra, like the Diggy logo, that's also a link to navigate you home.

Now [on to the mutations](./react-ui-graphql-mutations)!


# Tech Stack
Source: https://docs.hypermode.com/dgraph/guides/message-board-app/react-ui/tech-stack

The tech stack for this project includes Dgraph Cloud, React, Typescript, Apollo Client 3.x, Semantic UI React, Tailwind CSS, and Luxon.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The tech stack for the message board example app is as follows:

* [Dgraph Cloud](https://dgraph.io/cloud): backend database
* [React](https://reactjs.org/): JavaScript UI framework
* [Typescript](https://www.typescriptlang.org/): types for JavaScript
* [Apollo Client 3.x](https://www.apollographql.com/docs/react/): GraphQL client
  and React state management
* [Semantic UI React](https://react.semantic-ui.com/): UI component framework
* [Tailwind CSS](https://tailwindcss.com/): CSS framework
* [Luxon](https://moment.github.io/luxon/) : date and time formatting.

This app also uses the
[GraphQL Code Generator](https://graphql-code-generator.com/) to generate
TypeScript-friendly React hooks for the GraphQL queries and mutations.

Now, let's [get started building](./react-app-boiler-plate).


# Authorization Rules
Source: https://docs.hypermode.com/dgraph/guides/to-do-app/auth-rules

Use the @auth directive to limit access to the user’s to-dos. This step in the GraphQL tutorial walks you through authorization rules.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

<Note>This is part 3 of [Building a To-Do List App](./introduction).</Note>

In the current state of the app, we can view anyone's todos, but we want our
to-dos to be private to us. Let's do that using the `@auth` directive to limit
that to the user's to-dos.

We want to limit the user to its own to-dos, so we will define the query in
`auth` to filter depending on the user's username.

Let's update the schema to include that, and then let's understand what is
happening there -

```graphql
type Task
  @auth(
    query: {
      rule: """
      query($USER: String!) {
          queryTask {
              user(filter: { username: { eq: $USER } }) {
                  __typename
              }
          }
      }
      """
    }
  ) {
  id: ID!
  title: String! @search(by: [fulltext])
  completed: Boolean! @search
  user: User!
}
type User {
  username: String! @id @search(by: [hash])
  name: String
  tasks: [Task] @hasInverse(field: user)
}
```

Resubmit the updated schema -

```
curl -X POST localhost:8080/admin/schema --data-binary '@schema.graphql'
```

Now let's see what does the definition inside the `auth` directive means.
Firstly, we can see that this rule applies to `query` (similarly we can define
rules on `add`, `update` etc.).

```graphql
query ($USER: String!) {
  queryTask {
    user(filter: { username: { eq: $USER } }) {
      __typename
    }
  }
}
```

The rule contains a parameter `USER` which we will use to filter the todos by a
user. As we know `queryTask` returns an array of `task` that contains the `user`
also and we want to filter it by `user`, so we compare the `username` of the
user with the `USER` passed to the auth rule (logged in user).

Now the next thing you would be wondering is that how do we pass a value for the
`USER` parameter in the auth rule since its not something that you can call, the
answer is pretty simple actually that value will be extracted from the JWT token
which we pass to our GraphQL API as a header and then it will execute the rule.

Let's see how we can do that in the next step using Auth0 as an example.


# Using Auth0
Source: https://docs.hypermode.com/dgraph/guides/to-do-app/auth0-jwt

Get an app running with Auth0. This step in the GraphQL tutorial walks you through using Auth0 in an example to-do app tutorial.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

<Note>This is part 4 of [Building a To-Do List App](./introduction).</Note>

Let's start by going to our Auth0 dashboard where we can see the app which we
have already created and used in our frontend-app.

![Dashboard](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/dashboard.png)

Now we want to use the JWT that Auth0 generates, but we also need to add custom
claims to that token which will be used by our auth rules. So we can use
something known as "Rules" (left sidebar on dashboard page under "Auth
Pipeline") to add custom claims to a token. Let's create a new empty rule.

![Rule](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/rule.png)

Replace the content with the following -

```javascript
function (user, context, callback) {
  const namespace = "https://dgraph.io/jwt/claims";
  context.idToken[namespace] =
    {
      'USER': user.email,
    };

  return callback(null, user, context);
}
```

In the above function, we are only just adding the custom claim to the token
with a field as `USER` which if you recall from the last step is used in our
auth rules, so it needs to match exactly with that name.

Now let's go to `Settings` of our Auth0 app and then go down to view the
`Advanced Settings` to check the JWT signature algorithm (OAuth tab) and then
get the certificate (Certificates tab). We will be using `RS256` in this example
so let's make sure it's set to that and then copy the certificate which we will
use to get the public key. Use the download certificate button there to get the
certificate in `PEM`.

![Certificate](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/certificate.png)

Now let's run a command to get the public key from it, which we will add to our
schema. Just change the `file_name` and run the command.

```
openssl x509 -pubkey -noout -in file_name.pem
```

Copy the public key and now let's add it to our schema. For doing that we will
add something like this, to the bottom of our schema file -

```
# Dgraph.Authorization {"VerificationKey":"<AUTH0-APP-PUBLIC-KEY>","Header":"X-Auth-Token","Namespace":"https://dgraph.io/jwt/claims","Algo":"RS256","Audience":["<AUTH0-APP-CLIENT-ID>"]}
```

Let me just quickly explain what each thing means in that, so firstly we start
the line with a `#  Dgraph.Authorization`. Next is the `VerificationKey`, so
update `<AUTH0-APP-PUBLIC-KEY>` with your public key within the quotes and make
sure to have it in a single line and add `\n` where ever needed. Then set
`Header` to the name of the header `X-Auth-Token` (can be anything) which will
be used to send the value of the JWT. Next is the `Namespace` name
`https://dgraph.io/jwt/claims` (again can be anything, just needs to match with
the name specified in Auth0). Then next is the `Algo` which is `RS256`, the JWT
signature algorithm (another option is `HS256` but remember to use the same
algorithm in Auth0). Then for the `Audience`, add your app's Auth0 client ID.

The updated schema will look something like this (update the public key with
your key) -

```graphql
type Task
  @auth(
    query: {
      rule: """
      query($USER: String!) {
          queryTask {
              user(filter: { username: { eq: $USER } }) {
                  __typename
              }
          }
      }
      """
    }
  ) {
  id: ID!
  title: String! @search(by: [fulltext])
  completed: Boolean! @search
  user: User!
}
type User {
  username: String! @id @search(by: [hash])
  name: String
  tasks: [Task] @hasInverse(field: user)
}
# Dgraph.Authorization {"VerificationKey":"<AUTH0-APP-PUBLIC-KEY>","Header":"X-Auth-Token","Namespace":"https://dgraph.io/jwt/claims","Algo":"RS256","Audience":["<AUTH0-APP-CLIENT-ID>"]}
```

Resubmit the updated schema -

```
curl -X POST localhost:8080/admin/schema --data-binary '@schema.graphql'
```

Let's get that token and see what all it contains, then update the frontend
accordingly. For doing this, let's start our app again.

```
npm start
```

Now open a browser window, navigate to
[http://localhost:3000](http://localhost:3000) and open the developer tools, go
to the `network` tab and find a call called `token` to get your JWT from its
response JSON (field `id_token`).

![Token](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/token.png)

Now go to [jwt.io](https://jwt.io) and paste your token there.

![jwt](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/jwt.png)

The token also includes our custom claim like below.

```json
{
"https://dgraph.io/jwt/claims": {
    "USER": "vardhanapoorv"
  },
  ...
}
```

Now, you can check if the auth rule that we added is working as expected or not.
Open the GraphQL tool (Insomnia, GraphQL Playground) add the URL along with the
header `X-Auth0-Token` and its value as the JWT. Let's try the query to see the
todos and only the todos the logged-in user created should be visible.

```graphql
query {
  queryTask {
    title
    completed
    user {
      username
    }
  }
}
```

The above should give you only your todos and verifies that our auth rule
worked!

Now let's update our frontend app to include the `X-Auth0-Token` header with
value as JWT from Auth0 when sending a request.

To do this, we need to update the Apollo client setup to include the header
while sending the request, and we need to get the JWT from Auth0.

The value we want is in the field `idToken` from Auth0. We get that by quickly
updating `react-auth0-spa.js` to get `idToken` and pass it as a prop to our
`App`.

```javascript
...

const [popupOpen, setPopupOpen] = useState(false);
const [idToken, setIdToken] = useState("");

...

if (isAuthenticated) {
        const user = await auth0FromHook.getUser();
        setUser(user);
        const idTokenClaims = await auth0FromHook.getIdTokenClaims();
        setIdToken(idTokenClaims.__raw);
}

...

const user = await auth0Client.getUser();
const idTokenClaims = await auth0Client.getIdTokenClaims();

setIdToken(idTokenClaims.__raw);

...

{children}
      <App idToken={idToken} />
    </Auth0Context.Provider>

...

```

Check the updated file
[here](https://github.com/dgraph-io/graphql-sample-apps/blob/c94b6eb1cec051238b81482a049100b1cd15bbf7/todo-app-react/src/react-auth0-spa.js)

Now let's use that token while creating an Apollo client instance and give it to
a header `X-Auth0-Token` in our case. Let's update our `src/App.js` file.

```javascript
...

import { useAuth0 } from "./react-auth0-spa";
import { setContext } from "apollo-link-context";

// Updated to take token
const createApolloClient = token => {
  const httpLink = createHttpLink({
    uri: config.graphqlUrl,
    options: {
      reconnect: true,
    },
});

// Add header
const authLink = setContext((_, { headers }) => {
    // return the headers to the context so httpLink can read them
    return {
      headers: {
        ...headers,
        "X-Auth-Token": token,
      },
    };
});

// Include header
return new ApolloClient({
    link: httpLink,
    link: authLink.concat(httpLink),
    cache: new InMemoryCache()
});

// Get token from props and pass to function
const App = ({idToken}) => {
  const { loading } = useAuth0();
  if (loading) {
    return <div>Loading...</div>;
  }
const client = createApolloClient(idToken);

...
```

Check the updated file
[here](https://github.com/dgraph-io/graphql-sample-apps/blob/c94b6eb1cec051238b81482a049100b1cd15bbf7/todo-app-react/src/App.js).

Refer this step in
[GitHub](https://github.com/dgraph-io/graphql-sample-apps/commit/c94b6eb1cec051238b81482a049100b1cd15bbf7).

Let's now start the app.

```
npm start
```

Now you should have an app running with Auth0!


# Deploying on Dgraph Cloud
Source: https://docs.hypermode.com/dgraph/guides/to-do-app/deploy

In just two steps on Dgraph Cloud (deployment & schema), you get a GraphQL API that you can easily use in any app.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

<Note>This is part 6 of [Building a To-Do List App](./introduction).</Note>

Let's now deploy our fully functional app on Dgraph Cloud
[cloud.dgraph.io](https://cloud.dgraph.io).

### Create a deployment

After successfully logging into the site for the first time, your dashboard
should look something like this.

![Dgraph Cloud: Get Started](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/cloud-1.png)

Let's go ahead and launch a new deployment.

![Dgraph Cloud: Create deployment](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/cloud-2.png)

We named our deployment `todo-app-deployment` and set the optional subdomain as
`todo-app`, using which the deployment will be accessible. We can choose any
subdomain here as long as it's available.

Let's set it up in AWS, in the US region, and click on the *Launch* button.

![Dgraph Cloud: Deployment created](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/cloud-3.png)

Now the backend is ready.

Once the deployment is ready, let's add our schema there (insert your public
key) by going to the schema tab.

```graphql
type Task
  @auth(
    query: {
      rule: """
      query($USER: String!) {
          queryTask {
              user(filter: { username: { eq: $USER } }) {
                  __typename
              }
          }
      }
      """
    }
  ) {
  id: ID!
  title: String! @search(by: [fulltext])
  completed: Boolean! @search
  user: User!
}
type User {
  username: String! @id @search(by: [hash])
  name: String
  tasks: [Task] @hasInverse(field: user)
}
# Dgraph.Authorization {"VerificationKey":"<AUTH0-APP-PUBLIC-KEY>","Header":"X-Auth-Token","Namespace":"https://dgraph.io/jwt/claims","Algo":"RS256","Audience":["<AUTH0-APP-CLIENT-ID>"]}
```

Once the schema is submitted successfully, we can use the GraphQL API endpoint.

Let's update our frontend to use this URL instead of localhost. Open
`src/config.json` and update the `graphqlUrl` field with your GraphQL API
endpoint.

```json
{
    ...
    "graphqlUrl": "<Dgraph-Cloud-GraphQL-API>"
}
```

That's it! Just in two steps on Dgraph Cloud (deployment & schema), we got a
GraphQL API that we can now easily use in any app!


# Using Firebase Authentication
Source: https://docs.hypermode.com/dgraph/guides/to-do-app/firebase-jwt



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

<Note>This is part 5 of [Building a To-Do List App](./introduction).</Note>

In this step, we will add Firebase authentication per the sample
[Todo app with Firebase Authentication](https://github.com/dgraph-io/graphql-sample-apps/tree/master/todo-react-firebase).

### Create Project

Let's start by going to the
[Firebase console](https://console.firebase.google.com/u/0/project/_/authentication/users?pli=1)
and create a new project (Todo-app).

In the **Authentication** section, enable `Email/Password` login. You can add a
custom domain to `Authorized domains` below according to where you want to
deploy your app. By default localhost is added to the list.

![Authentication Section](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/firebase-domains.png)

Now we want to use the JWT that Firebase generates, but we also need to add
custom claims to that token which will be used by our authorization rules.

To add custom claims to the JWT we need to host a cloud function which will
insert claims into the JWT on user creation. This is our cloud function which
inserts `USER`: `email` claim under the Namespace
`https://dgraph.io/jwt/claims`.

```javascript
const functions = require("firebase-functions")
const admin = require("firebase-admin")
admin.initializeApp()

exports.addUserClaim = functions.https.onCall((data, context) => {
  return admin
    .auth()
    .getUserByEmail(data.email)
    .then((user) => {
      return admin.auth().setCustomUserClaims(user.uid, {
        "https://dgraph.io/jwt/claims": {
          USER: data.email,
        },
      })
    })
    .then(() => {
      return {
        message: `Success!`,
      }
    })
    .catch((err) => {
      return err
    })
})
```

### Using the Firebase CLI

Clone the Todo Firebase app repo and try to deploy the function to the Firebase
project created above.

```
git clone https://github.com/dgraph-io/graphql-sample-apps.git
cd graphql-sample-apps/todo-react-firebase
npm i
```

* Install the Firebase CLI tool `npm install -g firebase-tools`.
* Login into Firebase from the CLI `firebase login`.
* Run `firebase init functions` then select an existing project (that you
  created above).
* Select language as `JavaScript` for this example.
* Replace `index.js` with the snippet above.
* Deploy the function \`firebase deploy --only functions.

Please refer to the
[deployment guide](https://firebase.google.com/docs/functions/get-started) for
more info.

![Firebase CLI](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/firebase-cli.png)

### Create Webapp

Create a web app from your Firebase project settings page.

![Firebase Create Webapp](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/firebase-create-webapp.png)

After creating that, copy the config from there.

![Firebase Config](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/firebase-config.png)

Setup your Firebase configuration and `Dgraph Cloud` endpoint in the
[config.json](https://github.com/dgraph-io/graphql-sample-apps/blob/master/todo-react-firebase/src/config.json).
It looks like this:

```json
{
  "apiKey": "your-firebase-apiKey",
  "authDomain": "your-firebase-authDomain",
  "projectId": "your-firebase-projectId",
  "storageBucket": "your-firebase-storageBucket",
  "messagingSenderId": "your-firebase-messagingSenderId",
  "appId": "your-firebase-appId",
  "graphqlUrl": "your-graphql-endpoint"
}
```

Authentication with Firebase is done through the `JWKURL`, where the JSON Web
Key sets are hosted by Firebase. Since Firebase shares the JWKs among multiple
tenants, you must provide your Firebase `project-Id` to the `Audience` field. So
the `Dgraph.Authorization` header will look like this:

```
{"Header":"your-header", "Namespace":"namespace-of-custom-claims","JWKURL": "https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com", "Audience":[your-projectID]}
```

You don't need to set the `VerificationKey` and `Algo` in the `Authorization`
header. Doing so will cause an error.

Update the
[schema](https://github.com/dgraph-io/graphql-sample-apps/blob/master/todo-react-firebase/schema.graphql),
add the Authorization header (update the project-Id) -

```graphql
type Task
  @auth(
    query: {
      rule: """
      query($USER: String!) {
          queryTask {
              user(filter: { username: { eq: $USER } }) {
                  __typename
              }
          }
      }
      """
    }
    add: {
      rule: """
      query($USER: String!) {
          queryTask {
              user(filter: { username: { eq: $USER } }) {
                  __typename
              }
          }
      }
      """
    }
  ) {
  id: ID!
  title: String! @search(by: [fulltext])
  completed: Boolean! @search
  user: User!
}
type User {
  username: String! @id @search(by: [hash])
  name: String
  tasks: [Task] @hasInverse(field: user)
}

# Dgraph.Authorization {"JWKUrl":"https://www.googleapis.com/service_accounts/v1/jwk/securetoken@system.gserviceaccount.com", "Namespace": "https://dgraph.io/jwt/claims", "Audience": ["your-project-id"], "Header": "X-Auth-Token"}
```

Resubmit the updated schema to Dgraph or Dgraph Cloud.

### React App

For an example of how to initialize the Firebase app with the updated
configuration (`config`) settings, see
[base.js](https://github.com/dgraph-io/graphql-sample-apps/blob/master/todo-react-firebase/src/base.js).

```javascript
import firebase from "firebase/app"
import "firebase/auth"
import config from "./config.json"

const app = firebase.initializeApp({
  apiKey: config.apiKey,
  authDomain: config.authDomain,
  projectId: config.projectId,
  storageBucket: config.storageBucket,
  messagingSenderId: config.messagingSenderId,
  appId: config.appId,
})

export default app
```

To understand how the client gets the token and sends it along with each GraphQL
request, see
[Auth.js](https://github.com/dgraph-io/graphql-sample-apps/blob/master/todo-react-firebase/src/Auth.js).
We can see from the code that whenever there will be `state` change,
`currentUser` will be set to the `new user` and context will return `App` with
the new `idToken`. `App` will initialize the Apollo Client which will send this
`idToken` in header along with every GraphQL request.

```javascript
import React, { useEffect, useState } from "react"
import app from "./base.js"
import firebase from "firebase/app"
import "firebase/functions"

import App from "./App"
export const AuthContext = React.createContext()

export const AuthProvider = ({ children }) => {
  const [currentUser, setCurrentUser] = useState(null)
  const [loading, setLoading] = useState(true)
  const [idToken, setIdToken] = useState("")
  const addUserClaim = firebase.functions().httpsCallable("addUserClaim")

  useEffect(() => {
    app.auth().onAuthStateChanged(async (user) => {
      setLoading(false)
      setCurrentUser(user)
      if (user) {
        addUserClaim({ email: user.email })
        const token = await user.getIdToken()
        setIdToken(token)
      }
    })
  }, [])

  if (loading) {
    return <>Loading...</>
  }
  return (
    <AuthContext.Provider
      value={{
        loading,
        currentUser,
      }}
    >
      {children}
      <App idToken={idToken} />
    </AuthContext.Provider>
  )
}
```

To review the Apollo Client setup, see
[App.js](https://github.com/dgraph-io/graphql-sample-apps/blob/master/todo-react-firebase/src/App.js).

```javascript
...

const createApolloClient = token => {
  const httpLink = createHttpLink({
    uri: config.graphqlUrl,
    options: {
      reconnect: true,
    },
  });

  const authLink = setContext((_, { headers }) => {
    // return the headers to the context so httpLink can read them
    return {
      headers: {
        ...headers,
        "X-Auth-Token": token,
      },
    };
  });

  return new ApolloClient({
    link: authLink.concat(httpLink),
    cache: new InMemoryCache()
  });
}

const App = ({idToken}) => {
  const { loading } = useContext(AuthContext);
  if (loading) {
    return <div>Loading...</div>;
  }
  console.log(idToken)
  const client = createApolloClient(idToken);
  return (
    <ApolloProvider client={client}>
      <div>
        <Router history={history}>
        <header className="navheader">
          <NavBar/>
        </header>
        <Switch>
        <PrivateRoute path="/" component= {TodoApp} exact />
        <PrivateRoute path="/profile" component={Profile} exact/>
        <Route exact path="/login" component = {Login} />
        <Route exact path="/signup" component={SignUp} />
      </Switch>
      </Router>
    </div>
    </ApolloProvider>
  );
}

export default App
```

Now that we have a basic understanding of how to integrate Firebase
authentication in our app, let's see it in action!

```
npm start
```

![SignUp Screen](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/firebase-webapp.png)

![Todos Screen](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/firebase-todo.png)


# Build a To-Do List App
Source: https://docs.hypermode.com/dgraph/guides/to-do-app/introduction

This is a simple tutorial that will take you through making a basic to-do app using Dgraph’s GraphQL API and integrating it with Auth0

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

This is a simple tutorial that takes you through making a basic to-do list app
using Dgraph's GraphQL API and integrating it with third-party authentication
(Auth0 or Firebase).

### Steps

* [Schema Design](./schema-design)
* [Basic UI](./ui)
* [Add Auth Rules](./auth-rules)
* [Use Auth0's JWT](./auth0-jwt)
* [Use Firebase's JWT](./firebase-jwt)
* [Deploy on Dgraph Cloud](./deploy)


# Schema Design
Source: https://docs.hypermode.com/dgraph/guides/to-do-app/schema-design

Starting with listing the entities that are involved in a basic to-do app, this step in the GraphQL tutorial walks you through schema design.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

<Note>This is part 1 of [Building a To-Do List App](./introduction).</Note>

Let's start with listing down the entities that are involved in a basic to do
app.

* Task
* User

![To do Graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/todo-graph.png)

Equivalent GraphQL schema for this graph is be as follow:

```graphql
type Task {
    ...
}

type User {
    ...
}
```

What are the fields that these two simple entities contain?

There is a title and a status to check if it was completed or not in the `Task`
type. Then the `User` type has a username (unique identifier), name and the
tasks.

So each user can have many tasks.

![To do Graph complete](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/guides/to-do-app/todo-graph-2.png)

<Note>' \* ' signifies one-to-many relationship</Note>

Now let's add `@id` directive to `username` which makes it the unique key & also
add `@hasInverse` directive to enable the relationship between tasks and user.
We represent that in the GraphQL schema shown below:

```graphql
type Task {
  id: ID!
  title: String!
  completed: Boolean!
  user: User!
}

type User {
  username: String! @id
  name: String
  tasks: [Task] @hasInverse(field: user)
}
```

Save the content in a file `schema.graphql`.

## Running

Before we begin, make sure that you have
[Docker](https://docs.docker.com/install/) installed on your machine.

Let's begin by starting Dgraph standalone by running the command below:

```sh
docker run -it -p 8080:8080 dgraph/standalone:%VERSION_HERE
```

Let's load up the GraphQL schema file to Dgraph:

```sh
curl -X POST localhost:8080/admin/schema --data-binary '@schema.graphql'
```

You can access that GraphQL endpoint with any of the great GraphQL developer
tools. Good choices include GraphQL Playground, Insomnia, GraphiQL and Altair.

Set up any of them and point it at `http://localhost:8080/graphql`. If you know
lots about GraphQL, you might want to explore the schema, queries and mutations
that were generated from the schema.

## Mutating data

Let's add a user and some to dos in our To Do app.

```graphql
mutation {
  addUser(
    input: [
      {
        username: "alice@dgraph.io"
        name: "Alice"
        tasks: [
          { title: "Avoid touching your face", completed: false }
          { title: "Stay safe", completed: false }
          { title: "Avoid crowd", completed: true }
          { title: "Wash your hands often", completed: true }
        ]
      }
    ]
  ) {
    user {
      username
      name
      tasks {
        id
        title
      }
    }
  }
}
```

## Querying data

Let's fetch the to dos to list in our To Do app:

```graphql
query {
  queryTask {
    id
    title
    completed
    user {
      username
    }
  }
}
```

Running this query should return JSON response as shown below:

```json
{
  "data": {
    "queryTask": [
      {
        "id": "0x3",
        "title": "Avoid touching your face",
        "completed": false,
        "user": {
          "username": "alice@dgraph.io"
        }
      },
      {
        "id": "0x4",
        "title": "Stay safe",
        "completed": false,
        "user": {
          "username": "alice@dgraph.io"
        }
      },
      {
        "id": "0x5",
        "title": "Avoid crowd",
        "completed": true,
        "user": {
          "username": "alice@dgraph.io"
        }
      },
      {
        "id": "0x6",
        "title": "Wash your hands often",
        "completed": true,
        "user": {
          "username": "alice@dgraph.io"
        }
      }
    ]
  }
}
```

## Querying data with filters

Before we get into querying data with filters, we're required to define search
indexes to the specific fields.

Let's say we want to run a query on the `completed` field, for which we add
`@search` directive to the field, as shown in the schema below:

```graphql
type Task {
  id: ID!
  title: String!
  completed: Boolean! @search
  user: User!
}
```

The `@search` directive is added to support the native search indexes of
**Dgraph**.

Resubmit the updated schema -

```sh
curl -X POST localhost:8080/admin/schema --data-binary '@schema.graphql'
```

Now, let's fetch all to dos which are completed:

```graphql
query {
  queryTask(filter: { completed: true }) {
    title
    completed
  }
}
```

Next, let's say we want to run a query on the `title` field, for which we add
another `@search` directive to the field, as shown in the schema below:

```graphql
type Task {
  id: ID!
  title: String! @search(by: [fulltext])
  completed: Boolean! @search
  user: User!
}
```

The `fulltext` search index provides the advanced search capability to perform
equality comparison as well as matching with language-specific stemming and
stopwords.

Resubmit the updated schema -

```sh
curl -X POST localhost:8080/admin/schema --data-binary '@schema.graphql'
```

Now, let's try to fetch to dos whose title has the word "avoid":

```graphql
query {
  queryTask(filter: { title: { alloftext: "avoid" } }) {
    id
    title
    completed
  }
}
```


# Creating a Basic UI
Source: https://docs.hypermode.com/dgraph/guides/to-do-app/ui

Create a simple to-do app and integrate it with Auth0. This step in the GraphQL tutorial walks you through creating a basic UI with React.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

<Note>This is part 2 of [Building a To-Do List App](./introduction).</Note>

In this step, we're creating a simple to do app (in React) and integrating it
with Auth0.

## Create React app

Let's start by creating a React app using the `create-react-app` command.

```sh
npx create-react-app todo-react-app
```

To verify navigate to the folder, start the dev server, and visit
[http://localhost:3000](http://localhost:3000).

```sh
cd todo-react-app
npm start
```

## Install dependencies

Now, let's install the various dependencies that we will need in the app.

```
npm install todomvc-app-css classnames graphql-tag history react-router-dom
```

## Setup Apollo Client

Let's start with installing the Apollo dependencies and then create a setup.

```
npm install @apollo/react-hooks apollo-cache-inmemory apollo-client apollo-link-http graphql apollo-link-context react-todomvc @auth0/auth0-react
```

Now, let's update our `src/App.js` with the below content to include the Apollo
client setup.

```javascript
import React from "react"

import ApolloClient from "apollo-client"
import { InMemoryCache } from "apollo-cache-inmemory"
import { ApolloProvider } from "@apollo/react-hooks"
import { createHttpLink } from "apollo-link-http"

import "./App.css"

const createApolloClient = () => {
  const httpLink = createHttpLink({
    uri: "http://localhost:8080/graphql",
    options: {
      reconnect: true,
    },
  })

  return new ApolloClient({
    link: httpLink,
    cache: new InMemoryCache(),
  })
}

const App = () => {
  const client = createApolloClient()
  return (
    <ApolloProvider client={client}>
      <div>
        <h1>todos</h1>
        <input
          className="new-todo"
          placeholder="What needs to be done?"
          autoFocus={true}
        />
      </div>
    </ApolloProvider>
  )
}

export default App
```

Here we have created a simple instance of the Apollo client and passed the URL
of our GraphQL API. Then we have passed the client to `ApolloProvider` and
wrapped our `App` so that its accessible throughout the app.

## Queries and Mutations

Now, let's add some queries and mutations.

First, let's see how we can add a todo and get todos. Create a file
`src/GraphQLData.js` and add the following.

```javascript
import gql from "graphql-tag"

export const GET_TODOS = gql`
  query {
    queryTodo: queryTask {
      id
      value: title
      completed
    }
  }
`

export const ADD_TODO = gql`
  mutation addTask($task: AddTaskInput!) {
    addTask(input: [$task]) {
      task {
        id
        value: title
        completed
      }
    }
  }
`
```

Now, let's see how to use this to add a todo. Let's import the dependencies
first in `src/App.js` replacing all the code. Let's now create the functions to
add a todo and get todos.

```javascript
import { useQuery, useMutation } from "@apollo/react-hooks"
import { Todos } from "react-todomvc"
import "react-todomvc/dist/todomvc.css"
import { useAuth0 } from "@auth0/auth0-react"
import { GET_TODOS, ADD_TODO } from "./GraphQLData"

function App() {
  const [add] = useMutation(ADD_TODO)

  const { user, isAuthenticated, loginWithRedirect, logout } = useAuth0()

  const { loading, error, data } = useQuery(GET_TODOS)
  if (loading) return <p>Loading</p>
  if (error) {
    return <p>`Error: ${error.message}`</p>
  }

  const addNewTodo = (title) =>
    add({
      variables: {
        task: {
          title: title,
          completed: false,
          user: { username: user.email },
        },
      },
      refetchQueries: [
        {
          query: GET_TODOS,
        },
      ],
    })

  return (
    <div>
      <Todos
        todos={data.queryTodo}
        addNewTodo={addNewTodo}
        todosTitle="Todos"
      />
    </div>
  )
}

export default App
```

## Auth0 Integration

Now, let's integrate Auth0 in our app and use that to add the logged-in user.
Let's first create an app in Auth0.

* Head over to Auth0 and create an account. Click 'sign up'
  [here](https://auth0.com/)
* Once the signup is done, click "Create Application" in "Integrate Auth0 into
  your app".
* Give your app a name and select "Single Page Web App" app type
* Select React as the technology
* No need to do the sample app, scroll down to "Configure Auth0" and select
  "Application Settings".
* Select your app and add the values of `domain` and `clientid` in the file
  `src/auth_template.json`. Check this
  [link](https://auth0.com/docs/quickstart/spa/react/01-login#configure-auth0)
  for more information.
* Add `http://localhost:3000` to "Allowed Callback URLs", "Allowed Web Origins"
  and "Allowed Logout URLs".

Now that we have prepared our `src/App.js` file let's update our `src/index.js`
file with the following code.

```javascript
import React from "react"
import ReactDOM from "react-dom"
import App from "./App"
import {
  ApolloClient,
  ApolloProvider,
  InMemoryCache,
  createHttpLink,
} from "@apollo/client"
import { setContext } from "@apollo/client/link/context"
import { Auth0Provider, useAuth0 } from "@auth0/auth0-react"
import config from "./auth_template.json"

const GRAPHQL_ENDPOINT = "http://localhost:8080/graphql"

const AuthorizedApolloProvider = ({ children }) => {
  const { isAuthenticated, getIdTokenClaims } = useAuth0()
  const httpLink = createHttpLink({
    uri: GRAPHQL_ENDPOINT,
  })

  const authLink = setContext(async (_, { headers }) => {
    if (!isAuthenticated) {
      return headers
    }

    const token = await getIdTokenClaims()

    return {
      headers: {
        ...headers,
        "X-Auth-Token": token ? token.__raw : "",
      },
    }
  })

  const apolloClient = new ApolloClient({
    link: authLink.concat(httpLink),
    cache: new InMemoryCache(),
  })

  return <ApolloProvider client={apolloClient}>{children}</ApolloProvider>
}

ReactDOM.render(
  <Auth0Provider
    domain={config.domain}
    clientId={config.clientId}
    redirectUri={window.location.origin}
  >
    <AuthorizedApolloProvider>
      <React.StrictMode>
        <App />
      </React.StrictMode>
    </AuthorizedApolloProvider>
  </Auth0Provider>,
  document.getElementById("root"),
)
```

Note that for the app to work from this point on, the `src/auth_template.json`
file must be configured with your auth0 credentials.

Here is a reference
[Here](https://github.com/dgraph-io/auth-webinar/blob/marcelo/fix-finished-app/src/auth_template.json)

Let's also add definitions for updating, deleting and clearing all tasks to
`src/GraphQLData.js`. Let's also add the constants `user`, `isAuthenticated`,
`loginWithRedirect` and `logout` which they receive from the variable
`useAuth0`. We also create a constant called `logInOut` that contains the logic
to know if the user is logged in or not. This variable will show a button to
login or logout depending on the status of logged in or logged out. Note that
before calling the component Todos we call our variable `{logInOut}` so that our
login button appears above the app.

```javascript
import React from "react"
import { useQuery, useMutation } from "@apollo/client"
import { Todos } from "react-todomvc"

import "react-todomvc/dist/todomvc.css"
import { useAuth0 } from "@auth0/auth0-react"
import {
  GET_TODOS,
  ADD_TODO,
  UPDATE_TODO,
  DELETE_TODO,
  CLEAR_COMPLETED_TODOS,
} from "./GraphQLData"

function App() {
  const [add] = useMutation(ADD_TODO)
  const [del] = useMutation(DELETE_TODO)
  const [upd] = useMutation(UPDATE_TODO)
  const [clear] = useMutation(CLEAR_COMPLETED_TODOS)

  const { user, isAuthenticated, loginWithRedirect, logout } = useAuth0()

  const { loading, error, data } = useQuery(GET_TODOS)
  if (loading) return <p>Loading</p>
  if (error) {
    return <p>`Error: ${error.message}`</p>
  }

  const addNewTodo = (title) =>
    add({
      variables: {
        task: {
          title: title,
          completed: false,
          user: { username: user.email },
        },
      },
      refetchQueries: [
        {
          query: GET_TODOS,
        },
      ],
    })

  const updateTodo = (modifiedTask) =>
    upd({
      variables: {
        id: modifiedTask.id,
        task: {
          value: modifiedTask.title,
          completed: modifiedTask.completed,
        },
      },
      update(cache, { data }) {
        data.updateTask.task.map((t) =>
          cache.modify({
            id: cache.identify(t),
            fields: {
              title: () => t.title,
              completed: () => t.completed,
            },
          }),
        )
      },
    })

  const deleteTodo = (id) =>
    del({
      variables: { id },
      update(cache, { data }) {
        data.deleteTask.task.map((t) => cache.evict({ id: cache.identify(t) }))
      },
    })

  const clearCompletedTodos = () =>
    clear({
      update(cache, { data }) {
        data.deleteTask.task.map((t) => cache.evict({ id: cache.identify(t) }))
      },
    })

  const logInOut = !isAuthenticated ? (
    <p>
      <a href="#" onClick={loginWithRedirect}>
        Log in
      </a>
      to use the app.
    </p>
  ) : (
    <p>
      <a
        href="#"
        onClick={() => {
          logout({ returnTo: window.location.origin })
        }}
      >
        Log out
      </a>
      once you are finished, {user.email}.
    </p>
  )

  return (
    <div>
      {logInOut}
      <Todos
        todos={data.queryTodo}
        addNewTodo={addNewTodo}
        updateTodo={updateTodo}
        deleteTodo={deleteTodo}
        clearCompletedTodos={clearCompletedTodos}
        todosTitle="Todos"
      />
    </div>
  )
}

export default App
```

For our app to work correctly we need to update the `src/GraphQLData.js` file
with the remaining queries.

```javascript
import gql from "graphql-tag"

export const GET_TODOS = gql`
  query {
    queryTodo: queryTask {
      id
      value: title
      completed
    }
  }
`

export const ADD_TODO = gql`
  mutation addTask($task: AddTaskInput!) {
    addTask(input: [$task]) {
      task {
        id
        value: title
        completed
      }
    }
  }
`

export const UPDATE_TODO = gql`
  mutation updateTask($id: ID!, $task: TaskPatch!) {
    updateTask(input: { filter: { id: [$id] }, set: $task }) {
      task {
        id
        value: title
        completed
      }
    }
  }
`

export const DELETE_TODO = gql`
  mutation deleteTask($id: ID!) {
    deleteTask(filter: { id: [$id] }) {
      task {
        id
      }
    }
  }
`

export const CLEAR_COMPLETED_TODOS = gql`
  mutation updateTask {
    deleteTask(filter: { completed: true }) {
      task {
        id
      }
    }
  }
`
```

Here is the complete code
[Here](https://github.com/dgraph-io/auth-webinar/tree/marcelo/fix-finished-app/src)

Let's now start the app.

```
npm start
```

Now you should have an app running!


# HTTP
Source: https://docs.hypermode.com/dgraph/http



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

It is also possible to interact with Dgraph directly via its HTTP endpoints.
This allows clients to be built for languages that don't have access to a
working gRPC implementation.

In the examples shown here, regular command line tools such as `curl` and
[`jq`](https://stedolan.github.io/jq/) are used. However, the real intention
here is to show other programmers how they could implement a client in their
language on top of the HTTP API.

For an example of how to build a client on top of gRPC, refer to the
implementation of the Go client.

Similar to the Go client example, we use a bank account transfer example.

## Create the client

A client built on top of the HTTP API needs to track three pieces of state for
each transaction.

1. A start timestamp (`start_ts`). This uniquely identifies a transaction, and
   doesn't change over the transaction lifecycle.

2. The set of keys modified by the transaction (`keys`). This aids in
   transaction conflict detection.

   Every mutation would send back a new set of keys. The client must merge them
   with the existing set. Optionally, a client can de-dup these keys while
   merging.

3. The set of predicates modified by the transaction (`preds`). This aids in
   predicate move detection.

   Every mutation would send back a new set of predicates. The client must merge
   them with the existing set. Optionally, a client can de-dup these keys while
   merging.

## Alter the DQL schema

You may need to alter the DQL schema to declare predicate types, to add
predicate search indexes and to declare the predicates expected in entities of
specific type.

Update the DQL schema is done by posting schema data to the `/alter` endpoint:

```sh
curl "localhost:8080/alter" --silent --request POST \
  --data $'
name: string @index(term) .
release_date: datetime @index(year) .
revenue: float .
running_time: int .
starring: [uid] .
director: [uid] .

type Person {
  name
}

type Film {
  name
  release_date
  revenue
  running_time
  starring
  director
}
' | python -m json.tool
```

The success response looks like:

```json
{
  "data": {
    "code": "Success",
    "message": "Done"
  }
}
```

In case of errors, the API returns an error message such as:

```json
{
  "errors": [
    {
      "extensions": {
        "code": "Error"
      },
      "message": "line 5 column 18: Invalid ending"
    }
  ]
}
```

<Note>
  The request updates or creates the predicates and types present in the
  request. It doesn't modify or delete other schema information that may be
  present.
</Note>

## Query current DQL schema

Obtain the DQL schema by issuing a DQL query on `/query` endpoint.

```sh
$ curl -X POST \
  -H "Content-Type: application/dql" \
  localhost:8080/query -d $'schema {}' | python -m json.tool
```

## Start a transaction

Assume some initial accounts with balances have been populated. We now want to
transfer money from one account to the other. This is done in four steps:

1. Create a new transaction.

2. Inside the transaction, run a query to determine the current balances.

3. Perform a mutation to update the balances.

4. Commit the transaction.

Starting a transaction doesn't require any interaction with Dgraph itself. Some
state needs to be set up for the transaction to use. The `start_ts` can
initially be set to 0. `keys` can start as an empty set.

**For both query and mutation if the `start_ts` is provided as a path parameter,
then the operation is performed as part of the ongoing transaction. Otherwise, a
new transaction is initiated.**

## Run a query

To query the database, the `/query` endpoint is used. Remember to set the
`Content-Type` header to `application/dql` to ensure that the body of the
request is parsed correctly.

To get the balances for both accounts:

```sh
$ curl -H "Content-Type: application/dql" -X POST localhost:8080/query -d $'
{
  balances(func: anyofterms(name, "Alice Bob")) {
    uid
    name
    balance
  }
}' | jq

```

The result should look like this:

```json
{
  "data": {
    "balances": [
      {
        "uid": "0x1",
        "name": "Alice",
        "balance": "100"
      },
      {
        "uid": "0x2",
        "name": "Bob",
        "balance": "70"
      }
    ]
  },
  "extensions": {
    "server_latency": {
      "parsing_ns": 70494,
      "processing_ns": 697140,
      "encoding_ns": 1560151
    },
    "txn": {
      "start_ts": 4
    }
  }
}
```

Notice that along with the query result under the `data` field is additional
data in the `extensions -> txn` field. This data has to be tracked by the
client.

For queries, there is a `start_ts` in the response. This `start_ts` needs to be
used in all subsequent interactions with Dgraph for this transaction, and so
should become part of the transaction state.

## Run a mutation

Mutations can be done over HTTP by making a `POST` request to an Alpha's
`/mutate` endpoint. We need to send a mutation to Dgraph with the updated
balances. If Bob transfers \$10 to Alice, then the RDF triples to send are:

```txt
<0x1> <balance> "110" .
<0x1> <dgraph.type> "Balance" .
<0x2> <balance> "60" .
<0x2> <dgraph.type> "Balance" .
```

Note that refer to the Alice and Bob nodes by UID in the RDF format.

We now send the mutations via the `/mutate` endpoint. We need to provide our
transaction start timestamp as a path parameter, so that Dgraph knows which
transaction the mutation should be part of. We also need to set `Content-Type`
header to `application/rdf` to specify that mutation is written in RDF format.

```sh
$ curl -H "Content-Type: application/rdf" -X POST localhost:8080/mutate?startTs=4 -d $'
{
  set {
    <0x1> <balance> "110" .
    <0x1> <dgraph.type> "Balance" .
    <0x2> <balance> "60" .
    <0x2> <dgraph.type> "Balance" .
  }
}
' | jq
```

The result:

```json
{
  "data": {
    "code": "Success",
    "message": "Done",
    "uids": {}
  },
  "extensions": {
    "server_latency": {
      "parsing_ns": 50901,
      "processing_ns": 14631082
    },
    "txn": {
      "start_ts": 4,
      "keys": ["2ahy9oh4s9csc", "3ekeez23q5149"],
      "preds": ["1-balance"]
    }
  }
}
```

The result contains `keys` and `predicates` which should be added to the
transaction state.

## Committing the transaction

<Note>
  It is possible to commit immediately after a mutation is made (without
  requiring to use the `/commit` endpoint as explained in this section). To do
  this, add the parameter `commitNow` in the URL `/mutate?commitNow=true`.
</Note>

Finally, we can commit the transaction using the `/commit` endpoint. We need the
`start_ts` we've been using for the transaction along with the list of `keys`
and the list of predicates. If we had performed multiple mutations in the
transaction instead of just one, then the keys and predicates provided during
the commit would be the union of all keys and predicates returned in the
responses from the `/mutate` endpoint.

The `preds` field is used to cancel the transaction in cases where some of the
predicates are moved. This field isn't required and the `/commit` endpoint also
accepts the old format, which was a single array of keys.

```sh
$ curl -X POST localhost:8080/commit?startTs=4 -d $'
{
  "keys": [
    "2ahy9oh4s9csc",
    "3ekeez23q5149"
  ],
  "preds": [
    "1-balance"
  ]
}' | jq
```

The result:

```json
{
  "data": {
    "code": "Success",
    "message": "Done"
  },
  "extensions": {
    "txn": {
      "start_ts": 4,
      "commit_ts": 5
    }
  }
}
```

The transaction is now complete.

If another client were to perform another transaction concurrently affecting the
same keys, then it's possible that the transaction would *not* be successful.
This is indicated in the response when the commit is attempted.

```json
{
  "errors": [
    {
      "code": "Error",
      "message": "Transaction has been aborted. Please retry."
    }
  ]
}
```

In this case, it should be up to the user of the client to decide if they wish
to retry the transaction.

## Cancelling the transaction

To cancel a transaction, use the same `/commit` endpoint with the `abort=true`
parameter while specifying the `startTs` value for the transaction.

```sh
curl -X POST "localhost:8080/commit?startTs=4&abort=true" | jq
```

The result:

```json
{
  "code": "Success",
  "message": "Done"
}
```

## Running read-only queries

You can set the query parameter `ro=true` to `/query` to set it as a
[read-only](/dgraph/sdks/go#read-only-transactions) query.

```sh
$ curl -H "Content-Type: application/dql" -X POST "localhost:8080/query?ro=true" -d $'
{
  balances(func: anyofterms(name, "Alice Bob")) {
    uid
    name
    balance
  }
}
```

## Running best-effort queries

```sh
$ curl -H "Content-Type: application/dql" -X POST "localhost:8080/query?be=true" -d $'
{
  balances(func: anyofterms(name, "Alice Bob")) {
    uid
    name
    balance
  }
}
```

## Compression via HTTP

Dgraph supports gzip-compressed requests to and from Dgraph Alphas for `/query`,
`/mutate`, and `/alter`.

Compressed requests: to send compressed requests, set the HTTP request header
`Content-Encoding: gzip` along with the gzip-compressed payload.

Compressed responses: to receive compressed responses, set the HTTP request
header `Accept-Encoding: gzip`.

Example of a compressed request via curl:

```sh
$ curl -X POST \
  -H 'Content-Encoding: gzip' \
  -H "Content-Type: application/rdf" \
  localhost:8080/mutate?commitNow=true --data-binary @mutation.gz
```

Example of a compressed request via curl:

```sh
$ curl -X POST \
  -H 'Accept-Encoding: gzip' \
  -H "Content-Type: application/dql" \
  localhost:8080/query -d $'schema {}' | gzip --decompress
```

Example of a compressed request and response via curl:

```sh
$ zcat query.gz # query.gz is gzipped compressed
{
  all(func: anyofterms(name, "Alice Bob")) {
    uid
    balance
  }
}
```

```sh
$ curl -X POST \
  -H 'Content-Encoding: gzip' \
  -H 'Accept-Encoding: gzip' \
  -H "Content-Type: application/dql" \
  localhost:8080/query --data-binary @query.gz | gzip --decompress
```

<Note>
  Curl has a `--compressed` option that automatically
  requests for a compressed response (`Accept-Encoding` header) and decompresses
  the compressed response.

  ```sh
  curl -X POST --compressed -H "Content-Type: application/dql" localhost:8080/query -d $'schema {}'
  ```
</Note>

## Run a query in JSON format

The HTTP API also accepts requests in JSON format. For queries you have the keys
`query` and `variables`. The JSON format is required to set
[GraphQL Variables](/dgraph/graphql/query/variables) with the HTTP API.

This query:

```dql
{
  balances(func: anyofterms(name, "Alice Bob")) {
    uid
    name
    balance
  }
}
```

Should be escaped to this:

```sh
curl -H "Content-Type: application/json" localhost:8080/query -XPOST -d '{
    "query": "{\n balances(func: anyofterms(name, \"Alice Bob\")) {\n uid\n name\n balance\n }\n }"
}' | python -m json.tool | jq
```


# Dgraph
Source: https://docs.hypermode.com/dgraph/overview



Dgraph is a graph database most commonly used for building knowledge graphs. It
is the only open, complete graph database used at terabyte-scale to power
real-time use cases.

<Card title="Dgraph Labs has been acquired by Hypermode" img="https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/dgraph-hypermode.png">
  [Why we're continuing to invest in Dgraph
  →](https://hypermode.com/blog/why-invest-in-dgraph/)
</Card>

## Quick Links

<CardGroup cols={2}>
  <Card title="Quickstart" icon="play" href="./quickstart">
    Get hands-on with Dgraph and learn the basics of graph structures
  </Card>

  <Card title="What is a knowledge graph?" icon="brain-circuit" href="https://hypermode.com/blog/smarter-ai-knowledge-graphs">
    Learn how a knowledge graph can help you build smarter AI apps
  </Card>

  <Card title="Ultimate guide to graph databases" icon="book-open-cover" href="https://hypermode.com/blog/ultimate-guide-graph-databases">
    Read about what use cases are the best fit for graph databases
  </Card>

  <Card title="New in Dgraph v24.1" icon="square-plus" href="https://hypermode.com/blog/dgraph-v241-knowledge-graphs-faster">
    Explore the new query planner, faster writes, improved caching, and expanded
    data capabilities
  </Card>
</CardGroup>

## More Resources

<CardGroup cols={3}>
  <Card title="Discord" icon="discord" href="https://discord.hypermode.com">
    Join a community of Dgraph users and committers
  </Card>

  <Card title="GitHub" icon="github" href="https://github.com/hypermodeinc/dgraph">
    Raise an issue or submit your first pull request
  </Card>

  <Card title="Need Help?" icon="phone" href="https://hypermode.com">
    Experts for migrations, data modeling, sizing, and more
  </Card>
</CardGroup>


# Quickstart
Source: https://docs.hypermode.com/dgraph/quickstart



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

In this Dgraph quick start guide we walk through creating a graph, inserting
data, and querying the graph using [DQL](./glossary#dql).

This guide helps you to understand how to:

* Create a new Dgraph graph
* Connect your graph to the Ratel web client
* Add data using mutations
* Query the graph using DQL
* Update the graph schema to support more advanced queries

## Run Dgraph and connect the Ratel web UI

The easiest way to get Dgraph up and running in the cloud is using
[Dgraph on Hypermode](https://hypermode.com/sign-in). If you prefer a local
development experience with Dgraph we recommend using the official Dgraph Docker
image. Both options are described below.

In this section we'll create a new graph, then we'll connect our new graph to
[Ratel](./glossary#ratel), the web-based UI for Dgraph.

<Tabs>
  <Tab title="On Hypermode">
    <Steps>
      <Step title="Sign in to Hypermode and create your workspace">
        Open [`hypermode.com/sign-in`](https://hypermode.com/sign-in) in your web browser and sign
        in to Hypermode to create your account.

        After signing in you'll be prompted to choose a name for your workspace.

        ![Create your Hypermode workspace](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/quickstart/create-workspace.png)

        Enter a name for your workspace and then select **Create workspace**.
      </Step>

      <Step title="Create your graph">
        After creating your Hypermode workspace you'll be prompted to create your first
        graph.

        ![Create your first Hypermode graph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/quickstart/create-graph-2.png)

        Enter a name for your graph and choose the hosting location, then select
        **Create graph**.
      </Step>

      <Step title="View your graph details">
        Once your graph is created you'll see the graph details including its status,
        connection string, and API key.

        ![Graph detail view](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/quickstart/graph-details.png)

        We'll use the Dgraph connection string and the API key to connect to our graph
        via Dgraph clients such as Ratel or language SDKs.

        Click on the copy icon next to the connection string to copy the Dgraph
        connection string.
      </Step>

      <Step title="Connect the Ratel graph client">
        Ratel is a web-based Dgraph client for interacting with your graph. We'll use
        Ratel to execute DQL queries and update the graph schema.

        Navigate to [ratel.hypermode.com](https://ratel.hypermode.com) and paste the
        Dgraph connection string you copied in the previous step into the "Dgraph Conn
        String" text box in Ratel then select **Connect** to verify the connection and
        then select **Continue** to access the Ratel console.

        ![Connecting to Ratel](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/quickstart/ratel-connection-string.png)

        The Ratel console is where we can execute DQL queries and mutations and view the
        results of these operations, including visualizing graph data.

        ![Ratel overview](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/quickstart/ratel-overview.png)

        Now we're ready to add data to our graph.
      </Step>
    </Steps>
  </Tab>

  <Tab title="Locally">
    <Steps>
      <Step title="Run Dgraph with Docker">
        The [`dgraph/standalone`](https://hub.docker.com/r/dgraph/standalone) Docker image has everything needed to run Dgraph locally.

        Ensure you have [Docker installed](https://www.docker.com/), then run the following command to start a local Dgraph instance:

        ```bash
        docker run --rm -it -p 8080:8080 -p 9080:9080 dgraph/standalone:latest
        ```

        This will create a local Dgraph instance and expose the ports necessary to connect to Dgraph via HTTP and gRPC. Specifically:

        * `docker run` - initiates a new Docker container
        * `--rm` - automatically removes the container when it exits, helping with cleanup
        * `-it` - uses interactive mode to show output of the container
        * `-p 8080:8080` - maps port 8080 from the host machine to port 8080 in the Docker container to allow Dgraph HTTP connections
        * `-p 9080:9080` - maps port 9080 from the host machine to port 9080 in the Docker container to allow Dgraph gRPC connections
        * `dgraph/standalone:latest` - specifies the Docker image to use, this is the official Dgraph image with latest tag
      </Step>

      <Step title="Connect Ratel">
        Ratel is a web-based UI dashboard for interacting with Dgraph using Dgraph's query language,[DQL](./glossary#DQL)

        Navigate to the hosted version of Ratel at `https://ratel.hypermode.com` and enter `http://localhost:8080` for the "Dgraph Conn String".
        This will allow Ratel to connect to our local Dgraph instance and execute DQL queries.

        ![Setting up Ratel](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/quickstart/ratel-docker-connection.png)

        <Note>
          You can also run Ratel locally by running the `dgraph/ratel` container with the following command:

          ```bash
          docker run --rm -it -p 8000:8000 dgraph/ratel:latest
          ```
        </Note>

        Now select **Connect** to verify the connection and then select **Continue** to access the Ratel console.

        {/* TODO: updated image showing local connection to Ratel */}

        ![Setting up Ratel local connection](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/quickstart/ratel-docker-overview.png)

        Now we're ready to add data to our graph.
      </Step>
    </Steps>
  </Tab>
</Tabs>

## Add data to the graph with a mutation

Graph databases like Dgraph use a data model called the **property graph**,
which consists of [**nodes**](./glossary#node),
[**relationships**](./glossary#relationship) that connect nodes, and key-value
pair **properties** that describe nodes and relationships.

With Dgraph, we use **triples** to describe each piece of our graph, which when
combined together make up our property graph. Triples are composed of a subject,
predicate, and object.

```text
<subject> <predicate> <object> .
```

The subject always refers to a [node](./glossary#node),
[predicates](./glossary#predicate) can be a relationship or property, and the
object can be a node or property value. You can read more about triples in the
[RDF section of the docs](/dgraph/dql/rdf), but for now let's move on to
creating data in Dgraph using triples.

Let's create data about movies, characters, and their genres. Here's the
property graph representation of the data we'll create:

![Movie and actor data model](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/quickstart/data-model.png)

<Steps>
  <Step title="Add mutation in Ratel">
    The create, update, and delete operations in Dgraph are called mutations.

    In the Ratel **Console** page, select the **Mutate** tab, then paste the
    following mutation:

    ```dql
    {
      set {

        _:scifi <dgraph.type> "Genre" .
        _:scifi <Genre.name> "Sci-Fi" .

        _:starwars <dgraph.type> "Movie" .
        _:starwars <Movie.title> "Star Wars: Episode IV - A New Hope" .
        _:starwars <Movie.release_date> "1977-05-25"^^<xs:dateTime> .


        _:startrek <dgraph.type> "Movie" .
        _:startrek <Movie.title> "Star Trek: The Motion Picture" .
        _:startrek <Movie.release_date> "1979-12-07"^^<xs:dateTime> .

        _:george <dgraph.type> "Person" .
        _:george <Person.name> "George Lucas" .

        _:luke <dgraph.type> "Character" .
        _:luke <Character.name> "Luke Skywalker" .

        _:leia <dgraph.type> "Character" .
        _:leia <Character.name> "Princess Leia" .

        _:han <dgraph.type> "Character" .
        _:han <Character.name> "Han Solo" .

        _:starwars <Movie.genre> _:scifi .
        _:startrek <Movie.genre> _:scifi .

        _:starwars <Movie.director> _:george .

        _:starwars <Movie.character> _:luke .
        _:starwars <Movie.character> _:leia .
        _:starwars <Movie.character> _:han .

      }
    }
    ```

    The preceding DQL mutation uses
    [N-Quad RDF format](/dgraph/dql/rdf#n-quads-format) to define the triples that
    make up the property graph we want to create.
  </Step>

  <Step title="View mutation results">
    Select **Run** to execute the mutation. In the JSON tab we can see the result of
    this mutation.

    ```json
    {
      "data": {
        "code": "Success",
        "message": "Done",
        "queries": null,
        "uids": {
          "george": "0x4",
          "han": "0x7",
          "leia": "0x6",
          "luke": "0x5",
          "scifi": "0x1",
          "startrek": "0x3",
          "starwars": "0x2"
        }
      }
    }
    ```

    Dgraph displays the universal identifiers ([UID](/dgraph/glossary#uid)) of the
    nodes that were created.
  </Step>
</Steps>

## Query the graph

<Steps>
  <Step title="Query for all movies">
    In the **Console** page, select the **Query** tab and run this query:

    ```dql
    {
      movies(func: type(Movie)) {
        Movie.title
        Movie.genre {
          Genre.name
        }
        Movie.director {
          Person.name
        }
        Movie.character {
          Character.name
        }
      }
    }
    ```

    This query searches for all `Movie` nodes as the start of the traversal using
    the `type(Movie)` function to define the starting point of our query traversal,
    then finds any genres, directors, and characters connected to each movie.
  </Step>

  <Step title="View results in JSON and graph visualization">
    In Ratel's JSON tab we can view the results of this query as JSON:

    ```json
    {
      "data": {
        "movies": [
          {
            "Movie.title": "Star Wars: Episode IV - A New Hope",
            "Movie.genre": [
              {
                "Genre.name": "Sci-Fi"
              }
            ],
            "Movie.director": [
              {
                "Person.name": "George Lucas"
              }
            ],
            "Movie.character": [
              {
                "Character.name": "Luke Skywalker"
              },
              {
                "Character.name": "Princess Leia"
              },
              {
                "Character.name": "Han Solo"
              }
            ]
          },
          {
            "Movie.title": "Star Trek: The Motion Picture",
            "Movie.genre": [
              {
                "Genre.name": "Sci-Fi"
              }
            ]
          }
        ]
      }
    }
    ```

    In the response panel, Select **Graph** to view a graph visualization of the
    results of our query:

    ![Query result graph visualization](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/quickstart/query-result-1.png)
  </Step>
</Steps>

## Update the graph schema and query using an index

The previous query used the `type()` function to find the starting point of our
graph traversal. We can use more complex functions to filter by string
comparison operator, and others, however to use these function we must first
update the graph schema to create an index on the predicates we want to use in
these functions.

The [function documentation](/dgraph/dql/functions/) specifies which kind of
index is needed for each function.

We'll use Ratel to alter the schema to add indexes on some of the data so
queries can use term matching, filtering, and sorting.

<Steps>
  <Step title="Create an index for movie title">
    In Ratel's **Schema** page, select **Predicates**. Here we can see all the
    predicates used in the graph. A [predicate](/dgraph/glossary#predicate) is
    Dgraph's internal representation of a node, property, or relationship.

    Select the `Movie.title` predicate. Ratel displays details about the predicate
    type and indexes.

    Change the type to **string** then select **index** and select **term** for the
    `Movie.title` predicate, then select **Update** to apply the index.

    ![Adding an index for movie title](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/quickstart/schema-title.png)
  </Step>

  <Step title="Create an index for movie release date">
    Next, we'll create an index for the `Movie.release_date` predicate.

    Select the `Movie.release_date` predicate. Change the type to **dateTime**.
    Select **index** and choose **year** for the index tokenizer. Click **Update**
    to apply the index on the `release-date` predicate.

    ![Adding an index for movie release date](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/quickstart/schema-date.png)
  </Step>

  <Step title="Query using indexes">
    Now let's find all movies with the term "Star" in their title and released
    before 1979.

    In the **Console** page select the **Query** tab and run this query:

    ```dql
    {
      movieSearch(func: allofterms(Movie.title, "Star"), orderasc: Movie.release_date) @filter(lt(Movie.release_date, "1979")) {
        Movie.title
        Movie.release_date
        Movie.director {
        Person.name
        }
        Movie.character (orderasc: Character.name) {
        Character.name
        }
      }
    }
    ```

    We can see the JSON result in the JSON tab:

    ```JSON
    {
      "data": {
        "movieSearch": [
          {
            "Movie.title": "Star Wars: Episode IV - A New Hope",
            "Movie.release_date": "1977-05-25T00:00:00Z",
            "Movie.director": [
              {
                "Person.name": "George Lucas"
              }
            ],
            "Movie.character": [
              {
                "Character.name": "Han Solo"
              },
              {
                "Character.name": "Luke Skywalker"
              },
              {
                "Character.name": "Princess Leia"
              }
            ]
          }
        ]
      }
    }
    ```

    And also view the graph visualization of the result in the Graph tab:

    ![Graph visualization of query result](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/quickstart/query-result-2.png)

    Try changing the release date and the search terms conditions to see Dgraph
    search and filtering in action.
  </Step>
</Steps>

## Reverse relationship query

<Steps>
  <Step title="Add reverse relationship">
    In the previous queries we traversed from the movie node to its connected genre
    node, but what if we want to find all movies connected to a genre node? In order
    to traverse from a genre node to a movie node we need to explicitly define the
    `Movie.genre` predicate as a reverse relationship.

    To define a reverse relationship for the `Movie.genre` predicate we'll return to
    the Schema page in Ratel, select the `Movie.genre` predicate and toggle the
    **reverse** checkbox. Then select **Update** to apply this schema change.

    ![Define a reverse relationship in the graph schema](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/quickstart/schema-reverse.png)
  </Step>

  <Step title="Query using the reverse relationship">
    In a DQL query the `~` operator is used to specify a reverse relationship. To
    traverse from a genre node to a movie node we use the syntax `~Movie.genre`.

    In this query we find all movies connected to the "Sci-Fi" genre:

    ```dql
    {
      genreSearch(func: type(Genre)) {
        Genre.name
        movies: ~Movie.genre {
          Movie.title
        }
      }
    }
    ```

    Note that we can also alias the field name to "movies" in our result JSON using
    the syntax `movies: ~Movie.genre`.

    ```json
    {
      "data": {
        "genreSearch": [
          {
            "Genre.name": "Sci-Fi",
            "movies": [
              {
                "Movie.title": "Star Wars: Episode IV - A New Hope"
              },
              {
                "Movie.title": "Star Trek: The Motion Picture"
              }
            ]
          }
        ]
      }
    ```
  </Step>
</Steps>

In this quick start we created a new graph instance using Dgraph on Hypermode,
added data, queried the graph, visualized the results, and updated the schema of
our graph.

## Where to go from here

* Learn more about using [DQL](/dgraph/dql/query) to query your graph.
* Go to [Clients](/dgraph/sdks/overview) to see how to communicate with Dgraph
  from your app.
* Learn how to build intelligent applications using Dgraph and Modus such as
  [natural language search.](https://docs.hypermode.com/semantic-search)

## Need help

* Join the [Hypermode Discord server](https://discord.hypermode.com) for
  questions, issues, feature requests, and discussions.


# Backups
Source: https://docs.hypermode.com/dgraph/ratel/backups



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## Backup

Here you find options to back up your server.

<Note>
  This backup option is an [Enterprise
  feature](/dgraph/enterprise/binary-backups).
</Note>

![Ratel Backup](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/ratel/ratel_backup.png)

### Creating a backup

Click `Create Backup`. On the dialog box, choose the destination details. After
the successful backup, it's listed on the main panel.


# Cluster
Source: https://docs.hypermode.com/dgraph/ratel/cluster



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## Cluster management

Here you find the basic information about the cluster.

The Zero list with license and the list of zeros connected:

* Each card represents a zero node. The card has a green sign which shows the
  health. It shows the address of the node and a little blue banner indicating
  that this node is the leader.

The Alpha list separated into groups:

* You have a list of tablets that exist on that group and their approximate
  size. The card has the same pattern as the zero ones.

<Tip>
  By clicking on the Node Card you can remove that node (Alpha or Zero).
</Tip>


# Connection
Source: https://docs.hypermode.com/dgraph/ratel/connection



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## Recent servers

This section provides a list of recent connected clusters. You can select any
item on the list to connect.

The list also has an icon which indicates the version of the cluster running:

* Green icon: Running the latest version.
* Yellow icon: Running a specific version.
* Red icon: No connection found.
* Delete icon: Remove the address form the list.

![Ratel UI](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/ratel/ratel_ui.png)

## URL Input box

In this box you add a valid Dgraph Alpha address. When you click `Connect` Ratel
establishes a connection with the cluster. After Ratel has established a
connection (all icons are green), click the `Continue` button.

Under the input box you have tree icons which gives you the status of the
connection.

* Network Access: Uses an "Energy Plug" icon.
* Server Health: Uses a "Heart" icon.
* Logging in: a "lock" icon.

<Tip>
  To connect to a standard Dgraph instance, you only need to click `Connect`.
  There's a specific section to [login using ACL](#acl-account) ([Enterprise
  feature](/dgraph/enterprise/access-control-lists)).
</Tip>

## Cluster settings

### ACL account

The ACL account login is necessary only when you have ACL features enabled.

<Note>
  The default password for a cluster started from scratch is `password` and the
  user is `groot`.
</Note>

### Dgraph Zero

If you use a custom address for Zero instance, you should inform here.

### Extra settings

Query timeout (seconds): this is a timeout for queries and mutations. If the
operation takes too long, it is dropped after `x` seconds in the cluster.


# Console
Source: https://docs.hypermode.com/dgraph/ratel/console



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## Query panel

You can execute two kinds of operations: queries and mutations. The history
section holds either queries or mutations.

![Ratel Console](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/ratel/ratel_console.png)

### Query

On this panel, you can only run DQL (former GraphQL+-). You can use `#` to
comment on something. You also have the DQL Variable. See more at
[DQL](/dgraph/dql/schema).

### Mutation

On this panel, you can run RDF and JSON mutations.

## Result panel

### Graph

On this tab you can view the query results in a Graph format. This allows you to
visualize the Nodes and their relations.

### JSON

On this tab you have the JSON response from the cluster. The actual data comes
in the `data` key. You also have the `extensions` key which returns
`server_latency`, `txn`, and other metrics.

### Request

On this tab you have the actual request sent to the cluster.

### Geo

On this tab you can visualize a query that provides Geodata.

<Note>
  Your objects must contain a predicate or alias named `location` to use the geo
  display. To show a label, use a predicate or alias named `name`.
</Note>


# Ratel
Source: https://docs.hypermode.com/dgraph/ratel/overview

Ratel is a tool for data visualization and cluster management that's designed from the ground-up to work with Dgraph. Clone and build Ratel to get started.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Ratel is a tool for data visualization and cluster management that's designed
from the ground-up to work with Dgraph and [DQL](/dgraph/dql/schema). You can
use it for the following types of tasks:

* Connect to a backend and manage cluster settings (see
  [Connection](./connection))
* Run DQL queries and mutations, and see results (see [Console](./console))
* Update or replace your DQL schema, and drop data (see [Schema](./schema))
* Get information on cluster nodes and remove nodes (see [Cluster](./cluster))
* Backup your server if you are using self-managed Dgraph (see
  [Backup](./backups))

To get started with Ratel, use it online with the
[Dgraph Ratel Dashboard](https://play.dgraph.io) or clone and build Ratel using
the
[instructions from the Ratel repository on GitHub](https://github.com/hypermodeinc/ratel/blob/main/INSTRUCTIONS.md).


# Schema
Source: https://docs.hypermode.com/dgraph/ratel/schema



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## Predicate section

You have two panels:

* The left panel with a predicate list in a table. The table consists of three
  columns, the name of the `predicate`, the `type`, and the `indices`.
* On the right panel you have the properties related to the selection from the
  right panel.

![Ratel Schema](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/ratel/ratel_schema.png)

You can add new predicates using the `Add Predicate` button on the top left
corner. In the dialog box, you can add the name of the predicate, the type,
language if required, and its indices.

In the predicate's `Properties` panel you can edit the type, the indexation, or
drop it. In the tab `Samples & Statistics` you have information about your
dataset. It has a sample sneak-peek of the predicate's data.

## Type definition section

You have two panels:

* The left panel provides you a table with a type list, with two columns: `Type`
  and `Field Count`.
* On the right panel you have the properties related to the selected `Type`.

![Ratel Schema](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/ratel/ratel_schema_types.png)

You can add new Types using the `Add Type` button on the top left corner. In the
dialog box, you can add the name of the Type and select which predicates belong
to this Type. The list shows only existing predicates.

## Bulk edit schema & drop data

With this option you can edit the schema directly in plain-text. You also have
the option to Drop the data.

<Note>
  There are two ways to drop the DB. The default option drops the data but keep
  the `Schema`. To drop everything you have to select the check-box `Also drop
        Schema and Types`.
</Note>

![Ratel Schema](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/ratel/ratel_schema_bulk.png)


# .NET
Source: https://docs.hypermode.com/dgraph/sdks/dotnet



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

An implementation for a Dgraph client in C#, using [gRPC](https://grpc.io/).
This client follows the [Dgraph Go client](./go) closely.

<Tip>
  The official C# client [can be found
  here](https://github.com/hypermodeinc/dgraph.net). Follow the [install
  instructions](https://github.com/hypermodeinc/dgraph.net#install) to get it up
  and running.
</Tip>

## Supported Versions

More details on the supported versions can be found at
[this link](https://github.com/hypermodeinc/dgraph.net#supported-versions).

## Using a Client

### Creating a Client

Make a new client by passing in one or more gRPC channels pointing to alphas.

```c#
var client = new DgraphClient(new Channel("127.0.0.1:9080", ChannelCredentials.Insecure));
```

### Multi-tenancy

In [multi-tenancy](/dgraph/enterprise/multitenancy) environments, Dgraph
provides a new method `LoginRequest()`, which will allow the users to login to a
specific namespace.

In order to create a Dgraph client, and make the client login into namespace
`123`:

```c#
var lr = new Api.LoginRequest() {
  UserId = "userId",
  Password = "password",
  Namespace = 123
}
client.Login(lr)
```

In the example above, the client logs into namespace `123` using username
`userId` and password `password`. Once logged in, the client can perform all the
operations allowed to the `userId` user of namespace `123`.

### Altering the Database

To set the schema, pass the schema into the `DgraphClient.Alter` function, as
seen below:

```c#
var schema = "name: string @index(exact) .";
var result = client.Alter(new Operation{ Schema = schema });
```

The returned result object is based on the FluentResults library. You can check
the status using `result.isSuccess` or `result.isFailed`. More information on
the result object can be found [here](https://github.com/altmann/FluentResults).

### Creating a Transaction

To create a transaction, call `DgraphClient.NewTransaction` method, which
returns a new `Transaction` object. This operation incurs no network overhead.

It is good practice to call to wrap the `Transaction` in a `using` block, so
that the `Transaction.Dispose` function is called after running the transaction.

```c#
using(var transaction = client.NewTransaction()) {
    ...
}
```

You can also create Read-Only transactions. Read-Only transactions only allow
querying, and can be created using `DgraphClient.NewReadOnlyTransaction`.

### Running a Mutation

`Transaction.Mutate(RequestBuilder)` runs a mutation. It takes in a json
mutation string.

We define a person object to represent a person and serialize it to a json
mutation string. In this example, we are using the
[JSON.NET](https://www.newtonsoft.com/json) library, but you can use any JSON
serialization library you prefer.

```c#
using(var txn = client.NewTransaction()) {
    var alice = new Person{ Name = "Alice" };
    var json = JsonConvert.SerializeObject(alice);

    var transactionResult = await txn.Mutate(new RequestBuilder().WithMutations(new MutationBuilder{ SetJson = json }));
}
```

You can also set mutations using RDF format, if you so prefer, as seen below:

```c#
var mutation = "_:alice <name> \"Alice\" .";
var transactionResult = await txn.Mutate(new RequestBuilder().WithMutations(new MutationBuilder{ SetNquads = mutation }));
```

Check out the example in `source/Dgraph.tests.e2e/TransactionTest.cs`.

### Running a Query

You can run a query by calling `Transaction.Query(string)`. You will need to
pass in a DQL query string. If you want to pass an additional map of any
variables that you might want to set in the query, call
`Transaction.QueryWithVars(string, Dictionary<string,string>)` with the
variables dictionary as the second argument.

The response would contain the response string.

Let’s run the following query with a variable `$a`:

```console
query all($a: string) {
  all(func: eq(name, $a))
  {
    name
  }
}
```

Run the query, deserialize the result from Uint8Array (or base64) encoded JSON
and print it out:

```c#
// Run query.
var query = @"query all($a: string) {
  all(func: eq(name, $a))
  {
    name
  }
}";

var vars = new Dictionary<string,string> { { $a: "Alice" } };
var res = await dgraphClient.NewReadOnlyTransaction().QueryWithVars(query, vars);

// Print results.
Console.Write(res.Value.Json);
```

### Running an Upsert: Query + Mutation

The `Transaction.Mutate` function allows you to run upserts consisting of one
query and one mutation.

```c#
var query = @"
  query {
    user as var(func: eq(email, \"wrong_email@dgraph.io\"))
  }";

var mutation = new MutationBuilder{ SetNquads = "uid(user) <email> \"correct_email@dgraph.io\" ." };

var request = new RequestBuilder{ Query = query, CommitNow = true }.withMutation(mutation);

// Upsert: If wrong_email found, update the existing data
// or else perform a new mutation.
await txn.Mutate(request);
```

### Committing a Transaction

A transaction can be committed using the `Transaction.Commit` method. If your
transaction consisted solely of calls to `Transaction.Query` or
`Transaction.QueryWithVars`, and no calls to `Transaction.Mutate`, then calling
`Transaction.Commit` is not necessary.

An error will be returned if other transactions running concurrently modify the
same data that was modified in this transaction. It is up to the user to retry
transactions when they fail.

```c#
using(var txn = client.NewTransaction()) {
    var result = txn.Commit();
}
```


# Go
Source: https://docs.hypermode.com/dgraph/sdks/go



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

[![GoDoc](https://godoc.org/github.com/dgraph-io/dgo?status.svg)](https://godoc.org/github.com/dgraph-io/dgo)

The Go client communicates with the server on the gRPC port (default value
9080\).

The client can be obtained in the usual way via `go get`:

```sh
 Requires at least Go 1.23
export GO111MODULE=on
go get -u -v github.com/dgraph-io/dgo/v210
```

The full [GoDoc](https://godoc.org/github.com/dgraph-io/dgo) contains
documentation for the client API along with examples showing how to use it.

More details on the supported versions can be found at
[this link](https://github.com/hypermodeinc/dgo#supported-versions).

## Create the client

To create a client, dial a connection to Dgraph's external gRPC port (typically
`9080`). The following code snippet shows just one connection. You can connect
to multiple Dgraph Alphas to distribute the workload evenly.

```go
func newClient() *dgo.Dgraph {
  // Dial a gRPC connection. The address to dial to can be configured when
  // setting up the dgraph cluster.
  d, err := grpc.Dial("localhost:9080", grpc.WithInsecure())
  if err != nil {
    log.Fatal(err)
  }

  return dgo.NewDgraphClient(
    api.NewDgraphClient(d),
  )
}
```

The client can be configured to use gRPC compression:

```go
func newClient() *dgo.Dgraph {
  // Dial a gRPC connection. The address to dial to can be configured when
  // setting up the dgraph cluster.
  dialOpts := append([]grpc.DialOption{},
    grpc.WithInsecure(),
    grpc.WithDefaultCallOptions(grpc.UseCompressor(gzip.Name)))
  d, err := grpc.Dial("localhost:9080", dialOpts...)

  if err != nil {
    log.Fatal(err)
  }

  return dgo.NewDgraphClient(
    api.NewDgraphClient(d),
  )
}

```

### Multi-tenancy

In [multi-tenancy](/dgraph/enterprise/multitenancy) environments, Dgraph
provides a new method `LoginIntoNamespace()`, which allows the users to login to
a specific namespace.

In order to create a dgo client, and make the client login into namespace `123`:

```go
conn, err := grpc.Dial("127.0.0.1:9080", grpc.WithInsecure())
if err != nil {
  glog.Error("While trying to dial gRPC, got error", err)
}
dc := dgo.NewDgraphClient(api.NewDgraphClient(conn))
ctx := context.Background()
// Login to namespace 123
if err := dc.LoginIntoNamespace(ctx, "groot", "password", 123); err != nil {
  glog.Error("Failed to login: ",err)
}
```

In this example, the client logs into namespace `123` using username `groot` and
password `password`. Once logged in, the client can perform all the operations
allowed to the `groot` user of namespace `123`.

## Alter the database

To set the schema, set it on a `api.Operation` object, and pass it down to the
`Alter` method.

```go
func setup(c *dgo.Dgraph) {
  // Install a schema into dgraph. Accounts have a `name` and a `balance`.
  err := c.Alter(context.Background(), &api.Operation{
    Schema: `
      name: string @index(term) .
      balance: int .
    `,
  })
}
```

`api.Operation` contains other fields as well, including drop predicate and drop
all. Drop all is useful if you wish to discard all the data, and start from a
clean slate, without bringing the instance down.

```go
  // Drop all data including schema from the dgraph instance. This is useful
  // for small examples such as this, since it puts dgraph into a clean
  // state.
  err := c.Alter(context.Background(), &api.Operation{DropOp: api.Operation_ALL})
```

`api.Operation` also supports a drop data operation. This operation drops all
the data but preserves the schema. This is useful when the schema is large and
needs to be reused, such as in between unit tests.

```go
  // Drop all data including schema from the dgraph instance. This is useful
  // for small examples such as this, since it puts dgraph into a clean
  // state.
  err := c.Alter(context.Background(), &api.Operation{DropOp: api.Operation_DATA})
```

## Create a transaction

Dgraph supports running distributed ACID transactions. To create a transaction,
just call `c.NewTxn()`. This operation doesn't incur in network calls.
Typically, you'd also want to call a `defer txn.Discard(ctx)` to let it
automatically rollback in case of errors. Calling `Discard` after `Commit` would
be a no-op.

```go
func runTxn(c *dgo.Dgraph) {
  txn := c.NewTxn()
  defer txn.Discard(ctx)
  ...
}
```

### Read-only transactions

Read-only transactions can be created by calling `c.NewReadOnlyTxn()`. Read-only
transactions are useful to increase read speed because they can circumvent the
usual consensus protocol. Read-only transactions can't contain mutations and
trying to call `txn.Commit()` results in an error. Calling `txn.Discard()` is a
no-op.

Read-only queries can optionally be set as best-effort. Using this flag requests
the Dgraph Alpha to try to get timestamps from memory on a best-effort basis to
reduce the number of outbound requests to Zero. This may yield improved
latencies in read-bound workloads where linearizable reads are not strictly
needed.

## Run a query

You can run a query by calling `txn.Query`. The response would contain a `JSON`
field, which has the JSON encoded result. You can unmarshal it into Go struct
via `json.Unmarshal`.

```go
  // Query the balance for Alice and Bob.
  const q = `
    {
      all(func: anyofterms(name, "Alice Bob")) {
        uid
        balance
      }
    }
  `
  resp, err := txn.Query(context.Background(), q)
  if err != nil {
    log.Fatal(err)
  }

  // After we get the balances, we have to decode them into structs so that
  // we can manipulate the data.
  var decode struct {
    All []struct {
      Uid     string
      Balance int
    }
  }
  if err := json.Unmarshal(resp.GetJson(), &decode); err != nil {
    log.Fatal(err)
  }
```

## Query with RDF response

You can get query result as a RDF response by calling `txn.QueryRDF`. The
response would contain a `Rdf` field, which has the RDF encoded result.

<Note>
  If you are querying only for `uid` values, use a JSON format response.
</Note>

```go
  // Query the balance for Alice and Bob.
  const q = `
    {
      all(func: anyofterms(name, "Alice Bob")) {
        name
        balance
      }
    }
  `
  resp, err := txn.QueryRDF(context.Background(), q)
  if err != nil {
    log.Fatal(err)
  }

  // <0x17> <name> "Alice" .
  // <0x17> <balance> 100 .
  fmt.Println(resp.Rdf)
```

## Run a mutation

`txn.Mutate` would run the mutation. It takes in a `api.Mutation` object, which
provides two main ways to set data: JSON and RDF N-Quad. You can choose
whichever way is convenient.

To use JSON, use the fields SetJson and DeleteJson, which accept a string
representing the nodes to be added or removed respectively (either as a JSON map
or a list). To use RDF, use the fields SetNquads and DeleteNquads, which accept
a string representing the valid RDF triples (one per line) to added or removed
respectively. This protobuf object also contains the Set and Del fields which
accept a list of RDF triples that have already been parsed into our internal
format. As such, these fields are mainly used internally and users should use
the SetNquads and DeleteNquads instead if planning on using RDF.

We're going to continue using JSON. You could modify the Go structs parsed from
the query, and marshal them back into JSON.

```go
  // Move $5 between the two accounts.
  decode.All[0].Bal += 5
  decode.All[1].Bal -= 5

  out, err := json.Marshal(decode.All)
  if err != nil {
    log.Fatal(err)
  }

  _, err := txn.Mutate(context.Background(), &api.Mutation{SetJson: out})
```

Sometimes, you only want to commit mutation, without querying anything further.
In such cases, you can use a `CommitNow` field in `api.Mutation` to indicate
that the mutation must be immediately committed.

## Commit the transaction

Once all the queries and mutations are done, you can commit the transaction. It
returns an error in case the transaction couldn't be committed.

```go
  // Finally, we can commit the transactions. An error will be returned if
  // other transactions running concurrently modify the same data that was
  // modified in this transaction. It is up to the library user to retry
  // transactions when they fail.

  err := txn.Commit(context.Background())
```

## Complete example

This is an example from the [GoDoc](https://godoc.org/github.com/dgraph-io/dgo).
It shows how to create a `Node` with name `Alice`, while also creating her
relationships with other nodes.

<Note>
  `loc` predicate is of type `geo` and can be easily marshaled and unmarshaled
  into a Go struct. More such examples are present as part of the GoDoc.
</Note>

<Tip>
  You can also download this complete example file from our [GitHub
  repository](https://github.com/hypermodeinc/dgo/blob/main/example_set_object_test.go).
</Tip>

```go
package dgo_test

import (
  "context"
  "encoding/json"
  "fmt"
  "log"
  "time"

  "github.com/dgraph-io/dgo/v200/protos/api"
)

type School struct {
  Name  string   `json:"name,omitempty"`
  DType []string `json:"dgraph.type,omitempty"`
}

type loc struct {
  Type   string    `json:"type,omitempty"`
  Coords []float64 `json:"coordinates,omitempty"`
}

// If omitempty is not set, then edges with empty values (0 for int/float, "" for string, false
// for bool) would be created for values not specified explicitly.

type Person struct {
  Uid      string     `json:"uid,omitempty"`
  Name     string     `json:"name,omitempty"`
  Age      int        `json:"age,omitempty"`
  Dob      *time.Time `json:"dob,omitempty"`
  Married  bool       `json:"married,omitempty"`
  Raw      []byte     `json:"raw_bytes,omitempty"`
  Friends  []Person   `json:"friend,omitempty"`
  Location loc        `json:"loc,omitempty"`
  School   []School   `json:"school,omitempty"`
  DType    []string   `json:"dgraph.type,omitempty"`
}

func Example_setObject() {
  dg, cancel := getDgraphClient()
  defer cancel()

  dob := time.Date(1980, 01, 01, 23, 0, 0, 0, time.UTC)
  // While setting an object if a struct has a Uid then its properties in the graph are updated
  // else a new node is created.
  // In the example below new nodes for Alice, Bob and Charlie and school are created (since they
  // don't have a Uid).
  p := Person{
    Uid:     "_:alice",
    Name:    "Alice",
    Age:     26,
    Married: true,
    DType:   []string{"Person"},
    Location: loc{
      Type:   "Point",
      Coords: []float64{1.1, 2},
    },
    Dob: &dob,
    Raw: []byte("raw_bytes"),
    Friends: []Person{{
      Name:  "Bob",
      Age:   24,
      DType: []string{"Person"},
    }, {
      Name:  "Charlie",
      Age:   29,
      DType: []string{"Person"},
    }},
    School: []School{{
      Name:  "Crown Public School",
      DType: []string{"Institution"},
    }},
  }

  op := &api.Operation{}
  op.Schema = `
    name: string @index(exact) .
    age: int .
    married: bool .
    loc: geo .
    dob: datetime .
    Friend: [uid] .
    type: string .
    coords: float .
    type Person {
      name: string
      age: int
      married: bool
      Friend: [Person]
      loc: Loc
    }
    type Institution {
      name: string
    }
    type Loc {
      type: string
      coords: float
    }
  `

  ctx := context.Background()
  if err := dg.Alter(ctx, op); err != nil {
    log.Fatal(err)
  }

  mu := &api.Mutation{
    CommitNow: true,
  }
  pb, err := json.Marshal(p)
  if err != nil {
    log.Fatal(err)
  }

  mu.SetJson = pb
  response, err := dg.NewTxn().Mutate(ctx, mu)
  if err != nil {
    log.Fatal(err)
  }

  // Assigned uids for nodes which were created would be returned in the response.Uids map.
  variables := map[string]string{"$id1": response.Uids["alice"]}
  q := `query Me($id1: string){
    me(func: uid($id1)) {
      name
      dob
      age
      loc
      raw_bytes
      married
      dgraph.type
      friend @filter(eq(name, "Bob")){
        name
        age
        dgraph.type
      }
      school {
        name
        dgraph.type
      }
    }
  }`

  resp, err := dg.NewTxn().QueryWithVars(ctx, q, variables)
  if err != nil {
    log.Fatal(err)
  }

  type Root struct {
    Me []Person `json:"me"`
  }

  var r Root
  err = json.Unmarshal(resp.Json, &r)
  if err != nil {
    log.Fatal(err)
  }

  out, _ := json.MarshalIndent(r, "", "\t")
  fmt.Printf("%s\n", out)
}
```

Example output result:

```json
 Output: {
   "me": [
     {
       "name": "Alice",
       "age": 26,
       "dob": "1980-01-01T23:00:00Z",
       "married": true,
       "raw_bytes": "cmF3X2J5dGVz",
       "friend": [
         {
           "name": "Bob",
           "age": 24,
           "loc": {},
           "dgraph.type": [
             "Person"
           ]
         }
       ],
       "loc": {
         "type": "Point",
         "coordinates": [
           1.1,
           2
         ]
       },
       "school": [
         {
           "name": "Crown Public School",
           "dgraph.type": [
             "Institution"
           ]
         }
       ],
       "dgraph.type": [
         "Person"
       ]
     }
   ]
 }
```


# Java
Source: https://docs.hypermode.com/dgraph/sdks/java



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

A minimal implementation for a Dgraph client for Java 1.11+, using
[gRPC](https://grpc.io/). This client follows the [Dgraph Go client](./go)
closely.

<Tip>
  The official Java client [can be found
  here](https://github.com/hypermodeinc/dgraph4j). Follow the [install
  instructions](https://github.com/hypermodeinc/dgraph4j#download) to get it up
  and running.
</Tip>

## Supported versions

More details on the supported versions can be found at
[this link](https://github.com/hypermodeinc/dgraph4j#supported-versions).

## Quickstart

Build and run the
[DgraphJavaSample](https://github.com/hypermodeinc/dgraph4j/tree/main/samples/DgraphJavaSample)
project in the `samples` folder, which contains an end-to-end example of using
the Dgraph Java client. Follow the instructions in the
[README](https://github.com/hypermodeinc/dgraph4j/tree/main/samples/DgraphJavaSample/README.md)
of that project.

## Intro

This library supports two styles of clients, the synchronous client
`DgraphClient` and the async client `DgraphAsyncClient`. A `DgraphClient` or
`DgraphAsyncClient` can be initialized by passing it a list of
`DgraphBlockingStub` clients. The `anyClient()` API can randomly pick a stub,
which can then be used for gRPC operations.

## Using the synchronous client

<Tip>
  You can find a
  [DgraphJavaSample](https://github.com/hypermodeinc/dgraph4j/tree/main/samples/DgraphJavaSample)
  project, which contains an end-to-end working example of how to use the Java
  client.
</Tip>

### Creating a client

The following code snippet shows how to create a synchronous client using three
connections.

```java
ManagedChannel channel1 = ManagedChannelBuilder
    .forAddress("localhost", 9080)
    .usePlaintext().build();
DgraphStub stub1 = DgraphGrpc.newStub(channel1);

ManagedChannel channel2 = ManagedChannelBuilder
    .forAddress("localhost", 9082)
    .usePlaintext().build();
DgraphStub stub2 = DgraphGrpc.newStub(channel2);

ManagedChannel channel3 = ManagedChannelBuilder
    .forAddress("localhost", 9083)
    .usePlaintext().build();
DgraphStub stub3 = DgraphGrpc.newStub(channel3);

DgraphClient dgraphClient = new DgraphClient(stub1, stub2, stub3);
```

### Login using access control lists

If [Access Control Lists (ACL)](/dgraph/enterprise/access-control-lists) is
enabled then you can log-in to the default namespace (`0`) with the following
method:

```java
dgraphClient.login(USER_ID, USER_PASSWORD);
```

### Multi-tenancy

If [multi-tenancy](/dgraph/enterprise/multitenancy) is enabled, by default the
login method on client logs into the namespace `0`. In order to log into a
different namespace, use the `loginIntoNamespace` method on the client:

```java
dgraphClient.loginIntoNamespace(USER_ID, USER_PASSWORD, NAMESPACE);
```

Once logged-in, the `dgraphClient` object can be used to do any further
operations.

### Creating a secure client using TLS

To setup a client using TLS, you could use the following code snippet. The
server needs to be setup using the instructions provided
[here](/dgraph/self-managed/tls-configuration).

If you are doing client verification, you need to convert the client key from
PKCS#1 format to PKCS#8 format. By default, g doesn't support reading PKCS#1
format keys. To convert the format, you could use the `openssl` tool.

First, let's install the `openssl` tool:

```sh
apt install openssl
```

Now, use the following command to convert the key:

```sh
openssl pkcs8 -in client.name.key -topk8 -nocrypt -out client.name.java.key
```

Now, you can use the following code snippet to connect to Alpha over TLS:

```java
SslContextBuilder builder = GrpcSslContexts.forClient();
builder.trustManager(new File("<path to ca.crt>"));
// Skip the next line if you are not performing client verification.
builder.keyManager(new File("<path to client.name.crt>"), new File("<path to client.name.java.key>"));
SslContext sslContext = builder.build();

ManagedChannel channel = NettyChannelBuilder.forAddress("localhost", 9080)
    .sslContext(sslContext)
    .build();
DgraphGrpc.DgraphStub stub = DgraphGrpc.newStub(channel);
DgraphClient dgraphClient = new DgraphClient(stub);
```

### Check Dgraph version

Checking the version of the Dgraph server this client is interacting with is as
easy as:

```java
Version v = dgraphClient.checkVersion();
System.out.println(v.getTag());
```

Checking the version, before doing anything else can be used as a test to find
out if the client is able to communicate with the Dgraph server. This also helps
reduce the latency of the first query/mutation which results from some dynamic
library loading and linking that happens in JVM (see
[this issue](https://github.com/hypermodeinc/dgraph4j/issues/108) for more
details).

### Altering the database

To set the schema, create an `Operation` object, set the schema and pass it to
`DgraphClient#alter` method.

```java
String schema = "name: string @index(exact) .";
Operation operation = Operation.newBuilder().setSchema(schema).build();
dgraphClient.alter(operation);
```

Starting Dgraph version 20.03.0, indexes can be computed in the background. You
can call the function `setRunInBackground(true)` as shown below before calling
`alter`. You can find more details
[here](/dgraph/admin/update-types#indexes-in-background).

```java
String schema = "name: string @index(exact) .";
Operation operation = Operation.newBuilder()
        .setSchema(schema)
        .setRunInBackground(true)
        .build();
dgraphClient.alter(operation);
```

`Operation` contains other fields as well, including drop predicate and drop
all. Drop all is useful if you wish to discard all the data, and start from a
clean slate, without bringing the instance down.

```java
// Drop all data including schema from the dgraph instance. This is useful
// for small examples such as this, since it puts dgraph into a clean
// state.
dgraphClient.alter(Operation.newBuilder().setDropAll(true).build());
```

### Creating a transaction

There are two types of transactions in dgraph, queries (reads) and mutations
(writes). Both the synchronous `DgraphClient` and the asynchronous
`DgraphAsyncClient` clients support the two types of transactions by providing
the `newTransaction` and the `newReadOnlyTransaction` APIs. Creating a
transaction is a local operation and incurs no network overhead.

In most of the cases, the normal read-write transactions is used, which can have
any number of query or mutate operations. However, if a transaction only has
queries, you might benefit from a read-only transaction, which can share the
same read timestamp across multiple such read-only transactions and can result
in lower latencies.

For normal read-write transactions, it's a good practice to call
`Transaction#discard()` in a `finally` block after running the transaction.
Calling `Transaction#discard()` after `Transaction#commit()` is a no-op and you
can call `discard()` multiple times with no additional side-effects.

```java
Transaction txn = dgraphClient.newTransaction();
try {
    // Do something here
    // ...
} finally {
    txn.discard();
}
```

For read-only transactions, there is no need to call `Transaction.discard`,
which is equivalent to a no-op.

```java
Transaction readOnlyTxn = dgraphClient.newReadOnlyTransaction();
```

Read-only transactions can be set as best-effort. Best-effort queries relax the
requirement of linearizable reads. This is useful when running queries that do
not require a result from the latest timestamp.

```java
Transaction bestEffortTxn = dgraphClient.newReadOnlyTransaction()
    .setBestEffort(true);
```

### Running a mutation

`Transaction#mutate` runs a mutation. It takes in a `Mutation` object, which
provides two main ways to set data: JSON and RDF N-Quad. You can choose
whichever way is convenient.

We're going to use JSON. First we define a `Person` class to represent a person.
This data is serialized into JSON.

```java
class Person {
    String name
    Person() {}
}
```

Next, we initialize a `Person` object, serialize it and use it in `Mutation`
object.

```java
// Create data
Person person = new Person();
person.name = "Alice";

// Serialize it
Gson gson = new Gson();
String json = gson.toJson(person);
// Run mutation
Mutation mu = Mutation.newBuilder()
    .setSetJson(ByteString.copyFromUtf8(json.toString()))
    .build();
txn.mutate(mu);
```

Sometimes, you only want to commit mutation, without querying anything further.
In such cases, you can use a `CommitNow` field in `Mutation` object to indicate
that the mutation must be immediately committed.

Mutation can be run using the `doRequest` function as well.

```java
Request request = Request.newBuilder()
    .addMutations(mu)
    .build();
txn.doRequest(request);
```

### Committing a transaction

A transaction can be committed using the `Transaction#commit()` method. If your
transaction consisted solely of calls to `Transaction#query()`, and no calls to
`Transaction#mutate()`, then calling `Transaction#commit()` isn't necessary.

An error is returned if other transactions running concurrently modify the same
data that was modified in this transaction. It is up to the user to retry
transactions when they fail.

```java
Transaction txn = dgraphClient.newTransaction();

try {
    // …
    // Perform any number of queries and mutations
    // …
    // and finally …
    txn.commit()
} catch (TxnConflictException ex) {
    // Retry or handle exception.
} finally {
    // Clean up. Calling this after txn.commit() is a no-op
    // and hence safe.
    txn.discard();
}
```

### Running a query

You can run a query by calling `Transaction#query()`. You need to pass in a DQL
query string, and a map (optional, could be empty) of any variables that you
might want to set in the query.

The response would contain a `JSON` field, which has the JSON encoded result.
You need to decode it before you can do anything useful with it.

Let’s run the following query:

```dql
query all($a: string) {
  all(func: eq(name, $a)) {
            name
  }
}
```

First we must create a `People` class that helps us deserialize the JSON result:

```java
class People {
    List<Person> all;
    People() {}
}
```

Then we run the query, deserialize the result and print it out:

```java
// Query
String query =
"query all($a: string){\n" +
"  all(func: eq(name, $a)) {\n" +
"    name\n" +
"  }\n" +
"}\n";

Map<String, String> vars = Collections.singletonMap("$a", "Alice");
Response response = dgraphClient.newReadOnlyTransaction().queryWithVars(query, vars);

// Deserialize
People ppl = gson.fromJson(response.getJson().toStringUtf8(), People.class);

// Print results
System.out.printf("people found: %d\n", ppl.all.size());
ppl.all.forEach(person -> System.out.println(person.name));
```

This should print:

```sh
people found: 1
Alice
```

You can also use `doRequest` function to run the query.

```java
Request request = Request.newBuilder()
    .setQuery(query)
    .build();
txn.doRequest(request);
```

### Running a Query with RDF response

You can get query results as an RDF response by calling either `queryRDF()` or
`queryRDFWithVars()`. The response contains the `getRdf()` method, which
provides the RDF encoded output.

<Note>
  If you are querying for `uid` values only, use a JSON format response.
</Note>

```java
// Query
String query = "query me($a: string) { me(func: eq(name, $a)) { name }}";
Map<String, String> vars = Collections.singletonMap("$a", "Alice");
Response response =
    dgraphAsyncClient.newReadOnlyTransaction().queryRDFWithVars(query, vars).join();

// Print results
System.out.println(response.getRdf().toStringUtf8());
```

This should print (assuming Alice's `uid` is `0x2`):

```sh
<0x2> <name> "Alice" .
```

### Running an upsert: query + mutation

The `txn.doRequest` function allows you to run upserts consisting of one query
and one mutation. Variables can be defined in the query and used in the
mutation. You could also use `txn.doRequest` to perform a query followed by a
mutation.

```java
String query = "query {\n" +
  "user as var(func: eq(email, \"wrong_email@dgraph.io\"))\n" +
  "}\n";
Mutation mu = Mutation.newBuilder()
    .setSetNquads(ByteString.copyFromUtf8("uid(user) <email> \"correct_email@dgraph.io\" ."))
    .build();
Request request = Request.newBuilder()
    .setQuery(query)
    .addMutations(mu)
    .setCommitNow(true)
    .build();
txn.doRequest(request);
```

### Running a conditional upsert

The upsert block also allows specifying a conditional mutation block using an
`@if` directive. The mutation is executed only when the specified condition is
true. If the condition is false, the mutation is silently ignored.

See more about Conditional Upsert
[Here](/dgraph/dql/mutation#conditional-upsert).

```java
String query = "query {\n" +
    "user as var(func: eq(email, \"wrong_email@dgraph.io\"))\n" +
    "}\n";
Mutation mu = Mutation.newBuilder()
    .setSetNquads(ByteString.copyFromUtf8("uid(user) <email> \"correct_email@dgraph.io\" ."))
    .setCond("@if(eq(len(user), 1))")
    .build();
Request request = Request.newBuilder()
    .setQuery(query)
    .addMutations(mu)
    .setCommitNow(true)
    .build();
txn.doRequest(request);
```

### Setting deadlines

It is recommended that you always set a deadline for each client call, after
which the client terminates. This is in line with the recommendation for any
gRPC client. Read [this forum post][deadline-post] for more details.

```java
channel = ManagedChannelBuilder.forAddress("localhost", 9080).usePlaintext(true).build();
DgraphGrpc.DgraphStub stub = DgraphGrpc.newStub(channel);
ClientInterceptor timeoutInterceptor = new ClientInterceptor(){
    @Override
    public <ReqT, RespT> ClientCall<ReqT, RespT> interceptCall(
            MethodDescriptor<ReqT, RespT> method, CallOptions callOptions, Channel next) {
        return next.newCall(method, callOptions.withDeadlineAfter(500, TimeUnit.MILLISECONDS));
    }
};
stub.withInterceptors(timeoutInterceptor);
DgraphClient dgraphClient = new DgraphClient(stub);
```

[deadline-post]: https://discuss.hypermode.com/t/dgraph-java-client-setting-deadlines-per-call/3056

### Setting metadata headers

Certain headers such as authentication tokens need to be set globally for all
subsequent calls. Below is an example of setting a header with the name
"auth-token":

```java
// create the stub first
ManagedChannel channel =
ManagedChannelBuilder.forAddress(TEST_HOSTNAME, TEST_PORT).usePlaintext(true).build();
DgraphStub stub = DgraphGrpc.newStub(channel);

// use MetadataUtils to augment the stub with headers
Metadata metadata = new Metadata();
metadata.put(
        Metadata.Key.of("auth-token", Metadata.ASCII_STRING_MARSHALLER), "the-auth-token-value");
stub = MetadataUtils.attachHeaders(stub, metadata);

// create the DgraphClient wrapper around the stub
DgraphClient dgraphClient = new DgraphClient(stub);

// trigger a RPC call using the DgraphClient
dgraphClient.alter(Operation.newBuilder().setDropAll(true).build());
```

### Helper methods

#### Delete multiple edges

The example below uses the helper method `Helpers#deleteEdges` to delete
multiple edges corresponding to predicates on a node with the given UID. The
helper method takes an existing mutation, and returns a new mutation with the
deletions applied.

```java
Mutation mu = Mutation.newBuilder().build()
mu = Helpers.deleteEdges(mu, uid, "friends", "loc");
dgraphClient.newTransaction().mutate(mu);
```

### Closing the database connection

To disconnect from Dgraph, call `ManagedChannel#shutdown` on the gRPC channel
object created when [creating a Dgraph client](#creating-a-client).

```java
channel.shutdown();
```

## Using the asynchronous client

Dgraph Client for Java also bundles an asynchronous API, which can be used by
instantiating the `DgraphAsyncClient` class. The usage is almost exactly the
same as the `DgraphClient` (show in previous section) class. The main
differences is that the `DgraphAsyncClient#newTransacation()` returns an
`AsyncTransaction` class. The API for `AsyncTransaction` is exactly
`Transaction`. The only difference is that instead of returning the results
directly, it returns immediately with a corresponding `CompletableFuture<T>`
object. This object represents the computation which runs asynchronously to
yield the result in the future. Read more about `CompletableFuture<T>` in the
[Java 8 documentation][futuredocs].

[futuredocs]: https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html

Here is the asynchronous version of the preceding code, which runs a query.

```java
// Query
String query =
"query all($a: string){\n" +
"  all(func: eq(name, $a)) {\n" +
"    name\n" +
 "}\n" +
"}\n";

Map<String, String> vars = Collections.singletonMap("$a", "Alice");

AsyncTransaction txn = dgraphAsyncClient.newTransaction();
txn.query(query).thenAccept(response -> {
    // Deserialize
    People ppl = gson.fromJson(res.getJson().toStringUtf8(), People.class);

    // Print results
    System.out.printf("people found: %d\n", ppl.all.size());
    ppl.all.forEach(person -> System.out.println(person.name));
});
```

## Checking the request latency

If you would like to see the latency for either a mutation or query request, the
latency field in the returned result can be helpful. Here is an example to log
the latency of a query request:

```java
Response resp = txn.query(query);
Latency latency = resp.getLatency();
logger.info("parsing latency:" + latency.getParsingNs());
logger.info("processing latency:" + latency.getProcessingNs());
logger.info("encoding latency:" + latency.getEncodingNs());
```

Similarly you can get the latency of a mutation request:

```java
Assigned assignedIds = dgraphClient.newTransaction().mutate(mu);
Latency latency = assignedIds.getLatency();
```

## Concurrent mutations and conflicts

This how-to guide provides an example on how to handle concurrent modifications
using a multi-threaded Java Program. The example demonstrates
[transaction](./overview#transactions) conflicts in Dgraph.

Steps to run this example are as follows.

Step 1: start a new terminal and launch Dgraph with the following command line.

```sh
docker run -it -p 8080:8080 -p 9080:9080 dgraph/standalone:%VERSION_HERE
```

Step 2: check out the source code from the 'samples' directory in
[dgraph4j repo](https://github.com/hypermodeinc/dgraph4j). This particular
example can found at the path `samples/concurrent-modification`. In order to run
this example, execute the following maven command from the
'concurrent-modification' folder.

```sh
mvn clean install exec:java
```

Step 3: on running the example, the program initializes Dgraph with the
following schema.

```sh
<clickCount>: int @index(int) .
<name>: string @index(exact) .
```

Step 4: the program also initializes user "Alice" with a 'clickCount' of value
'1', and then proceeds to increment 'clickCount' concurrently in two threads.
Dgraph throws an exception if a transaction is updating a given predicate that
is being concurrently modified. As part of the exception handling logic, the
program sleeps for 1 second on receiving a concurrent modification exception
(“TxnConflictException”), and then retries.

The logs below show that two threads are increasing clickCount for the same user
named Alice (note the same UID). Thread #1 succeeds immediately, and Dgraph
throws a concurrent modification conflict on Thread 2. Thread 2 sleeps for 1
second and retries, and this time succeeds.

```sh
1599628015260 Thread #2 increasing clickCount for uid 0xe, Name: Alice
1599628015260 Thread #1 increasing clickCount for uid 0xe, Name: Alice
1599628015291 Thread #1 succeeded after 0 retries
1599628015297 Thread #2 found a concurrent modification conflict, sleeping for 1 second...
1599628016297 Thread #2 resuming
1599628016310 Thread #2 increasing clickCount for uid 0xe, Name: Alice
1599628016333 Thread #2 succeeded after 1 retries
```

Step 5: please note that the final value of clickCount is 3 (initial value was
1\), which is correct. Query:

```json
{
  Alice(func: has(<name>)) @filter(eq(name,"Alice" )) {
    uid
    name
    clickCount
  }
}
```

Response:

```json
{
  "data": {
    "Alice": [
      {
        "uid": "0xe",
        "name": "Alice",
        "clickCount": 3
      }
    ]
  }
}
```

## Summary

Concurrent modifications to the same predicate causes the "TxnConflictException"
exception. When several transactions hit the same node's predicate at the same
time, the first one succeeds, while the other receives the
“TxnConflictException”. Upon constantly retrying, the transactions begin to
succeed one after another, and given enough retries, correctly completes its
work.


# JavaScript
Source: https://docs.hypermode.com/dgraph/sdks/javascript



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

The official Dgraph client implementation for JavaScript, using
[gRPC-js](https://www.npmjs.com/package/@grpc/grpc-js) (the original
[gRPC](https://grpc.io/) client for JavaScript is now deprecated).

This client follows the [Dgraph Go client](./go) closely.

<Tip>
  You can find the official Dgraph JavaScript gRPC client at:
  [https://github.com/hypermodeinc/dgraph-js](https://github.com/hypermodeinc/dgraph-js). Follow the [installation
  instructions](https://github.com/hypermodeinc/dgraph-js#install) to get it up
  and running.
</Tip>

## Supported versions

More details on the supported versions can be found at
[this link](https://github.com/hypermodeinc/dgraph-js#supported-versions).

## Quickstart

Build and run the
[simple project](https://github.com/hypermodeinc/dgraph-js/tree/main/examples/simple),
which contains an end-to-end example of using the Dgraph JavaScript client.
Follow the instructions in the
[README](https://github.com/hypermodeinc/dgraph-js/tree/main/examples/simple/README.md)
of that project.

### Examples

* [Simple](https://github.com/hypermodeinc/dgraph-js/tree/main/examples/simple):
  Quickstart example of using dgraph-js.
* [TLS](https://github.com/hypermodeinc/dgraph-js/tree/main/examples/tls):
  Example of using dgraph-js with a Dgraph cluster secured with TLS.

## Using a client

<Tip>
  You can find a [simple
  example](https://github.com/hypermodeinc/dgraph-js/tree/main/examples/simple)
  project, which contains an end-to-end working example of how to use the
  JavaScript gRPC client, for Node.js >= v6.
</Tip>

### Creating a client

A `DgraphClient` object can be initialized by passing it a list of
`DgraphClientStub` clients as arguments. Connecting to multiple Dgraph servers
in the same cluster allows for better distribution of workload.

The following code snippet shows just one connection.

```js
const dgraph = require("dgraph-js")
const grpc = require("grpc")

const clientStub = new dgraph.DgraphClientStub(
  // addr: optional, default: "localhost:9080"
  "localhost:9080",
  // credentials: optional, default: grpc.credentials.createInsecure()
  grpc.credentials.createInsecure(),
)
const dgraphClient = new dgraph.DgraphClient(clientStub)
```

To facilitate debugging, [debug mode](#debug-mode) can be enabled for a client.

### Multi-tenancy

In [multi-tenancy](/dgraph/enterprise/multitenancy) environments, `dgraph-js`
provides a new method `loginIntoNamespace()`, which allows the users to login to
a specific namespace.

In order to create a JavaScript client, and make the client login into namespace
`123`:

```js
const dgraphClientStub = new dgraph.DgraphClientStub("localhost:9080")
await dgraphClientStub.loginIntoNamespace("groot", "password", 123) // where 123 is the namespaceId
```

In this example, the client logs into namespace `123` using username `groot` and
password `password`. Once logged in, the client can perform all the operations
allowed to the `groot` user of namespace `123`.

### Altering the database

To set the schema, create an `Operation` object, set the schema and pass it to
`DgraphClient#alter(Operation)` method.

```js
const schema = "name: string @index(exact) ."
const op = new dgraph.Operation()
op.setSchema(schema)
await dgraphClient.alter(op)
```

Starting Dgraph version 20.03.0, indexes can be computed in the background. You
can set `setRunInBackground` field of the `Operation` object to `true` before
passing it to the `DgraphClient#alter(Operation)` method. You can find more
details [here](/dgraph/admin/update-types#indexes-in-background).

```js
const schema = "name: string @index(exact) ."
const op = new dgraph.Operation()
op.setSchema(schema)
op.setRunInBackground(true)
await dgraphClient.alter(op)
```

> NOTE: Many of the examples here use the `await` keyword which requires
> `async/await` support which is available on Node.js >= v7.6.0. For prior
> versions, the expressions following `await` can be used just like normal
> `Promise`:
>
> ```js
> dgraphClient.alter(op)
>     .then(function(result) { ... }, function(err) { ... })
> ```

`Operation` contains other fields as well, including drop predicate and drop
all. Drop all is useful if you wish to discard all the data, and start from a
clean slate, without bringing the instance down.

```js
// Drop all data including schema from the Dgraph instance. This is useful
// for small examples such as this, since it puts Dgraph into a clean
// state.
const op = new dgraph.Operation()
op.setDropAll(true)
await dgraphClient.alter(op)
```

### Creating a transaction

To create a transaction, call `DgraphClient#newTxn()` method, which returns a
new `Txn` object. This operation incurs no network overhead.

It is good practice to call `Txn#discard()` in a `finally` block after running
the transaction. Calling `Txn#discard()` after `Txn#commit()` is a no-op and you
can call `Txn#discard()` multiple times with no additional side-effects.

```js
const txn = dgraphClient.newTxn()
try {
  // Do something here
  // ...
} finally {
  await txn.discard()
  // ...
}
```

To create a read-only transaction, set `readOnly` boolean to `true` while
calling `DgraphClient#newTxn()` method. Read-only transactions can't contain
mutations and trying to call `Txn#mutate()` or `Txn#commit()` results in an
error. Calling `Txn.Discard()` is a no-op.

You can optionally set the `bestEffort` boolean to `true`. This may yield
improved latencies in read-bound workloads where the guaranteed latest value is
not strictly needed.

```js
const txn = dgraphClient.newTxn({
  readOnly: true,
  bestEffort: false,
})
// ...
const res = await txn.queryWithVars(query, vars)
```

### Running a mutation

`Txn#mutate(Mutation)` runs a mutation. It takes in a `Mutation` object, which
provides two main ways to set data, JSON and RDF N-Quad. You can choose
whichever way is convenient.

We define a person object to represent a person and use it in a `Mutation`
object.

```js
// Create data.
const p = {
  name: "Alice",
}

// Run mutation.
const mu = new dgraph.Mutation()
mu.setSetJson(p)
await txn.mutate(mu)
```

For a more complete example with multiple fields and relationships, look at the
\[simple] project in the `examples` folder.

Sometimes, you only want to commit a mutation, without querying anything
further. In such cases, you can use `Mutation#setCommitNow(true)` to indicate
that the mutation must be immediately committed.

`Mutation#setIgnoreIndexConflict(true)` can be applied on a `Mutation` object to
not run conflict detection over the index, which would decrease the number of
transaction conflicts and aborts. However, this would come at the cost of
potentially inconsistent upsert operations.

Mutation can be run using `txn.doRequest` as well.

```js
const mu = new dgraph.Mutation()
mu.setSetJson(p)

const req = new dgraph.Request()
req.setCommitNow(true)
req.setMutationsList([mu])

await txn.doRequest(req)
```

### Running a query

You can run a query by calling `Txn#query(string)`. You need to pass in a DQL
query string. If you want to pass an additional map of any variables that you
might want to set in the query, call `Txn#queryWithVars(string, object)` with
the variables object as the second argument.

The response would contain the method `Response#getJSON()`, which returns the
response JSON.

Let’s run the following query with a variable \$a:

```console
query all($a: string) {
  all(func: eq(name, $a))
  {
    name
  }
}
```

Run the query, deserialize the result from Uint8Array (or base64) encoded JSON
and print it out:

```js
// Run query.
const query = `query all($a: string) {
  all(func: eq(name, $a))
  {
    name
  }
}`
const vars = { $a: "Alice" }
const res = await dgraphClient.newTxn().queryWithVars(query, vars)
const ppl = res.getJson()

// Print results.
console.log(`Number of people named "Alice": ${ppl.all.length}`)
ppl.all.forEach((person) => console.log(person.name))
```

This should print:

```console
Number of people named "Alice": 1
Alice
```

You can also use `txn.doRequest` function to run the query.

```js
const req = new dgraph.Request()
const vars = req.getVarsMap()
vars.set("$a", "Alice")
req.setQuery(query)

const res = await txn.doRequest(req)
console.log(JSON.stringify(res.getJson()))
```

### Running an upsert: query + mutation

The `txn.doRequest` function allows you to run upserts consisting of one query
and one mutation. Query variables could be defined and can then be used in the
mutation. You can also use the `txn.doRequest` function to perform just a query
or a mutation.

```js
const query = `
  query {
      user as var(func: eq(email, "wrong_email@dgraph.io"))
  }`

const mu = new dgraph.Mutation()
mu.setSetNquads(`uid(user) <email> "correct_email@dgraph.io" .`)

const req = new dgraph.Request()
req.setQuery(query)
req.setMutationsList([mu])
req.setCommitNow(true)

// Upsert: If wrong_email found, update the existing data
// or else perform a new mutation.
await dgraphClient.newTxn().doRequest(req)
```

### Running a conditional upsert

The upsert block allows specifying a conditional mutation block using an `@if`
directive. The mutation is executed only when the specified condition is true.
If the condition is false, the mutation is silently ignored.

See more about Conditional Upsert
[Here](/dgraph/dql/mutation#conditional-upsert).

```js
const query = `
  query {
      user as var(func: eq(email, "wrong_email@dgraph.io"))
  }`

const mu = new dgraph.Mutation()
mu.setSetNquads(`uid(user) <email> "correct_email@dgraph.io" .`)
mu.setCond(`@if(eq(len(user), 1))`)

const req = new dgraph.Request()
req.setQuery(query)
req.addMutations(mu)
req.setCommitNow(true)

await dgraphClient.newTxn().doRequest(req)
```

### Committing a transaction

A transaction can be committed using the `Txn#commit()` method. If your
transaction consisted solely of calls to `Txn#query` or `Txn#queryWithVars`, and
no calls to `Txn#mutate`, then calling `Txn#commit()` isn't necessary.

An error is returned if other transactions running concurrently modify the same
data that was modified in this transaction. It is up to the user to retry
transactions when they fail.

```js
const txn = dgraphClient.newTxn()
try {
  // ...
  // Perform any number of queries and mutations
  // ...
  // and finally...
  await txn.commit()
} catch (e) {
  if (e === dgraph.ERR_ABORTED) {
    // Retry or handle exception.
  } else {
    throw e
  }
} finally {
  // Clean up. Calling this after txn.commit() is a no-op
  // and hence safe.
  await txn.discard()
}
```

### Clean up resources

To clean up resources, you have to call `DgraphClientStub#close()` individually
for all the instances of `DgraphClientStub`.

```js
const SERVER_ADDR = "localhost:9080"
const SERVER_CREDENTIALS = grpc.credentials.createInsecure()

// Create instances of DgraphClientStub.
const stub1 = new dgraph.DgraphClientStub(SERVER_ADDR, SERVER_CREDENTIALS)
const stub2 = new dgraph.DgraphClientStub(SERVER_ADDR, SERVER_CREDENTIALS)

// Create an instance of DgraphClient.
const dgraphClient = new dgraph.DgraphClient(stub1, stub2)

// ...
// Use dgraphClient
// ...

// Cleanup resources by closing all client stubs.
stub1.close()
stub2.close()
```

### Debug mode

Debug mode can be used to print helpful debug messages while performing alters,
queries, and mutations. It can be set using
the`DgraphClient#setDebugMode(boolean?)` method.

```js
// Create a client.
const dgraphClient = new dgraph.DgraphClient(...);

// Enable debug mode.
dgraphClient.setDebugMode(true);
// OR simply dgraphClient.setDebugMode();

// Disable debug mode.
dgraphClient.setDebugMode(false);
```

### Setting metadata headers

Metadata headers such as authentication tokens can be set through the context of
gRPC methods. Below is an example of how to set a header named `auth-token`.

```js
// The following piece of code shows how one can set metadata with
// auth-token, to allow Alter operation, if the server requires it.

var meta = new grpc.Metadata()
meta.add("auth-token", "mySuperSecret")

await dgraphClient.alter(op, meta)
```


# JavaScript HTTP Client
Source: https://docs.hypermode.com/dgraph/sdks/javascript-http



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

A Dgraph client implementation for JavaScript using HTTP. It supports both
browser and Node.js environments. This client follows the
[Dgraph JavaScript gRPC client](./javascript) closely.

<Tip>
  The official JavaScript HTTP client [can be found
  here](https://github.com/dgraph-io/dgraph-js-http). Follow the [install
  instructions](https://github.com/dgraph-io/dgraph-js-http#install) to get it
  up and running.
</Tip>

## Supported versions

More details on the supported versions can be found at
[this link](https://github.com/dgraph-io/dgraph-js-http#supported-versions).

## Quickstart

Build and run the
[simple project](https://github.com/dgraph-io/dgraph-js-http/tree/master/examples/simple),
which contains an end-to-end example of using the Dgraph JavaScript HTTP client.
Follow the instructions in the
[README](https://github.com/dgraph-io/dgraph-js-http/tree/master/examples/simple/README.md)
of that project.

## Using a client

<Tip>
  You can find a [simple
  example](https://github.com/dgraph-io/dgraph-js-http/tree/master/examples/simple)
  project, which contains an end-to-end working example of how to use the
  JavaScript HTTP client, for Node.js >= v6.
</Tip>

### Create a client

A `DgraphClient` object can be initialized by passing it a list of
`DgraphClientStub` clients as arguments. Connecting to multiple Dgraph servers
in the same cluster allows for better distribution of workload.

The following code snippet shows just one connection.

```js
const dgraph = require("dgraph-js-http")

const clientStub = new dgraph.DgraphClientStub(
  // addr: optional, default: "http://localhost:8080"
  "http://localhost:8080",
  // legacyApi: optional, default: false. Set to true when connecting to Dgraph v1.0.x
  false,
)
const dgraphClient = new dgraph.DgraphClient(clientStub)
```

To facilitate debugging, [debug mode](#debug-mode) can be enabled for a client.

### Login into Dgraph

If your Dgraph server has Access Control Lists enabled (Dgraph v1.1 or above),
the clientStub must be logged in for accessing data:

```js
await clientStub.login("groot", "password")
```

Calling `login` will obtain and remember the access and refresh JWT tokens. All
subsequent operations via the logged in `clientStub` will send along the stored
access token.

Access tokens expire after 6 hours, so in long-lived apps (e.g. business logic
servers) you need to `login` again on a periodic basis:

```js
// When no parameters are specified the clientStub uses existing refresh token
// to obtain a new access token.
await clientStub.login()
```

### Configure access tokens

Some Dgraph configurations require extra access tokens. Alpha servers can be
configured with
[Secure Alter Operations](/dgraph/self-managed/overview#secure-alter-operations).
In this case the token needs to be set on the client instance:

```js
dgraphClient.setAlphaAuthToken("My secret token value")
```

### Create https connection

If your cluster is using TLS/mTLS you can pass a node `https.Agent` configured
with you certificates as follows:

```js
const https = require("https")
const fs = require("fs")
// read your certificates
const cert = fs.readFileSync("./certs/client.crt", "utf8")
const ca = fs.readFileSync("./certs/ca.crt", "utf8")
const key = fs.readFileSync("./certs/client.key", "utf8")

// create your https.Agent
const agent = https.Agent({
  cert,
  ca,
  key,
})

const clientStub = new dgraph.DgraphClientStub(
  "https://localhost:8080",
  false,
  { agent },
)
const dgraphClient = new dgraph.DgraphClient(clientStub)
```

### Alter the database

To set the schema, pass the schema to `DgraphClient#alter(Operation)` method.

```js
const schema = "name: string @index(exact) ."
await dgraphClient.alter({ schema: schema })
```

> NOTE: Many of the examples here use the `await` keyword which requires
> `async/await` support which isn't available in all javascript environments.
> For unsupported environments, the expressions following `await` can be used
> just like normal `Promise` instances.

`Operation` contains other fields as well, including drop predicate and drop
all. Drop all is useful if you wish to discard all the data, and start from a
clean slate, without bringing the instance down.

```js
// Drop all data including schema from the Dgraph instance. This is useful
// for small examples such as this, since it puts Dgraph into a clean
// state.
await dgraphClient.alter({ dropAll: true })
```

### Create a transaction

To create a transaction, call `DgraphClient#newTxn()` method, which returns a
new `Txn` object. This operation incurs no network overhead.

It is good practice to call `Txn#discard()` in a `finally` block after running
the transaction. Calling `Txn#discard()` after `Txn#commit()` is a no-op and you
can call `Txn#discard()` multiple times with no additional side-effects.

```js
const txn = dgraphClient.newTxn()
try {
  // Do something here
  // ...
} finally {
  await txn.discard()
  // ...
}
```

You can make queries read-only and best effort by passing `options` to
`DgraphClient#newTxn`. For example:

```js
const options = { readOnly: true, bestEffort: true }
const res = await dgraphClient.newTxn(options).query(query)
```

Read-only transactions are useful to increase read speed because they can
circumvent the usual consensus protocol. Best effort queries can also increase
read speed in read bound system. Please note that best effort requires readonly.

### Run a mutation

`Txn#mutate(Mutation)` runs a mutation. It takes in a `Mutation` object, which
provides two main ways to set data: JSON and RDF N-Quad. You can choose
whichever way is convenient.

We define a person object to represent a person and use it in a `Mutation`
object.

```js
// Create data.
const p = {
  name: "Alice",
}

// Run mutation.
await txn.mutate({ setJson: p })
```

For a more complete example with multiple fields and relationships, look at the
\[simple] project in the `examples` folder.

For setting values using N-Quads, use the `setNquads` field. For delete
mutations, use the `deleteJson` and `deleteNquads` fields for deletion using
JSON and N-Quads respectively.

Sometimes, you only want to commit a mutation, without querying anything
further. In such cases, you can use `Mutation#commitNow = true` to indicate that
the mutation must be immediately committed.

```js
// Run mutation.
await txn.mutate({ setJson: p, commitNow: true })
```

### Run a query

You can run a query by calling `Txn#query(string)`. You will need to pass in a
GraphQL+- query string. If you want to pass an additional map of any variables
that you might want to set in the query, call
`Txn#queryWithVars(string, object)` with the variables object as the second
argument.

The response would contain the `data` field, `Response#data`, which returns the
response JSON.

Let’s run the following query with a variable \$a:

```console
query all($a: string) {
  all(func: eq(name, $a))
  {
    name
  }
}
```

Run the query and print out the response:

```js
// Run query.
const query = `query all($a: string) {
  all(func: eq(name, $a))
  {
    name
  }
}`
const vars = { $a: "Alice" }
const res = await dgraphClient.newTxn().queryWithVars(query, vars)
const ppl = res.data

// Print results.
console.log(`Number of people named "Alice": ${ppl.all.length}`)
ppl.all.forEach((person) => console.log(person.name))
```

This should print:

```console
Number of people named "Alice": 1
Alice
```

### Commit a transaction

A transaction can be committed using the `Txn#commit()` method. If your
transaction consisted solely of calls to `Txn#query` or `Txn#queryWithVars`, and
no calls to `Txn#mutate`, then calling `Txn#commit()` is not necessary.

An error will be returned if other transactions running concurrently modify the
same data that was modified in this transaction. It is up to the user to retry
transactions when they fail.

```js
const txn = dgraphClient.newTxn()
try {
  // ...
  // Perform any number of queries and mutations
  // ...
  // and finally...
  await txn.commit()
} catch (e) {
  if (e === dgraph.ERR_ABORTED) {
    // Retry or handle exception.
  } else {
    throw e
  }
} finally {
  // Clean up. Calling this after txn.commit() is a no-op
  // and hence safe.
  await txn.discard()
}
```

### Check request latency

To see the server latency information for requests, check the
`extensions.server_latency` field from the Response object for queries or from
the Assigned object for mutations. These latencies show the amount of time the
Dgraph server took to process the entire request. It does not consider the time
over the network for the request to reach back to the client.

```js
// queries
const res = await txn.queryWithVars(query, vars)
console.log(res.extensions.server_latency)
// { parsing_ns: 29478,
//  processing_ns: 44540975,
//  encoding_ns: 868178 }

// mutations
const assigned = await txn.mutate({ setJson: p })
console.log(assigned.extensions.server_latency)
// { parsing_ns: 132207,
//   processing_ns: 84100996 }
```

### Debug mode

Debug mode can be used to print helpful debug messages while performing alters,
queries and mutations. It can be set using
the`DgraphClient#setDebugMode(boolean?)` method.

```js
// Create a client.
const dgraphClient = new dgraph.DgraphClient(...);

// Enable debug mode.
dgraphClient.setDebugMode(true);
// OR simply dgraphClient.setDebugMode();

// Disable debug mode.
dgraphClient.setDebugMode(false);
```


# Overview
Source: https://docs.hypermode.com/dgraph/sdks/overview

Dgraph client libraries in various programming languages.

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph client libraries allow you to run DQL transactions, queries and mutations
in various programming languages.

If you are interested in clients for GraphQL endpoint, please refer to
[GraphQL clients](/dgraph/graphql/connecting) section.

Go, python, Java, C# and JavaScript clients are using
**[gRPC](https://grpc.io/):** protocol and
[Protocol Buffers](https://developers.google.com/protocol-buffers) (the .proto
file used by Dgraph is located at
[api.proto](https://github.com/hypermodeinc/dgo/blob/main/protos/api.proto)).

A JavaScript client using **HTTP:** is also available.

It is possible to interface with Dgraph directly via gRPC or HTTP. However, if a
client library exists for your language, that's a simpler option.

<Tip>
  For multi-node setups, predicates are assigned to the group that first sees
  that predicate. Dgraph also automatically moves predicate data to different
  groups to balance predicate distribution. This occurs automatically every 10
  minutes. It is possible for clients to aid this process by communicating with
  all Dgraph instances. For the Go client, this means passing in one
  `*grpc.ClientConn` per Dgraph instance, or routing traffic through a load
  balancer. Mutations are made in a round robin fashion, resulting in a
  semi-random initial predicate distribution.
</Tip>

### Transactions

Dgraph clients perform mutations and queries using transactions. A transaction
bounds a sequence of queries and mutations that are committed by Dgraph as a
single unit: that's, on commit, either all the changes are accepted by Dgraph or
none are.

A transaction always sees the database state at the moment it began, plus any
changes it makes --- changes from concurrent transactions aren't visible.

On commit, Dgraph aborts a transaction, rather than committing changes, when a
conflicting, concurrently running transaction has already been committed. Two
transactions conflict when both transactions:

* write values to the same scalar predicate of the same node (e.g both
  attempting to set a particular node's `address` predicate); or
* write to a singular `uid` predicate of the same node (changes to `[uid]`
  predicates can be concurrently written); or
* write a value that conflicts on an index for a predicate with `@upsert` set in
  the schema (see [upserts](/dgraph/dql/upsert)).

When a transaction is aborted, all its changes are discarded. Transactions can
be manually aborted.

<Note>
  For additional clients, see [Unofficial
  Clients](/dgraph/sdks/unofficial-clients).
</Note>


# Python
Source: https://docs.hypermode.com/dgraph/sdks/python



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Official Dgraph client implementation for Python (Python >= v2.7 and >= v3.5),
using [gRPC](https://grpc.io/). This client follows the [Dgraph Go client](./go)
closely.

<Tip>
  The official Python client [can be found
  here](https://github.com/hypermodeinc/pydgraph). Follow the [install
  instructions](https://github.com/hypermodeinc/pydgraph#install) to get it up
  and running.
</Tip>

## Supported versions

More details on the supported versions can be found at
[this link](https://github.com/hypermodeinc/pydgraph#supported-versions).

## Using a client

<Tip>
  You can get a [simple
  example](https://github.com/hypermodeinc/pydgraph/tree/main/examples/simple)
  project, which contains an end-to-end working example of how to use the Python
  client.
</Tip>

### Creating a client

You can initialize a `DgraphClient` object by passing it a list of
`DgraphClientStub` clients as arguments. Connecting to multiple Dgraph servers
in the same cluster allows for better distribution of workload.

The following code snippet shows just one connection.

```python
import pydgraph

client_stub = pydgraph.DgraphClientStub('localhost:9080')
client = pydgraph.DgraphClient(client_stub)
```

### Multi-tenancy

In [multi-tenancy](/dgraph/enterprise/multitenancy) environments, PyDgraph
provides a new method `login_into_namespace()`, which allows the users to login
to a specific namespace.

In order to create a python client, and make the client login into namespace
`123`:

```python
client_stub = pydgraph.DgraphClientStub('localhost:9080')
client = pydgraph.DgraphClient(client_stub)
// Login to namespace groot user of namespace 123
client.login_into_namespace("groot", "password", "123")
```

In the example above, the client logs into namespace `123` using username
`groot` and password `password`. Once logged in, the client can perform all the
operations allowed to the `groot` user of namespace `123`.

### Altering the database

To set the schema, create an `Operation` object, set the schema and pass it to
`DgraphClient#alter(Operation)` method.

```python
schema = 'name: string @index(exact) .'
op = pydgraph.Operation(schema=schema)
client.alter(op)
```

Starting with Dgraph version 20.03.0, indexes can be computed in the background.
You can set the `run_in_background` field of `pydgraph.Operation` to `True`
before passing it to the `Alter` function. You can find more details
[here](/dgraph/dql/schema#indexes-in-background).

```python
schema = 'name: string @index(exact) .'
op = pydgraph.Operation(schema=schema, run_in_background=True)
client.alter(op)
```

`Operation` contains other fields as well, including the `drop` predicate and
`drop all`. Drop all is useful if you wish to discard all the data, and start
with a clean slate, without bringing the instance down.

```python
# Drop all data including schema from the Dgraph instance. This is a useful
# for small examples such as this since it puts Dgraph into a clean state.
op = pydgraph.Operation(drop_all=True)
client.alter(op)
```

### Creating a transaction

To create a transaction, call the `DgraphClient#txn()` method, which returns a
new `Txn` object. This operation incurs no network overhead.

It is good practice to call `Txn#discard()` in a `finally` block after running
the transaction. Calling `Txn#discard()` after `Txn#commit()` is a no-op and you
can call `Txn#discard()` multiple times with no additional side-effects.

```python
txn = client.txn()
try:
  # Do something here
  # ...
finally:
  txn.discard()
  # ...
```

To create a read-only transaction, call `DgraphClient#txn(read_only=True)`.
Read-only transactions are ideal for transactions which only involve queries.
Mutations and commits aren't allowed.

```python
txn = client.txn(read_only=True)
try:
  # Do some queries here
  # ...
finally:
  txn.discard()
  # ...
```

To create a read-only transaction that executes best-effort queries, call
`DgraphClient#txn(read_only=True, best_effort=True)`. Best-effort queries are
faster than normal queries because they bypass the normal consensus protocol.
For this same reason, best-effort queries can't guarantee to return the latest
data. Best-effort queries are only supported by read-only transactions.

### Running a mutation

`Txn#mutate(mu=Mutation)` runs a mutation. It takes in a `Mutation` object,
which provides two main ways to set data, JSON and RDF N-Quad. You can choose
whichever way is convenient.

`Txn#mutate()` provides convenience keyword arguments `set_obj` and `del_obj`
for setting JSON values and `set_nquads` and `del_nquads` for setting N-Quad
values. See examples below for usage.

We define a person object to represent a person and use it in a transaction.

```python
# Create data.
p = {
    'name': 'Alice',
}

# Run mutation.
txn.mutate(set_obj=p)

# If you want to use a mutation object, use this instead:
# mu = pydgraph.Mutation(set_json=json.dumps(p).encode('utf8'))
# txn.mutate(mu)

# If you want to use N-Quads, use this instead:
# txn.mutate(set_nquads='_:alice <name> "Alice" .')
```

```python
# Delete data.

query = """query all($a: string)
 {
   all(func: eq(name, $a))
    {
      uid
    }
  }"""

variables = {'$a': 'Bob'}

res = txn.query(query, variables=variables)
ppl = json.loads(res.json)

# For a mutation to delete a node, use this:
txn.mutate(del_obj=person)
```

For a complete example with multiple fields and relationships, look at the
simple project in the `examples` folder.

Sometimes, you only want to commit a mutation, without querying anything
further. In such cases, you can set the keyword argument `commit_now=True` to
indicate that the mutation must be immediately committed.

A mutation can be executed using `txn.do_request` as well.

```python
mutation = txn.create_mutation(set_nquads='_:alice <name> "Alice" .')
request = txn.create_request(mutations=[mutation], commit_now=True)
txn.do_request(request)
```

### Committing a transaction

A transaction can be committed using the `Txn#commit()` method. If your
transaction consist solely of `Txn#query` or `Txn#queryWithVars` calls, and no
calls to `Txn#mutate`, then calling `Txn#commit()` isn't necessary.

An error is raised if another transaction modifies the same data concurrently
that was modified in the current transaction. It is up to the user to retry
transactions when they fail.

```python
txn = client.txn()
try:
  # ...
  # Perform any number of queries and mutations
  # ...
  # and finally...
  txn.commit()
except pydgraph.AbortedError:
  # Retry or handle exception.
finally:
  # Clean up. Calling this after txn.commit() is a no-op
  # and hence safe.
  txn.discard()
```

### Running a query

You can run a query by calling `Txn#query(string)`. You need to pass in a
[DQL](/dgraph/dql/query) query string. If you want to pass an additional
dictionary of any variables that you might want to set in the query, call
`Txn#query(string, variables=d)` with the variables dictionary `d`.

The query response contains the `json` field, which returns the JSON response.

Let’s run a query with a variable `$a`, deserialize the result from JSON and
print it out:

```python
# Run query.
query = """query all($a: string) {
  all(func: eq(name, $a))
  {
    name
  }
}"""
variables = {'$a': 'Alice'}

res = txn.query(query, variables=variables)

# If not doing a mutation in the same transaction, simply use:
# res = client.txn(read_only=True).query(query, variables=variables)

ppl = json.loads(res.json)

# Print results.
print('Number of people named "Alice": {}'.format(len(ppl['all'])))
for person in ppl['all']:
  print(person)
```

This should print:

```console
Number of people named "Alice": 1
Alice
```

You can also use `txn.do_request` function to run the query.

```python
request = txn.create_request(query=query)
txn.do_request(request)
```

### Running an upsert: query + mutation

The `txn.do_request` function allows you to use upsert blocks. An upsert block
contains one query block and one or more mutation blocks, so it lets you perform
queries and mutations in a single request. Variables defined in the query block
can be used in the mutation blocks using the `uid` and `val` functions
implemented by DQL.

To learn more about upsert blocks, see the
[Upsert Block documentation](/dgraph/dql/mutation#upsert-block).

```python
query = """{
  u as var(func: eq(name, "Alice"))
}"""
nquad = """
  uid(u) <name> "Alice" .
  uid(u) <age> "25" .
"""
mutation = txn.create_mutation(set_nquads=nquad)
request = txn.create_request(query=query, mutations=[mutation], commit_now=True)
txn.do_request(request)
```

### Running a conditional upsert

The upsert block also allows specifying a conditional mutation block using an
`@if` directive. The mutation is executed only when the specified condition is
true. If the condition is false, the mutation is silently ignored.

See more about Conditional Upserts
[here](/dgraph/dql/mutation#conditional-upsert).

```python
query = """
  {
    user as var(func: eq(email, "wrong_email@dgraph.io"))
  }
"""
cond = "@if(eq(len(user), 1))"
nquads = """
  uid(user) <email> "correct_email@dgraph.io" .
"""
mutation = txn.create_mutation(cond=cond, set_nquads=nquads)
request = txn.create_request(mutations=[mutation], query=query, commit_now=True)
txn.do_request(request)
```

### Cleaning up resources

To clean up resources, you have to call `DgraphClientStub#close()` individually
for all the instances of `DgraphClientStub`.

```python
SERVER_ADDR = "localhost:9080"

# Create instances of DgraphClientStub.
stub1 = pydgraph.DgraphClientStub(SERVER_ADDR)
stub2 = pydgraph.DgraphClientStub(SERVER_ADDR)

# Create an instance of DgraphClient.
client = pydgraph.DgraphClient(stub1, stub2)

# ...
# Use client
# ...

# Clean up resources by closing all client stubs.
stub1.close()
stub2.close()
```

### Setting metadata headers

Metadata headers such as authentication tokens can be set through the metadata
of gRPC methods. Below is an example of how to set a header named `auth-token`.

```python
# The following piece of code shows how one can set metadata with
# auth-token, to allow Alter operation, if the server requires it.
# metadata is a list of arbitrary key-value pairs.
metadata = [("auth-token", "the-auth-token-value")]
dg.alter(op, metadata=metadata)
```

### Setting a timeout

A timeout value representing the number of seconds can be passed to the `login`,
`alter`, `query`, and `mutate` methods using the `timeout` keyword argument.

For example, the following alters the schema with a timeout of ten seconds:
`dg.alter(op, timeout=10)`

### Passing credentials

A `CallCredentials` object can be passed to the `login`, `alter`, `query`, and
`mutate` methods using the `credentials` keyword argument.

### Authenticating to a reverse TLS proxy

If the Dgraph instance is behind a reverse TLS proxy, credentials can also be
passed through the methods available in the gRPC library. Note that in this case
every request needs to include the credentials. In the example below, we're
trying to add authentication to a proxy that requires an API key. This value is
expected to be included in the metadata using the key `authorization`.

```python
creds = grpc.ssl_channel_credentials()
call_credentials = grpc.metadata_call_credentials(
    lambda context, callback: callback((("authorization", "<api-key>"),), None))
composite_credentials = grpc.composite_channel_credentials(creds, call_credentials)
client_stub = pydgraph.DgraphClientStub(
    '{host}:{port}'.format(host=GRPC_HOST, port=GRPC_PORT), composite_credentials)
client = pydgraph.DgraphClient(client_stub)
```

### Async methods

The `alter` method in the client has an asynchronous version called
`async_alter`. The async methods return a future. You can directly call the
`result` method on the future. However. The DgraphClient class provides a static
method `handle_alter_future` to handle any possible exception.

```python
alter_future = self.client.async_alter(pydgraph.Operation(
  schema="name: string @index(term) ."))
response = pydgraph.DgraphClient.handle_alter_future(alter_future)
```

The `query` and `mutate` methods int the `Txn` class also have async versions
called `async_query` and `async_mutation` respectively. These functions work
just like `async_alter`.

You can use the `handle_query_future` and `handle_mutate_future` static methods
in the `Txn` class to retrieve the result. A short example is given below:

```python
txn = client.txn()
query = "query body here"
future = txn.async_query()
response = pydgraph.Txn.handle_query_future(future)
```

A working example can be found in the `test_asycn.py` test file.

Keep in mind that due to the nature of async calls, the async functions cannot
retry the request if the login is invalid. You will have to check for this error
and retry the login (with the function `retry_login` in both the `Txn` and
`Client` classes). A short example is given below:

```python
client = DgraphClient(client_stubs) # client_stubs is a list of gRPC stubs.
alter_future = client.async_alter()
try:
    response = alter_future.result()
except Exception as e:
  # You can use this function in the util package to check for JWT
    # expired errors.
    if pydgraph.util.is_jwt_expired(e):
        # retry your request here.
```


# Unofficial Dgraph Clients
Source: https://docs.hypermode.com/dgraph/sdks/unofficial-clients



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

<Note>
  These third-party clients are contributed by the community and are not
  officially supported by Dgraph.
</Note>

## Apache Spark Connector

* [https://github.com/G-Research/spark-dgraph-connector](https://github.com/G-Research/spark-dgraph-connector)

## Dart

* [https://github.com/marceloneppel/dgraph](https://github.com/marceloneppel/dgraph)

## Elixir

* [https://github.com/liveforeverx/dlex](https://github.com/liveforeverx/dlex)
* [https://github.com/ospaarmann/exdgraph](https://github.com/ospaarmann/exdgraph)

## Rust

* [https://github.com/Swoorup/dgraph-rs](https://github.com/Swoorup/dgraph-rs)
* [https://github.com/selmeci/dgraph-tonic](https://github.com/selmeci/dgraph-tonic)

## C\#

* [https://github.com/schivei/dgraph4net](https://github.com/schivei/dgraph4net) - DQL Client with migration management


# Cluster Checklist
Source: https://docs.hypermode.com/dgraph/self-managed/cluster-checklist



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

In setting up a cluster be sure the check the following.

* Is at least one Dgraph Zero node running?
* Is each Dgraph Alpha instance in the cluster set up correctly?
* Will each Dgraph Alpha instance be accessible to all peers on 7080 (+ any port
  offset)?
* Does each instance have a unique ID on startup?
* Has `--bindall=true` been set for networked communication?

See the [Production Checklist](./production-checklist) docs for more info.


# Cluster Setup
Source: https://docs.hypermode.com/dgraph/self-managed/cluster-setup



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## Understanding Dgraph cluster

Dgraph is a truly distributed graph database. It shards by predicate and
replicates predicates across the cluster, queries can be run on any node and
joins are handled over the distributed data. A query is resolved locally for
predicates the node stores, and using distributed joins for predicates stored on
other nodes.

To effectively running a Dgraph cluster, it's important to understand how
sharding, replication and rebalancing works.

### Sharding

Dgraph colocates data per predicate (\_ P \_, in RDF terminology), thus the
smallest unit of data is one predicate. To shard the graph, one or many
predicates are assigned to a group. Each Alpha node in the cluster serves a
single group. Dgraph Zero assigns a group to each Alpha node.

### Shard rebalancing

Dgraph Zero tries to rebalance the cluster based on the disk usage in each
group. If Zero detects an imbalance, it will try to move a predicate along with
its indices to a group that has lower disk usage. This can make the predicate
temporarily read-only. Queries for the predicate will still be serviced, but any
mutations for the predicate will be rejected and should be retried after the
move is finished.

Zero would continuously try to keep the amount of data on each server even,
typically running this check on a 10-min frequency. Thus, each additional Dgraph
Alpha instance would allow Zero to further split the predicates from groups and
move them to the new node.

### Consistent Replication

When starting Zero nodes, you can pass, to each one, the `--replicas` flag to
assign the same group to multiple nodes. The number passed to the `--replicas`
flag causes that Zero node to assign the same group to the specified number of
nodes. These nodes will then form a Raft group (or quorum), and every write will
be consistently replicated to the quorum.

To achieve consensus, it's important that the size of quorum be an odd number.
Therefore, we recommend setting `--replicas` to 1, 3 or 5 (not 2 or 4). This
allows 0, 1, or 2 nodes serving the same group to be down, respectively, without
affecting the overall health of that group.


# Cluster Types
Source: https://docs.hypermode.com/dgraph/self-managed/cluster-types



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

### Terminology

An N-node cluster is a Dgraph cluster that contains N number of Dgraph
instances. For example, a 6-node cluster means six Dgraph instances. The
replication setting specifies the number of Dgraph Alpha replicas that are in
each group. If this is higher than 1, each Alpha in a group holds a full copy of
that group's data. The replication setting is a configuration flag
(`--replicas`) on Dgraph Zero. Sharding is done (typically for databases near 1
TB in size) by creating multiple Dgraph Alpha groups. Every Dgraph Alpha group
is automatically assigned a set of distinct predicates to store and serve, thus
dividing up the data.

Examples of different cluster settings:

* No sharding
  * 2-node cluster: 1 Zero, 1 Alpha (one group).
  * HA equivalent: x3 = 6-node cluster.
* With 2-way sharding:
  * 3-node cluster: 1 Zero, 2 Alphas (two groups).
  * HA equivalent: x3 = 9-node cluster.

In the following examples we outline the two most common cluster configurations:
a 2-node cluster and a 6-node cluster.

### Basic setup: 2-node cluster

We provide sample configuration for both
[Docker Compose](https://github.com/hypermodeinc/dgraph/blob/master/contrib/config/docker/docker-compose.yml)
and
[Kubernetes](https://github.com/hypermodeinc/dgraph/tree/master/contrib/config/kubernetes/dgraph-single)
for a 2-node cluster. You can also run Dgraph directly on your host machines.

![2-node cluster](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/self-managed/deploy-guide-1.png)

Configuration can be set either as command-line flags, environment variables, or
in a file (see [Configuration](./config)).

Dgraph Zero:

* The `--my` flag should be set to the address:port (the internal-gRPC port)
  that's accessible to the Dgraph Alpha (default: `localhost:5080`).
* The `--raft` superflag's `idx` option should be set to a unique Raft ID within
  the Dgraph Zero group (default: `1`).
* The `--wal` flag should be set to the directory path to store write-ahead-log
  entries on disk (default: `zw`).
* The `--bindall` flag should be set to true for machine-to-machine
  communication (default: `true`).
* Recommended: For better issue diagnostics, set the log level verbosity to 2
  with the option `--v=2`.

Dgraph Alpha:

* The `--my` flag should be set to the address:port (the internal-gRPC port)
  that's accessible to the Dgraph Zero (default: `localhost:7080`).
* The `--zero` flag should be set to the corresponding Zero address set for
  Dgraph Zero's `--my` flag.
* The `--postings` flag should be set to the directory path for data storage
  (default: `p`).
* The `--wal` flag should be set to the directory path for write-ahead-log
  entries (default: `w`)
* The `--bindall` flag should be set to true for machine-to-machine
  communication (default: `true`).
* Recommended: For better issue diagnostics, set the log level verbosity to 2
  `--v=2`.

### High availability setup: 6-node cluster

We provide sample configuration for both
[Docker Compose](https://github.com/hypermodeinc/dgraph/blob/master/contrib/config/docker/docker-compose-ha.yml)
and
[Kubernetes](https://github.com/hypermodeinc/dgraph/tree/master/contrib/config/kubernetes/dgraph-ha)
for a 6-node cluster with 3 Alpha replicas per group. You can also run Dgraph
directly on your host machines.

A Dgraph cluster can be configured in a high-availability setup with Dgraph Zero
and Dgraph Alpha each set up with peers. These peers are part of Raft consensus
groups, which elect a single leader among themselves. The non-leader peers are
called followers. In the event that the peers can't communicate with the leader
(for example, a network partition or a machine shuts down), the group
automatically elects a new leader to continue.

Configuration can be set either as command-line flags, environment variables, or
in a file (see [Configuration](./config)).

In this setup, we assume the following hostnames are set:

* `zero1`
* `zero2`
* `zero3`
* `alpha1`
* `alpha2`
* `alpha3`

We configure the cluster with 3 Alpha replicas per group. The cluster
group-membership topology looks like the following:

![Dgraph cluster image](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/dgraph/self-managed/deploy-guide-2.png)

#### Set up Dgraph Zero group

In the Dgraph Zero group you must set unique Raft IDs (`--raft` superflag's
`idx` option) per Dgraph Zero. Dgraph will not auto-assign Raft IDs to Dgraph
Zero instances.

The first Dgraph Zero that starts will initiate the database cluster. Any
following Dgraph Zero instances must connect to the cluster via the `--peer`
flag to join. If the `--peer` flag is omitted from the peers, then the Dgraph
Zero will create its own independent Dgraph cluster.

**First Dgraph Zero** example:
`dgraph zero --replicas=3 --raft idx=1 --my=zero1:5080`

The `--my` flag must be set to the address:port of this instance that peers will
connect to. The `--raft` superflag's `idx` option sets its Raft ID to `1`.

**Second Dgraph Zero** example:
`dgraph zero --replicas=3 --raft idx=2 --my=zero2:5080 --peer=zero1:5080`

The `--my` flag must be set to the address:port of this instance that peers will
connect to. The `--raft` superflag's `idx` option sets its Raft ID to 2, and the
`--peer` flag specifies a request to connect to the Dgraph cluster of zero1
instead of initializing a new one.

**Third Dgraph Zero** example:
`dgraph zero --replicas=3 --raft idx=3 --my=zero3:5080 --peer=zero1:5080`:

The `--my` flag must be set to the address:port of this instance that peers will
connect to. The `--raft` superflag's `idx` option sets its Raft ID to 3, and the
`--peer` flag specifies a request to connect to the Dgraph cluster of zero1
instead of initializing a new one.

Dgraph Zero configuration options:

* The `--my` flag should be set to the address:port (the internal-gRPC port)
  that will be accessible to Dgraph Alpha (default: `localhost:5080`).
* The `--raft` superflag's `idx` option should be set to a unique Raft ID within
  the Dgraph Zero group (default: `1`).
* The `--wal` flag should be set to the directory path to store write-ahead-log
  entries on disk (default: `zw`).
* The `--bindall` flag should be set to true for machine-to-machine
  communication (default: `true`).
* Recommended: For more informative log info, set the log level verbosity to 2
  with the option `--v=2`.

#### Set up Dgraph Alpha group

The number of replica members per Alpha group depends on the setting of Dgraph
Zero's `--replicas` flag. Above, it's set to 3. So when Dgraph Alphas join the
cluster, Dgraph Zero will assign it to an Alpha group to fill in its members up
to the limit per group set by the `--replicas` flag.

First Alpha example: `dgraph alpha --my=alpha1:7080 --zero=zero1:5080`

Second Alpha example: `dgraph alpha --my=alpha2:7080 --zero=zero1:5080`

Third Alpha example: `dgraph alpha --my=alpha3:7080 --zero=zero1:5080`

Dgraph Alpha configuration options:

* The `--my` flag should be set to the address:port (the internal-gRPC port)
  that will be accessible to the Dgraph Zero (default: `localhost:7080`).
* The `--zero` flag should be set to the corresponding Zero address set for
  Dgraph Zero's `--my`flag.
* The `--postings` flag should be set to the directory path for data storage
  (default: `p`).
* The `--wal` flag should be set to the directory path for write-ahead-log
  entries (default: `w`)
* The `--bindall` flag should be set to true for machine-to-machine
  communication (default: `true`).
* Recommended: For more informative log info, set the log level verbosity to 2
  `--v=2`.


# Config
Source: https://docs.hypermode.com/dgraph/self-managed/config



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

You can see the list of available subcommands with `dgraph --help`. You can view
the full set of configuration options for a given subcommand with
`dgraph <subcommand> --help` (for example, `dgraph zero --help`).

You can configure options in multiple ways, which are listed below from highest
precedence to lowest precedence:

* Using command line flags (as described in the help output).
* Using environment variables.
* Using a configuration file.

If no configuration for an option is used, then the default value as described
in the `--help` output applies.

You can use multiple configuration methods at the same time, so a core set of
options could be set in a config file, and instance specific options could be
set using environment vars or flags.

## Command line flags

Dgraph has *global flags* that apply to all subcommands and flags specific to a
subcommand.

Some flags have been deprecated and replaced in release `v21.03`, and flags for
several commands (`alpha`, `backup`, `bulk`,`debug`, `live`, and `zero`) now
have superflags. Superflags are compound flags that contain one or more options
that let you define multiple settings in a semicolon-delimited list. The general
syntax for superflags is as follows:
`--<flagname> option-a=value-a; option-b=value-b`.

The following example shows how to use superflags when running the
`dgraph alpha` command.

```sh
dgraph alpha --my=alpha.example.com:7080 --zero=zero.example.com:5080 \
  --badger "compression=zstd:1" \
  --block_rate "10" \
  --trace "jaeger=http://jaeger:14268" \
  --tls "ca-cert=/dgraph/tls/ca.crt;client-auth-type=REQUIREANDVERIFY;server-cert=/dgraph/tls/node.crt;server-key=/dgraph/tls/node.key;use-system-ca=true;internal-port=true;client-cert=/dgraph/tls/client.dgraphuser.crt;client-key=/dgraph/tls/client.dgraphuser.key"
  --security "whitelist=10.0.0.0/8,172.0.0.0/8,192.168.0.0/16"
```

## Environment variables

The environment variable names for Dgraph mirror the flag names shown in the
Dgraph CLI `--help` output. These environment variable names are formed the
concatenation of `DGRAPH`, the subcommand invoked (`ALPHA`, `ZERO`, `LIVE`, or
`BULK`), and then the name of the flag (in uppercase). For example, instead
running a command like `dgraph alpha --block_rate 10`, you could set the
following environment variable: `DGRAPH_ALPHA_BLOCK_RATE=10 dgraph alpha`.

So, the environment variable syntax for a superflag
(`--<superflag-name> option-a=value; option-b=value`) is
`<SUPERFLAG-NAME>="option-a=value;option-b=value"`.

The following is an example of environment variables for `dgraph alpha`:

```sh
DGRAPH_ALPHA_BADGER="compression=zstd:1"
DGRAPH_ALPHA_BLOCK_RATE="10"
DGRAPH_ALPHA_TRACE="jaeger=http://jaeger:14268"
DGRAPH_ALPHA_TLS="ca-cert=/dgraph/tls/ca.crt;client-auth-type=REQUIREANDVERIFY;server-cert=/dgraph/tls/node.crt;server-key=/dgraph/tls/node.key;use-system-ca=true;internal-port=true;client-cert=/dgraph/tls/client.dgraphuser.crt;client-key=/dgraph/tls/client.dgraphuser.key"
DGRAPH_ALPHA_SECURITY="whitelist=10.0.0.0/8,172.0.0.0/8,192.168.0.0/16"
```

## Configuration file

You can specify a configuration file using the Dgraph CLI with the `--config`
flag (for example, `dgraph alpha --config my_config.json`), or using an
environment variable, (for example,
`DGRAPH_ALPHA_CONFIG=my_config.json dgraph alpha`).

Dgraph supports configuration file formats that it detects based on file
extensions ([`.json`](https://www.json.org/json-en.html),
[`.yml`](https://yaml.org/) or [`.yaml`](https://yaml.org/)). In these files,
the name of the superflag is used as a key that points to a hash. The hash
consists of `key: value` pairs that correspond to the superflag's list of
`option=value` pairs.

<Note>
  The formats [`.toml`](https://toml.io/en/),
  [`.hcl`](https://github.com/hashicorp/hcl), and
  [`.properties`](https://en.wikipedia.org/wiki/.properties) are not supported
  in release `v21.03.0`.
</Note>

<Tip>
  When representing the superflag options in the hash, you can use either
  *kebab-case* or *snake\_case* for names of the keys.
</Tip>

### JSON config file

In JSON, you can represent a superflag and its options
(`--<superflag-name> option-a=value;option-b=value`) as follows:

```json
{
  "<superflag-name>": {
    "option-a": "value",
    "option-b": "value"
  }
}
```

The following example JSON config file (`config.json`) using *kebab-case*:

```json
{
  "badger": { "compression": "zstd:1" },
  "trace": { "jaeger": "http://jaeger:14268" },
  "security": { "whitelist": "10.0.0.0/8,172.0.0.0/8,192.168.0.0/16" },
  "tls": {
    "ca-cert": "/dgraph/tls/ca.crt",
    "client-auth-type": "REQUIREANDVERIFY",
    "server-cert": "/dgraph/tls/node.crt",
    "server-key": "/dgraph/tls/node.key",
    "use-system-ca": true,
    "internal-port": true,
    "client-cert": "/dgraph/tls/client.dgraphuser.crt",
    "client-key": "/dgraph/tls/client.dgraphuser.key"
  }
}
```

The following example JSON config file (`config.json`) using *snake\_case*:

```json
{
  "badger": { "compression": "zstd:1" },
  "trace": { "jaeger": "http://jaeger:14268" },
  "security": { "whitelist": "10.0.0.0/8,172.0.0.0/8,192.168.0.0/16" },
  "tls": {
    "ca_cert": "/dgraph/tls/ca.crt",
    "client_auth_type": "REQUIREANDVERIFY",
    "server_cert": "/dgraph/tls/node.crt",
    "server_key": "/dgraph/tls/node.key",
    "use_system_ca": true,
    "internal_port": true,
    "client_cert": "/dgraph/tls/client.dgraphuser.crt",
    "client_key": "/dgraph/tls/client.dgraphuser.key"
  }
}
```

### YAML config file

In YAML, you can represent a superflag and its options
(`--<superflag-name> option-a=value;option-b=value`) as follows:

```yaml
<superflag-name>:
  option-a: value
  option-b: value
```

The following example YAML config file (`config.yml`) uses *kebab-case*:

```yaml
badger:
  compression: zstd:1
trace:
  jaeger: http://jaeger:14268
security:
  whitelist: 10.0.0.0/8,172.0.0.0/8,192.168.0.0/16
tls:
  ca-cert: /dgraph/tls/ca.crt
  client-auth-type: REQUIREANDVERIFY
  server-cert: /dgraph/tls/node.crt
  server-key: /dgraph/tls/node.key
  use-system-ca: true
  internal-port: true
  client-cert: /dgraph/tls/client.dgraphuser.crt
  client-key: /dgraph/tls/client.dgraphuser.key
```

The following example YAML config file (`config.yml`) uses *snake\_case*:

```yaml
badger:
  compression: zstd:1
trace:
  jaeger: http://jaeger:14268
security:
  whitelist: 10.0.0.0/8,172.0.0.0/8,192.168.0.0/16
tls:
  ca_cert: /dgraph/tls/ca.crt
  client_auth_type: REQUIREANDVERIFY
  server_cert: /dgraph/tls/node.crt
  server_key: /dgraph/tls/node.key
  use_system_ca: true
  internal_port: true
  client_cert: /dgraph/tls/client.dgraphuser.crt
  client_key: /dgraph/tls/client.dgraphuser.key
```


# Data Compression
Source: https://docs.hypermode.com/dgraph/self-managed/data-compression



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph Alpha lets you configure the compression of data on disk using the
`--badger` superflag's `compression` option. You can choose between the
[Snappy](https://github.com/golang/snappy) and
[Zstandard](https://github.com/facebook/zstd) compression algorithms, or choose
not to compress data on disk.

<Note>
  This option replaces the `--badger.compression_level` and
  `--badger.compression` options used in earlier Dgraph versions.
</Note>

The following disk compression settings are available:

| Setting      | Notes                                                                |
| ------------ | -------------------------------------------------------------------- |
| `none`       | Data on disk will not be compressed.                                 |
| `zstd:level` | Use Zstandard compression, with a compression level specified (1-3). |
| `snappy`     | Use Snappy compression (this is the default value).                  |

For example, you could choose to use Zstandard compression with the highest
compression level using the following command:

```sh
dgraph alpha --badger compression=zstd:3
```

This compression setting (Zstandard, level 3) is more CPU-intensive than other
options, but offers the highest compression ratio. To change back to the default
compression setting, use the following command:

```sh
dgraph alpha --badger compression=snappy
```

Using this compression setting (Snappy) provides a good compromise between the
need for a high compression ratio and efficient CPU usage.


# Data Decryption
Source: https://docs.hypermode.com/dgraph/self-managed/decrypt



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

You might need to decrypt data from an encrypted Dgraph cluster for a variety of
reasons, including:

* Migration of data from an encrypted cluster to a non-encrypted cluster
* Changing your data or schema by directly editing an RDF file or schema file

To support these scenarios, Dgraph includes a `decrypt` command that decrypts
encrypted RDF and schema files. To learn how to export RDF and schema files from
Dgraph, see [export database](/dgraph/admin/export).

The `decrypt` command supports a variety of symmetric key lengths, which
determine the AES cypher used for encryption and decryption, as follows:

| Symmetric key length | AES encryption cypher |
| -------------------- | --------------------- |
| 128 bits (16-bytes)  | AES-128               |
| 192 bits (24-bytes)  | AES-192               |
| 256 bits (32-bytes)  | AES-256               |

The `decrypt` command also supports the use of
[HashiCorp Vault](https://www.vaultproject.io/) to store secrets, including
support for Vault's
[AppRole authentication](https://developer.hashicorp.com/vault/docs/auth/approle).

## Decryption options

The following decryption options (or *flags*) are available for the `decrypt`
command:

| Flag or Superflag | Superflag Option | Notes                                                                                                     |
| ----------------- | ---------------- | --------------------------------------------------------------------------------------------------------- |
| `--encryption`    | `key-file`       | Encryption key filename                                                                                   |
| `-f`, `--file`    |                  | Path to file for the encrypted RDF or schema **.gz** file                                                 |
| `-h`, `--help`    |                  | Help for the decrypt command                                                                              |
| `-o`, `--out`     |                  | Path to file for the decrypted **.gz** file that decrypt creates                                          |
| `--vault`         | `addr`           | Vault server address, in **http\://\<*ip-address*>:\<*port*>** format (default: `http://localhost:8200` ) |
|                   | `enc-field`      | Name of the Vault server's key/value store field that holds the Base64 encryption key                     |
|                   | `enc-format`     | Vault server field format; can be `raw` or `base64` (default: `base64`)                                   |
|                   | `path`           | Vault server key/value store path (default: `secret/data/dgraph`)                                         |
|                   | `role-id-file`   | File containing the [Vault](https://www.vaultproject.io/) `role_id` used for AppRole authentication       |
|                   | `secret-id-file` | File containing the [Vault](https://www.vaultproject.io/) `secret_id` used for AppRole authentication     |

To learn more about the `--vault` superflag and its options that have replaced
the `--vault_*` options in release v20.11 and earlier, see
[Dgraph CLI Command Reference](/dgraph/cli/command-reference).

## Data decryption examples

For example, you could use the following command with an encrypted RDF file
(**encrypted.rdf.gz**) and an encryption key file (**enc\_key\_file**), to create
a decrypted RDF file:

```sh
# Encryption Key from the file path
dgraph decrypt --file "encrypted.rdf.gz" --out "decrypted_rdf.gz" --encryption key-file="enc-key-file"

# Encryption Key from HashiCorp Vault
dgraph decrypt --file "encrypted.rdf.gz" --out "decrypted_rdf.gz" \
  --vault addr="http://localhost:8200";enc-field="enc_key";enc-format="raw";path="secret/data/dgraph/alpha";role-id-file="./role_id";secret-id-file="./secret_id"
```

You can use similar syntax to create a decrypted schema file:

```sh
# Encryption Key from the file path
dgraph decrypt --file "encrypted.schema.gz" --out "decrypted_schema.gz" --encryption key-file="enc-key-file"

# Encryption Key from HashiCorp Vault
dgraph decrypt --file "encrypted.schema.gz" --out "decrypted_schema.gz" \
  --vault addr="http://localhost:8200";enc-field="enc_key";enc-format="raw";path="secret/data/dgraph/alpha";role-id-file="./role_id";secret-id-file="./secret_id"
```


# Dgraph Alpha
Source: https://docs.hypermode.com/dgraph/self-managed/dgraph-alpha



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph Alpha provides several HTTP endpoints for administrators, as follows:

* `/health?all` returns information about the health of all the servers in the
  cluster.
* `/admin/shutdown` initiates a proper
  [shutdown](./overview#shutting-down-database) of the Alpha.

By default the Alpha listens on `localhost` for admin actions (the loopback
address only accessible from the same machine). The `--bindall=true` option
binds to `0.0.0.0` and thus allows external connections.

<Tip>
  Set max file descriptors to a high value like 10000 if you are going to load a
  lot of data.
</Tip>

## Querying Health

You can query the `/admin` graphql endpoint with a query like the one below to
get a JSON consisting of basic information about health of all the servers in
the cluster.

```graphql
query {
  health {
    instance
    address
    version
    status
    lastEcho
    group
    uptime
    ongoing
    indexing
  }
}
```

Here’s an example of JSON returned from the above query:

```json
{
  "data": {
    "health": [
      {
        "instance": "zero",
        "address": "localhost:5080",
        "version": "v2.0.0-rc1",
        "status": "healthy",
        "lastEcho": 1582827418,
        "group": "0",
        "uptime": 1504
      },
      {
        "instance": "alpha",
        "address": "localhost:7080",
        "version": "v2.0.0-rc1",
        "status": "healthy",
        "lastEcho": 1582827418,
        "group": "1",
        "uptime": 1505,
        "ongoing": ["opIndexing"],
        "indexing": ["name", "age"]
      }
    ]
  }
}
```

* `instance`: Name of the instance. Either `alpha` or `zero`.
* `status`: Health status of the instance. Either `healthy` or `unhealthy`.
* `version`: Version of Dgraph running the Alpha or Zero server.
* `uptime`: Time in nanoseconds since the Alpha or Zero server is up and
  running.
* `address`: IP\_ADDRESS:PORT of the instance.
* `group`: Group assigned based on the replication factor. Read more
  [here](./cluster-setup).
* `lastEcho`: Last time, in Unix epoch, when the instance was contacted by
  another Alpha or Zero server.
* `ongoing`: List of ongoing operations in the background.
* `indexing`: List of predicates for which indexes are built in the background.
  Read more [here](/dgraph/dql/schema#indexes-in-background).

The same information (except `ongoing` and `indexing`) is available from the
`/health` and `/health?all` endpoints of Alpha server.


# Dgraph Zero
Source: https://docs.hypermode.com/dgraph/self-managed/dgraph-zero



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph Zero controls the Dgraph cluster, and stores information about it. It
automatically moves data between different Dgraph Alpha instances based on the
size of the data served by each Alpha instance.

Before you can run `dgraph alpha`, you must run at least one `dgraph zero` node.
You can see the options available for `dgraph zero` by using the following
command:

```sh
dgraph zero --help
```

The `--replicas` option controls the replication factor: the number of replicas
per data shard, including the original shard. For consensus, the replication
factor must be set to an odd number, and the following error occurs if it's set
to an even number (for example, `2`):

```nix
ERROR: Number of replicas must be odd for consensus. Found: 2
```

When a new Alpha joins the cluster, it's assigned to a group based on the
replication factor. If the replication factor is set to `1`, then each Alpha
node serves a different group. If the replication factor is set to `3` and you
then launch six Alpha nodes, the first three Alpha nodes serve group 1 and next
three nodes serve group 2. Zero monitors the space occupied by predicates in
each group and moves predicates between groups as-needed to re-balance the
cluster.

## Endpoints

Like Alpha, Zero also exposes HTTP on port 6080 (plus any ports specified by
`--port_offset`). You can query this port using a **GET** request to access the
following endpoints:

* `/state` returns information about the nodes that are part of the cluster.
  This includes information about the size of predicates and which groups they
  belong to.
* `/assign?what=uids&num=100` allocates a range of UIDs specified by the `num`
  argument, and returns a JSON map containing the `startId` and `endId` that
  defines the range of UIDs (inclusive). This UID range can be safely assigned
  externally to new nodes during data ingestion.
* `/assign?what=timestamps&num=100` requests timestamps from Zero. This is
  useful to "fast forward" the state of the Zero node when starting from a
  postings directory that already has commits higher than Zero's leased
  timestamp.
* `/removeNode?id=3&group=2` removes a dead Zero or Alpha node. When a replica
  node goes offline and can't be recovered, you can remove it and add a new node
  to the quorum. To remove dead Zero nodes, pass `group=0` and the id of the
  Zero node to this endpoint.

<Note>
  Before using the API ensure that the node is down and ensure that it doesn't
  come back up ever again. Don't use the same `idx` of a node that was removed
  earlier.
</Note>

* `/moveTablet?tablet=name&group=2` Moves a tablet to a group. Zero already
  re-balances shards every 8 minutes, but this endpoint can be used to force
  move a tablet.

You can also use the following **POST** endpoint on HTTP port 6080:

* `/enterpriseLicense` applies an enterprise license to the cluster by supplying
  it as part of the body.

### More about the /state endpoint

The `/state` endpoint of Dgraph Zero returns a JSON document of the current
group membership info, which includes the following:

* Instances which are part of the cluster.
* Number of instances in Zero group and each Alpha groups.
* Current leader of each group.
* Predicates that belong to a group.
* Estimated size in bytes of each predicate.
* Enterprise license information.
* Max Leased transaction ID.
* Max Leased UID.
* CID (Cluster ID).

Here’s an example of JSON for a cluster with three Alpha nodes and three Zero
nodes returned from the `/state` endpoint:

```json
{
  "counter": "22",
  "groups": {
    "1": {
      "members": {
        "1": {
          "id": "1",
          "groupId": 1,
          "addr": "alpha2:7082",
          "leader": true,
          "amDead": false,
          "lastUpdate": "1603350485",
          "clusterInfoOnly": false,
          "forceGroupId": false
        },
        "2": {
          "id": "2",
          "groupId": 1,
          "addr": "alpha1:7080",
          "leader": false,
          "amDead": false,
          "lastUpdate": "0",
          "clusterInfoOnly": false,
          "forceGroupId": false
        },
        "3": {
          "id": "3",
          "groupId": 1,
          "addr": "alpha3:7083",
          "leader": false,
          "amDead": false,
          "lastUpdate": "0",
          "clusterInfoOnly": false,
          "forceGroupId": false
        }
      },
      "tablets": {
        "dgraph.cors": {
          "groupId": 1,
          "predicate": "dgraph.cors",
          "force": false,
          "space": "0",
          "remove": false,
          "readOnly": false,
          "moveTs": "0"
        },
        "dgraph.graphql.schema": {
          "groupId": 1,
          "predicate": "dgraph.graphql.schema",
          "force": false,
          "space": "0",
          "remove": false,
          "readOnly": false,
          "moveTs": "0"
        },
        "dgraph.graphql.schema_created_at": {
          "groupId": 1,
          "predicate": "dgraph.graphql.schema_created_at",
          "force": false,
          "space": "0",
          "remove": false,
          "readOnly": false,
          "moveTs": "0"
        },
        "dgraph.graphql.schema_history": {
          "groupId": 1,
          "predicate": "dgraph.graphql.schema_history",
          "force": false,
          "space": "0",
          "remove": false,
          "readOnly": false,
          "moveTs": "0"
        },
        "dgraph.graphql.xid": {
          "groupId": 1,
          "predicate": "dgraph.graphql.xid",
          "force": false,
          "space": "0",
          "remove": false,
          "readOnly": false,
          "moveTs": "0"
        },
        "dgraph.type": {
          "groupId": 1,
          "predicate": "dgraph.type",
          "force": false,
          "space": "0",
          "remove": false,
          "readOnly": false,
          "moveTs": "0"
        }
      },
      "snapshotTs": "22",
      "checksum": "18099480229465877561"
    }
  },
  "zeros": {
    "1": {
      "id": "1",
      "groupId": 0,
      "addr": "zero1:5080",
      "leader": true,
      "amDead": false,
      "lastUpdate": "0",
      "clusterInfoOnly": false,
      "forceGroupId": false
    },
    "2": {
      "id": "2",
      "groupId": 0,
      "addr": "zero2:5082",
      "leader": false,
      "amDead": false,
      "lastUpdate": "0",
      "clusterInfoOnly": false,
      "forceGroupId": false
    },
    "3": {
      "id": "3",
      "groupId": 0,
      "addr": "zero3:5083",
      "leader": false,
      "amDead": false,
      "lastUpdate": "0",
      "clusterInfoOnly": false,
      "forceGroupId": false
    }
  },
  "maxUID": "10000",
  "maxTxnTs": "10000",
  "maxRaftId": "3",
  "removed": [],
  "cid": "2571d268-b574-41fa-ae5e-a6f8da175d6d",
  "license": {
    "user": "",
    "maxNodes": "18446744073709551615",
    "expiryTs": "1605942487",
    "enabled": true
  }
}
```

This JSON provides information that includes the following, with node members
shown with their node name and HTTP port number:

* Group 1 members:
  * alpha2:7082, id: 1, leader
  * alpha1:7080, id: 2
  * alpha3:7083, id: 3
* Group 0 members (Dgraph Zero nodes)
  * zero1:5080, id: 1, leader
  * zero2:5082, id: 2
  * zero3:5083, id: 3
* `maxUID`
  * The current maximum lease of UIDs used for blank node UID assignment.
  * This increments in batches of 10,000 IDs. Once the maximum lease is reached,
    another 10,000 IDs are leased. In the event that the Zero leader is lost,
    the new leader starts a new lease from `maxUID`+1. Any UIDs lost between
    these leases is never used for blank-node UID assignment.
  * An admin can use the Zero endpoint HTTP GET `/assign?what=uids&num=1000` to
    reserve a range of UIDs (in this case, 1000) to use externally. Zero never
    use these UIDs for blank node UID assignment, so the user can use the range
    to assign UIDs manually to their own data sets.
* `maxTxnTs`
  * The current maximum lease of transaction timestamps used to hand out start
    timestamps and commit timestamps. This increments in batches of 10,000 IDs.
    After the max lease is reached, another 10,000 IDs are leased. If the Zero
    leader is lost, then the new leader starts a new lease from `maxTxnTs`+1 .
    Any lost transaction IDs between these leases is never used.
  * An admin can use the Zero endpoint HTTP GET
    `/assign?what=timestamps&num=1000` to increase the current transaction
    timestamp (in this case, by 1000). This is mainly useful in special-case
    scenarios; for example, using an existing `-p directory` to create a fresh
    cluster to be able to query the latest data in the DB.
* `maxRaftId`
  * The number of Zeros available to serve as a leader node. Used by the
    [Raft](/dgraph/concepts/raft) consensus algorithm.
* `CID`
  * This is a unique UUID representing the *cluster-ID* for this cluster. It is
    generated during the initial DB startup and is retained across restarts.
* Enterprise license
  * Enabled
  * `maxNodes`: unlimited
  * License expiration, shown in seconds since the Unix epoch.

<Note>
  The terms tablet, predicate, and edge are currently synonymous in this
  context. In future, Dgraph might improve data scalability to shard a predicate
  into separate tablets that can be assigned to different groups.
</Note>


# Download
Source: https://docs.hypermode.com/dgraph/self-managed/download

Download the images and source files to build and install for a production-ready Dgraph cluster

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

You can obtain Dgraph binary for the latest version as well as previous releases
using automatic install script, manual download, through Docker images or by
building the binary from the open source code.

<Tabs>
  <Tab title="Docker">
    1. Install Docker.

    2. Pull the latest Dgraph image using docker:

       ```sh
          docker pull dgraph/dgraph:latest
       ```

       To set up a [learning environment](./single-host-setup), you may pull the
       [dgraph standalone](https://hub.docker.com/r/dgraph/standalone) image :

       ```sh
          docker pull dgraph/standalone:latest
       ```

    3. Verify that the image is downloaded:

       ```sh
          docker images
       ```
  </Tab>

  <Tab title="Automatic">
    On Linux system, you can get the binary using the automatic script:

    1. Download the Dgraph installation script to install Dgraph automatically:

       ```sh
          curl https://get.dgraph.io -sSf | bash
       ```

    2. Verify that it works fine, by running: `dgraph version` For more information
       about the various installation scripts that you can use, see
       [install scripts](https://github.com/dgraph-io/Install-Dgraph).
  </Tab>

  <Tab title="Manual">
    On Linux system, you can download a tar file and install manually. Download the
    appropriate tar for your platform from
    **[Dgraph releases](https://github.com/hypermodeinc/dgraph/releases)**. After
    downloading the tar for your platform from GitHub, extract the binary to
    `/usr/local/bin` like so.

    1. Download the installation file:

       ```sh
       sudo tar -C /usr/local/bin -xzf dgraph-linux-amd64-VERSION.tar.gz
       ```

    2. Verify that it works fine, by running: `dgraph version`
  </Tab>

  <Tab title="Source">
    You can also build **Dgraph** and **Ratel** from the source code by following
    the instructions from
    [Contributing to Dgraph](https://github.com/hypermodeinc/dgraph/blob/main/CONTRIBUTING.md)
    or
    [Building and running Ratel](https://github.com/hypermodeinc/ratel/blob/main/INSTRUCTIONS.md).
  </Tab>
</Tabs>


# Highly Available Cluster Setup
Source: https://docs.hypermode.com/dgraph/self-managed/ha-cluster



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

You can run three Dgraph Alpha servers and three Dgraph Zero servers in a highly
available cluster setup. For a highly available setup, start the Dgraph Zero
server with `--replicas 3` flag, so that all data is replicated on three Alpha
servers and forms one Alpha group. You can install a highly available cluster
using:

* [dgraph-ha.yaml](https://github.com/hypermodeinc/dgraph/blob/master/contrib/config/kubernetes/dgraph-ha/dgraph-ha.yaml)
  file
* Helm charts.

### Install a highly available Dgraph cluster using YAML or Helm

<Tabs>
  <Tab title="YAML">
    #### Before you begin

    * Install the
      [Kubernetes command line tool](https://kubernetes.io/docs/tasks/tools/install-kubectl/).
    * Ensure that you have a production-ready Kubernetes cluster with at least three
      worker nodes running in a cloud provider of your choice.
    * (Optional) To run Dgraph Alpha with TLS, see
      [TLS Configuration](./tls-configuration).

    #### Installing a highly available Dgraph cluster

    1. Verify that you are able to access the nodes in the Kubernetes cluster:

       ```sh
       kubectl get nodes
       ```

       An output similar to this appears:

       ```sh
         NAME                                          STATUS   ROLES    AGE   VERSION
        <aws-ip-hostname>.<region>.compute.internal   Ready    <none>   1m   v1.15.11-eks-af3caf
        <aws-ip-hostname>.<region>.compute.internal   Ready    <none>   1m   v1.15.11-eks-af3caf
        <aws-ip-hostname>.<region>.compute.internal   Ready    <none>   1m   v1.15.11-eks-af3caf
       ```

       After your Kubernetes cluster is up, you can use
       [dgraph-ha.yaml](https://github.com/hypermodeinc/dgraph/blob/master/contrib/config/kubernetes/dgraph-ha/dgraph-ha.yaml)
       to start the cluster.

    2. Start a StatefulSet that creates Pods with `Zero`, `Alpha`, and `Ratel UI`:

       ```sh
       kubectl create --filename https://raw.githubusercontent.com/hypermodeinc/dgraph/master/contrib/config/kubernetes/dgraph-ha/dgraph-ha.yaml
       ```

       An output similar to this appears:

       ```sh
       service/dgraph-zero-public created
       service/dgraph-alpha-public created
       service/dgraph-ratel-public created
       service/dgraph-zero created
       service/dgraph-alpha created
       statefulset.apps/dgraph-zero created
       statefulset.apps/dgraph-alpha created
       deployment.apps/dgraph-ratel created
       ```

    3. Confirm that the Pods were created successfully.

       ```sh
       kubectl get pods
       ```

       An output similar to this appears:

       ```sh
        NAME                  READY   STATUS    RESTARTS   AGE
       dgraph-alpha-0        1/1     Running   0          6m24s
       dgraph-alpha-1        1/1     Running   0          5m42s
       dgraph-alpha-2        1/1     Running   0          5m2s
       dgraph-ratel-<pod-id> 1/1     Running   0          6m23s
       dgraph-zero-0         1/1     Running   0          6m24s
       dgraph-zero-1         1/1     Running   0          5m41s
       dgraph-zero-2         1/1     Running   0          5m6s
       ```

       You can check the logs for the Pod using `kubectl logs --follow <POD_NAME>`..

    4. Port forward from your local machine to the Pod:

       ```sh
          kubectl port-forward service/dgraph-alpha-public 8080:8080
          kubectl port-forward service/dgraph-ratel-public 8000:8000
       ```

    5. Go to `http://localhost:8000` to access Dgraph using the Ratel UI.

    <Note> You can also access the service on its External IP address.</Note>

    #### Deleting highly available Dgraph resources

    Delete all the resources using:

    ```sh
    kubectl delete --filename https://raw.githubusercontent.com/hypermodeinc/dgraph/master/contrib/config/kubernetes/dgraph-ha/dgraph-ha.yaml
    kubectl delete persistentvolumeclaims --selector app=dgraph-zero
    kubectl delete persistentvolumeclaims --selector app=dgraph-alpha
    ```
  </Tab>

  <Tab title="Helm">
    #### Before you begin

    * Install the
      [Kubernetes command line tool](https://kubernetes.io/docs/tasks/tools/install-kubectl/).
    * Ensure that you have a production-ready Kubernetes cluster with at least three
      worker nodes running in a cloud provider of your choice.
    * Install [Helm](https://helm.sh/docs/intro/install/).
    * (Optional) To run Dgraph Alpha with TLS, see
      [TLS Configuration](./tls-configuration).

    #### Installing a highly available Dgraph cluster using Helm

    1. Verify that you are able to access the nodes in the Kubernetes cluster:

       ```sh
       kubectl get nodes
       ```

       An output similar to this appears:

       ```sh
         NAME                                          STATUS   ROLES    AGE   VERSION
        <aws-ip-hostname>.<region>.compute.internal   Ready    <none>   1m   v1.15.11-eks-af3caf
        <aws-ip-hostname>.<region>.compute.internal   Ready    <none>   1m   v1.15.11-eks-af3caf
        <aws-ip-hostname>.<region>.compute.internal   Ready    <none>   1m   v1.15.11-eks-af3caf
       ```

       After your Kubernetes cluster is up and running, you can use of the
       [Dgraph Helm chart](https://github.com/dgraph-io/charts/) to install a highly
       available Dgraph cluster

    2. Add the Dgraph Helm repository:

       ```sh
       helm repo add dgraph https://charts.dgraph.io
       ```

    3. Install the chart with `<RELEASE-NAME>`:

       ```sh
       helm install <RELEASE-NAME> dgraph/dgraph
       ```

       You can also specify the version using:

       ```sh
         helm install <RELEASE-NAME> dgraph/dgraph --set image.tag="[version]"
       ```

       When configuring the Dgraph image tag, be careful not to use `latest` or
       `main` in a production environment. These tags may have the Dgraph version
       change, causing a mixed-version Dgraph cluster that can lead to an outage and
       potential data loss.

       An output similar to this appears:

       ```sh
       NAME: <RELEASE-NAME>
       LAST DEPLOYED: Wed Feb  1 21:26:32 2023
       NAMESPACE: default
       STATUS: deployed
       REVISION: 1
       TEST SUITE: None
       NOTES:
       1. You have just deployed Dgraph, version 'v21.12.0'.

           For further information:
           * Documentation: https://docs.hypermode.com/dgraph
           * Community and Issues: https://discuss.hypermode.com/
       2. Get the Dgraph Alpha HTTP/S endpoint by running these commands.
           export ALPHA_POD_NAME=$(kubectl get pods --namespace default --selector "statefulset.kubernetes.io/pod-name=<RELEASE-NAME>-dgraph-alpha-0,release=<RELEASE-NAME>-dgraph" --output jsonpath="{.items[0].metadata.name}")
           echo "Access Alpha HTTP/S using http://localhost:8080"
           kubectl --namespace default port-forward $ALPHA_POD_NAME 8080:8080

         NOTE: Change "http://" to "https://" if TLS was added to the Ingress, Load Balancer, or Dgraph Alpha service.
       ```

    4. Get the name of the Pods in the cluster using `kubectl get pods`:

       ```sh
           NAME                          READY   STATUS    RESTARTS   AGE
         <RELEASE-NAME>-dgraph-alpha-0   1/1     Running   0          4m48s
         <RELEASE-NAME>-dgraph-alpha-1   1/1     Running   0          4m2s
         <RELEASE-NAME>-dgraph-alpha-2   1/1     Running   0          3m31s
         <RELEASE-NAME>-dgraph-zero-0    1/1     Running   0          4m48s
         <RELEASE-NAME>-dgraph-zero-1    1/1     Running   0          4m10s
         <RELEASE-NAME>-dgraph-zero-2    1/1     Running   0          3m50s
       ```

    5. Get the Dgraph Alpha HTTP/S endpoint by running these commands:

       ```sh
           export ALPHA_POD_NAME=$(kubectl get pods --namespace default --selector "statefulset.kubernetes.io/pod-name=<RELEASE-NAME>-dgraph-alpha-0,release=<RELEASE-NAME>-dgraph" --output jsonpath="{.items[0].metadata.name}")
           echo "Access Alpha HTTP/S using http://localhost:8080"
           kubectl --namespace default port-forward $ALPHA_POD_NAME 8080:8080
       ```

    #### Deleting the resources from the cluster

    1. Delete the Helm deployment using:

       ```sh
         helm delete my-release
       ```

    2. Delete associated Persistent Volume Claims:

       ```sh
       kubectl delete pvc --selector release=my-release
       ```
  </Tab>
</Tabs>

### Dgraph configuration files

You can create a Dgraph [configuration](./config) files for Alpha server and
Zero server with Helm chart configuration values, `<MY-CONFIG-VALUES>`. For more
information about the values, see the latest
[configuration settings](https://github.com/dgraph-io/charts/blob/master/charts/dgraph/README.md#configuration).

1. Open an editor of your choice and create a configuration file named
   `<MY-CONFIG-VALUES>.yaml`:

   ```yaml
   # <MY-CONFIG-VALUES>.yaml
   alpha:
     configFile:
       config.yaml: |
         alsologtostderr: true
         badger:
           compression_level: 3
           tables: mmap
           vlog: mmap
         postings: /dgraph/data/p
         wal: /dgraph/data/w
   zero:
     configFile:
       config.yaml: |
         alsologtostderr: true
         wal: /dgraph/data/zw
   ```

2. Change to the director in which you created `<MY-CONFIG-VALUES>`.yaml and
   then install with Alpha and Zero configuration using:

   ```sh
   helm install <RELEASE-NAME> dgraph/dgraph --values <MY-CONFIG-VALUES>.yaml
   ```

### Exposing Alpha and Ratel Services

By default Zero and Alpha services are exposed only within the Kubernetes
cluster as Kubernetes service type `ClusterIP`.

In order to expose the Alpha service and Ratel service publicly you can use
Kubernetes service type `LoadBalancer` or an Ingress resource.

<Tabs>
  <Tab title="LoadBalancer">
    #### Public Internet

    To use an external load balancer, set the service type to `LoadBalancer`.

    <Note>
      For security purposes we recommend limiting access to any public endpoints,
      such as using a white list.
    </Note>

    1. To expose Alpha service to the Internet use:

       ```sh
       helm install <RELEASE-NAME> dgraph/dgraph --set alpha.service.type="LoadBalancer"
       ```

    2. To expose Alpha and Ratel services to the Internet use:

       ```sh
       helm install <RELEASE-NAME> dgraph/dgraph --set alpha.service.type="LoadBalancer" --set ratel.service.type="LoadBalancer"
       ```

    ##### Private network

    An external load balancer can be configured to face internally to a private
    subnet rather the public Internet. This way it can be accessed securely by
    clients on the same network, through a VPN, or from a jump server. In
    Kubernetes, this is often configured through service annotations by the
    provider. Here's a small list of annotations from cloud providers:

    | Provider     | Documentation Reference                                                                                        | Annotation                                                        |
    | ------------ | -------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |
    | AWS          | [Amazon EKS: Load Balancing](https://docs.aws.amazon.com/eks/latest/userguide/load-balancing.html)             | `service.beta.kubernetes.io/aws-load-balancer-internal: "true"`   |
    | Azure        | [AKS: Internal Load Balancer](https://docs.microsoft.com/azure/aks/internal-lb)                                | `service.beta.kubernetes.io/azure-load-balancer-internal: "true"` |
    | Google Cloud | [GKE: Internal Load Balancing](https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balancing) | `cloud.google.com/load-balancer-type: "Internal"`                 |

    As an example, using Amazon [EKS](https://aws.amazon.com/eks/) as the provider.

    1. Create a Helm chart configuration values file `<MY-CONFIG-VALUES>`.yaml file:

       ```yaml
       # <MY-CONFIG-VALUES>.yaml
       alpha:
         service:
           type: LoadBalancer
           annotations:
             service.beta.kubernetes.io/aws-load-balancer-internal: "true"
       ratel:
         service:
           type: LoadBalancer
           annotations:
             service.beta.kubernetes.io/aws-load-balancer-internal: "true"
       ```

    2. To expose Alpha and Ratel services privately, use:

       ```sh
       helm install <RELEASE-NAME> dgraph/dgraph --values <MY-CONFIG-VALUES>.yaml
       ```
  </Tab>

  <Tab title="Ingress Resource">
    You can expose Alpha and Ratel using an
    [ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)
    resource that can route traffic to service resources. Before using this option
    you may need to install an
    [ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)
    first, as is the case with [AKS](https://docs.microsoft.com/azure/aks/) and
    [EKS](https://aws.amazon.com/eks/), while in the case of
    [GKE](https://cloud.google.com/kubernetes-engine), this comes bundled with a
    default ingress controller. When routing traffic based on the `hostname`, you
    may want to integrate an addon like
    [ExternalDNS](https://github.com/kubernetes-sigs/external-dns) so that DNS
    records can be registered automatically when deploying Dgraph.

    As an example, you can configure a single ingress resource that uses
    [ingress-nginx](https://github.com/kubernetes/ingress-nginx) for Alpha and Ratel
    services.

    1. Create a Helm chart configuration values file, `<MY-CONFIG-VALUES>`.yaml
       file:

       ```yaml
       # <MY-CONFIG-VALUES>.yaml
       global:
         ingress:
           enabled: false
           annotations:
             kubernetes.io/ingress.class: nginx
           ratel_hostname: "ratel.<my-domain-name>"
           alpha_hostname: "alpha.<my-domain-name>"
       ```

    2. To expose Alpha and Ratel services through an ingress:

       ```sh
       helm install <RELEASE-NAME> dgraph/dgraph --values <MY-CONFIG-VALUES>.yaml
       ```

    You can run `kubectl get ingress` to see the status and access these through
    their hostname, such as `http://alpha.<my-domain-name>` and
    `http://ratel.<my-domain-name>`

    <Tip>
      Ingress controllers likely have an option to configure access for private
      internal networks. Consult documentation from the ingress controller provider
      for further information.
    </Tip>
  </Tab>
</Tabs>

### Upgrading the Helm chart

You can update your cluster configuration by updating the configuration of the
Helm chart. Dgraph is a stateful database that requires some attention on
upgrading the configuration carefully to update your cluster to your desired
configuration.

In general, you can use [`helm upgrade`][helm-upgrade] to update the
configuration values of the cluster. Depending on your change, you may need to
upgrade the configuration in multiple steps.

[helm-upgrade]: https://helm.sh/docs/helm/helm_upgrade/

To upgrade to an [HA cluster setup](./#ha-cluster-setup-using-kubernetes):

1. Ensure that the shard replication setting is more than one and
   `zero.shardReplicaCount`. For example, set the shard replica flag on the Zero
   node group to 3,`zero.shardReplicaCount=3`.

2. Run the Helm upgrade command to restart the Zero node group:

   ```sh
   helm upgrade <RELEASE-NAME> dgraph/dgraph [options]
   ```

3. Set the Alpha replica count flag. For example: `alpha.replicaCount=3`.

4. Run the Helm upgrade command again:

   ```sh
   helm upgrade <RELEASE-NAME> dgraph/dgraph [options]
   ```


# Lambda Server
Source: https://docs.hypermode.com/dgraph/self-managed/lambda-server

Setup a Dgraph database with a lambda server. Dgraph Lambda is a serverless platform for running JavaScript with Dgraph

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

In this article you'll learn how to setup a Dgraph database with a lambda
server.

## Dgraph Lambda

[Dgraph Lambda](https://github.com/dgraph-io/dgraph-lambda) is a serverless
platform for running JavaScript with Dgraph.

You can
[download the latest version](https://github.com/dgraph-io/dgraph-lambda/releases/latest)
or review the implementation in the
[repo](https://github.com/dgraph-io/dgraph-lambda).

### Running with Docker

To run a Dgraph Lambda server with Docker:

```sh
docker run -it --rm -p 8686:8686 -v /path/to/script.js:/app/script/script.js -e DGRAPH_URL=http://host.docker.internal:8080 dgraph/dgraph-lambda
```

<Note>
  `host.docker.internal` doesn't work on older versions of Docker on Linux. You
  can use `DGRAPH_URL=http://172.17.0.1:8080` instead.
</Note>

### Adding libraries

If you would like to add libraries to Dgraph Lambda, use
`webpack --target=webworker` to compile your script.

### Working with TypeScript

You can import `@slash-graphql/lambda-types` to get types for
`addGraphQLResolver` and `addGraphQLMultiParentResolver`.

## Dgraph Alpha

To set up Dgraph Alpha, you need to define the `--graphql` superflag's
`lambda-url` option, which is used to set the URL of the lambda server. All the
`@lambda` fields are resolved through the lambda functions implemented on the
given lambda server.

For example:

```sh
dgraph alpha --graphql lambda-url=http://localhost:8686/graphql-worker
```

Then test it out with the following `curl` command:

```sh
curl localhost:8686/graphql-worker -H "Content-Type: application/json" -d '{"resolver":"MyType.customField","parent":[{"customField":"Dgraph Labs"}]}'
```

### Docker settings

If you're using Docker, you need to add the `--graphql` superflag's `lambda-url`
option to your Alpha configuration. For example:

```yml
command:
  /gobin/dgraph alpha --zero=zero1:5180 -o 100 --expose_trace --trace ratio=1.0
  --profile_mode block --block_rate 10 --logtostderr -v=2 --security
  whitelist=10.0.0.0/8,172.16.0.0/12,192.168.0.0/16 --my=alpha1:7180 --graphql
  lambda-url=http://lambda:8686/graphql-worker
```

Next, you need to add the Dgraph Lambda server configuration, and map the
JavaScript file that contains the code for lambda functions to the
`/app/script/script.js` file. Remember to set the `DGRAPH_URL` environment
variable to your Alpha server.

Here's a complete Docker example that uses the base Dgraph image and adds Lambda
server support:

```yml
services:
  dgraph:
    image: dgraph/standalone:latest
    environment:
      DGRAPH_ALPHA_GRAPHQL: "lambda-url=http://dgraph_lambda:8686/graphql-worker"
    ports:
      - "8080:8080"
      - "9080:9080"
      - "8000:8000"
    volumes:
      - dgraph:/dgraph

  dgraph_lambda:
    image: dgraph/dgraph-lambda:latest

    ports:
      - "8686:8686"
    environment:
      DGRAPH_URL: http://dgraph:8080
    volumes:
      - ./gql/script.js:/app/script/script.js:ro

volumes:
  dgraph: {}
```


# Load Balancing Queries with NGINX
Source: https://docs.hypermode.com/dgraph/self-managed/load-balancing-nginx



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

There might be times when you'll want to set up a load balancer to accomplish
goals such as increasing the utilization of your database by sending queries
from the app to multiple database server replicas. You can follow these steps to
get started with that.

## Setting up NGINX load balancer using Docker Compose

### Download ZIP

Download the contents of this gist's ZIP file and extract it to a directory
called `graph-nginx`, as follows:

```sh
mkdir dgraph-nginx
cd dgraph-nginx
wget -O dgraph-nginx.zip https://gist.github.com/danielmai/0cf7647b27c7626ad8944c4245a9981e/archive/5a2f1a49ca2f77bc39981749e4783e3443eb3ad9.zip
unzip -j dgraph-nginx.zip
```

Two files will be created: `docker-compose.yml` and `nginx.conf`.

### Start Dgraph cluster

Start a 6-node Dgraph cluster (3 Dgraph Zero, 3 Dgraph Alpha, replication
setting 3) by starting the Docker Compose config:

```sh
docker-compose up
```

## Setting up NGINX load balancer with Dgraph running directly on the host machine

You can start your Dgraph cluster directly on the host machine (for example,
with systemd) as follows:

### Install NGINX using the following `apt-get` command

After you have set up your Dgraph cluster, install the latest stable NGINX. On
Debian and Ubuntu systems use the following command:

```sh
apt-get install nginx
```

### Configure NGINX as a load balancer

Make sure that your Dgraph cluster is up and running (it this case we will refer
to a 6 node cluster). After installing NGINX, you can configure it for load
balancing. You do this by specifying which types of connections to listen to,
and where to redirect them. Create a new configuration file called
`load-balancer.conf`:

```sh
sudo vim /etc/nginx/conf.d/load-balancer.conf
```

and edit it to read as follows:

```sh
upstream alpha_grpc {
  server alpha1:9080;
  server alpha2:9080;
  server alpha3:9080;
}

upstream alpha_http {
  server alpha1:8080;
  server alpha2:8080;
  server alpha3:8080;
}

# $upstream_addr is the ip:port of the Dgraph Alpha defined in the upstream
# Example: 172.25.0.2, 172.25.0.7, 172.25.0.5 are the IP addresses of alpha1, alpha2, and alpha3
# /var/log/nginx/access.log will contain these logs showing "localhost to <upstream address>"
# for the different backends. By default, NGINX load balancing is round robin.

log_format upstreamlog '[$time_local] $remote_addr - $remote_user - $server_name $host to: $upstream_addr: $request $status upstream_response_time $upstream_response_time msec $msec request_time $request_time';

server {
  listen 9080 http2;
  access_log /var/log/nginx/access.log upstreamlog;
  location / {
    grpc_pass grpc://alpha_grpc;
  }
}

server {
  listen 8080;
  access_log /var/log/nginx/access.log upstreamlog;
  location / {
    proxy_pass http://alpha_http;
  }
}
```

Next, disable the default server configuration; on Debian and Ubuntu systems
you’ll need to remove the default symbolic link from the **sites-enabled**
folder.

```sh
rm /etc/nginx/sites-enabled/default
```

Now you can restart `nginx`:

```sh
systemctl restart nginx
```

## Use the increment tool to start a gRPC LB

In a different shell, run the `dgraph increment`
([docs](/dgraph/admin/increment-tool)) tool against the NGINX gRPC load balancer
(`nginx:9080`):

```sh
docker-compose exec alpha1 dgraph increment --alpha nginx:9080 --num=10
```

If you have Dgraph installed on your host machine, then you can also run this
from the host:

```sh
dgraph increment --alpha localhost:9080 --num=10
```

The increment tool uses the Dgraph Go client to establish a gRPC connection
against the `--alpha` flag and transactionally increments a counter predicate
`--num` times.

## Check logs

In the NGINX access logs (in the `docker-compose` up shell window), or if you
are not using Docker Compose you can tail logs from `/var/log/nginx/access.log`.
You'll see access logs like the following:

<Note>
  With gRPC load balancing, each request can hit a different Alpha node. This
  can increase read throughput.
</Note>

```sh
nginx_1   | [15/Jan/2020:03:12:02 +0000] 172.20.0.9 - - -  nginx to: 172.20.0.7:9080: POST /api.Dgraph/Query HTTP/2.0 200 upstream_response_time 0.008 msec 1579057922.135 request_time 0.009
nginx_1   | [15/Jan/2020:03:12:02 +0000] 172.20.0.9 - - -  nginx to: 172.20.0.2:9080: POST /api.Dgraph/Query HTTP/2.0 200 upstream_response_time 0.012 msec 1579057922.149 request_time 0.013
nginx_1   | [15/Jan/2020:03:12:02 +0000] 172.20.0.9 - - -  nginx to: 172.20.0.5:9080: POST /api.Dgraph/Query HTTP/2.0 200 upstream_response_time 0.008 msec 1579057922.162 request_time 0.012
nginx_1   | [15/Jan/2020:03:12:02 +0000] 172.20.0.9 - - -  nginx to: 172.20.0.7:9080: POST /api.Dgraph/Query HTTP/2.0 200 upstream_response_time 0.012 msec 1579057922.176 request_time 0.013
nginx_1   | [15/Jan/2020:03:12:02 +0000] 172.20.0.9 - - -  nginx to: 172.20.0.2:9080: POST /api.Dgraph/Query HTTP/2.0 200 upstream_response_time 0.012 msec 1579057922.188 request_time 0.011
nginx_1   | [15/Jan/2020:03:12:02 +0000] 172.20.0.9 - - -  nginx to: 172.20.0.5:9080: POST /api.Dgraph/Query HTTP/2.0 200 upstream_response_time 0.016 msec 1579057922.202 request_time 0.013
```

These logs show that traffic is being load balanced to the following upstream
addresses defined in alpha\_grpc in nginx.conf:

* `nginx to: 172.20.0.7`
* `nginx to: 172.20.0.2`
* `nginx to: 172.20.0.5`

## Load balancing methods

By default, NGINX load balancing is done round-robin. By the way There are other
load-balancing methods available such as least connections or IP hashing. To use
a different method than round-robin, specify the desired load-balancing method
in the upstream section of `load-balancer.conf`.

```sh
# use least connection method
upstream alpha_grpc {
  least_conn;
  server alpha1:9080;
  server alpha2:9080;
  server alpha3:9080;
}

upstream alpha_http {
  least_conn;
  server alpha1:8080;
  server alpha2:8080;
  server alpha3:8080;
}
```


# Monitoring
Source: https://docs.hypermode.com/dgraph/self-managed/monitoring



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph exposes metrics via the `/debug/vars` endpoint in JSON format and the
`/debug/prometheus_metrics` endpoint in Prometheus' text-based format. Dgraph
doesn't store the metrics and only exposes the value of the metrics at that
instant. You can either poll this endpoint to get the data in your monitoring
systems or install
[Prometheus](https://prometheus.io/docs/introduction/install/). Replace targets
in the configuration file below with the IP address of your Dgraph instances and
run prometheus using the command `prometheus --config.file my_config.yaml`.

```sh
scrape_configs:
  - job_name: "dgraph"
    metrics_path: "/debug/prometheus_metrics"
    scrape_interval: "2s"
    static_configs:
    - targets:
      - 172.31.9.133:6080     # For Dgraph zero, 6080 is the http endpoint exposing metrics.
      - 172.31.15.230:8080    # For Dgraph alpha, 8080 is the http endpoint exposing metrics.
      - 172.31.0.170:8080
      - 172.31.8.118:8080
```

<Note>
  Raw data exported by Prometheus is available via `/debug/prometheus_metrics`
  endpoint on Dgraph alphas.
</Note>

Install [Grafana](http://docs.grafana.org/installation/) to plot the metrics.
Grafana runs at port 3000 in default settings. Create a Prometheus data source
by following these
[steps](https://prometheus.io/docs/visualization/grafana/#creating-a-prometheus-data-source).
Import
[`grafana_dashboard.json`](https://github.com/hypermodeinc/dgraph-benchmarks/blob/main/scripts/grafana_dashboard.json)
by following these
[instructions](http://docs.grafana.org/reference/export_import/#importing-a-dashboard).

## Amazon CloudWatch

Route53's health checks can be leveraged to create standard CloudWatch alarms to
notify on change in the status of the `/health` endpoints of Alpha and Zero.

Considering that the endpoints to monitor are publicly accessible and you have
the AWS credentials and [awscli](https://aws.amazon.com/cli/) setup, we’ll go
through an example of setting up a simple CloudWatch alarm configured to alert
via email for the Alpha endpoint `alpha.acme.org:8080/health`. Dgraph Zero's
`/health` endpoint can also be monitored in a similar way.

### Create the Route53 health check

```sh
aws route53 create-health-check \
    --caller-reference $(date "+%Y%m%d%H%M%S") \
    --health-check-config file:///tmp/create-healthcheck.json \
    --query 'HealthCheck.Id'
```

The file `/tmp/create-healthcheck.json` would need to have the values for the
parameters required to create the health check as such:

```sh
{
  "Type": "HTTPS",
  "ResourcePath": "/health",
  "FullyQualifiedDomainName": "alpha.acme.org",
  "Port": 8080,
  "RequestInterval": 30,
  "FailureThreshold": 3
}
```

The reference for the values one can specify while creating or updating a health
check can be found on the AWS
[documentation](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-creating-values.html).

The response to the command id the ID of the created health check.

```sh
"29bdeaaa-f5b5-417e-a5ce-7dba1k5f131b"
```

Make a note of the health check ID. This is used to integrate CloudWatch alarms
with the health check.

<Note>
  Currently, Route53 metrics are only
  [available](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/monitoring-health-checks.html)
  in the **US East (N. Virginia)** region. The CloudWatch Alarm (and the SNS
  Topic) should therefore be created in `us-east-1`.
</Note>

### \[Optional] Creating an SNS topic

SNS topics are used to create message delivery channels. If you do not have any
SNS topics configured, one can be created by running the following command:

```sh
aws sns create-topic --region=us-east-1 --name ops --query 'TopicArn'
```

The response to this command is:

```sh
"arn:aws:sns:us-east-1:123456789012:ops"
```

Be sure to make a note of the topic ARN. This is used to configure the
CloudWatch alarm's action parameter.

Run the following command to subscribe your email to the SNS topic:

```sh
aws sns subscribe \
    --topic-arn arn:aws:sns:us-east-1:123456789012:ops \
    --protocol email \
    --notification-endpoint ops@acme.org
```

The subscription needs to be confirmed through *AWS Notification - Subscription
Confirmation* sent through email. Once the subscription is confirmed, CloudWatch
can be configured to use the SNS topic to trigger the alarm notification.

### Creating a CloudWatch alarm

The following command creates a CloudWatch alarm with `--alarm-actions` set to
the ARN of the SNS topic and the `--dimensions` of the alarm set to the health
check ID.

```sh
aws cloudwatch put-metric-alarm \
    --region=us-east-1 \
    --alarm-name dgraph-alpha \
    --alarm-description "Alarm for when Alpha is down" \
    --metric-name HealthCheckStatus \
    --dimensions "Name=HealthCheckId,Value=29bdeaaa-f5b5-417e-a5ce-7dba1k5f131b" \
    --namespace AWS/Route53 \
    --statistic Minimum \
    --period 60 \
    --threshold 1 \
    --comparison-operator LessThanThreshold \
    --evaluation-periods 1 \
    --treat-missing-data breaching \
    --alarm-actions arn:aws:sns:us-east-1:123456789012:ops
```

One can verify the alarm status from the CloudWatch or Route53 consoles.

#### Internal endpoints

If the Alpha endpoint is internal to the VPC network, create a Lambda function
that periodically (triggered using CloudWatch Event Rules) requests the
`/health` path and creates CloudWatch metrics which could then be used to create
the required CloudWatch alarms. The architecture and the CloudFormation template
to achieve the same can be found
[here](https://aws.amazon.com/blogs/networking-and-content-delivery/performing-route-53-health-checks-on-private-resources-in-a-vpc-with-aws-lambda-and-amazon-cloudwatch/).


# Monitoring the Cluster
Source: https://docs.hypermode.com/dgraph/self-managed/monitoring-cluster



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

## Monitoring the Kubernetes cluster

Dgraph exposes Prometheus metrics to monitor the state of various components
involved in the cluster, including Dgraph Alpha and Zero nodes. You can setup
Prometheus monitoring for your cluster.

You can use Helm to install
[kube-prometheus-stack](https://github.com/prometheus-operator/kube-prometheus)
chart. This Helm chart is a collection of Kubernetes manifests,
[Grafana](http://grafana.com/) dashboards,
[Prometheus rules](https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/)
combined with scripts to provide monitoring with
[Prometheus](https://prometheus.io/) using the
[Prometheus Operator](https://github.com/prometheus-operator/prometheus-operator).
This Helm chart also installs [Grafana](http://grafana.com/),
[node\_exporter](https://github.com/prometheus/node_exporter),
[kube-state-metrics](https://github.com/kubernetes/kube-state-metrics).

### Before you begin

* Install the
  [Kubernetes command line tool](https://kubernetes.io/docs/tasks/tools/install-kubectl/).
* Ensure that you have a production-ready Kubernetes cluster with at least three
  worker nodes running in a cloud provider of your choice.
* Install [Helm](https://helm.sh/docs/intro/install/)

### Install using Helm chart

1. Create a `YAML` file named `dgraph-prometheus-operator.yaml` and edit the
   values as appropriate for adding endpoints, adding alert rules, adjusting
   alert manager configuration, adding Grafana dashboard, and others. For more
   information see,
   [Dgraph Helm chart values](https://github.com/hypermodeinc/dgraph/tree/master/contrib/config/monitoring/prometheus/chart-values).

   ```yaml
   prometheusOperator:
     createCustomResource: true

   grafana:
     enabled: true
     persistence:
       enabled: true
       accessModes: ["ReadWriteOnce"]
       size: 5Gi
     defaultDashboardsEnabled: true
     service:
       type: ClusterIP

   alertmanager:
     service:
       labels:
         app: dgraph-io
     alertmanagerSpec:
       storage:
         volumeClaimTemplate:
           spec:
             accessModes: ["ReadWriteOnce"]
             resources:
               requests:
                 storage: 5Gi
       replicas: 1
       logLevel: debug
     config:
       global:
         resolve_timeout: 2m
       route:
         group_by: ['job']
         group_wait: 30s
         group_interval: 5m
         repeat_interval: 12h
         receiver: 'null'
         routes:
         - match:
             alertname: Watchdog
              receiver: 'null'
       receivers:
       - name: 'null'

   prometheus:
     service:
         type: ClusterIP
     serviceAccount:
       create: true
       name: prometheus-dgraph-io

     prometheusSpec:
       storageSpec:
         volumeClaimTemplate:
           spec:
             accessModes: ["ReadWriteOnce"]
             resources:
               requests:
                 storage: 25Gi
       resources:
         requests:
           memory: 400Mi
       enableAdminAPI: false

     additionalServiceMonitors:
       - name: zero-dgraph-io
         endpoints:
           - port: http-zero
             path: /debug/prometheus_metrics
         namespaceSelector:
           any: true
         selector:
           matchLabels:
             monitor: zero-dgraph-io
       - name: alpha-dgraph-io
         endpoints:
           - port: http-alpha
             path: /debug/prometheus_metrics
         namespaceSelector:
           any: true
         selector:
           matchLabels:
             monitor: alpha-dgraph-io
   ```

2. Create a `YAML` file named `secrets.yaml` that has the credentials for
   Grafana.

   ```yaml
   grafana:
     adminPassword: <GRAFANA-PASSWORD>
   ```

3. Add the `prometheus-operator` Helm chart:

   ```sh
   helm repo add stable https://charts.helm.sh/stable
   helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
   helm repo update
   ```

4. Install
   [kube-prometheus-stack](https://github.com/prometheus-operator/kube-prometheus)
   with the `<MY-RELEASE-NAME>` in the namespace named `monitoring`:

   ```sh
   helm install <MY-RELEASE-NAME>\
     --values dgraph-prometheus-operator.yaml \
     --values secrets.yaml \
     prometheus-community/kube-prometheus-stack --namespace monitoring
   ```

   An output similar to the following appears:

   ```sh
   NAME: dgraph-prometheus-release
   LAST DEPLOYED: Sun Feb  5 21:35:45 2023
   NAMESPACE: monitoring
   STATUS: deployed
   REVISION: 1
   NOTES:
   kube-prometheus-stack has been installed. Check its status by running:
     kubectl --namespace monitoring get pods -l "release=dgraph-prometheus-release"

   Visit https://github.com/prometheus-operator/kube-prometheus instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
   ```

5. Check the list of services in the `monitoring` namespace using
   `kubectl get svc -n monitoring`:

   ```sh
   NAME                                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
   alertmanager-operated                                ClusterIP   None             <none>        9093/TCP,9094/TCP,9094/UDP   29s
   dgraph-prometheus-release-alertmanager               ClusterIP   10.128.239.240   <none>        9093/TCP                     32s
   dgraph-prometheus-release-grafana                    ClusterIP   10.128.213.70    <none>        80/TCP                       32s
   dgraph-prometheus-release-kube-state-metrics         ClusterIP   10.128.139.145   <none>        8080/TCP                     32s
   dgraph-prometheus-release-operator                   ClusterIP   10.128.6.5       <none>        443/TCP                      32s
   dgraph-prometheus-release-prometheus                 ClusterIP   10.128.255.88    <none>        9090/TCP                     32s
   dgraph-prometheus-release-prometheus-node-exporter   ClusterIP   10.128.103.131   <none>        9100/TCP                     32s
   prometheus-operated                                  ClusterIP   None             <none>        9090/TCP                     29s

   ```

6. Use
   `kubectl port-forward svc/dgraph-prometheus-release-prometheus -n monitoring 9090`
   to access Prometheus at `localhost:9090`.

7. Use `kubectl --namespace monitoring port-forward svc/grafana 3000:80` to
   access Grafana at `localhost:3000`.

8. Log in to Grafana using the password that you had set in the `secrets.yaml`
   file.

9. In the **Dashboards** menu of Grafana, select **Import**.

10. In the **Dashboards/Import dashboard** page copy the contents of the
    [`dgraph-kubernetes-grafana-dashboard.json`](https://github.com/hypermodeinc/dgraph/blob/master/contrib/config/monitoring/grafana/dgraph-kubernetes-grafana-dashboard.json)
    file in **Import via panel JSON** and click **Load**.

    You can visualize all Dgraph Alpha and Zero Kubernetes Pods, using the
    regular expression pattern `"/dgraph-.*-[0-9]*$/`. You can change this in
    the dashboard configuration and select the variable Pod. For example, if you
    have multiple releases, and only want to visualize the current release named
    `my-release-3`, change the regular expression pattern to
    `"/my-release-3.*dgraph-.*-[0-9]*$/"` in the Pod variable of the dashboard
    configuration. By default, the Prometheus that you installed is configured
    as the `Datasource` in Grafana.

## Kubernetes storage

The Kubernetes configurations in the previous sections were configured to run
Dgraph with any storage type (`storage-class: anything`). On the common cloud
environments like AWS, Google Cloud, and Azure, the default storage type are
slow disks like hard disks or low IOPS SSDs. We highly recommend using faster
disks for ideal performance when running Dgraph.

### Local storage

The AWS storage-optimized i-class instances provide locally attached NVMe-based
SSD storage which provide consistent very high IOPS. The Dgraph team uses
i3.large instances on AWS to test Dgraph.

You can create a Kubernetes `StorageClass` object to provision a specific type
of storage volume which you can then attach to your Dgraph Pods. You can set up
your cluster with local SSDs by using
[Local Persistent Volumes](https://kubernetes.io/blog/2018/04/13/local-persistent-volumes-beta/).
This Kubernetes feature is in beta at the time of this writing (Kubernetes
v1.13.1). You can first set up an EC2 instance with locally attached storage.
Once it's formatted and mounted properly, then you can create a StorageClass to
access it.:

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: <your-local-storage-class-name>
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

Currently, Kubernetes doesn't allow automatic provisioning of local storage. So
a PersistentVolume with a specific mount path should be created:

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: <your-local-pv-name>
spec:
  capacity:
    storage: 475Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: <your-local-storage-class-name>
  local:
    path: /data
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - <node-name>
```

Then, in the StatefulSet configuration you can claim this local storage in
.spec.volumeClaimTemplate:

```yaml
kind: StatefulSet
---
volumeClaimTemplates:
  - metadata:
      name: datadir
    spec:
      accessModes:
        - ReadWriteOnce
      storageClassName: <your-local-storage-class-name>
      resources:
        requests:
          storage: 500Gi
```

You can repeat these steps for each instance that's configured with local node
storage.

### Non-local persistent disks

EBS volumes on AWS and PDs on Google Cloud are persistent disks that can be
configured with Dgraph. The disk performance is much lower than locally attached
storage but can be sufficient for your workload such as testing environments.

When using EBS volumes on AWS, we recommend using Provisioned IOPS SSD EBS
volumes (the io1 disk type) which provide consistent IOPS. The available IOPS
for AWS EBS volumes is based on the total disk size. With Kubernetes, you can
request io1 disks to be provisioned with this configuration with 50 IOPS/GB
using the `iopsPerGB` parameter:

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: <your-storage-class-name>
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io1
  iopsPerGB: "50"
  fsType: ext4
```

Example: requesting a disk size of 250Gi with this storage class would provide
12.5K IOPS.

## Removing a Dgraph pod

In the event that you need to completely remove a Pod (for example, the disk
became corrupted and data can't be recovered), you can use the `/removeNode` API
to remove the node from the cluster. With a Kubernetes StatefulSet, you'll need
to remove the node in this order:

1. On the Zero leader, call `/removeNode` to remove the Dgraph instance from the
   cluster (see [More about Dgraph Zero](/dgraph/self-managed/dgraph-zero)). The
   removed instance will immediately stop running. Any further attempts to join
   the cluster fails for that instance since it has been removed.
2. Remove the PersistentVolumeClaim associated with the Pod to delete its data.
   This prepares the Pod to join with a clean state.
3. Restart the Pod. This creates a new PersistentVolumeClaim to create new data
   directories.

When an Alpha Pod restarts in a replicated cluster, it joins as a new member of
the cluster, be assigned a group and an unused index from Zero, and receive the
latest snapshot from the Alpha leader of the group.

When a Zero Pod restarts, it must join the existing group with an unused index
ID. You set the index ID with the `--raft` superflag's `idx` option. This might
require you to update the StatefulSet configuration.

## Kubernetes and Bulk Loader

You may want to initialize a new cluster with an existing data set such as data
from the [Dgraph Bulk Loader](/dgraph/admin/bulk-loader). You can use
[Init Containers](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/)
to copy the data to the Pod volume before the Alpha process runs.

See the `initContainers` configuration in
[dgraph-ha.yaml](https://github.com/hypermodeinc/dgraph/tree/master/contrib/config/kubernetes/dgraph-ha/dgraph-ha.yaml)
to learn more.


# Self-Managed Cluster
Source: https://docs.hypermode.com/dgraph/self-managed/overview



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

You can deploy and manage Dgraph database in a variety of self-managed
deployment scenarios, including:

* Running Dgraph on your on-premises infrastructure (virtual or bare-metal
  physical servers)
* Running Dgraph on your cloud infrastructure (AWS, Google Cloud and Azure)

This section focuses exclusively on deployment and management for these
self-managed scenarios. To learn about fully managed options that let you focus
on building apps and websites, rather than managing infrastructure, or
[Try Dgraph on Hypermode](https://hypermode.com/sign-up).

## Overview

A Dgraph cluster consists of the following:

* **Dgraph Alpha database server nodes**: The Dgraph Alpha server nodes in your
  deployment host and serve data. These nodes also host an `/admin` HTTP and
  gRPC endpoint that can be used for data and node administration tasks such as
  backup, export, draining, and shutdown.
* **Dgraph Zero management server nodes**: The Dgraph Zero nodes in your
  deployment control the nodes in your Dgraph cluster. Dgraph Zero automatically
  moves data between different Dgraph Alpha instances based on the volume of
  data served by each Alpha instance.

You need at least one node of each type to run Dgraph. You need three nodes of
each type to run Dgraph in a high-availability (HA) cluster configuration. To
learn more about 2-node and 6-node deployment options, see the
[Production Checklist](./production-checklist).

Each Dgraph Alpha exposes various administrative (admin) endpoints both over
HTTP and GraphQL, for example endpoints to export data and to perform a clean
shutdown. All such admin endpoints are protected by three layers of
authentication:

1. IP White-listing (use the `--security` superflag's `whitelist` option on
   Dgraph Alpha to whitelist IP addresses other than localhost).
2. Poor-man's auth, if Dgraph Alpha is started with the `--security` superflag's
   `token` option, then you should pass the token as an `X-Dgraph-AuthToken`
   header while making the HTTP request.
3. Guardian-only access, if Access Control Lists (ACL) is enabled. In this case
   you should pass the ACL-JWT of a Guardian user using the
   `X-Dgraph-AccessToken` header while making the HTTP request.

An administration endpoint is any HTTP endpoint which provides admin
capabilities. Administration endpoints usually start with the `/admin` path. The
current list of admin endpoints includes the following:

* `/admin`
* `/admin/config/cache_mb`
* `/admin/draining`
* `/admin/shutdown`
* `/admin/schema`
* `/admin/schema/validate`
* `/alter`
* `/login`

There are a few exceptions to this general rule:

* `/login`: This endpoint logs-in an ACL user, and provides them with a JWT.
  Only IP Whitelisting and Poor-man's auth checks are performed for this
  endpoint.
* `/admin`: This endpoint provides GraphQL queries/mutations corresponding to
  the HTTP admin endpoints. All of the queries/mutations on `/admin` have all
  three layers of authentication, except for `login (mutation)`, which has the
  same behavior as the HTTP `/login` endpoint.

## Whitelisting admin operations

By default, admin operations can only be initiated from the machine on which the
Dgraph Alpha runs.

You can use the `--security` superflag's `whitelist` option to specify a
comma-separated whitelist of IP addresses, IP ranges, CIDR ranges, or hostnames
for hosts from which admin operations can be initiated.

**IP Address**

```sh
dgraph alpha --security whitelist=127.0.0.1 ...
```

This would allow admin operations from hosts with IP 127.0.0.1 (that's
`localhost` only).

**IP Range**

```sh
dgraph alpha --security whitelist=172.17.0.0:172.20.0.0,192.168.1.1 ...
```

This would allow admin operations from hosts with IP between `172.17.0.0` and
`172.20.0.0` along with the server which has IP address as `192.168.1.1`.

**CIDR Range**

```sh
dgraph alpha --security whitelist=172.17.0.0/16,172.18.0.0/15,172.20.0.0/32,192.168.1.1/32 ...
```

This would allow admin operations from hosts that matches the CIDR range
`172.17.0.0/16`, `172.18.0.0/15`, `172.20.0.0/32`, or `192.168.1.1/32` (the same
range as the IP Range example).

You can set whitelist IP to `0.0.0.0/0` to whitelist all IP addresses.

**Hostname**

```sh
dgraph alpha --security whitelist=admin-bastion,host.docker.internal ...
```

This would allow admin operations from hosts with hostnames `admin-bastion` and
`host.docker.internal`.

## Restrict mutation operations

By default, you can perform mutation operations for any predicate. If the
predicate in mutation doesn't exist in the schema, the predicate gets added to
the schema with an appropriate [Dgraph Type](/dgraph/dql/schema).

You can use `--limit "mutations=disallow"` to disable all mutations, which is
set to `allow` by default.

```sh
dgraph alpha --limit "mutations=disallow;"
```

Enforce a strict schema by setting `--limit "mutations=strict`. This mode allows
mutations only on predicates already in the schema. Before performing a mutation
on a predicate that doesn't exist in the schema, you need to perform an alter
operation with that predicate and its schema type.

```sh
dgraph alpha --limit "mutations=strict; mutations-nquad=1000000"
```

## Secure alter operations

Clients can use alter operations to apply schema updates and drop particular or
all predicates from the database. By default, all clients are allowed to perform
alter operations. You can configure Dgraph to only allow alter operations when
the client provides a specific token. You can use this "Simple ACL" token to
prevent clients from making unintended or accidental schema updates or predicate
drops.

You can specify the auth token with the `--security` superflag's `token` option
for each Dgraph Alpha in the cluster. Clients must include the same auth token
to make alter requests.

```sh
dgraph alpha --security token=<authtokenstring>
```

```sh
curl -s localhost:8080/alter -d '{ "drop_all": true }'
# Permission denied. No token provided.
```

```sh
curl -s -H 'X-Dgraph-AuthToken: <wrongsecret>' localhost:8080/alter -d '{ "drop_all": true }'
# Permission denied. Incorrect token.
```

```sh
curl -H 'X-Dgraph-AuthToken: <authtokenstring>' localhost:8080/alter -d '{ "drop_all": true }'
# Success. Token matches.
```

<Note>
  To fully secure alter operations in the cluster, the authentication token must
  be set for every Alpha node.
</Note>

## Export database

As an `Administrator` you might want to export data from Dgraph to:

* backup your data
* migrate your data from one instance to another
* share your data

For more information about exporting your database, see
[Export data](/dgraph/admin/export)

## Shut down database

A clean exit of a single Dgraph node is initiated by running the following
GraphQL mutation on /admin endpoint.

<Warning>
  This won't work if called from outside the server where Dgraph is running. You
  can specify a list or range of whitelisted IP addresses from which shutdown or
  other admin operations can be initiated using the `--security` superflag's
  `whitelist` option on `dgraph alpha`.
</Warning>

```graphql
mutation {
  shutdown {
    response {
      message
      code
    }
  }
}
```

This stops the Alpha on which the command is executed and not the entire
cluster.

## Delete database

To drop all data, you could send a `DropAll` request via `/alter` endpoint.

Alternatively, you could:

* [Shutdown Dgraph](#shut-down-database) and wait for all writes to complete,
* Delete (maybe do an export first) the `p` and `w` directories, then
* Restart Dgraph.

## Upgrade database

Doing periodic exports is always a good idea. This is particularly useful if you
wish to upgrade Dgraph or reconfigure the sharding of a cluster. The following
are the right steps to safely export and restart.

1. Start an [export](#export-database)
2. Ensure it's successful
3. [Shutdown Dgraph](#shut-down-database) and wait for all writes to complete
4. Start a new Dgraph cluster using new data directories (this can be done by
   passing empty directories to the options `-p` and `-w` for Alphas and `-w`
   for Zeros)
5. Reload the data via [Bulk Loader](/dgraph/admin/bulk-loader)
6. Verify the correctness of the new Dgraph cluster. If all looks good, you can
   delete the old directories (export serves as an insurance)

These steps are necessary because Dgraph's underlying data format could have
changed, and reloading the export avoids encoding incompatibilities.

Blue-green deployment is a common approach to minimize downtime during the
upgrade process. This approach involves switching your app to read-only mode. To
make sure that no mutations are executed during the maintenance window you can
do a rolling restart of all your Alpha using the option `--mutations disallow`
when you restart the Alpha nodes. This ensures the cluster is in read-only mode.

At this point your app can still read from the old cluster and you can perform
steps 4 and 5. When the new cluster (that uses the upgraded version of Dgraph)
is up and running, you can point your app to it, and shutdown the old cluster.

## Post Installation

Now that Dgraph is up and running, to understand how to add and query data to
Dgraph, follow [Query Language Spec](/dgraph/dql/schema).


# Ports Usage
Source: https://docs.hypermode.com/dgraph/self-managed/ports-usage



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Dgraph cluster nodes use a range of ports to communicate over gRPC and HTTP.
Choose these ports carefully based on your topology and mode of deployment, as
this impacts the access security rules or firewall configurations required for
each port.

## Types of ports

Dgraph Alpha and Dgraph Zero nodes use a variety of gRPC and HTTP ports, as
follows:

* **gRPC-internal-private**: Used between the cluster nodes for internal
  communication and message exchange. Communication using these ports is
  TLS-encrypted.
* **gRPC-external-private**: Used by Dgraph Live Loader and Dgraph Bulk Loader
  to access APIs over gRPC.
* **gRPC-external-public**: Used by Dgraph clients to access APIs in a session
  that can persist after a query.
* **HTTP-external-private**: Used for monitoring and administrative tasks.
* **HTTP-external-public:** Used by clients to access APIs over HTTP.

## Default ports used by different nodes

| Dgraph Node Type | gRPC-internal-private | gRPC-external-private | gRPC-external-public | HTTP-external-private | HTTP-external-public |
| ---------------- | --------------------- | --------------------- | -------------------- | --------------------- | -------------------- |
| zero             | 5080<sup>1</sup>      | 5080<sup>1</sup>      |                      | 6080<sup>2</sup>      |                      |
| alpha            | 7080                  |                       | 9080                 |                       | 8080                 |
| ratel            |                       |                       |                      |                       | 8000                 |

<sup>1</sup>: Dgraph Zero uses port 5080 for internal communication within the
cluster, and to support the [data import](/dgraph/admin/import) tools.

<sup>2</sup>: Dgraph Zero uses port 6080 for
[administrative](/dgraph/self-managed/dgraph-zero) operations. Dgraph clients
can't access this port.

Users must modify security rules or open firewall ports depending upon their
underlying network to allow communication between cluster nodes, between the
Dgraph instances, and between Dgraph clients. In general, you should configure
the gRPC and HTTP `external-public` ports for open access by Dgraph clients, and
configure the gRPC-internal ports for open access by the cluster nodes.

**Ratel UI** accesses Dgraph Alpha on the `HTTP-external-public port` (which
defaults to localhost:8080) and can be configured to talk to a remote Dgraph
cluster. This way you can run Ratel on your local machine and point to a remote
cluster. But, if you are deploying Ratel along with Dgraph cluster, then you may
have to expose port 8000 to the public.

**Port Offset** To make it easier for users to set up a cluster, Dgraph has
default values for the ports used by Dgraph nodes. To support multiple nodes
running on a single machine or VM, you can set a node to use different ports
using an offset (using the command option `--port_offset`). This command
increments the actual ports used by the node by the offset value provided. You
can also use port offsets when starting multiple Dgraph Zero nodes in a
development environment.

For example, when a user runs Dgraph Alpha with the `--port_offset 2` setting,
then the Alpha node binds to port 7082 (`gRPC-internal-private`), 8082
(`HTTP-external-public`) and 9082 (`gRPC-external-public`), respectively.

**Ratel UI** by default listens on port 8000. You can use the `-port` flag to
configure it to listen on any other port.

## High availability cluster configuration

In a high availability (HA) cluster configuration, you should run three or five
replicas for the Zero node, and three or five replicas for the Alpha node. A
Dgraph cluster is divided into Raft groups, where Dgraph Zero is group 0 and
each shard of Dgraph Alpha is a subsequent numbered group (group 1, group 2,
etc.). The number of replicas in each Raft group must be an odd number for the
group to have consensus, which exists when the majority of nodes in a group are
available.

<Tip>
  If the number of replicas in a Raft group is **2N + 1**, up to **N** nodes can
  go offline without any impact on reads or writes. So, if there are five
  replicas, three must be online to avoid an impact to reads or writes.
</Tip>

### Dgraph Zero

Run three Dgraph Zero instances, assigning a unique integer ID to each using the
`--raft` superflag's `idx` option, and passing the address of any healthy Dgraph
Zero instance using the `--peer` flag.

To run three replicas for the Alpha nodes, set `--replicas=3`. Each time a new
Alpha node is added, the Zero node checks the existing groups and assign them as
appropriate.

### Dgraph Alpha

You can run as many Dgraph Alpha nodes as you want. You can manually set the
`--raft` superflag's `idx` option, or you can leave that flag empty, and the
Zero node assigns an id to the Alpha node. This id persists in the write-ahead
log, so be careful not to delete it.

The new Alpha nodes automatically detect each other by communicating with Dgraph
Zero and establish connections to each other. If you don't have a proxy or load
balancer for the Zero nodes, you can provide a list of Zero node addresses for
Alpha nodes to use at startup with the `--zero` flag. The Alpha node tries to
connect to one of the Zero nodes starting from the first Zero node address in
the list. For example: `--zero=zero1,zero2,zero3` where `zero1` is the
`host:port` of a Zero instance.

Typically, a Zero node would first attempt to replicate a group, by assigning a
new Alpha node to run the same group previously assigned to another. After the
group has been replicated per the `--replicas` flag, Dgraph Zero creates a new
group.

Over time, the data is evenly split across all of the groups. So, it's important
to ensure that the number of Alpha nodes is a multiple of the replication
setting. For example, if you set `--replicas=3` in for a Zero node, and then run
three Alpha nodes for no sharding, but 3x replication. Or, if you run six Alpha
nodes, sharding the data into two groups, with 3x replication.


# Production Checklist
Source: https://docs.hypermode.com/dgraph/self-managed/production-checklist

Requirements to install Dgraph in a production environment

<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

This guide describes important setup recommendations for a production-ready
Dgraph cluster, ensuring high availability with external persistent storage,
automatic recovery of failed services, automatic recovery of failed systems such
as virtual machines, and disaster recovery such as backup/restore or
export/import with automation.

<Note>
  In this guide, a node refers to a Dgraph instance unless specified otherwise.
</Note>

A **Dgraph cluster** is comprised of multiple **Dgraph instances** or nodes
connected together to form a single distributed database. A Dgraph instance is
either a **Dgraph Zero** or **Dgraph Alpha**, each of which serves a different
role in the cluster.

Once installed you may also install or use a **Dgraph client** to communicate
with the database and perform queries, mutations, alter schema operations and so
on. Pure HTTP calls from curl, Postman, or another program are also possible
without a specific client, but there are a range of clients that provide
higher-level language bindings, and which use optimized gRPC for communications
to the database. Any standards-compliant GraphQL client works with Dgraph to run
GraphQL operations. To run DQL and other Dgraph-specific operations, use a
Dgraph client.

Dgraph provides official clients for Go, Java, Python, and JavaScript, and C#,
and the JavaScript client supports both gRPC and HTTP to run more easily in a
browser. Community-developed Dgraph clients for other languages are also
available. The full list of clients can be found in
[Clients](/dgraph/sdks/overview) page. One particular client, Dgraph Ratel, is a
more sophisticated UI tool used to visualize queries, run mutations, and manage
schemas in both GraphQL and DQL. Note that clients aren't part of a database
cluster, and simply connect to one or more Dgraph Alpha instances.

### Cluster requirements

A minimum of one Dgraph Zero and one Dgraph Alpha is needed for a working
cluster.

There can be multiple Dgraph Zeros and Dgraph Alphas running in a single
cluster.

### Machine requirements

To ensure predictable performance characteristics, Dgraph instances should
**not** run on "burstable" or throttled machines that limit resources. That
includes t2 class machines on AWS.

To ensure that Dgraph is highly available, we recommend each Dgraph instance be
deployed to a different underlying host machine, and ideally that machines are
in different availability zones or racks. In the event of an underlying machine
failure, it's critical that only one Dgraph Alpha and one Dgraph Zero be offline
so that 2 of the 3 instances in each group maintain a quorum. Also when using
virtual machines or containers, ensure machines aren't over-subscribed and
ideally not co-resident with other processes that interrupt and delay Dgraph
processing.

If you'd like to run Dgraph with fewer machines, then the recommended
configuration is to run a single Dgraph Zero and a single Dgraph Alpha per
machine. In a high availability setup, that allows the cluster to lose a single
machine (simultaneously losing a Dgraph Zero and a Dgraph Alpha) with continued
availability of the database.

Don't run multiple Dgraph Zeros or Dgraph Alpha processes on a single machine.
This can affect performance due to shared resource issues and reduce
availability in the event of machine failures.

### Operating system

Dgraph is designed to run on Linux. To run Dgraph on Windows and macOS, use the
standalone Docker image.

### CPU and memory

We recommend 8 vCPUs or cores on each of three HA Alpha instances for production
loads, with 16 GiB+ memory per node.

You'll want a ensure that your CPU and memory resources are sufficient for your
production workload. A common configuration for Dgraph is 16 vCPUs and 32 GiB of
memory per machine. Dgraph is designed with concurrency in mind, so more cores
means quicker processing and higher throughput of requests.

You may find you'll need more CPU cores and memory for your specific use case.

In addition, we highly recommend that your CPU clock rate is greater than or
equal to 3.4GHz.

### Disk

Dgraph instances make heavy use of disks, so storage with high IOPS is highly
recommended to ensure reliable performance. Specifically SSDs, not HDDs.

Regarding disk IOPS, the recommendation is:

* 1000 IOPS minimum
* 3000 IOPS for medium and large datasets

Instances such as c5d.4xlarge have locally attached NVMe SSDs with high IOPS.
You can also use EBS volumes with provisioned IOPS (io1). If you are not running
performance-critical workloads, you can also choose to use cheaper gp2 EBS
volumes. Typically, AWS
[gp3](https://aws.amazon.com/about-aws/whats-new/2020/12/introducing-new-amazon-ebs-general-purpose-volumes-gp3/?nc1=h_ls)
disks are a good option and have 3000 Baseline IOPS at any disk size.

Recommended disk sizes for Dgraph Zero and Dgraph Alpha:

* Dgraph Zero: 200 GB to 300 GB. Dgraph Zero stores cluster metadata information
  and maintains a write-ahead log for cluster operations.
* Dgraph Alpha: 250 GB to 750 GB. Dgraph Alpha stores database data, including
  the schema, indices, and the data values. It maintains a write-ahead log of
  changes to the database. Your cloud provider may provide better disk
  performance based on the volume size.
* If you plan to store over 1.1 TB per Dgraph Alpha instance, you must increase
  either the MaxLevels or TableSizeMultiplier.

Additional recommendations:

* The recommended Linux filesystem is ext4.
* Avoid using shared storage such as NFS, CIFS, and CEPH storage.

### Firewall rules

Dgraph instances communicate over several ports. Firewall rules should be
configured appropriately for the ports documented in
[Ports Usage](./ports-usage).

Internal ports must be accessible by all Zero and Alpha peers for proper
cluster-internal communication. Database clients must be able to connect to
Dgraph Alpha external ports either directly or through a load balancer.

Dgraph Zeros can be set up in a private network where communication is only with
Dgraph Alphas, database administrators, internal services (such as Prometheus or
Jaeger), and possibly developers (see note below). Dgraph Zero's 6080 external
port is only necessary for database administrators. For example, it can be used
to inspect the cluster metadata (/state), allocate UIDs or set transaction
timestamps (/assign), move data shards (/moveTablet), or remove cluster nodes
(/removeNode). The full docs about Zero's administrative tasks are in
[More About Dgraph Zero](/dgraph/self-managed/dgraph-zero).

<Note>
  Developers using Dgraph Live Loader or Dgraph Bulk Loader require access to
  both Dgraph Zero port 5080 and Dgraph Alpha port 9080. When using those tools,
  consider using them within your environment that has network access to both
  ports of the cluster.
</Note>

### Operating system tuning

The OS should be configured with the recommended settings to ensure that Dgraph
runs properly.

#### File descriptors limit

Dgraph can use a large number of open file descriptors. Most operating systems
set a default limit that's lower than what's required.

It is recommended to set the file descriptors limit to unlimited. If that's not
possible, set it to at least a million (1,048,576) which is recommended to
account for cluster growth over time.

### Deployment

A Dgraph instance is run as a single process from a single static binary. It
doesn't require any additional dependencies or separate services to run (see the
[Supplementary Services](#supplementary-services) section for third-party
services that work alongside Dgraph). A Dgraph cluster is set up by running
multiple Dgraph processes networked together.

### Backup policy

A backup policy is a predefined, set schedule used to schedule backups of
information from business apps. A backup policy helps to ensure data recovery in
the event of accidental data deletion, data corruption, or a system outage.

For Dgraph, backups are created using the
[backups enterprise feature](/dgraph/enterprise/binary-backups). You can also
create full exports of your data and schema using
[data exports](/dgraph/admin/export) available as an open source feature.

We **strongly** recommend that you have a backup policy in place before moving
your app to the production phase, and we also suggest that you have a backup
policy even for pre-production apps supported by Dgraph database instances
running in development, staging, QA or pre-production clusters.

We suggest that your policy include frequent full and incremental backups.
Accordingly, we suggest the following backup policy for your production apps:

* [full backup](/dgraph/enterprise/binary-backups#forcing-a-full-backup) every
  24 hours
* incremental backup every 2-4 hours

### Supplementary services

These services aren't required for a Dgraph cluster to function but are
recommended for better insight when operating a Dgraph cluster.

* [Metrics](/dgraph/admin/metrics) with Prometheus and Grafana.
* [Distributed tracing](/dgraph/admin/traces) with Jaeger.


# Single Host Setup
Source: https://docs.hypermode.com/dgraph/self-managed/single-host-setup



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

To learn about Dgraph and the components, you can install and run Dgraph cluster
on a single host using Docker, Docker Compose, or Dgraph command line.

<Tabs>
  <Tab title="Docker">
    Dgraph cluster can be setup running as containers on a single host.

    ## Before you begin

    Ensure that you have installed:

    * Docker [Desktop](https://docs.docker.com/desktop/) (required for windows or
      mac)
    * Docker [Engine](https://docs.docker.com/engine/install/)

    ## Launch a Dgraph standalone cluster using Docker

    1. Select a name `<CONTAINER_NAME>` for you Docker container and create a
       directory `<GRAPH_DATA_PATH>` that for Dgraph data on your local file system.

    2. Run a container with the dgraph/standalone image:

       ```sh
          docker run --name <CONTAINER_NAME> -d -p "8080:8080" -p "9080:9080" -v <DGRAPH_DATA_PATH>:/dgraph dgraph/standalone:latest
       ```

    3. Optionally launch [Ratel](dgraph/ratel/overview) using the `dgraph/ratel`
       docker image:

       ```sh
       docker run --name ratel  -d -p "8000:8000"  dgraph/ratel:latest
       ```

       You can now use Ratel UI on your browser at localhost:8000 and connect to you
       Dgraph cluster at localhost:8080

    ## Setup a Dgraph cluster on a single host using Docker

    1. Get the `<IP_ADDRESS>` of the host using:

       ```sh
          ip addr  # On Arch Linux
          ifconfig # On Ubuntu/Mac
       ```

    2. Pull the latest Dgraph image using docker:

       ```sh
          docker pull dgraph/dgraph:latest
       ```

    3. Verify that the image is downloaded:

       ```sh
          docker images
       ```

    4. Create a `<DGRAPH_NETWORK>` using:

       ```sh
          docker network create <DGRAPH_NETWORK>
       ```

    5. Create a directory `<ZERO_DATA>`to store data for Dgraph Zero and run the
       container:

       ```sh
       mkdir ~/<ZERO> # Or any other directory where data should be stored.

       docker run -it -p 5080:5080 --network <DGRAPH_NETWORK> -p 6080:6080 -v ~/<ZERO_DATA>:/dgraph dgraph/dgraph:latest dgraph zero --my=<IP_ADDRESS>:5080
       ```

    6. Create a directory `<ALPHA_DATA_1>` to store for Dgraph Alpha and run the
       container:

       ```sh
       mkdir ~/<ALPHA_DATA_1> # Or any other directory where data should be stored.

       docker run -it -p 7080:7080 --network <DGRAPH_NETWORK> -p 8080:8080 -p 9080:9080 -v ~/<ALPHA_DATA_1>:/dgraph dgraph/dgraph:latest dgraph alpha --zero=<IP_ADDRESS>:5080 --my=<IP_ADDRESS>:7080
       ```

    7. Create a directory `<ALPHA_DATA_2>` to store for the second Dgraph Alpha and
       run the container:

       ```sh
       mkdir ~/<ALPHA_DATA_2> # Or any other directory where data should be stored.

       docker run -it -p 7081:7081 --network <DGRAPH_NETWORK> -p 8081:8081 -p 9081:9081 -v ~/<ALPHA_DATA_2>:/dgraph dgraph/dgraph:latest dgraph alpha --zero=<IP_ADDRESS>:5080 --my=<IP_ADDRESS>:7081  -o=1
       ```

       To override the default ports for the second Alpha use `-o`.

    8. Connect the Dgraph cluster that are running using [https://play.dgraph.io/](https://play.dgraph.io/).
       For information about connecting, see [Ratel UI](/dgraph/ratel/connection).
  </Tab>

  <Tab title="Dgraph Command Line">
    You can run Dgraph directly on a single Linux host.

    ## Before you begin

    Ensure that you have:

    * Installed [Dgraph](./download) on the Linux host.
    * Made a note of the `<IP_ADDRESS>` of the host.

    ## Using Dgraph CLI

    You can start Dgraph on a single host using the dgraph command line.

    1. Run Dgraph Zero

       ```sh
       dgraph zero --my=<IP_ADDRESS>:5080
       ```

       The `--my` flag is the connection that Dgraph alphas dial to talk to Zero.
       So, the port `5080` and the IP address must be visible to all the Dgraph
       alphas. For all other various flags, run `dgraph zero --help`.

    2. Run two Dgraph Alpha nodea:

       ```sh
       dgraph alpha --my=<IP_ADDRESS>:7080 --zero=localhost:5080
       dgraph alpha --my=<IP_ADDRESS>:7081 --zero=localhost:5080 -o=1
       ```

       Dgraph Alpha nodes use two directories to persist data and
       [WAL logs](/dgraph/concepts/consistency-model), and these directories must be
       different for each Alpha if they're running on the same host. You can use
       `-p` and `-w` to change the location of the data and WAL directories.To learn
       more about other flags, run `dgraph alpha --help`.

    3. Connect the Dgraph cluster that are running using [https://play.dgraph.io/](https://play.dgraph.io/).
       For information about connecting, see [Ratel UI](/dgraph/ratel/connection).
  </Tab>

  <Tab title="Docker Compose">
    You can install Dgraph using the Docker Compose on a system hosted on any of the
    cloud provider.

    ## Before you begin

    * Ensure that you have installed Docker
      [Compose](https://docs.docker.com/compose/).
    * IP address of the system on cloud `<CLOUD_IP_ADDRESS>`.
    * IP address of the local host `<IP_ADDRESS>`.

    ## Using Docker Compose

    1. Download the Dgraph `docker-compose.yml` file:

       ```sh
       wget https://github.com/hypermodeinc/dgraph/raw/main/contrib/config/docker/docker-compose.yml
       ```

       By default only the localhost IP 127.0.0.1 is allowed. When you run Dgraph on
       Docker, the containers are assigned IPs and those IPs need to be added to the
       allowed list.

    2. Add a list of IPs allowed for Dgraph so that you can create the schema. Use
       an editor of your choice and add the `<IP_ADDRESS>` of the local host in
       `docker-compose.yml` file:

       ```txt
       # This Docker Compose file can be used to quickly boot up Dgraph Zero
       # and Alpha in different Docker containers.
       # It mounts /tmp/data on the host machine to /dgraph within the
       # container. You will need to change /tmp/data to a more appropriate location.
       # Run `docker-compose up` to start Dgraph.
       version: "3.2"
       services:
         zero:
           image: dgraph/dgraph:latest
           volumes:
             - /tmp/data:/dgraph
           ports:
             - 5080:5080
             - 6080:6080
           restart: on-failure
           command: dgraph zero --my=zero:5080
         alpha:
           image: dgraph/dgraph:latest
           volumes:
             - /tmp/data:/dgraph
           ports:
             - 8080:8080
             - 9080:9080
           restart: on-failure
           command: dgraph alpha --my=alpha:7080 --zero=zero:5080 --security whitelist=<IP_ADDRESS>
         ratel:
           image: dgraph/ratel:latest
           ports:
             - 8000:8000

       ```

    3. Run the `docker-compose` command to start the Dgraph services in the docker
       container:

       ```sh
       sudo docker-compose up
       ```

       After Dgraph is installed on Docker, you can view the images and the
       containers running in Docker for Dgraph.

    4. View the containers running for Dgraph using:

       ```sh
       sudo docker ps -a
       ```

       An output similar to the following appears:

       ```sh
       CONTAINER ID   IMAGE                  COMMAND                  CREATED
       4b67157933b6   dgraph/dgraph:latest   "dgraph zero --my=ze—"   2 days ago
       3faf9bba3a5b   dgraph/ratel:latest    "/usr/local/bin/dgra—"   2 days ago
       a6b5823b668d   dgraph/dgraph:latest   "dgraph alpha --my=a—"   2 days ago
       a6b5823b668d   dgraph/dgraph:latest   "dgraph alpha --my=a…"   2 days ago
       ```

    5. To access the Ratel UI for queries, mutations, and altering schema, open your
       web browser and navigate to `http://<CLOUD_IP_ADDRESS>:8000`.

    6. Click **Launch Latest** to access the latest stable release of Ratel UI.

    7. In the **Dgraph Server Connection** dialog that set the **Dgraph server URL**
       as `http://<CLOUD_IP_ADDRESS>:8080`

    8. Click **Connect** . The connection health appears green.

    9. Click **Continue** to query or run mutations.
  </Tab>
</Tabs>


# Single Server Cluster Setup
Source: https://docs.hypermode.com/dgraph/self-managed/single-server-cluster



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

You can install a single server Dgraph cluster in Kubernetes.

## Before you begin

* Install the
  [Kubernetes command line tool](https://kubernetes.io/docs/tasks/tools/install-kubectl/).
* Ensure that you have a production-ready Kubernetes cluster running in a cloud
  provider of your choice.
* (Optional) To run Dgraph Alpha with TLS, see
  [TLS Configuration](./tls-configuration).

## Installing a single server Dgraph cluster

1. Verify that you are able to access the nodes in the Kubernetes cluster:

   ```sh
   kubectl get nodes
   ```

   An output similar to this appears:

   ```sh
   NAME                                          STATUS   ROLES    AGE   VERSION
   <aws-ip-hostname>.<region>.compute.internal   Ready    <none>   1m   v1.15.11-eks-af3caf
   <aws-ip-hostname>.<region>.compute.internal   Ready    <none>   1m   v1.15.11-eks-af3caf
   ```

   After your Kubernetes cluster is up, you can use
   [dgraph-single.yaml](https://github.com/hypermodeinc/dgraph/blob/master/contrib/config/kubernetes/dgraph-single/dgraph-single.yaml)
   to start a Zero, Alpha, and Ratel UI services.

2. Start a StatefulSet that creates a single Pod with `Zero`, `Alpha`, and
   `Ratel UI`:

   ```sh
   kubectl create --filename https://raw.githubusercontent.com/hypermodeinc/dgraph/master/contrib/config/kubernetes/dgraph-single/dgraph-single.yaml
   ```

   An output similar to this appears:

   ```sh
   service/dgraph-public created
   statefulset.apps/dgraph created
   ```

3. Confirm that the Pod was created successfully.

   ```sh
   kubectl get pods
   ```

   An output similar to this appears:

   ```sh
   NAME       READY     STATUS    RESTARTS   AGE
   dgraph-0   3/3       Running   0          1m
   ```

4. List the containers running in the Pod `dgraph-0`:

   ```sh
   kubectl get pods dgraph-0 -o jsonpath='{range .spec.containers[*]}{.name}{"\n"}{end}'
   ```

   An output similar to this appears:

   ```sh
   ratel
   zero
   alpha
   ```

   You can check the logs for the containers in the pod using
   `kubectl logs --follow dgraph-0 <CONTAINER_NAME>`.

5. Port forward from your local machine to the Pod:

   ```sh
   kubectl port-forward pod/dgraph-0 8080:8080
   kubectl port-forward pod/dgraph-0 8000:8000
   ```

6. Go to `http://localhost:8000` to access Dgraph using the Ratel UI.

## Deleting Dgraph single server resources

Delete all the resources using:

```sh
kubectl delete --filename https://raw.githubusercontent.com/hypermodeinc/dgraph/master/contrib/config/kubernetes/dgraph-single/dgraph-single.yaml
kubectl delete persistentvolumeclaims --selector app=dgraph
```


# TLS Configuration
Source: https://docs.hypermode.com/dgraph/self-managed/tls-configuration



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

Connections between Dgraph database and its clients can be secured using TLS. In
addition, Dgraph can now secure gRPC communications between Dgraph Alpha and
Dgraph Zero server nodes using mutual TLS (mTLS). Dgraph can now also secure
communications over the Dgraph Zero `gRPC-external-private` port used by
Dgraph's Live Loader and Bulk Loader clients. To learn more about the HTTP and
gRPC ports used by Dgraph Alpha and Dgraph Zero, see
[Ports Usage](./ports-usage). Password-protected private keys are **not
supported**.

To further improve TLS security, only TLS v1.2 cypher suites that use 128-bit or
greater RSA or AES encryption are supported.

<Tip>
  If you're generating encrypted private keys with `openssl`, be sure to specify
  the encryption algorithm explicitly (like `-aes256`). This forces `openssl` to
  include `DEK-Info` header in private key, which is required to decrypt the key
  by Dgraph. When default encryption is used, `openssl` doesn't write that
  header and key can't be decrypted.
</Tip>

## Dgraph certificate management tool

<Note>
  This section refers to the `dgraph cert` command which was introduced in
  v1.0.9. For previous releases, see the previous [TLS configuration
  documentation](https://github.com/hypermodeinc/dgraph/blob/release/v1.0.7/wiki/content/deploy/index.md#tls-configuration).
</Note>

The `dgraph cert` program creates and manages CA-signed certificates and private
keys using a generated Dgraph Root CA. There are three types of certificate/key
pairs:

1. Root CA certificate/key pair: This is used to sign and verify node and client
   certificates. If the root CA certificate is changed then you must regenerate
   all certificates, and this certificate must be accessible to the Alpha nodes.
2. Node certificate/key pair: This is shared by the Dgraph Alpha nodes and used
   for accepting TLS connections.
3. Client certificate/key pair: This is used by the clients (such as Live Loader
   or Ratel) to communicate with Dgraph Alpha server nodes where client
   authentication with mTLS is required.

```sh
# To see the available flags.
$ dgraph cert --help

# Create Dgraph Root CA, used to sign all other certificates.
$ dgraph cert

# Create node certificate and private key
$ dgraph cert -n localhost

# Create client certificate and private key for mTLS (mutual TLS)
$ dgraph cert -c dgraphuser

# Combine all in one command
$ dgraph cert -n localhost -c dgraphuser

# List all your certificates and keys
$ dgraph cert ls
```

The default location where the *cert* command stores certificates (and keys) is
`tls` under the Dgraph working directory. The default directory path can be
overridden using the `--dir` option. For example:

```sh
dgraph cert --dir ~/mycerts
```

### File naming conventions

The following file naming conventions are used by Dgraph for proper TLS setup.

| File name         | Description                | Use                                               |
| ----------------- | -------------------------- | ------------------------------------------------- |
| ca.crt            | Dgraph Root CA certificate | Verify all certificates                           |
| ca.key            | Dgraph CA private key      | Validate CA certificate                           |
| node.crt          | Dgraph node certificate    | Shared by all nodes for accepting TLS connections |
| node.key          | Dgraph node private key    | Validate node certificate                         |
| client.*name*.crt | Dgraph client certificate  | Authenticate a client *name*                      |
| client.*name*.key | Dgraph client private key  | Validate *name* client certificate                |

For client authentication, each client must have their own certificate and key.
These are then used to connect to the Dgraph server nodes.

The node certificate `node.crt` can support multiple node names using multiple
host names and/or IP address. Just separate the names with commas when
generating the certificate.

```sh
dgraph cert -n localhost,104.25.165.23,dgraph.io,2400:cb00:2048:1::6819:a417
```

<Tip>
  You must delete the old node cert and key before you can generate a new pair.
</Tip>

<Note>
  When using host names for node certificates, including `localhost`, your
  clients must connect to the matching host name -- such as `localhost` not
  `127.0.0.1`. If you need to use IP addresses, then add them to the node
  certificate.
</Note>

### Certificate inspection

The command `dgraph cert ls` lists all certificates and keys in the `--dir`
directory (default `dgraph-tls`), along with details to inspect and validate
cert/key pairs.

Example of command output:

```sh
-rw-r--r-- ca.crt - Dgraph Root CA certificate
        Issuer: Dgraph Labs, Inc.
           S/N: 043c4d8fdd347f06
    Expiration: 02 Apr 29 16:56 UTC
SHA-256 Digest: 4A2B0F0F 716BF5B6 C603E01A 6229D681 0B2AFDC5 CADF5A0D 17D59299 116119E5

-r-------- ca.key - Dgraph Root CA key
SHA-256 Digest: 4A2B0F0F 716BF5B6 C603E01A 6229D681 0B2AFDC5 CADF5A0D 17D59299 116119E5

-rw-r--r-- client.admin.crt - Dgraph client certificate: admin
        Issuer: Dgraph Labs, Inc.
     CA Verify: PASSED
           S/N: 297e4cb4f97c71f9
    Expiration: 03 Apr 24 17:29 UTC
SHA-256 Digest: D23EFB61 DE03C735 EB07B318 DB70D471 D3FE8556 B15D084C 62675857 788DF26C

-rw------- client.admin.key - Dgraph Client key
SHA-256 Digest: D23EFB61 DE03C735 EB07B318 DB70D471 D3FE8556 B15D084C 62675857 788DF26C

-rw-r--r-- node.crt - Dgraph Node certificate
        Issuer: Dgraph Labs, Inc.
     CA Verify: PASSED
           S/N: 795ff0e0146fdb2d
    Expiration: 03 Apr 24 17:00 UTC
         Hosts: 104.25.165.23, 2400:cb00:2048:1::6819:a417, localhost, dgraph.io
SHA-256 Digest: 7E243ED5 3286AE71 B9B4E26C 5B2293DA D3E7F336 1B1AFFA7 885E8767 B1A84D28

-rw------- node.key - Dgraph Node key
SHA-256 Digest: 7E243ED5 3286AE71 B9B4E26C 5B2293DA D3E7F336 1B1AFFA7 885E8767 B1A84D28
```

Important points:

* The cert/key pairs should always have matching SHA-256 digests. Otherwise, the
  cert must be regenerated. If the Root CA pair differ, all cert/key must be
  regenerated; the flag `--force` can help.
* All certificates must pass Dgraph CA verification.
* All key files should have the least access permissions, especially the
  `ca.key`, but be readable.
* Key files won't be overwritten if they have limited access, even with
  `--force`.
* Node certificates are only valid for the hosts listed.
* Client certificates are only valid for the named client/user.

## TLS options

Starting in release v21.03, pre-existing TLS configuration options have been
replaced by the `--tls` [superflag](/dgraph/cli/command-reference) and its
options. The following `--tls` configuration options are available for Dgraph
Alpha and Dgraph Zero nodes:

* `ca-cert <path>` - Path and filename of the Dgraph Root CA (for example,
  `ca.crt`)
* `server-cert <path>` - Path and filename of the node certificate (for example,
  `node.crt`)
* `server-key <path>` - Path and filename of the node certificate private key
  (for example, `node.key`)
* `use-system-ca` - Include System CA with Dgraph Root CA.
* `client-auth-type <string>` - TLS client authentication used to validate
  client connections from external ports. To learn more, see
  [Client Authentication Options](#client-authentication-options).

<Note>
  Dgraph now allows you to specify the path and filename of the CA root
  certificate, the node certificate, and the node certificate private key. So,
  these files don't need to have specific filenames or exist in the same
  directory, as in previous Dgraph versions that used the `--tls_dir` flag.
</Note>

You can configure Dgraph Live Loader with the following `--tls` options:

* `ca-cert <path>` - Dgraph root CA, such as `./tls/ca.crt`
* `use-system-ca` - Include System CA with Dgraph Root CA.
* `client-cert` - User cert file provided by the client to Alpha
* `client-key` - User private key file provided by the client to Alpha
* `server-name <string>` - Server name, used for validating the server's TLS
  host name.

### Using TLS with only external ports encrypted

To encrypt communication between Dgraph server nodes and clients over external
ports, you can configure certificates and run Dgraph Alpha and Dgraph Zero using
the following commands:

Dgraph Alpha:

```sh
# First, create the root CA, Alpha node certificate and private keys, if not already created.
# Note that you must specify in node.crt the host name or IP addresses that clients use connect:
$ dgraph cert -n localhost,104.25.165.23,104.25.165.25,104.25.165.27
# Set up Dgraph Alpha nodes using the following default command (after generating certificates and private keys)
$ dgraph alpha --tls "ca-cert=/dgraph-tls/ca.crt; server-cert=/dgraph-tls/node.crt; server-key=/dgraph-tls/node.key"
```

Dgraph Zero:

```sh
# First, copy the root CA, node certificates and private keys used to set up Dgraph Alpha (above) to the Dgraph Zero node.
# Optionally, you can generate and use a separate Zero node certificate, where you specify the host name or IP addresses used by Live Loader and Bulk Loader to connect to Dgraph Zero.
# Next, set up Dgraph Zero nodes using the following default command:
$ dgraph zero --tls "ca-cert=/dgraph-tls/ca.crt; server-cert=/dgraph-tls/node.crt; server-key=/dgraph-tls/node.key"
```

You can then run Dgraph Live Loader on a Dgraph Alpha node using the following
command:

```sh
# Now, connect to server using TLS
$ dgraph live --tls "ca-cert=./dgraph-tls/ca.crt; server-name=localhost" -s 21million.schema -f 21million.rdf.gz
```

### Using TLS with internal and external ports encrypted

If you require client authentication (mutual TLS, or mTLS), you can configure
certificates and run Dgraph Alpha and Dgraph Zero with settings that encrypt
both internal ports (those used within the cluster) as well as external ports
(those used by clients that connect to the cluster, including Bulk Loader and
Live Loader).

The following example shows how to encrypt both internal and external ports:

Dgraph Alpha:

```sh
# First create the root CA, node certificates and private keys, if not already created.
# Note that you must specify the host name or IP address for other nodes that will share node.crt.
$ dgraph cert -n localhost,104.25.165.23,104.25.165.25,104.25.165.27
# Set up Dgraph Alpha nodes using the following default command (after generating certificates and private keys)
$ dgraph alpha
      --tls "ca-cert=/dgraph-tls/ca.crt; server-cert=/dgraph-tls/node.crt; server-key=/dgraph-tls/node.key;
internal-port=true; client-cert=/dgraph-tls/client.alpha1.crt; client-key=/dgraph-tls/client.alpha1.key"
```

Dgraph Zero:

```sh
# First, copy the certificates and private keys used to set up Dgraph Alpha (above) to the Dgraph Zero node.
# Next, set up Dgraph Zero nodes using the following default command:
$ dgraph zero
      --tls "ca-cert=/dgraph-tls/ca.crt; server-cert=/dgraph-tls/node.crt; server-key=/dgraph-tls/node.key; internal-port=true; client-cert=/dgraph-tls/client.zero1.crt; client-key=/dgraph-tls/client.zero1.key"
```

You can then run Dgraph Live Loader using the following:

```sh
# Now, connect to server using mTLS (mutual TLS)
$ dgraph live
   --tls "ca-cert=./tls/ca.crt; client-cert=./tls/client.dgraphuser.crt; client-key=./tls/client.dgraphuser.key; server-name=localhost; internal-port=true" \
   -s 21million.schema \
   -f 21million.rdf.gz
```

### Client authentication options

The server always requests client authentication. There are four different
values for the `client-auth-type` option that change the security policy of the
client certificate.

| Value              | Client Cert/Key | Client Certificate Verified                                   |
| ------------------ | --------------- | ------------------------------------------------------------- |
| `REQUEST`          | optional        | Client certificate isn't VERIFIED if provided. (least secure) |
| `REQUIREANY`       | required        | Client certificate is never VERIFIED                          |
| `VERIFYIFGIVEN`    | optional        | Client certificate is VERIFIED if provided (default)          |
| `REQUIREANDVERIFY` | required        | Client certificate is always VERIFIED (most secure)           |

`REQUIREANDVERIFY` is the most secure but also the most difficult to configure
for clients. When using this value, the value of `server-name` is matched
against the certificate SANs values and the connection host.

<Note>
  If mTLS is enabled using `internal-port=true`, internal ports (by default 5080
  and 7080) use the `REQUIREANDVERIFY` setting. Unless otherwise configured,
  external ports (by default 9080, 8080, and 6080) use the `VERIFYIFGIVEN`
  setting. Changing the `client-auth-type` option to another setting only
  affects client authentication on external ports.
</Note>

## Using Ratel with client authentication

Ratel UI (and any other JavaScript clients built on top of `dgraph-js-http`)
connect to Dgraph servers via HTTP, when TLS is enabled servers begin to expect
HTTPS requests only.

If you haven't already created the CA certificate and the node certificate for
Alpha servers from the earlier instructions (see
[Dgraph Certificate Management Tool](#dgraph-certificate-management-tool)), the
first step would be to generate these certificates, it can be done by the
following command:

```sh
# Create rootCA and node certificates/keys
$ dgraph cert -n localhost
```

If Dgraph Alpha's `client-auth-type` option is set to `REQUEST` or
`VERIFYIFGIVEN` (default), then client certificate isn't mandatory. The steps
after generating CA/node certificate are as follows:

### Step 1: Install Dgraph root CA into system CA

#### Linux

```sh
# Copy the generated CA to the ca-certificates directory
$ cp /path/to/ca.crt /usr/local/share/ca-certificates/ca.crt
# Update the CA store
$ sudo update-ca-certificates`
```

### Step 2: Install Dgraph root CA into web browsers trusted CA list

#### Firefox

* Choose Preferences -> Privacy & Security -> View Certificates -> Authorities
* Click on Import and import the `ca.crt`

#### Chrome

* Choose Settings -> Privacy and Security -> Security -> Manage Certificates ->
  Authorities
* Click on Import and import the `ca.crt`

### Step 3. Point Ratel to the `https://` endpoint of Alpha server

* Change the Dgraph Alpha server address to `https://` instead of `http://`, for
  example `https://localhost:8080`.

For `REQUIREANY` and `REQUIREANDVERIFY` as `client-auth-type` option, you need
to follow the preceding steps and install client certificate on your browser:

1. Generate a client certificate: `dgraph cert -c laptopuser`.

2. Convert it to a `.p12` file:

   ```sh
   openssl pkcs12 -export \
      -out laptopuser.p12 \
      -in tls/client.laptopuser.crt \
      -inkey tls/client.laptopuser.key
   ```

   Use any password you like for export, it's used to encrypt the p12 file.

3. Import the client certificate to your browser. It can be done in Chrome as
   follows:
   * Choose Settings -> Privacy and Security -> Security -> Manage Certificates
     -> Your Certificates
   * Click on Import and import the `laptopuser.p12`.

<Note>
  Mutual TLS may not work in Firefox because Firefox is unable to send privately
  signed client certificates, this issue is filed
  [here](https://bugzilla.mozilla.org/show_bug.cgi?id=1662607).
</Note>

Next time you use Ratel to connect to an Alpha with Client authentication
enabled the browser prompts you for a client certificate to use. Select the
client's certificate you've imported in the preceding step and queries/mutations
should succeed.

## Using Curl with Client authentication

When TLS is enabled, `curl` requests to Dgraph need some specific options to
work. For instance (for changing draining mode):

```sh
curl --silent https://localhost:8080/admin/draining
```

If you are using `curl` with
[Client Authentication](#client-authentication-options) set to `REQUIREANY` or
`REQUIREANDVERIFY`, you need to provide the client certificate and private key.
For instance (for an export request):

```sh
curl --silent --cacert ./tls/ca.crt --cert ./tls/client.dgraphuser.crt --key ./tls/client.dgraphuser.key https://localhost:8080/admin/draining
```

Refer to the `curl` documentation for further information on its TLS options.

## Access data using a client

Some examples of connecting via a [Client](/dgraph/sdks/overview) when TLS is in
use can be found below:

* [dgraph4j](https://github.com/hypermodeinc/dgraph4j#creating-a-secure-client-using-tls)
* [dgraph-js](https://github.com/hypermodeinc/dgraph-js/tree/main/examples/tls)
* [dgo](https://github.com/hypermodeinc/dgraph/blob/main/tlstest/acl/acl_over_tls_test.go)
* [pydgraph](https://github.com/hypermodeinc/pydgraph/tree/main/examples/tls)

## Troubleshooting Ratel's Client authentication

If you are getting errors in Ratel when TLS is enabled, try opening your Dgraph
Alpha URL as a web page.

Assuming you are running Dgraph on your local machine, opening
`https://localhost:8080/` in the browser should produce a message
`Dgraph browser is available for running separately using the dgraph-ratel binary`.

In case you are getting a connection error, try not passing the
`client-auth-type` flag when starting an Alpha. If you are still getting an
error, check that your host name is correct and the port is open. Then, make
sure that "Dgraph Root CA" certificate is installed and trusted correctly.

After that, if things work without passing `client-auth-type` but stop working
when `REQUIREANY` and `REQUIREANDVERIFY` are set, make sure the `.p12` file is
installed correctly.


# Troubleshooting
Source: https://docs.hypermode.com/dgraph/self-managed/troubleshooting



<Info>
  We're overhauling Dgraph's docs to make them clearer and more approachable. If
  you notice any issues during this transition or have suggestions, please
  [let us know](https://github.com/hypermodeinc/docs/issues).
</Info>

This page provides tips on how to troubleshoot issues with running Dgraph.

### Running out of memory

When you [bulk load](dgraph/admin/bulk-loader) or
[backup](dgraph/enterprise/binary-backups) your data, Dgraph can consume more
memory than usual due to a high volume of writes. This can cause Out Of Memory
(OOM) crashes.

You can take the following steps to help avoid OOM crashes:

* **Increase the amount of memory available**: If you run Dgraph with
  insufficient memory, that can result in OOM crashes. The recommended minimum
  RAM to run Dgraph on desktops and laptops (single-host deployment) is 16 GB.
  For servers in a cluster deployment, the recommended minimum is 8 GB per
  server. This applies to EC2 and GCE instances, as well as on-premises servers.
* **Reduce the number of Go routines**: You can troubleshoot OOM issues by
  reducing the number of Go routines (`goroutines`) used by Dgraph from the
  default value of eight. For example, you can reduce the `goroutines` that
  Dgraph uses to four by calling the `dgraph alpha` command with the following
  option:

  `--badger "goroutines=4"`

### "Too many open files" errors

If Dgraph logs "too many open files" errors, you should increase the per-process
open file descriptor limit to permit more open files. During normal operations,
Dgraph must be able to open many files. Your operating system may have an open
file descriptor limit with a low default value that isn't adequate for a
database like Dgraph. If so, you might need to increase this limit.

On Linux and Mac, you can get file descriptor limit settings with the `ulimit`
command, as follows:

* Get hard limit: `ulimit -n -H`
* Get soft limit: `ulimit -n -S`

A soft limit of `1048576` open files is the recommended minimum to use Dgraph in
production, but you can try increasing this soft limit if you continue to see
this error. To learn more, see the `ulimit` documentation for your operating
system.

<Note>
  Depending on your OS, your shell session limits might not be the same as the
  Dgraph process limits.
</Note>

For example, to properly set up the `ulimit` values on Ubuntu 20.04 systems:

```sh
sudo sed -i 's/#DefaultLimitNOFILE=/DefaultLimitNOFILE=1048576/' /etc/systemd/system.conf
sudo sed -i 's/#DefaultLimitNOFILE=/DefaultLimitNOFILE=1048576/' /etc/systemd/user.conf
```

This affects the base limits for all processes. After a reboot, your OS picks up
the new values.


# Why Dgraph?
Source: https://docs.hypermode.com/dgraph/why-dgraph

Developers at startups to Fortune 500 companies choose Dgraph for their most intensive graph features

Dgraph is designed for real-time workloads, horizontal scalability, and data
flexibility. Implemented as a distributed system, Dgraph processes queries in
parallel to deliver the fastest results, even for the most complex workloads.

### Fully open source with an enormous community

Dgraph is a thriving open source project with 20,000+ GitHub stars and over 200
contributors. This means rapid bug fixes, feature enhancements, and a wealth of
shared expertise. There’s no vendor lock-in and your development efforts are
fully portable.

### Seamless horizontal scaling

Dgraph's distributed architecture ensures that your database can scale with your
data and user needs. Your data is automatically rebalanced and highly available
across multiple RAFT groups, even during multi-node failures. Graph traversals
are parallelized to enable efficient query processing, delivering near-linear
scale-out performance with no single-leader architecture.

### AI-native primitives

Dgraph's [vector indexing](./dql/indexes#vector-indices), search, and storage
allow you to store multiple embeddings on any given node or relationship.
Uniquely able to store multiple vector embeddings, Dgraph allows you to compare
and combine embedding models to get the best results for your similarity search.
You can combine HNSW vector similarity, keyword-search, geospatial polygons and
graph traversals to power your multi-modal search. Dgraph’s multi-tenancy
capabilities enable your AI apps to have logically separated knowledge graphs.

### Battle-tested for your most critical projects

With thousands of deployments globally, Dgraph is behind some of the world's
most important applications. From building rocket ships, to self-driving cars,
to beloved personal computers, to producing clean energy— Dgraph is a trusted
part of their applications. Self-manage Dgraph or deploy your database to
Hypermode’s managed Dgraph service.

### Turn-key hosting platform

Hypermode's fully managed Dgraph service eliminates operational overhead while
maximizing performance. Deploy production-ready graph databases in minutes with
automatic scaling, backups, and monitoring. Our expert team handles
infrastructure management, upgrades, and optimization. With enterprise-grade
security, 99.995% SLA uptime guarantee, and technical support, you can trust
your most mission-critical workloads to our platform. Migrate existing
deployments or start fresh with flexible pricing that scales with your needs.


# Quickstart: Hyper Commerce
Source: https://docs.hypermode.com/getting-started-with-hyper-commerce

Learn how to instantly get started with the Hypermode Commerce template

This guide walks you through getting started with Hypermode using the
[Instant Vector Search template](https://github.com/hypermodeinc/hyper-commerce).
You’ll also learn how to customize your functions to tailor the app to your
needs.

## What's Hypermode?

[Hypermode](/introduction) is framework designed to simplify the development and
deployment of AI features and assistants. With its
[Functions SDK](/modus/overview) and runtime, developers can quickly deploy
intelligent features into production without the need for extensive data
wrangling. Hypermode supports rapid iteration, allowing you to start with a
large language model and later fine-tune smaller, open source models. Its
production-ready GraphQL API auto-generates and enables seamless integration.
Hypermode is ideal for developers looking to quickly prototype and evolve AI
capabilities.

## Quickstart

You can get started with Hypermode’s
[Instant Vector Search](https://github.com/hypermodeinc/hyper-commerce) template
using either sample data or your own, without needing to write any code or set
up a GitHub repository. This lets you explore the template and Hypermode’s
features before committing any time and effort.

### Step 1: Creating a new project

1. Visit the [Hypermode website](https://hypermode.com/sign-in) and sign up with
   your GitHub, Google, or email address. Once signed in, you'll see the
   Hypermode projects dashboard.

2. After accessing your dashboard, click the “Create New Project” button

   <Frame>
     <img className="block" src="https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/getting-started-guide/create-project.png" alt="The Hypermode projects dashboard." />
   </Frame>

3. Select “Deploy Instant Vector Search” from the options.

   <Frame>
     <img className="block" src="https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/getting-started-guide/deploy-template-dropdown.png" alt="Selecting the 'Deploy instant vector search' template in the dropdown when creating a new project." />
   </Frame>

4. Choose between using sample data or your own data:

   <Frame>
     <img className="block" src="https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/getting-started-guide/sample-data-select.png" alt="Selecting to use sample data when creating the new Hypermode project." />
   </Frame>

   > If you choose to use your own data, you'll need to upload it using a CSV
   > file and a Python script provided in Step 2.

5. Once created, Hypermode provisions a runtime with instant connectivity to
   shared AI models that bring the functions within your project to life. The
   source code for the project is open source, available in
   [this repository](https://github.com/hypermodeinc/hyper-commerce).

Behind the scenes, Hypermode automatically exposes the functions from the
backend directory as GraphQL APIs, allowing you to interact with them like any
other GraphQL endpoint.

### Step 2: Adding your data

If you selected sample data, your project is fully setup. You can move on to
Step 3 and immediately start exploring the project and its features. If you
chose to use your own data, follow these steps to seed your data into the
collections:

1. Ensure you have [Python](https://www.python.org/downloads/) installed on your
   machine. You can verify this by running `python --version` or
   `python3 --version` in your terminal.

2. Prepare your CSV file with the following headers:
   `Unique Id,Product Name,Category,Selling Price,About Product,Image,Num Stars,In Stock`.

3. Copy the script from
   [this file](https://github.com/hypermodeinc/hyper-commerce/blob/main/backend/extras/ecommerce_populate.py).

4. Update the script with your endpoint (located in your projects dashboard),
   API key (located in the settings tab of your Hypermode console), and the path
   to your CSV file.

5. Install the necessary dependencies:

   ```sh
   pip install requests gql requests_toolbelt pandas numpy
   ```

6. Run the script to populate your data:

   ```sh
   python3 ecommerce_populate.py
   ```

### Step 3: Exploring the Console

In the Hypermode console, you’ll see several key components of your project:

<Frame>
  <img className="block" src="https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/getting-started-guide/project-dash.png" alt="The Hypermode project dashboard." />
</Frame>

* **[Functions](/modus/overview):** These are serverless functions written in
  AssemblyScript (a TypeScript-like language) that are automatically exposed as
  GraphQL APIs. Once deployed, you can query these functions within your app.
* **[Collections](/modus/sdk/assemblyscript/collections):** Hypermode offers
  built-in key-value storage, known as collections, with support for vector
  embeddings. This allows you to store and retrieve data efficiently, without
  requiring a database for basic use cases.
* **[Models](/modus/sdk/assemblyscript/models):** This section represents the AI
  models defined for your project. These models handle tasks like embedding text
  into vectors for search. Hypermode provides open source shared and dedicated
  model hosting for rapid experimentation. You can also connect to your
  preferred large language model, including OpenAI, Anthropic, and Gemini.
* **[Connections](/modus/app-manifest#connections):** You define all external
  connections, with the runtime denying all other egress for secure-by-default
  processing.
* **Endpoint:** The GraphQL endpoint for your project, which you’ll use to
  interact with your APIs and query your data.

### Step 4: Querying your data

Now that you set up, deployed, and seeded your project with data, you can test
your functions using the query interface in the Hypermode console.

1. Navigate to the Query tab in your Hypermode console to test your data.

2. Paste the following query to retrieve sample product data:

   ```graphql
   query {
     searchProducts(maxItems: 4, query: "sparkly shoes") {
       searchObjs {
         product {
           name
           description
           id
           stars
           isStocked
         }
       }
     }
   }
   ```

3. You should see the data for 4 items returned from the `searchProducts`
   endpoint that match your query, `sparkly shoes`. Feel free to experiment with
   the query—adjust the `maxItems` value to return more items, or change the
   search query to see how the returned data matches your input. Additionally,
   notice that the function ranks items based on their star rating and whether
   they're in stock.

   <Frame>
     <img className="block" src="https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/getting-started-guide/query-interface.png" alt="The query interface." />
   </Frame>

### Step 5: Testing in the Frontend

Now that you've set up your project and queried your data in the console, you
can test the capability in a frontend UI.

1. Clone the repository that contains a pre-built UI for testing. You can find
   the repo
   [here](https://github.com/hypermodeinc/hyper-commerce/tree/main/frontend).

2. Retrieve your API key from the Settings section of your Hypermode console.

3. Create a `.env.local` file in the root of your project and add your API key
   and endpoint to it, like this:

   ```sh
   HYPERMODE_API_TOKEN=YOUR_API_TOKEN
   HYPERMODE_API_ENDPOINT=YOUR_API_ENDPOINT
   ```

4. **Run the project locally** by executing the following command:

   ```sh
   npm run dev
   ```

5. With the project running locally, you can now test the search capability in
   the provided UI. Try searching for products to see how your Hypermode
   project's API integrates and returns data.

> Note: The intent of this quickstart is for proof of concepts. For more
> advanced usage, such as customizing search re-ranking logic, you'll need to
> clone the template to your own repository to make and push changes. Refer to
> the next section for further instructions.

***

## Customizing the app

In this section, you’ll learn how to tailor the template to fit your specific
needs. We’ll show you how to edit your backend functions and deploy those
changes to Hypermode.

### Step 1: Clone the template repository

1. Go to the template repo
   [hyper-commerce](https://github.com/hypermodeinc/hyper-commerce).

2. Clone the repo by clicking `Use this template` in the upper-right corner and
   selecting `Create new repo.` This clones the code into your own GitHub
   repository

3. Visit the [Hypermode website](https://hypermode.com/sign-in) and sign up with
   your GitHub, Google, or email address. Once signed in, you'll see your
   Hypermode projects dashboard.

4. In the Hypermode console, click `New Project`.

5. Select `Import a GitHub Repo`.

   <Frame>
     <img className="block" src="https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/getting-started-guide/import-github-repo.png" alt="Selecting 'import a GitHub repo' from the dropdown." />
   </Frame>

6. Choose the repo you just created.

   <Frame>
     <img className="block" src="https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/getting-started-guide/select-repo.png" alt="Selecting the repo we want to import." />
   </Frame>

7. Click “Deploy” to finish creating your project.

8. Once deployed, your functions and collections are visible in the Hypermode
   console.

### Step 2: Seeding your data

1. Make your first commit to the repo to trigger a deployment.

2. Ensure you have [Python](https://www.python.org/downloads/) installed on your
   machine. You can verify this by running `python --version` or
   `python3 --version` in your terminal.

3. The template includes a script at `/backend/extras/ecommerce_populate.py` to
   seed your data, as well as sample data located in
   `/backend/extras/hyper_toys.csv`.

4. If you want to use your own data, replace the content of the sample CSV
   (`hyper_toys.csv`) with your own data. Make sure the headers in your CSV
   match the following headers:
   `Uniq Id,Product Name,Category,Selling Price,About Product,Image,Num Stars,In Stock`

5. Install the required dependencies by running the following command in your
   project directory:

   ```sh
   pip install -r requirements.txt
   ```

6. Edit the `ecommerce_populate.py` file to include your endpoint and API key,
   which you can find in your Hypermode dashboard.

7. Run the script to seed the data into your project:

   ```sh
   python3 ecommerce_populate.py
   ```

8. The script batch inserts the data and displays the time taken for each
   operation. Inserting the full dataset (10,000 rows) may take around 18
   minutes. If you want to test with a smaller dataset, feel free to reduce the
   size of the CSV.

### Step 3: Customizing your functions

You can modify the template to suit your needs by customizing the functions in
the `/backend/functions/assembly` directory.

#### Example: Customizing product rankings

If you'd like to rank products based solely on their star rating, without
considering whether they're in stock, follow these steps:

1. Go to the `search.ts` file and locate the
   `reRankAndFilterSearchResultObjects` function.
2. Modify the function to only rank based on the star rating, like this:

```tsx
function reRankAndFilterSearchResultObjects(
  objs: collections.CollectionSearchResultObject[],
  thresholdStars: f32,
): collections.CollectionSearchResultObject[] {
  for (let i = 0; i < objs.length; i++) {
    const starRes = collections.getText(
      consts.productStarCollection,
      objs[i].key,
    )
    const stars = parseFloat(starRes)

    objs[i].score *= stars * 0.1
  }

  objs.sort((a, b) => (a.score < b.score ? -1 : a.score > b.score ? 1 : 0))

  const filteredResults: collections.CollectionSearchResultObject[] = []
  for (let i = 0; i < objs.length; i++) {
    const starRes = collections.getText(
      consts.productStarCollection,
      objs[i].key,
    )
    const stars = parseFloat(starRes)
    if (stars >= thresholdStars) {
      filteredResults.push(objs[i])
    }
  }

  return filteredResults
}
```

#### Deploying the change

1. Once you’ve updated the function, commit the changes to your repo.
2. Any commit to the `main` branch automatically triggers a Hypermode
   deployment.
3. After deployment, when you query the `searchProducts` endpoint again, it
   ranks products solely based on their star rating.

### Step 4: Testing your functions

Once you’ve made changes to your backend functions and deployed them to
Hypermode, it's time to test the updates.

#### Test in the console IDE

In the Hypermode console, navigate to the Query tab to test your modified
functions directly. Run queries similar to the ones you used earlier to see how
the changes impact the results.

#### Run the frontend locally

The repo you cloned includes a frontend. Move into the frontend directory and
add the following values to your `.env.local` file:

```jsx
HYPERMODE_API_TOKEN = YOUR_API_TOKEN
HYPERMODE_API_ENDPOINT = YOUR_API_ENDPOINT
```

> Note: Both of these values are available in the Hypermode console

Next, just run the command `npm run dev` in your terminal to run the app
locally. Now you can test the changes you made to your backend functions.


# Connect to Graph
Source: https://docs.hypermode.com/graphs/connect

Connect to your graph from a SDK, IDE, or Modus

<Info>
  Graphs on Hypermode is currently in developer preview. New features are
  shipping weekly.
</Info>

## Connection strings

A connection string is a URL that contains all of the necessary information to
connect to your graph from a Dgraph client or Modus. It is displayed after you
deploy your graph.

## Authentication

Your graph endpoint is protected by bearer token authorization passed in the
`Authorization` header as an API key. You can find your API key in the console
alongside your graph connection details. The API key is already included in the
connection string.

<Warning>
  If you remove your API key from your graph, it is made publicly accessible
  with no authentication.
</Warning>

## Clients

Dgraph includes a web client and language-specific SDKs for programmatically
working with your graph. When using the auto-generated GraphQL API, you can use
any GraphQL client to work with your graph.

### Ratel

Dgraph includes Ratel, a web client for easily connecting to and exploring your
graph. Ratel is available hosted at [https://ratel.hypermode.com](https://ratel.hypermode.com). To connect to
your graph, paste your connection string into the Ratel interface and click
`Connect`.

### Dgraph SDKs

Dgraph SDKs are available for a number of languages, including
[Go](/dgraph/sdks/go), [Python](/dgraph/sdks/python), [Java](/dgraph/sdks/java),
and [JavaScript](/dgraph/sdks/javascript). The SDKs have been updated to support
the connection string for connecting to your graph on Hypermode. Ensure you're
running the latest version of the SDK for your language.

### GraphQL clients

When working with your graph through the auto-generated GraphQL API, you can use
any [GraphQL IDE or client](/dgraph/graphql/connecting) to connect to your
graph.

### Modus

Modus provides a framework for integrating data from your graph into a complete
AI-ready backend. Learn more on
[connecting Modus to your graph](/modus/data-fetching#dgraph).


# Manage Data
Source: https://docs.hypermode.com/graphs/manage-data

Query, mutate, and bulk manage data from your graph

<Info>
  Graphs on Hypermode is currently in developer preview. New features are
  shipping weekly.
</Info>

Querying (reading) and mutating (writing) data to your graph is simple using an
[IDE or SDK](./connect). For bulk operations, there are tools for importing,
exporting, and dropping data.

## Import data

Dgraph provides multiple tools for importing data to your graph, whether an
[initial import to an empty graph](/dgraph/admin/bulk-loader) or an
[incremental import](/dgraph/admin/live-loader).

You can also [load CSV-formatted data](/dgraph/admin/import-csv).

## Export data

If you need to export data from your graph, please reach out to support and we
can assist.

<Note>Self-service for exporting data is coming soon.</Note>

## Drop data

Dropping data from your graph is a permanent action and can't be undone. To drop
data from your graph, you can use the `/alter` endpoint.

To drop all data from your graph, while maintaining the schema:

```sh
curl -X POST https://<my-database>.hypermode.host/dgraph/alter \
    --header "Authorization: Bearer $BEARER_TOKEN" \
    --header "Content-Type: application/json"  \
    --data '{"drop_op": "DATA"}'
```

For more options, see [Dgraph's documentation](/dgraph/admin/drop-data).


# Manage Graph
Source: https://docs.hypermode.com/graphs/manage-graph

Manage your graph's resources and configuration

<Info>
  Graphs on Hypermode is currently in developer preview. New features are
  shipping weekly.
</Info>

## Region availability

Graphs on Hypermode are available in the following regions:

| Region | Location              |
| :----- | :-------------------- |
| `iad1` | Washington, D.C., USA |
| `pdx1` | Portland, Oregon, USA |

Additional regions are planned to be available in the near future:

* Frankfurt, Germany
* Dublin, Ireland
* Singapore

## Scaling

Graph compute is measured in gigabytes (GB) of memory, with 1 vCPU allocated per
4 gigabytes of memory. On Hypermode, you're charged for the compute allocated to
your graph, billed to the second. That means when your graph is scaled to zero,
you're only billed for the underlying storage.

### Scale to zero

By default, new graphs scale to zero after five minutes of inactivity. When a
new query or mutation is received, the graph scales up and process that request.

You can configure the scaling behavior for your graph by increasing the idle
period. Set the idle period to `0` to turn off scale to zero.

### Autoscaling

Autoscaling is a feature that automatically scales your graph based on the
compute and memory load. When under sustained load, the graph scales up to
handle the additional load. When the load subsides, the graph scales down
automatically.

You can configure the autoscaling behavior for your graph by setting the `min`
and `max` values. The `min` value is the minimum number of compute units the
graph has, and the `max` value is the maximum number of compute units the graph
can scale to.

### Storage

You are billed only for the storage used by your graph. The storage is
automatically provisioned and expanded based on the growth of data within your
graph.

## Backups

Your graph on Hypermode is automatically backed up every four hours, with
backups stored with zonal isolation. To restore your graph to a previous state,
please reach out to support via the console.

<Note>User control of backup frequency and restore is coming soon.</Note>


# Manage Schema
Source: https://docs.hypermode.com/graphs/manage-schema

Load and update the schema of your graph

<Info>
  Graphs on Hypermode is currently in developer preview. New features are
  shipping weekly.
</Info>

When working with Dgraph, defining a schema is optional! Start schema-less and
layer on a DQL or GraphQL-based modeling approach when needed.

## Schema-less default

Graphs running in Hypermode use Dgraph's `flexible` schema mode. This mode
allows you to run a mutation without declaring the predicate in your schema.

When a mutation introduces a new predicate, Dgraph automatically adds it to the
schema. This can be useful when you're in the early stages of a project and the
schema is evolving frequently and for allowing AI agents to augment your
knowledge graph.

## Deploy a DQL schema

To deploy a [DQL schema](/dgraph/dql/schema), place the schema in a file
(`schema.dql` in this case) and make a POST request to the `/dgraph/alter`
endpoint for your host.

```sh
curl -X POST https://<my-database>.hypermode.host/dgraph/alter \
    --header "Authorization: Bearer $BEARER_TOKEN" \
    --header "Content-Type: application/json"  \
    --data-binary "@schema.dql"
```

## Deploy a GraphQL schema

To deploy a [GraphQL schema](/dgraph/graphql/schema/overview), place the schema
in a file (`schema.graphql` in this case) and make a POST request to the
`/dgraph/admin/schema` endpoint for your host.

```sh
curl -X POST https://<my-database>.hypermode.host/dgraph/admin/schema \
    --header "Authorization: Bearer $BEARER_TOKEN" \
    --header "Content-Type: application/json"  \
    --data-binary "@schema.graphql"
```


# Graphs
Source: https://docs.hypermode.com/graphs/overview

A fully managed graph database service powered by Dgraph

<Info>
  Graphs on Hypermode is currently in developer preview. New features are
  shipping weekly.
</Info>

Graphs on Hypermode provides a fully managed [Dgraph](/dgraph/overview) service
for building and deploying knowledge graphs. This service combines the power of
Dgraph's distributed graph database with Hypermode's AI development platform,
offering a seamless experience for developers and organizations of all sizes.

## Key features

In addition to the [features provided by Dgraph](/dgraph/why-dgraph), Graphs on
Hypermode provides a turnkey experience for building and deploying knowledge
graphs.

| Feature                                                   | Description                                                                      |
| :-------------------------------------------------------- | :------------------------------------------------------------------------------- |
| [Scale to Zero](./manage-graph#scale-to-zero)             | automatically scale down after periods of inactivity and scale back up on demand |
| [Autoscaling](./manage-graph#autoscaling)                 | dynamic resource allocation based on workload demands                            |
| [Flexible Schema Options](./manage-schema)                | start schema-less or define structured schemas using DQL or GraphQL              |
| [Regional Deployment](./manage-graph#region-availability) | deploy in multiple regions for low-latency access                                |
| [Modus Integration](/modus/data-fetching#dgraph)          | seamlessly integrate with Hypermode's Modus framework for AI-ready backends      |
| [Fully Managed Service](./manage-graph)                   | deploy, scale, and operate your graph database without infrastructure headaches  |

## Getting started

To get started with Graphs on Hypermode:

1. **[Deploy a new graph](./provision)**: create your graph database
2. **[Connect to your graph](./connect)**: use connection strings to access your
   graph from various clients
3. **[Manage your schema](./manage-schema)**: define your graph structure using
   DQL or GraphQL *(optional)*
4. **[Manage data](./manage-data)**: add, query, and manipulate data in your
   graph

## Use cases

Graphs on Hypermode is ideal for:

* Knowledge graphs for AI apps
* Social networks and recommendation systems
* Fraud detection and pattern recognition
* Content management systems with rich interconnections
* Complex relationship modeling
* Any app requiring efficient traversal of connected data

## Why Graphs on Hypermode?

Traditional data stores struggle with highly connected data and complex
relationship queries, which are often found in knowledge graphs. Graph stores
like Dgraph excel at modeling and querying relationships, making them perfect
for building with AI services, where connections between data points are as
important as the data itself.

Hypermode's managed service eliminates the operational complexity of running
Dgraph, allowing you to focus on building your app while we handle scaling,
backups, and infrastructure management.


# Provision Graph
Source: https://docs.hypermode.com/graphs/provision

Create a new graph and configure its resources

<Info>
  Graphs on Hypermode is currently in developer preview. New features are
  shipping weekly.
</Info>

From your workspace home, click `Create new graph`. You are prompted to name
your graph and select the [region](./manage-graph#region-availability) for
provisioning. Once provisioned, your [connection string](./connect) is
displayed.

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/graphs/create-graph.png" alt="Create graph" />
</Frame>


# Hosted Models
Source: https://docs.hypermode.com/hosted-models

Iterate quickly with seamless access to the most popular models

Hypermode includes a set of shared models available for integration into your
app.

Need a bespoke model? You can include a model from
[Hugging Face](https://huggingface.co/) in your
[app manifest](/modus/app-manifest) and Hypermode runs and manages it for you.

## Setup

To use a Hypermode-hosted model, in your [app manifest](/modus/app-manifest) set
`connection: "hypermode"`, `provider: "hugging-face"`, and set `sourceModel` to
be the model name as specified on Hugging Face.

```json modus.json {3-7}
{
  ...
  "models": {
    "text-generator": {
      "sourceModel": "meta-llama/Llama-3.2-3B-Instruct",
      "provider": "hugging-face",
      "connection": "hypermode"
    }
  }
  ...
}
```

## Deployment mode

We run our most popular models as multi-tenant, shared instances across
workspaces and customers.

By default, if the model you use is available as a shared model, your app uses
these shared models at runtime. If the model you use isn't available as a shared
model, Hypermode automatically spins up a dedicated instance of the model for
your project.

## Shared models

These are the models available currently with shared instances:

* [`meta-llama/Llama-3.2-3B-Instruct`](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)
* [`deepseek-ai/DeepSeek-R1-Distill-Llama-8B`](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)
* [`sentence-transformers/all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
* [`AntoineMC/distilbart-mnli-github-issues`](https://huggingface.co/AntoineMC/distilbart-mnli-github-issues)
* [`distilbert/distilbert-base-uncased-finetuned-sst-2-english`](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english)

<note>
  We're constantly evaluating model usage in determining new models to add to
  our shared catalog. Interested in consuming an open source model not listed
  here by the token? Let us know at
  [help@hypermode.com](mailto:help@hypermode.com).
</note>


# Hyp CLI
Source: https://docs.hypermode.com/hyp-cli

Comprehensive reference for the Hyp CLI commands and usage

Hyp CLI is a command-line tool for managing your Hypermode account and apps.
When using Hyp CLI alongside the Modus CLI, your apps automatically connect to
[Hypermode-hosted models](/hosted-models) when working locally.

## Install

Install Hyp CLI via npm.

```sh
npm install -g @hypermode/hyp-cli
```

## Commands

### `login`

Log in to your Hypermode account. When executed, this command redirects to the
Hypermode console to authenticate. It stores an authentication token locally and
prompts you to select your organization context.

### `logout`

Log out of your Hypermode account. This command clears your local authentication
token.

### `link`

Link your GitHub repo to a Hypermode project. The command adds a default GitHub
Actions workflow to build your Modus app and a Hypermode GitHub app for
auto-deployment on commit.

### `org switch`

Switch to a different org context within the CLI session.


# Integrate API
Source: https://docs.hypermode.com/integrate-api

Easily add intelligent features to your app

Hypermode makes it easy to incrementally add intelligence your app.

## API endpoint

You can find your project's API endpoint in the Hypermode Console, in your
project's Home tab, in the format `https://<slug>.hypermode.app/<path>`.

## API token

Hypermode protects your project's endpoint with an API key. In your project
dashboard, navigate to **Settings** → **API Keys** to find and manage your API
tokens.

From your app, you can call the API by passing the API token in the
`Authorization` header. Here's an example using the `fetch` API in JavaScript:

```javascript
const endpoint = "" // your API endpoint
const key = "" // your API key
// your graphql query
const query = `query {
  ...
}`

const response = await fetch(endpoint, {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    Authorization: key,
  },
  body: JSON.stringify({
    query: query,
  }),
})
const data = await response.json()
```

<Note>
  Additional authorization methods are under development. If your app requires a
  different protocol, reach out at
  [help@hypermode.com](mailto:help@hypermode.com).
</Note>


# Hypermode
Source: https://docs.hypermode.com/introduction

An AI development platform that provides hosting and management capabilities for agents and knowledge graphs

{/* vale Google.Contractions = NO */}

## What is Hypermode?

{/* vale Google.Contractions = YES */}

Hypermode is a managed service that provides the workbench and infrastructure to
create powerful, engaging, and secure agentic flows, AI features, and backend
services.

With a suite of tools and services, start with one Hypermode component or all of
them together as your complete backend.

<CardGroup cols={2}>
  <Card title="Apps" icon="rectangle-code" href="https://hypermode.typeform.com/agents-waitlist">
    Build and deploy agentic flows, agents, and AI-enabled services | Join our
    waitlist →
  </Card>

  <Card title="Graphs" icon="chart-network" href="graphs/overview">
    Organize your knowledge in a natural structure of nodes and relationships
  </Card>

  <Card title="Models" icon="cube" href="/hosted-models">
    Experiment rapidly with the most popular and newest open source models
  </Card>

  <Card title="Integrated Console" icon="display">
    Understand and control the end-to-end flow of your agents, data, and
    inferences
  </Card>
</CardGroup>

## Built on open source

Hypermode leads and builds on a strong foundation of open source projects:

<CardGroup cols={4}>
  <Card title="Modus" icon="" href="/modus/overview">
    An agent framework in Go with auto-generated AI APIs
  </Card>

  <Card title="Dgraph" icon="" href="/dgraph/overview">
    A graph database most commonly used for building knowledge graphs
  </Card>

  <Card title="Badger" icon="" href="/badger/overview">
    An embeddable key-value store in Go
  </Card>

  <Card title="Ristretto" icon="" href="https://github.com/hypermodeinc/ristretto">
    An embeddable memory-bound cache in Go
  </Card>
</CardGroup>


# Modify Organization
Source: https://docs.hypermode.com/modify-organization

Update organization attributes or delete your organization

Organizations contain a set of related projects and members. This forms the
billing envelope for Hypermode. You can modify your organization's name and slug
through the settings.

## Update organization name

Your organization name is a display name only and changing it doesn't impact
your running apps. To update your organization display name, click your
organization's name in the top navigation. Then, click **Settings** →
**General**. Enter a new name and click **Save**.

## Update organization slug

From your organization home view, click **Settings** → **General**. Enter a new
slug and click **Save**.

<Warning>
  When you update the organization slug, Hypermode reflects the new slug in your
  app endpoints across all projects. You must update any existing integrations
  that use the existing app endpoints. Make sure to communicate these changes
  with your team or stakeholders.
</Warning>

## Delete organization

If you no longer need an organization, you can permanently delete it. To delete
an organization, you must [delete all projects](/modify-project#delete-project)
first.

To delete an organization, click your organization's name in the top navigation.
Then, click **Settings** → **Delete**. Click **Delete** and enter the
organization's name as confirmation. This action is irreversible and permanently
deletes your organization.


# Modify Project
Source: https://docs.hypermode.com/modify-project

Update project attributes or delete your project

After you create a project, you can modify its name and slug through the project
settings.

## Update project name

Your project name is display-only and changing it doesn't impact your running
app. To update your project display name, click **Settings** → **General**.
Enter a new name and click **Save**.

## Update project slug

From your project view, click **Settings** → **General**. Enter a new slug and
click **Save**.

<Warning>
  If you update the project slug, you must update any existing integrations that
  use the existing app endpoints. Make sure to communicate these changes with
  your team or stakeholders.
</Warning>

## Delete project

If you no longer need a project, you can permanently delete it. Deleting a
project removes all associated endpoints, configurations, and data.

To delete a project, from the project in the console, click **Settings** →
**Delete**. Click **Delete** and enter the project’s name as confirmation. This
action is irreversible and permanently deletes all project data and
configuration.


# AI-Enabled Apps
Source: https://docs.hypermode.com/modus/ai-enabled-apps

Add intelligence to your app with AI models

Modus makes it easy to incrementally add intelligence to your apps. Whether
you're building app with Modus or starting with your first AI feature, Modus'
APIs give you a full pallette to build a modern app from.


# API Generation
Source: https://docs.hypermode.com/modus/api-generation

Create the signature for your API

Modus automatically creates an external API based on the endpoints defined in
your [app manifest](/modus/app-manifest#endpoints). Modus generates the API
signature based on the functions you export from your app.

## Exporting functions

Modus uses the default conventions for each language.

<Tabs>
  <Tab title="Go">
    Functions written in Go use starting capital letters to expose functions as public. Modus
    creates an external API for public functions from any file that belongs to the `main` package.

    The functions below generate an API endpoint with the signature

    ```graphql
    type Query {
      classifyText(text: String!, threshold: Float!): String!
    }
    ```

    Since the `classify` function isn't capitalized, Modus doesn't include it in the
    generated GraphQL API.

    ```go
    package main

    import (
      "errors"
      "fmt"
      "github.com/hypermodeinc/modus/sdk/go/models"
      "github.com/hypermodeinc/modus/sdk/go/models/experimental"
    )

    const modelName = "my-classifier"

    // this function takes input text and a probability threshold, and returns the
    // classification label determined by the model, if the confidence is above the
    // threshold; otherwise, it returns an empty string

    func ClassifyText(text string, threshold float32) (string, error) {
      predictions, err:= classify(text)
      if err != nil {
        return "", err
      }

      prediction := predictions[0]
      if prediction.Confidence < threshold {
        return "", nil
      }

      return prediction.Label, nil
    }

    func classify(texts ...string) ([]experimental.ClassifierResult, error) {
      model, err := models.GetModel[experimental.ClassificationModel](modelName)
      if err != nil {
        return nil, err
      }

      input, err := model.CreateInput(texts...)
      if err != nil {
        return nil, err
      }

      output, err := model.Invoke(input)
      if err != nil {
        return nil, err
      }

      if len(output.Predictions) != len(texts) {
        word := "prediction"
        if len(texts) > 1 {
          word += "s"
        }

        return nil, fmt.Errorf("expected %d %s, got %d", len(texts), word, len(output.Predictions))
      }

      return output.Predictions, nil
    }
    ```
  </Tab>

  <Tab title="AssemblyScript">
    Functions written in AssemblyScript use ES module-style `import` and `export` statements. With
    the default package configuration, Modus creates an external API for functions exported form the
    `index.ts` file located in the `functions/assembly` folder of your project.

    The functions below generate an API endpoint with the signature

    ```graphql
    type Query {
      classifyText(text: String!, threshold: Float!): String!
    }
    ```

    Since the `classify` function isn't exported from the module, Modus doesn't
    include it in the generated GraphQL API.

    ```ts
    import { models } from "@hypermode/modus-sdk-as"
    import {
      ClassificationModel,
      ClassifierResult,
    } from "@hypermode/modus-sdk-as/models/experimental/classification"

    const modelName: string = "my-classifier"

    // this function takes input text and a probability threshold, and returns the
    // classification label determined by the model, if the confidence is above the
    // threshold; otherwise, it returns an empty string
    export function classifyText(text: string, threshold: f32): string {
      const predictions = classify(text, threshold)

      const prediction = predictions[0]
      if (prediction.confidence < threshold) {
        return ""
      }

      return prediction.label
    }

    function classify(text: string, threshold: f32): ClassifierResult[] {
      const model = models.getModel<ClassificationModel>(modelName)
      const input = model.createInput([text])
      const output = model.invoke(input)

      return output.predictions
    }
    ```
  </Tab>
</Tabs>


# App Manifest
Source: https://docs.hypermode.com/modus/app-manifest

Define the resources for your app

The manifest for your Modus app allows you to configure the exposure and
resources for your functions at runtime. You define the manifest in the
`modus.json` file within the root of directory of your app.

## Structure

<CardGroup cols={2}>
  <Card title="Endpoints" icon="rectangle-code" href="#endpoints">
    Expose your functions for integration into your frontend or federated API
  </Card>

  <Card title="Connections" icon="router" href="#connections">
    Establish connectivity for external endpoints and model hosts
  </Card>

  <Card title="Models" icon="cube" href="#models">
    Define inference services for use in your functions
  </Card>

  <Card title="Collections" icon="table" href="#collections">
    Define sets of text data to enable natural language search
  </Card>
</CardGroup>

### Base manifest

A simple manifest, which exposes a single GraphQL endpoint with a bearer token
for authentication, looks like this:

```json modus.json
{
  "$schema": "https://schema.hypermode.com/modus.json",
  "endpoints": {
    "default": {
      "type": "graphql",
      "path": "/graphql",
      "auth": "bearer-token"
    }
  }
}
```

## Endpoints

Endpoints make your functions available outside of your Modus app. The
`endpoints` object in the app manifest allows you to define these endpoints for
integration into your frontend or federated API.

Each endpoint requires a unique name, specified as a key containing only
alphanumeric characters and hyphens.

<Note>
  Only a GraphQL endpoint is available currently, but the modular design of
  Modus allows for the introduction of additional endpoint types in the future.
</Note>

### GraphQL endpoint

This endpoint type supports the GraphQL protocol to communicate with external
clients. You can use a GraphQL client, such as
[urql](https://github.com/urql-graphql/urql) or
[Apollo Client](https://github.com/apollographql/apollo-client), to interact
with the endpoint.

**Example:**

```json modus.json
{
  "endpoints": {
    "default": {
      "type": "graphql",
      "path": "/graphql",
      "auth": "bearer-token"
    }
  }
}
```

<ResponseField name="type" type="string" required>
  Always set to `"graphql"` for this endpoint type.
</ResponseField>

<ResponseField name="path" type="string" required>
  The path for the endpoint. Must start with a forward slash `/`.
</ResponseField>

<ResponseField name="auth" type="string" required>
  The authentication method for the endpoint. Options are `"bearer-token"` or
  `"none"`. See [Authentication](/modus/authentication) for additional details.
</ResponseField>

## Connections

Connections establish connectivity and access to external services. They're used
for HTTP and GraphQL APIs, database connections, and externally hosted AI
models. The `connections` object in the app manifest allows you to define these
hosts, for secure access from within a function.

Each connection requires a unique name, specified as a key containing only
alphanumeric characters and hyphens.

Each connection has a `type` property, which controls how it's used and which
additional properties are available. The following table lists the available
connection types:

| Type         | Purpose                          | Function Classes            |
| :----------- | :------------------------------- | :-------------------------- |
| `http`       | Connect to an HTTP(S) web server | `http`, `graphql`, `models` |
| `dgraph`     | Connect to a Dgraph database     | `dgraph`                    |
| `mysql`      | Connect to a MySQL database      | `mysql`                     |
| `neo4j`      | Connect to a Neo4j database      | `neo4j`                     |
| `postgresql` | Connect to a PostgreSQL database | `postgresql`                |

<Warning>
  **Don't include secrets directly in the manifest!**

  If your connection requires authentication, you can include *placeholders* in
  connection properties which resolve to their respective secrets at runtime.

  When developing locally,
  [set secrets using environment variables](/modus/run-locally#environment-secrets).

  When deployed on Hypermode, set the actual secrets via the Hypermode Console,
  where they're securely stored until needed.
</Warning>

### HTTP connection

This connection type supports the HTTP and HTTPS protocols to communicate with
external hosts. You can use the [HTTP APIs](/modus/sdk/assemblyscript/http) in
the Modus SDK to interact with the host.

This connection type is also used for
[GraphQL APIs](/modus/sdk/assemblyscript/graphql) and to invoke externally
hosted AI [models](/modus/sdk/assemblyscript/models).

**Example:**

```json modus.json
{
  "connections": {
    "openai": {
      "type": "http",
      "baseUrl": "https://api.openai.com/",
      "headers": {
        "Authorization": "Bearer {{API_KEY}}"
      }
    }
  }
}
```

<ResponseField name="type" type="string" required>
  Always set to `"http"` for this connection type.
</ResponseField>

<ResponseField name="baseUrl" type="string" required>
  Base URL for connections to the host. Must end with a trailing slash and may
  contain path segments if necessary.

  Example: `"https://api.example.com/v1/"`
</ResponseField>

<ResponseField name="endpoint" type="string" required>
  Full URL endpoint for connections to the host.

  Example: `"https://models.example.com/v1/classifier"`
</ResponseField>

<Note>
  You must include either a `baseUrl` or an `endpoint`, but not both.

  * Use `baseUrl` for connections to a host with a common base URL.
  * Use `endpoint` for connections to a specific URL.

  Typically, you'll use the `baseUrl` field. However, some APIs, such as
  `graphql.execute`, require the full URL in the `endpoint` field.
</Note>

<ResponseField name="headers" type="object">
  If provided, requests on the connection include these headers. Each key-value pair is a header name and value.

  Values may include variables using the `{{VARIABLE}}` template syntax, which
  resolve at runtime to environment variables provided for each connection, via
  the Hypermode Console.

  <Accordion title="Examples">
    This example specifies a header named `Authorization` that uses the `Bearer`
    scheme. A secret named `AUTH_TOKEN` provides the token:

    ```json
    "headers": {
      "Authorization": "Bearer {{AUTH_TOKEN}}"
    }
    ```

    This example specifies a header named `X-API-Key` provided by a secret named
    `API_KEY`:

    ```json
    "headers": {
      "X-API-Key": "{{API_KEY}}"
    }
    ```

    You can use a special syntax for connections that require
    [HTTP basic authentication](https://en.wikipedia.org/wiki/Basic_access_authentication).
    In this example, secrets named `USERNAME` and `PASSWORD` combined and then are
    base64-encoded to form a compliant `Authorization` header value:

    ```json
    "headers": {
      "Authorization": "Basic {{base64(USERNAME:PASSWORD)}}"
    }
    ```
  </Accordion>
</ResponseField>

<ResponseField name="queryParameters" type="object">
  If provided, requests on the connection include these query parameters, appended
  to the URL. Each key-value pair is a parameter name and value.

  Values may include variables using the `{{VARIABLE}}` template syntax, which
  resolve at runtime to secrets provided for each connection, via the Hypermode
  Console.

  <Accordion title="Example">
    This example specifies a query parameter named `key` provided by a secret named
    `API_KEY`:

    ```json
    "queryParameters": {
      "key": "{{API_KEY}}"
    }
    ```
  </Accordion>
</ResponseField>

### Dgraph connection

This connection type supports connecting to Dgraph databases. You can use the
[Dgraph APIs](/modus/sdk/assemblyscript/dgraph) in the Modus SDK to interact
with the database.

There are two ways to connect to Dgraph:

* [Using a connection string](#using-a-dgraph-connection-string) (preferred
  method)
* [Using a gRPC target](#using-a-dgraph-grpc-target) (older method)

You can use either approach in Modus, but not both.

#### Using a Dgraph connection string

This is the preferred method for connecting to Dgraph. It uses a simplified URI
based connection string to specify all options, including host, port, options,
and authentication.

**Example:**

```json modus.json
{
  "connections": {
    "my-dgraph": {
      "type": "dgraph",
      "connString": "dgraph://example.hypermode.host:443?sslmode=verify-ca&bearertoken={{DGRAPH_API_KEY}}"
    }
  }
}
```

<ResponseField name="type" type="string" required>
  Always set to `"dgraph"` for this connection type.
</ResponseField>

<ResponseField name="connString" type="string" required>
  The connection string for the Dgraph database, in URI format.
</ResponseField>

#### Using a Dgraph gRPC target

This is the older method for connecting to Dgraph. It uses a gRPC target to
specify the host and port, and a separate key for authentication. It
automatically uses SSL mode (with full CA verification) for the connection -
*except* when connecting to `localhost`.

Additional options such as username/password authentication aren't supported. If
you need to use these options, use the connection string method instead.

**Example:**

```json modus.json
{
  "connections": {
    "my-dgraph": {
      "type": "dgraph",
      "grpcTarget": "example.grpc.region.aws.cloud.dgraph.io:443",
      "key": "{{DGRAPH_API_KEY}}"
    }
  }
}
```

<ResponseField name="type" type="string" required>
  Always set to `"dgraph"` for this connection type.
</ResponseField>

<ResponseField name="grpcTarget" type="string" required>
  The gRPC target for the Dgraph database.
</ResponseField>

<ResponseField name="key" type="string" required>
  The API key for the Dgraph database.
</ResponseField>

### MySQL connection

This connection type supports connecting to MySQL databases. You can use the
[MySQL APIs](/modus/sdk/assemblyscript/mysql) in the Modus SDK to interact with
the database.

**Example:**

```json modus.json
{
  "connections": {
    "my-database": {
      "type": "mysql",
      "connString": "mysql://{{USERNAME}}:{{PASSWORD}}@db.example.com:3306/dbname?tls=true"
    }
  }
}
```

<ResponseField name="type" type="string" required>
  Always set to `"mysql"` for this connection type.
</ResponseField>

<ResponseField name="connString" type="string" required>
  The connection string for the MySQL database.

  Values may include variables using the `{{VARIABLE}}` template syntax, which
  resolve at runtime to secrets provided for each connection, via the Hypermode
  Console.

  The connection string in the preceding example includes:

  * A username and password provided by secrets named `USERNAME` & `PASSWORD`
  * A host named `db.example.com` on port `3306`
  * A database named `dbname`
  * Encryption enabled via `tls=true` - which is highly recommended for secure
    connections

  Set the connection string using a URI format
  [as described in the MySQL documentation](https://dev.mysql.com/doc/refman/8.4/en/connecting-using-uri-or-key-value-pairs.html#connecting-using-uri).

  However, any optional parameters provided should be in the form specified by the
  Go MySQL driver used by the Modus Runtime,
  [as described here](https://github.com/go-sql-driver/mysql/blob/master/README.md#parameters)

  For example, use `tls=true` to enable encryption (not `sslmode=require`).
</ResponseField>

### Neo4j connection

This connection type supports connecting to Neo4j databases. You can use the
[Neo4j APIs](/modus/sdk/assemblyscript/neo4j) in the Modus SDK to interact with
the database.

**Example:**

```json modus.json
{
  "connections": {
    "my-neo4j": {
      "type": "neo4j",
      "dbUri": "bolt://localhost:7687",
      "username": "neo4j",
      "password": "{{NEO4J_PASSWORD}}"
    }
  }
}
```

<ResponseField name="type" type="string" required>
  Always set to `"neo4j"` for this connection type.
</ResponseField>

<ResponseField name="dbUri" type="string" required>
  The URI for the Neo4j database.
</ResponseField>

<ResponseField name="username" type="string" required>
  The username for the Neo4j database.
</ResponseField>

<ResponseField name="password" type="string" required>
  The password for the Neo4j database.
</ResponseField>

### PostgreSQL connection

This connection type supports connecting to PostgreSQL databases. You can use
the [PostgreSQL APIs](/modus/sdk/assemblyscript/postgresql) in the Modus SDK to
interact with the database.

**Example:**

```json modus.json
{
  "connections": {
    "my-database": {
      "type": "postgresql",
      "connString": "postgresql://{{PG_USER}}:{{PG_PASSWORD}}@db.example.com:5432/data?sslmode=require"
    }
  }
}
```

<ResponseField name="type" type="string" required>
  Always set to `"postgresql"` for this connection type.
</ResponseField>

<ResponseField name="connString" type="string" required>
  The connection string for the PostgreSQL database.

  Values may include variables using the `{{VARIABLE}}` template syntax, which
  resolve at runtime to secrets provided for each connection, via the Hypermode
  Console.

  The connection string in the preceding example includes:

  * A username and password provided by secrets named `PG_USER` & `PG_PASSWORD`
  * A host named `db.example.com` on port `5432`
  * A database named `data`
  * SSL mode set to `require` - which is highly recommended for secure connections

  Refer to
  [the PostgreSQL documentation](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING)
  for more details on connection strings.

  <Tip>
    Managed PostgreSQL providers often provide a pre-made connection string for you
    to copy. Check your provider's documentation for details.

    For example, if using Neon, refer to the
    [Neon documentation](https://neon.tech/docs/connect/connect-from-any-app).
  </Tip>
</ResponseField>

<Tip>
  See [Running locally with secrets](/modus/run-locally#environment-secrets) for
  more details on how to set secrets for local development.
</Tip>

## Models

AI models are a core resource for inferencing. The `models` object in the app
manifest allows you to easily define models, whether hosted by Hypermode or
another host.

Each model requires a unique name, specified as a key, containing only
alphanumeric characters and hyphens.

```json modus.json
{
  "models": {
    "text-generator": {
      "sourceModel": "meta-llama/Llama-3.2-3B-Instruct",
      "provider": "hugging-face",
      "connection": "hypermode"
    }
  }
}
```

<ResponseField name="sourceModel" type="string" required>
  Original relative path of the model within the provider's repository.
</ResponseField>

<ResponseField name="provider" type="string">
  Source provider of the model. If the `connection` value is `hypermode`, this
  field is mandatory. `hugging-face` is currently the only supported option.
</ResponseField>

<ResponseField name="connection" type="string" required>
  Connection for the model instance.

  * Specify `"hypermode"` for models that [Hypermode hosts](/hosted-models).
  * Otherwise, specify a name that matches a connection defined in the
    [`connections`](#connections) section of the manifest.
</ResponseField>

<Tip>
  When using `hugging-face` as the `provider` and `hypermode` as the `connection`,
  Hypermode automatically facilitates the connection to an instance of a shared or
  dedicated instance of the model. Your project's functions securely access the
  hosted model, with no further configuration required. For more details, see
  [hosted models](/hosted-models).
</Tip>

## Collections

Collections simplify the usage of vector embeddings to build natural language
search features. The `collections` object allows you to define indexed data
types that are automatically embedded and searchable based on the search method
you define.

Each collection requires a unique name, specified as a key, containing only
alphanumeric characters and hyphens.

For more detail on implementing Collections, see [Search](/modus/search).

```json modus.json
{
  "collections": {
    "myProducts": {
      "searchMethods": {
        "searchMethod": {
          "embedder": "myEmbedder",
          "index": {
            "type": "sequential"
          }
        }
      }
    }
  }
}
```

<ResponseField name="searchMethods" type="object" required>
  Search methods define a pair of an embedder and index to make available for
  searching the data in your collection.
</ResponseField>

<ResponseField name="embedder" type="string" required>
  The function name to embed text added to the collection.
</ResponseField>

<ResponseField name="index" type="string">
  If provided, describes the index mechanism used by the search method. `type`:
  specifies the type of the index. For example, `sequential` (default).
</ResponseField>


# Architecture
Source: https://docs.hypermode.com/modus/architecture



<Warning>name the pieces and projects we’re built on</Warning>


# Authentication
Source: https://docs.hypermode.com/modus/authentication

Protect your API

It is easy to secure your Modus app with authentication. Modus currently
supports bearer token authentication, with additional authentication methods
coming soon.

## Bearer tokens

Modus supports authentication via the `Authorization` header in HTTP requests.
You can use the `Authorization` header to pass a bearer JSON Web Token (JWT) to
your Modus app. The token authenticates the user and authorize access to
resources.

To use bearer token authentication for your Modus app, be sure to set the `auth`
property on your endpoint to `"bearer-token"` in your
[app manifest](/modus/app-manifest#endpoints).

### Setting verification keys

Once set, Modus verifies tokens passed in the `Authorization` header of incoming
requests against the public keys you provide. To enable this verification, you
must pass the public keys using the `MODUS_PEMS` or `MODUS_JWKS_ENDPOINTS`
environment variable.

The value of the `MODUS_PEMS` or `MODUS_JWKS_ENDPOINTS` environment variable
should be a JSON object with the public keys as key-value pairs. This is an
example of how to set the `MODUS_PEMS` and `MODUS_JWKS_ENDPOINTS` environment
variable:

<CodeGroup>
  ```sh MODUS_PEMS
  export MODUS_PEMS='{\"key1\":\"-----BEGIN PUBLIC KEY-----\\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAwJ9z1z1z1z1z1z\\n-----END PUBLIC KEY-----\"}'
  ```

  ```sh MODUS_JWKS_ENDPOINTS
  export MODUS_JWKS_ENDPOINTS='{"my-auth-provider":"https://myauthprovider.com/application/o/myappname/.wellknown/jwks.json"}'
  ```
</CodeGroup>

<Tip>
  When deploying your Modus app on Hypermode, the bearer token authentication is
  automatically set up.
</Tip>

### Verifying tokens

To verify the token, Modus uses the public keys passed via the `MODUS_PEMS`
environment variable. If the token is verifiable with any of the verification
keys provided, Modus decodes the JWT token and passes the decoded claims as an
environment variable.

### Accessing claims

The decoded claims are available through the `auth` API in the Modus SDK.

To access the decoded claims, use the `getJWTClaims()` function. The function
allows the user to pass in a class to deserialize the claims into, and returns
an instance of the class with the claims.

This allows users to access the claims in the token and use them to authenticate
and authorize users in their Modus app.

<CodeGroup>
  ```go Go
  import github.com/hypermodeinc/modus/sdk/go/pkg/auth

  type ExampleClaims struct {
      Sub string `json:"sub"`
      Exp int64  `json:"exp"`
      Iat int64  `json:"iat"`
  }

  func GetClaims() (*ExampleClaims, error) {
      return auth.GetJWTClaims[*ExampleClaims]()
  }
  ```

  ```ts AssemblyScript
  import { auth } from "@hypermode/modus-sdk-as"

  @json
  export class ExampleClaims {
    public sub!: string
    public exp!: i64
    public iat!: i64
  }

  export function getClaims(): ExampleClaims {
    return auth.getJWTClaims<ExampleClaims>()
  }
  ```
</CodeGroup>


# Basic Functions
Source: https://docs.hypermode.com/modus/basic-functions

Implement simple functions with Modus

We built Hypermode first to make the easy things easy. Here you'll find a
collection of examples demonstrating how to implement basic functions using the
Modus framework. We designed these examples to help you get started quickly and
understand the core concepts of Modus.

## Set up

Before diving into the examples, make sure you have Modus installed and set up.
If you haven't done this yet, please refer to the
[quickstart guide](modus/quickstart).

## Basic function implementations

### Hello world

Learn how to create a simple "Hello World" function using Modus. This example
covers the basics of setting up a function, deploying it, and invoking it.

### Data processing

Explore how to implement a function that processes data. This example
demonstrates how to handle input data, perform operations, and return results.

### API integration

See how to integrate external APIs into your Modus functions. This example shows
how to make API calls, handle responses, and use the data in your functions.

### Database operations

Understand how to perform database operations with Modus. This example covers
connecting to a database, executing queries, and managing data.

## Best practices

* **Modular Code**: Keep your code modular and organized to make it easier to
  maintain and extend.
* **Error Handling**: Implement robust error handling to ensure your functions
  can gracefully handle unexpected situations.
* **Logging**: Use logging to track the execution of your functions and
  troubleshoot issues.
* **Testing**: Write tests for your functions to ensure they work as expected
  and catch potential issues early.

## Additional resources

Once you've mastered the basics, explore adding intelligence to your app with
our [AI-enabled examples](/modus/ai-enabled-apps).

We hope these examples help you get started with Modus and inspire you to build
amazing apps. Happy coding!

<Note>
  If you have any questions or need further assistance, join the discussion on
  our [community forum](https://discord.hypermode.com).
</Note>


# Changelog
Source: https://docs.hypermode.com/modus/changelog

The latest changes and improvements in Modus

Welcome to the Modus changelog! Here you'll find the latest improvements to the
Modus framework. Stay informed about what's new and what's changed to make the
most out of Modus.

Here's a short summary of the larger items shipped with each major or minor
release. For a more detailed list of changes, please refer to
[the full change log in GitHub](https://github.com/hypermodeinc/modus/blob/main/CHANGELOG.md).

## Version history

| Version | Date       | Description                                                  |
| ------- | ---------- | ------------------------------------------------------------ |
| 0.17.x  | 2025-01-24 | MySQL support, local model tracing, and OpenAI improvements. |
| 0.16.x  | 2024-12-23 | Local time and time zone support                             |
| 0.15.x  | 2024-12-13 | Neo4j support                                                |
| 0.14.x  | 2024-11-23 | Modus API Explorer + in-code documentation                   |
| 0.13.x  | 2024-10-17 | First release of Modus as an open source framework 🎉        |

Stay tuned for more updates and improvements as we continue to enhance Modus!


# Contributing
Source: https://docs.hypermode.com/modus/contributing



<Warning>- why contribute? - guidelines and process for contributing</Warning>


# Data Fetching
Source: https://docs.hypermode.com/modus/data-fetching

Pull data into your app

Modus makes it simple to fetch data from external sources. The specific data
source you're retrieving from determines the method you use to interact with it.

## Fetching from databases

### PostgreSQL

PostgreSQL is a powerful, open source relational database system. Modus provides
a simple way to interact with PostgreSQL databases with the `postgresql` APIs.

Here is an example of fetching a person from a PostgreSQL database using the
Modus SDK:

<CodeGroup>
  ```go Go
  package main

  import (
    "github.com/hypermodeinc/modus/sdk/go/pkg/postgresql"
  )

  // the name of the PostgreSQL connection, as specified in the modus.json manifest
  const connection = "my-database"

  type Person struct {
    Name string `json:"name"`
    Age  int    `json:"age"`
  }

  func GetPerson(name string) (*Person, error) {
    const query = "select * from persons where name = $1"
    rows, _, _ := postgresql.Query[Person](connection, query, name)
    return &rows[0], nil
  }
  ```

  ```ts AssemblyScript
  import { postgresql } from "@hypermode/modus-sdk-as"

  // the name of the PostgreSQL connection, as specified in the modus.json manifest
  const connection = "my-database"

  @json
  class Person {
    name!: string
    age!: i32
  }

  export function getPerson(name: string): Person {
    const query = "select * from persons where name = $1"

    const params = new postgresql.Params()
    params.push(name)

    const response = postgresql.query<Person>(connection, query, params)
    return response.rows[0]
  }
  ```
</CodeGroup>

### Dgraph

Dgraph is a distributed, transactional graph database. Modus offers an easy way
to query and mutate data in Dgraph with the `dgraph` APIs.

Here is an example of fetching a person from a Dgraph database using the Modus
SDK:

<CodeGroup>
  ```go Go
  package main

  import (
    "encoding/json"
    "github.com/hypermodeinc/modus/sdk/go/pkg/dgraph"
  )

  // the name of the Dgraph connection, as specified in the modus.json manifest
  const connection = "my-dgraph"

  // declare structures used to parse the JSON document returned by Dgraph query.
  type Person struct {
    Name string `json:"name,omitempty"`
    Age  int32  `json:"age,omitempty"`
  }

  // Dgraph returns an array of Persons
  type GetPersonResponse struct {
    Persons []*Person `json:"persons"`
  }

  func GetPerson(name string) (*Person, error) {
    statement := `query getPerson($name: string!) {
      persons(func: eq(name, $name))  {
        name
        age
      }
    }
    `
    variables := map[string]string{
      "$name": name,
    }

    response, _ := dgraph.Execute(connection, &dgraph.Request{
      Query: &dgraph.Query{
        Query:     statement,
        Variables: variables,
      },
    })

    var data GetPersonResponse
    json.Unmarshal([]byte(response.Json), &data)

    return data.Persons[0], nil
  }

  ```

  ```ts AssemblyScript
  import { dgraph } from "@hypermode/modus-sdk-as"
  import { JSON } from "json-as"

  // the name of the Dgraph connection, as specified in the modus.json manifest
  const connection: string = "my-dgraph"

  // declare classes used to parse the JSON document returned by Dgraph query.
  @json
  class Person {
    name: string = ""
    age: i32 = 0
  }

  // Dgraph returns an array of objects
  @json
  class GetPersonResponse {
    persons: Person[] = []
  }

  export function getPerson(name: string): Person {
    const statement = `  
    query getPerson($name: string) {
      persons(func: eq(name, $name))  {
          name
          age
      }
    }`

    const vars = new dgraph.Variables()
    vars.set("$name", name)

    const resp = dgraph.execute(
      connection,
      new dgraph.Request(new dgraph.Query(statement, vars)),
    )
    const persons = JSON.parse<GetPersonResponse>(resp.Json).persons
    return persons[0]
  }
  ```
</CodeGroup>

### Neo4j

Neo4j is a graph database management system. Modus provides a simple way to
query and mutate data in Neo4j with the `neo4j` APIs.

Here is an example of mutating & fetching a person from a Neo4j database using
the Modus SDK:

<CodeGroup>
  ```go Go
  package main

  import (
    "github.com/hypermodeinc/modus/sdk/go/pkg/neo4j"
  )


  const host = "my-database"

  func CreatePeopleAndRelationships() (string, error) {
    people := []map[string]any{
      {"name": "Alice", "age": 42, "friends": []string{"Bob", "Peter", "Anna"}},
      {"name": "Bob", "age": 19},
      {"name": "Peter", "age": 50},
      {"name": "Anna", "age": 30},
    }

    for _, person := range people {
      _, err := neo4j.ExecuteQuery(host,
        "MERGE (p:Person {name: $person.name, age: $person.age})",
        map[string]any{"person": person})
      if err != nil {
        return "", err
      }
    }

    for _, person := range people {
      if person["friends"] != "" {
        _, err := neo4j.ExecuteQuery(host, `
          MATCH (p:Person {name: $person.name})
                  UNWIND $person.friends AS friend_name
                  MATCH (friend:Person {name: friend_name})
                  MERGE (p)-[:KNOWS]->(friend)
        `, map[string]any{
          "person": person,
        })
        if err != nil {
          return "", err
        }
      }
    }

    return "People and relationships created successfully", nil
  }

  type Person struct {
    Name string `json:"name"`
    Age  int64  `json:"age"`
  }

  func GetAliceFriendsUnder40() ([]Person, error) {
    response, err := neo4j.ExecuteQuery(host, `
          MATCH (p:Person {name: $name})-[:KNOWS]-(friend:Person)
          WHERE friend.age < $age
          RETURN friend
          `,
      map[string]any{
        "name": "Alice",
        "age":  40,
      },
      neo4j.WithDbName("neo4j"),
    )
    if err != nil {
      return nil, err
    }

    nodeRecords := make([]Person, len(response.Records))

    for i, record := range response.Records {
      node, _ := neo4j.GetRecordValue[neo4j.Node](record, "friend")
      name, err := neo4j.GetProperty[string](&node, "name")
      if err != nil {
        return nil, err
      }
      age, err := neo4j.GetProperty[int64](&node, "age")
      if err != nil {
        return nil, err
      }
      nodeRecords[i] = Person{
        Name: name,
        Age:  age,
      }
    }

    return nodeRecords, nil
  }
  ```

  ```ts AssemblyScript
  import { neo4j } from "@hypermode/modus-sdk-as"

  // This host name should match one defined in the modus.json manifest file.
  const hostName: string = "my-database"

  @json
  class Person {
    name: string
    age: i32
    friends: string[] | null

    constructor(name: string, age: i32, friends: string[] | null = null) {
      this.name = name
      this.age = age
      this.friends = friends
    }
  }

  export function CreatePeopleAndRelationships(): string {
    const people: Person[] = [
      new Person("Alice", 42, ["Bob", "Peter", "Anna"]),
      new Person("Bob", 19),
      new Person("Peter", 50),
      new Person("Anna", 30),
    ]

    for (let i = 0; i < people.length; i++) {
      const createPersonQuery = `
        MATCH (p:Person {name: $person.name})
        UNWIND $person.friends AS friend_name
        MATCH (friend:Person {name: friend_name})
        MERGE (p)-[:KNOWS]->(friend)
      `
      const peopleVars = new neo4j.Variables()
      peopleVars.set("person", people[i])
      const result = neo4j.executeQuery(hostName, createPersonQuery, peopleVars)
      if (!result) {
        throw new Error("Error creating person.")
      }
    }

    return "People and relationships created successfully"
  }

  export function GetAliceFriendsUnder40(): Person[] {
    const vars = new neo4j.Variables()
    vars.set("name", "Alice")
    vars.set("age", 40)

    const query = `
      MATCH (p:Person {name: $name})-[:KNOWS]-(friend:Person)
          WHERE friend.age < $age
          RETURN friend
    `

    const result = neo4j.executeQuery(hostName, query, vars)
    if (!result) {
      throw new Error("Error getting friends.")
    }

    const personNodes: Person[] = []

    for (let i = 0; i < result.Records.length; i++) {
      const record = result.Records[i]
      const node = record.getValue<neo4j.Node>("friend")
      const person = new Person(
        node.getProperty<string>("name"),
        node.getProperty<i32>("age"),
      )
      personNodes.push(person)
    }

    return personNodes
  }
  ```
</CodeGroup>

## Fetching from APIs

### HTTP

HTTP protocols underpin RESTful APIs with OpenAPI schemas. Modus provides a
convenient way to interact with any external HTTP API using the `http` APIs in
the Modus SDK.

Here is an example of fetching a person from an HTTP API using the Modus SDK:

<CodeGroup>
  ```go Go
  package main

  import (
    "encoding/json"
    "fmt"
    "github.com/hypermodeinc/modus/sdk/go/pkg/http"
  )

  // declare structures used to parse the JSON document returned by the REST API
  type Person struct {
    Name string    `json:"name,omitempty"`
    Age  int32     `json:"age,omitempty"`
  }

  func GetPerson(name string) (*Person, error) {
    url := fmt.Sprintf("https://example.com/api/person?name=%s", name)
    response, _ := http.Fetch(url)

    // The API returns Person object as JSON
    var person Person
    response.JSON(&person)

    return person, nil
  }

  ```

  ```ts AssemblyScript
  import { http } from "@hypermode/modus-sdk-as"

  @json
  class Person {
    name: string = ""
    age: i32 = 0
  }

  export function getPerson(name: string): Person {
    const url = `https://example.com/api/people?name=${name}`

    const response = http.fetch(url)

    // the API returns Person object as JSON
    return response.json<Person>()
  }
  ```
</CodeGroup>

### GraphQL

GraphQL is a data-centric query language for APIs that allows you to fetch only
the data you need. With the `graphql` APIs in the Modus SDK, you can easily
fetch data from any GraphQL endpoint.

Here is an example of fetching a person from a GraphQL API using the Modus SDK:

<CodeGroup>
  ```go Go
  import (
    "encoding/json"
    "github.com/hypermodeinc/modus/sdk/go/pkg/graphql"
  )

  // the name of the GraphQL connection, as specified in the modus.json manifest
  const connection = "my-graphql-api"

  // declare structures used to parse the JSON document returned
  type Person struct {
    Name string    `json:"name,omitempty"`
    Age  int32     `json:"age,omitempty"`
  }

  type GetPersonResponse struct {
    Person *Person `json:"getPerson"`
  }

  func GetPerson(name string) (*Person, error) {
    statement := `query getPerson($name: String!) {
        getPerson(name: $name) {
          age
          name
        }
      }`

    vars := map[string]any{
       "name": name,
    }

    response, _ := graphql.Execute[GetPersonResponse](connection, statement, vars)

    return response.Data.Person, nil
  }
  ```

  ```ts AssemblyScript
  import { graphql } from "@hypermode/modus-sdk-as"

  // the name of the GraphQL connection, as specified in the modus.json manifest
  const connection: string = "my-graphql-api"

  // declare classes used to parse the JSON document returned
  @json
  class Person {
    name: string = ""
    age: i32 = 0
  }
  @json
  class GetPersonResponse {
    getPerson: Person | null
  }

  export function getPerson(name: string): Person | null {
    const statement = `
      query getPerson($name: String!) {
        getPerson(name: $name) {
          age
          name
        }
      }`

    const vars = new graphql.Variables()
    vars.set("name", name)

    const response = graphql.execute<GetPersonResponse>(
      connection,
      statement,
      vars,
    )

    return response.data!.getPerson
  }
  ```
</CodeGroup>


# Using DeepSeek
Source: https://docs.hypermode.com/modus/deepseek-model

Use the DeepSeek-R1 Model with your Modus app

`DeepSeek-R1` is an open source AI reasoning model that rivals the performance
of frontier models such as OpenAI's o1 in complex reasoning tasks like math and
coding. Benefits of DeepSeek include:

* **Performance**: `DeepSeek-R1` achieves comparable results to OpenAI's o1
  model on several benchmarks.
* **Efficiency**: The model uses significantly fewer parameters and therefore
  operates at a lower cost relative to competing frontier models.
* **Open Source**: The open source license allows both commercial and
  non-commercial usage of the model weights and associated code.
* **Novel training approach**: The research team developed DeepSeek-R1 through a
  multi-stage approach that combines reinforcement learning, fine-tuning, and
  data distillation.
* **Distilled versions**: The DeepSeek team released smaller, distilled models
  based on DeepSeek-R1 that offer high reasoning capabilities with fewer
  parameters.

In this guide we review how to use the `DeepSeek-R1` model in your Modus app.

## Options for using DeepSeek with Modus

There are two options for using `DeepSeek-R1` in your Modus app:

1. [Use the distilled `DeepSeek-R1` model hosted by Hypermode](#using-the-distilled-deepseek-model-hosted-by-hypermode)
   Hypermode hosts and makes available the distilled DeepSeek model based on
   `Llama-3.1-8B` enabling Modus apps to use it in both local development
   environments and deployed apps.

2. [Use the DeepSeek Platform API with your Modus app](#using-the-deepseek-platform-api-with-modus)
   Access DeepSeek models hosted on the DeepSeek platform by configuring a
   DeepSeek connection in your Modus app and using your DeepSeek API key

## Using the distilled DeepSeek model hosted by Hypermode

The open source `DeepSeek-R1-Distill-Llama-8B` DeepSeek model is available on
Hypermode as a [shared model](/hosted-models#shared-models). This means that we
can invoke this model in a Modus app in both a local development environment and
also in an app deployed on Hypermode.

The `DeepSeek-R1-Distill-Llama-8B` model is a distilled version of the
DeepSeek-R1 model which has been fine-tuned using the `Llama-3.1-8B` model as a
base model, using samples generated by DeepSeek-R1.

Distilled models offer similar high reasoning capabilities with fewer
parameters.

<iframe
  src="https://www.youtube.com/embed/ICRwZ8ywR9Q"
  title="DeepSeek + Modus"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write;
encrypted-media; gyroscope; picture-in-picture"
  allowfullscreen
  style={{ aspectRatio: "16 / 9", width: "100%" }}
/>

<Steps>
  <Step title="Create a Modus app">
    If you haven't already, create a new modus app. Skip this step if you already
    have a Modus app.

    ```sh
    modus new
    ```

    <Tip>
      See [the Modus Quickstart](quickstart) for more information about creating
      Modus projects.
    </Tip>
  </Step>

  <Step title="Add the DeepSeek model to your app manifest">
    Update your Modus app manifest `modus.json` file to specify the
    `DeepSeek-R1-Distill-Llama-8B` model hosted on Hypermode.

    ```json modus.json
      "models": {
        "deepseek-reasoner": {
          "sourceModel": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
          "provider": "hugging-face",
          "connection": "hypermode"
        }
      },
    ```

    <Note>
      Note that we named the model `deepseek-reasoner` in our app manifest, which we
      use to access the model in our Modus function.
    </Note>
  </Step>

  <Step title="Use the Hyp CLI to sign in to Hypermode">
    To use Hypermode hosted models in our local development environment we use the
    `hyp` CLI to log in to Hypermode.

    Install the `hyp` CLI if not previously installed.

    ```sh
    npm i -g @hypermode/hyp-cli
    ```

    Log into Hypermode using the command:

    ```sh
    hyp login
    ```

    If signing in for the first time, Hypermode prompts to create an account and
    specify an organization.
  </Step>

  <Step title="Write a function to invoke the model">
    You can now invoke the model in your Modus app's functions using the Modus
    models interface.

    Here we write a function that takes a prompt as input, invokes the DeepSeek
    model, and returns the generated text as output. At runtime this function
    becomes a GraphQL Query field in the GraphQL API generated by Modus.

    <CodeGroup>
      ```go Go
      package main

      import (
         "strings"

         "github.com/hypermodeinc/modus/sdk/go/pkg/models"
         "github.com/hypermodeinc/modus/sdk/go/pkg/models/openai"
      )

      func GenerateText(prompt string) (string, error) {

         model, err := models.GetModel[openai.ChatModel]("deepseek-reasoner")
         if err != nil {
             return "", err
         }

         // DeepSeek recommends not using a system prompt and including all instructions in the user prompt
         input, err := model.CreateInput(
             openai.NewUserMessage(prompt),
         )
         if err != nil {
             return "", err
         }

         // DeepSeek recommends setting temperature within the range of 0.5-0.7 with 0.6 the recommended default
         input.Temperature = 0.6

         // Here we invoke the model with the input we created.
         output, err := model.Invoke(input)
         if err != nil {
             return "", err
         }

         // The output is also specific to the ChatModel interface.
         // Here we return the trimmed content of the first choice.
         return strings.TrimSpace(output.Choices[0].Message.Content), nil
      }
      ```

      ```ts AssemblyScript
      import { models } from "@hypermode/modus-sdk-as"

      import {
        OpenAIChatModel,
        SystemMessage,
        UserMessage,
      } from "@hypermode/modus-sdk-as/models/openai/chat"

      export function generateText(prompt: string): string {
        const model = models.getModel<OpenAIChatModel>("deepseek-reasoner")

        // DeepSeek recommends not using a system prompt and including all instructions in the user prompt
        const input = model.createInput([new UserMessage(prompt)])

        // DeepSeek recommends setting temperature within the range of 0.5-0.7 with 0.6 the recommended default
        input.temperature = 0.6

        // Here we invoke the model with the input we created.
        const output = model.invoke(input)

        // The output is also specific to the OpenAIChatModel interface.
        // Here we return the trimmed content of the first choice.
        return output.choices[0].message.content.trim()
      }
      ```
    </CodeGroup>

    <Info>
      DeepSeek recommends not using a system prompt with DeepSeek-R1 and setting the
      `temperature` parameter in the range of 0.5-0.7
    </Info>
  </Step>

  <Step title="Run your Modus app">
    Run your Modus app locally:

    ```sh
    modus dev
    ```

    This command compiles your Modus app and starts a local GraphQL API endpoint.
  </Step>

  <Step title="Query your function in the Modus API Explorer">
    Open the Modus API Explorer in your web browser at
    `http://localhost:8686/explorer`.

    Add your prompt as an input argument for the `generateText` query field and
    select "Run" to invoke the DeepSeek model.

    ![Querying the DeepSeek model using the Modus API Explorer](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/how-to-guides/deepseek/api-explorer-distilled-model.png)

    <Tip>
      For mathematical problems,use a directive in your prompt such as: "Please
      reason step by step, and include your final answer within `\boxed{}.`"
    </Tip>
  </Step>
</Steps>

This example demonstrated how to use the DeepSeek-R1 Distilled Model hosted by
Hypermode in a Modus app to create an endpoint that returns text generated by
the DeepSeek model. More advanced use cases for leveraging the DeepSeek
reasoning models in your Modus app include workflows like tool use / function
calling, generating structured outputs, problem solving, code generation, and
much more. [Let us know](mailto:hello@hypermode.com?subject=DeepSeek+Modus) how
you're leveraging DeepSeek with Modus.

## Using the DeepSeek platform API with Modus

This option involves using the DeepSeek models hosted by DeepSeek Platform with
your Modus app. You'll need to create an account with DeepSeek Platform and pay
for your model usage.

<Steps>
  <Step title="Create a DeepSeek API key">
    Create an account with [DeepSeek Platform](https://platform.deepseek.com).

    Once you've signed in select the "API keys" tab and "Create new API token" to
    generate your DeepSeek Platform API token.

    ![DeepSeek Platform API tokens](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/how-to-guides/deepseek/deepseek-platform-api-token.png)
  </Step>

  <Step title="Create a Modus app">
    If you haven't already, create a new modus app. Skip this step if you already have a Modus app.

    ```sh
    modus new
    ```

    <Tip>
      See [the Modus Quickstart](quickstart) for more information about creating
      Modus projects.
    </Tip>
  </Step>

  <Step title="Define the model and connection in your app manifest">
    Update your Modus app's `modus.json` app manifest file to include the DeepSeek
    model and a connection for the DeepSeek Platform API. Use `deepseek-reasoner` as
    the value for `sourceModel` for the DeepSeek-R1 reasoning model and
    `deepseek-chat` for the DeepSeek-V3 model.

    ```json modus.json
    {
      "models": {
        "deepseek-reasoner": {
          "sourceModel": "deepseek-reasoner",
          "connection": "deepseek",
          "path": "v1/chat/completions"
        }
      },
      "connections": {
        "deepseek": {
          "type": "http",
          "baseUrl": "https://api.deepseek.com/",
          "headers": {
            "Authorization": "Bearer {{API_TOKEN}}"
          }
        }
      }
    }
    ```

    <Note>
      At query time, Modus replaces the `{{API_TOKEN}}`secret placeholder with your `MODUS_DEEPSEEK_API_TOKEN` environment variable value.

      Set this environment variable value in the next step.
    </Note>
  </Step>

  <Step title="Create environment variable for your API token">
    Edit the `.env.dev.local` file to declare an environment variable for your DeepSeek Platform API key.

    ```env
    MODUS_DEEPSEEK_API_TOKEN=<YOUR_TOKEN_VALUE_HERE>
    ```

    <Note>
      Modus namespaces environment variables for secrets placeholders with `MODUS` and
      your connection's name from the app manifest. For more details on using secrets
      in Modus, refer to
      [working locally with secrets](/modus/app-manifest#working-locally-with-secrets).
    </Note>
  </Step>

  <Step title="Write a function to invoke the DeepSeek model">
    You can now invoke the model in your Modus app's functions using the Modus
    models interface.

    Here we write a function that takes a prompt as input, invokes the DeepSeek
    model, and returns the generated text as output. At runtime this function
    becomes a GraphQL Query field in the GraphQL API generated by Modus.

    <CodeGroup>
      ```go Go
          package main

          import (
              "strings"

              "github.com/hypermodeinc/modus/sdk/go/pkg/models"
              "github.com/hypermodeinc/modus/sdk/go/pkg/models/openai"
          )

          func GenerateText(prompt string) (string, error) {

              model, err := models.GetModel[openai.ChatModel]("deepseek-reasoner")
              if err != nil {
                  return "", err
              }

              // DeepSeek recommends not using a system prompt and including all instructions in the user prompt
              input, err := model.CreateInput(
                  openai.NewUserMessage(prompt),
              )
              if err != nil {
                  return "", err
              }

              // DeepSeek recommends setting temperature within the range of 0.5-0.7 with 0.6 the recommended default
              input.Temperature = 0.6

              // Here we invoke the model with the input we created.
              output, err := model.Invoke(input)
              if err != nil {
                  return "", err
              }

              // The output is also specific to the ChatModel interface.
              // Here we return the trimmed content of the first choice.
              return strings.TrimSpace(output.Choices[0].Message.Content), nil
          }
      ```

      ```ts AssemblyScript
      import { models } from "@hypermode/modus-sdk-as"

      import {
        OpenAIChatModel,
        SystemMessage,
        UserMessage,
      } from "@hypermode/modus-sdk-as/models/openai/chat"

      export function generateText(prompt: string): string {
        const model = models.getModel<OpenAIChatModel>("deepseek-reasoner")

        // DeepSeek recommends not using a system prompt and including all instructions in the user prompt
        const input = model.createInput([new UserMessage(prompt)])

        // DeepSeek recommends setting temperature within the range of 0.5-0.7 with 0.6 the recommended default
        input.temperature = 0.6

        // Here we invoke the model with the input we created.
        const output = model.invoke(input)

        // The output is also specific to the OpenAIChatModel interface.
        // Here we return the trimmed content of the first choice.
        return output.choices[0].message.content.trim()
      }
      ```
    </CodeGroup>

    <Info>
      DeepSeek recommends not using a system prompt with DeepSeek-R1 and setting the
      `temperature` parameter in the range of 0.5-0.7
    </Info>
  </Step>

  <Step title="Run your Modus app">
    Run your Modus app locally:

    ```sh
    modus dev
    ```

    This command compiles your Modus app and starts a local GraphQL API endpoint.
  </Step>

  <Step title="Query using the Modus API Explorer">
    Open the Modus API Explorer in your web browser at `http://localhost:8686/explorer`.

    Add your prompt as an input argument for the `generateText` query field and
    select "Run" to invoke the DeepSeek model.

    ![Querying the DeepSeek model using the Modus API Explorer](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/how-to-guides/deepseek/api-explorer-deepseek-api.png)

    <Tip>
      For mathematical problems,use a directive in your prompt such as: "Please
      reason step by step, and include your final answer within `\boxed{}.`"
    </Tip>
  </Step>
</Steps>

This example demonstrated how to use the DeepSeek Platform API in a Modus app to
create an endpoint that returns text generated by the DeepSeek-R1 model. More
advanced use cases for leveraging the DeepSeek reasoning models in your Modus
app include workflows like tool use / function calling, generating structured
outputs, problem solving, code generation, and much more.
[Let us know](mailto:hello@hypermode.com?subject=DeepSeek+Modus) how you're
leveraging DeepSeek with Modus.

## Resources

* [DeepSeek on Hugging Face](https://huggingface.co/deepseek-ai)
* [`DeepSeek-R1-Distill-Llama-8B` model card](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)
* [DeepSeek-R1 paper](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf)
* [DeepSeek Platform API docs](https://api-docs.deepseek.com/)


# Deploying
Source: https://docs.hypermode.com/modus/deploying



<Warning>- CI/CD integration - how to deploy to Hypermode</Warning>


# Error Handling
Source: https://docs.hypermode.com/modus/error-handling

Easily debug and handle errors

Error handling in Modus makes it simple to have both a clear debugging process
for developers and informative responses for end-users.

### Logging errors

The Console API in the Modus SDK is globally available in all functions. It
allows developers to capture log messages and errors, which are automatically
included in Modus runtime execution logs.

<Tip>
  When deployed to Hypermode, function logs are available in the Hypermode
  Console on the [Function Runs](../observe-functions#function-runs) page.
</Tip>

The Console API provides five logging functions:

```go
console.log("This is a simple log message.")
console.debug("This is a debug message.")
console.info("This is an info message.")
console.warn("This is a warning message.")
console.error("This is an error message.")
```

### Error reporting in GraphQL

GraphQL responses have a standard structure that includes both `data` and
`errors` sections. Modus allows for the inclusion of error codes or messages
within the `errors` section to allow for downstream processing in addition to
debugging.

<CodeGroup>
  ```ts Go
  // in Go, error handling is typically done by returning an `error` object as the
  // last result of a function; Modus transforms this into logging and error handling
  // automatically, ensuring all errors are logged before being sent to the response
  func TestError(input string) (string, error) {
      if input == "" {
          return "", errors.New("input is empty")
      }
      return "You said: " + input, nil
  }
  ```

  ```ts AssemblyScript
  export function TestError(input: string): string {
    if (input == "") {
      console.error("input is empty")
      // this is a non-fatal error reported in GraphQL response.
      // the function continues
      return "Can't process your input"
    }

    if (input == "error") {
      throw new Error(`This is a fatal error.`)
      // a fatal error appears in the log
      // the errors section in GraphQL responses contains only one entry with
      // "message": "error calling function" and the data section is empty
      // the function does not continue
    }

    return "You said: " + input
  }
  ```
</CodeGroup>

### Best practices for error handling

* **Handle non-fatal errors** using `console.error`, allowing the function to
  continue. The GraphQL response may have both a `data` section and an `errors`
  section.

* **Handle critical errors** using AssemblyScript `throw` keyword or the Go
  idiomatic error object to stop the function's execution.

* **Return clear error messages**: When returning errors, include concise and
  informative error messages that help diagnose the issue.


# Model Invoking
Source: https://docs.hypermode.com/modus/model-invoking

Invoke your models with the Modus Models API

Modus enables you to easily integrate AI models into your app. In just a few
steps, you can generate text, classify items, compute embeddings, and use models
in your app for many other use cases using the `models` API in the Modus SDK.

## Understanding key components

**Models**: your app can invoke models hosted on Hypermode, OpenAI, Anthropic,
and many more. You define models in your app manifest.

**Models API**: the `models` API in the Modus SDK provides a set of functions
that you can import and call from your app.

## Define your models

You define models in your [app manifest](./app-manifest#models). Here are some
examples:

<CodeGroup>
  ```json Llama-3.1-8B-Instruct {4-9}
  {
    ...
    "models": {
      // model card: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct
      "text-generator": {
        "sourceModel": "meta-llama/Llama-3.2-3B-Instruct", // model name on the provider
        "provider": "hugging-face", // provider for this model
        "connection": "hypermode" // host where the model is running
      }
    }
    ...
  }
  ```

  ```json GPT-4o {4-9,13-19}
  {
    ...
    "models": {
      // model docs: https://platform.openai.com/docs/models/gpt-4o
      "text-generator": {
        "sourceModel": "gpt-4o",
        "connection": "openai",
        "path": "v1/chat/completions"
      }
    },
    // for externally hosted models, explicitly define the connection
    "connections": {
      "openai": {
        "type": "http",
        "baseUrl": "https://api.openai.com/",
        "headers": {
          "Authorization": "Bearer {{API_KEY}}"
        }
      }
    }
    ...
  }
  ```

  ```json Claude 3.5 Sonnet {4-9,13-20}
  {
    ...
    "models": {
      // model docs: https://docs.anthropic.com/en/docs/about-claude/models#model-comparison-table
      "text-generator": {
        "sourceModel": "claude-3-5-sonnet-20240620",
        "connection": "anthropic",
        "path": "v1/messages"
      }
    },
    // for externally hosted models, explicitly define the connection
    "connections": {
      "anthropic": {
        "type": "http",
        "baseUrl": "https://api.anthropic.com/",
        "headers": {
          "x-api-key": "{{API_KEY}}",
          "anthropic-version": "2023-06-01"
        }
      }
    }
    ...
  }
  ```
</CodeGroup>

## Invoking a model for inference

To invoke a model within your app, import the `models` packages from the SDK.
Import the core `models` package and the package for the interface your model
uses. For example, to use the OpenAI interface for a text-generation model, you
would import the `openai` package in addition to the core `models` package.

### Generation models

Generation models are models that generate text, images, or other data based on
input. Currently, the Models API supports the OpenAI, Anthropic, and Gemini
interfaces. Let's see how to invoke a model using the OpenAI interface.

When using a model interface, you automatically get type-ahead guidance in your
code editor based on the available options for that interface.

<Note>
  Hypermode-hosted generation models implement the OpenAI API standard. To
  interact with these models, use the `openai` interface.
</Note>

<CodeGroup>
  ```go Go
  package main

  import (
      "encoding/json"
      "fmt"
      "strings"

      "github.com/hypermodeinc/modus/sdk/go/pkg/models"
      "github.com/hypermodeinc/modus/sdk/go/pkg/models/openai"
  )

  // this model name should match the one defined in the modus.json manifest file
  const modelName = "text-generator"

  func GenerateText(instruction, prompt string) (string, error) {
      model, err := models.GetModel[openai.ChatModel](modelName)
      if err != nil {
          return "", err
      }

      input, err := model.CreateInput(
          openai.NewSystemMessage(instruction),
          openai.NewUserMessage(prompt),
      )
      if err != nil {
          return "", err
      }

      // this is one of many optional parameters available for the OpenAI chat interface
      input.Temperature = 0.7

      output, err := model.Invoke(input)
      if err != nil {
          return "", err
      }

      return strings.TrimSpace(output.Choices[0].Message.Content), nil
  }
  ```

  ```ts AssemblyScript
  import { models } from "@hypermode/modus-sdk-as"
  import {
    OpenAIChatModel,
    ResponseFormat,
    SystemMessage,
    UserMessage,
  } from "@hypermode/modus-sdk-as/models/openai/chat"

  // this model name should match the one defined in the modus.json manifest file
  const modelName: string = "text-generator"

  export function generateText(instruction: string, prompt: string): string {
    const model = models.getModel<OpenAIChatModel>(modelName)
    const input = model.createInput([
      new SystemMessage(instruction),
      new UserMessage(prompt),
    ])

    // this is one of many optional parameters available for the OpenAI chat interface
    input.temperature = 0.7

    const output = model.invoke(input)
    return output.choices[0].message.content.trim()
  }
  ```
</CodeGroup>

### Classification models

Classification models provide a label for input data. You can use these models
to sort data into categories or classes. Let's see how to invoke a
classification model.

<CodeGroup>
  ```go Go

  import (
    "errors"
    "fmt"

    "github.com/hypermodeinc/modus/sdk/go/pkg/models"
    "github.com/hypermodeinc/modus/sdk/go/pkg/models/experimental"
  )

  // this model name should match the one defined in the modus.json manifest file
  const modelName = "my-classifier"

  // this function takes input text and a probability threshold, and returns the
  // classification label determined by the model, if the confidence is above the
  // threshold; otherwise, it returns an empty string
  func ClassifyText(text string, threshold float32) (string, error) {
    predictions, err := classify(text)
    if err != nil {
      return "", err
    }

    prediction := predictions[0]
    if prediction.Confidence < threshold {
      return "", nil
    }

    return prediction.Label, nil
  }
  ```

  ```ts AssemblyScript
  import { models } from "@hypermode/modus-sdk-as"
  import {
    ClassificationModel,
    ClassifierResult,
  } from "@hypermode/modus-sdk-as/models/experimental/classification"

  // this model name should match the one defined in the modus.json manifest file
  const modelName: string = "my-classifier"

  // this function takes input text and a probability threshold, and returns the
  // classification label determined by the model, if the confidence is above the
  // threshold; otherwise, it returns an empty string
  export function classifyText(text: string, threshold: f32): string {
    const model = models.getModel<ClassificationModel>(modelName)
    const input = model.createInput([text])
    const output = model.invoke(input)

    const prediction = output.predictions[0]
    if (prediction.confidence >= threshold) {
      return prediction.label
    }

    return ""
  }
  ```
</CodeGroup>

### Embedding models

Modus supports invoking embedding models for text, images, and other data types.
You use the outputs of these models for implementing search, recommendation, and
similarity functions in your app. Refer to [Search](/modus/search) for more
information on how to use embeddings in your app.


# Modus CLI
Source: https://docs.hypermode.com/modus/modus-cli

Comprehensive reference for the Modus CLI commands and usage

The Modus CLI is a command-line tool for interacting with your Modus app and
running it locally.

## Install

Install Modus CLI via npm.

```sh
npm install -g @hypermode/modus-cli
```

<Info>
  The Modus CLI automatically downloads various files and dependencies to the
  following directory:

  * Linux/macOS: `$HOME/.modus`
  * Windows: `%USERPROFILE%/.modus`

  If you would like to override the location of this directory, you can set the
  `MODUS_HOME` environment variable to the desired path.

  For example, if your local permissions on Windows don't allow creating a
  directory in the root of your user profile, you can try a different location.
  You can use the `AppData` directory, or any other directory where you have write
  permissions.

  ```cmd
  setx MODUS_HOME %APPDATA%\Modus
  ```

  *Note, the Windows `setx` command makes the environment variable setting
  permanent.*
</Info>

## Commands

### `new`

Initialize a new Modus app. The Modus CLI prompts you to enter the app name and
language of choice.

### `dev`

Run your Modus app locally. The Modus CLI starts a local server and provides a
URL to access the app.

<Tip>
  When using [Hyp CLI](/hyp-cli) alongside the Modus CLI, users get access to
  [Hypermode-hosted models](/hosted-models) for local development.
</Tip>

### `build`

Build your Modus app. The Modus CLI compiles your app and generates a `.build`
folder for the artifacts.

### `uninstall`

Uninstall the Modus CLI from your system.


# Modus
Source: https://docs.hypermode.com/modus/overview

Welcome to the Modus docs!

## What is Modus? {/* vale Google.Contractions = NO */}

Modus is an open source, serverless framework for building functions and APIs,
powered by WebAssembly.

You write your app logic in Go or AssemblyScript, and Modus provides additional
features to easily integrate models, data, and external services.

To **build apps that are thoughtful, fun, and effective**, we often need to
integrate models in different forms, whether generative large language models or
classical machine/deep learning models.

Your app might be a simple create, read, update, and delete (CRUD) function that
has a single model to identify similar entries. Or, it could be a complex
agentic reasoning system that chains dozens of models and data sources together.
**Modus creates a way of working with models that scales with your needs.**

Modus exists to make it easier for you to build the apps of your dreams.

<Note>
  Modus is a multi-language framework. It currently includes support for Go and
  AssemblyScript, a WebAssembly compatible TypeScript-like language. Additional
  language support is in development.
</Note>

## What is Modus good for? {/* vale Google.Contractions = NO */}

We designed Modus primarily as a general-purpose app framework, it just happens
to treat models as a first-class component. With Modus you can use models, as
appropriate, without additional complexity.

However, Modus is best for apps that require sub-second response times. We've
made trade-offs to optimize for speed and simplicity.

For more inspiration, check out the
[Modus recipes](https://github.com/hypermodeinc/modus-recipes).

## Main features

A few of the core Modus features include:

| Feature                                     | Description                                                                                  |
| ------------------------------------------- | -------------------------------------------------------------------------------------------- |
| [Multi-Language](/modus/project-structure)  | Write functions in Go and AssemblyScript, with additional language support in development    |
| [Auto-Generated API](/modus/api-generation) | A secure API is automatically generated from your function signatures                        |
| [Model Integration](/modus/model-invoking)  | Connect and invoke AI models from different providers, without learning a new SDK            |
| [Search](/modus/search)                     | Add natural language search and recommendations with integrated vector embeddings            |
| [Authentication](/modus/authentication)     | Secure your API endpoints with minimal configuration                                         |
| **WebAssembly Runtime**                     | Small and portable execution engine for deployment across server, edge, and desktop computes |


# Project Structure
Source: https://docs.hypermode.com/modus/project-structure

Understand the structure of a Modus app

{/* vale Google.Passive = NO */}

A Modus app is organized into a set of files and directories that define the
structure of your app. This structure is important for maintaining and scaling
your app as it grows.

{/* vale Google.Passive = YES */}

## Structure

Modus fits within the natural language structure of your app code, with
configuration separated from your source code.

<CodeGroup>
  ```sh Go
  .
  ├── main.go
  ├── ...
  ├── modus.json
  ├── go.mod
  └── go.sum
  ```

  ```sh AssemblyScript
  .
  ├── assembly
      ├── index.ts
      ├── ...
      └── tsconfig.json
  ├── modus.json
  ├── asconfig.json
  ├── package.json
  └── package-lock.json
  ```
</CodeGroup>

## App manifest

The `modus.json` [app manifest](/modus/app-manifest) is the central
configuration file for your Modus app. It defines the endpoints, connections,
models, and collections that your code has available to it during runtime.
Because Modus is a secure-by-default framework, only the resources defined in
this file are accessible to your app.

## Initializing your app

When you initialize your app with the `modus new` command, the Modus CLI
scaffolds your app with the necessary files and directories.


# Quickstart
Source: https://docs.hypermode.com/modus/quickstart

Run your first Modus app in a few minutes

In this quickstart we'll show you how to get set up with Modus and its CLI and
build a simple app that fetches a random quote from an external API. You'll
learn how to use the basic components of a Modus app and how to run it locally.

## Prerequisites

* [Node.js](https://nodejs.org/en/download/package-manager) - v22 or higher
* Text editor - we recommend [VS Code](https://code.visualstudio.com/)
* Terminal - access Modus through a command-line interface (CLI)

## Building your first Modus app

<Steps>
  <Step title="Install the Modus CLI">
    The Modus CLI provides a set of commands to help you create, build, and run your Modus apps.
    Install the CLI using npm.

    ```sh
    npm install -g @hypermode/modus-cli
    ```
  </Step>

  <Step title="Initialize your Modus app">
    To create a new Modus app, run the following command in your terminal:

    ```sh
    modus new
    ```

    This command prompts you to choose between Go and AssemblyScript as the language for your app. It
    then creates a new directory with the necessary files and folders for your app. You will also be asked if you would like to initialize a Git repository.
  </Step>

  <Step title="Build and run your app">
    To build and run your app, navigate to the app directory and run the following command:

    ```sh
    modus dev
    ```

    This command runs your app locally in development mode and provides you with a URL to access your
    app's generated API.
  </Step>

  <Step title="Access your local endpoint">
    Once your app is running, you can access the graphical interface for your API at the URL located in your terminal.

    ```sh
    View endpoint: http://localhost:8686/explorer
    ```

    The API Explorer interface allows you to interact with your app's API and test your functions.

    <img className="block" src="https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/api-explorer.png" alt="API Graphical Interface." />
  </Step>

  <Step title="Add a connection">
    Modus is a secure-by-default framework. To connect to external services, you need to add a connection
    in your app manifest.

    Add the following code into your `modus.json` manifest file:

    ```json modus.json
    {
      "connections": {
        "zenquotes": {
          "type": "http",
          "baseUrl": "https://zenquotes.io/"
        }
      }
    }
    ```
  </Step>

  <Step title="Add a function">
    Functions are the building blocks of your app. Let's add a function that fetches a random quote from
    the ZenQuotes connection you just created.

    <Tabs>
      <Tab title="Go">
        To add a function, create a new file in the root directory with the following code:

        ```go quotes.go
        package main

        import (
          "errors"
          "fmt"

          "github.com/hypermodeinc/modus/sdk/go/pkg/http"
        )

        type Quote struct {
          Quote  string `json:"q"`
          Author string `json:"a"`
        }

        // this function makes a request to an API that returns data in JSON format, and
        // returns an object representing the data
        func GetRandomQuote() (*Quote, error) {
          request := http.NewRequest("https://zenquotes.io/api/random")

          response, err := http.Fetch(request)
          if err != nil {
            return nil, err
          }
          if !response.Ok() {
            return nil, fmt.Errorf("Failed to fetch quote. Received: %d %s", response.Status, response.StatusText)
          }

          // the API returns an array of quotes, but we only want the first one
          var quotes []Quote
          response.JSON(&quotes)
          if len(quotes) == 0 {
            return nil, errors.New("expected at least one quote in the response, but none were found")
          }
          return &quotes[0], nil
        }
        ```
      </Tab>

      <Tab title="AssemblyScript">
        To add a function, create a new file in the `assembly` directory with the following code:

        ```ts quotes.ts
        import { http } from "@hypermode/modus-sdk-as";

        @json
        class Quote {
          @alias("q")
          quote!: string;

          @alias("a")
          author!: string;
        }

        // this function makes a request to an API that returns data in JSON format, and
        // returns an object representing the data
        export function getRandomQuote(): Quote {
          const request = new http.Request("https://zenquotes.io/api/random");

          const response = http.fetch(request);
          if (!response.ok) {
            throw new Error(
              `Failed to fetch quote. Received: ${response.status} ${response.statusText}`,
            );
          }

          // the API returns an array of quotes, but we only want the first one
          return response.json<Quote[]>()[0];
        }
        ```

        Then add the following to `index.ts`. This includes the `getRandomQuote` function on
        your generated API.

        ```ts index.ts
        export * from "./quotes";
        ```
      </Tab>
    </Tabs>

    After adding your function, you can use the API Explorer interface to test the `GetRandomQuote` function.
  </Step>

  <Step title="Add a model">
    Modus also supports AI models. You can define new models in your `modus.json` file. Let's add a new meta-llama model:

    ```json
      "models": {
        "text-generator": {
          "sourceModel": "meta-llama/Llama-3.2-3B-Instruct",
          "provider": "hugging-face",
          "connection": "hypermode"
        }
      },
    ```
  </Step>

  <Step title="Install the Hyp CLI and log in">
    Next, install the Hyp CLI. This allows you to access hosted models on the Hypermode platform.

    ```sh
      npm install -g @hypermode/hyp-cli
    ```

    You can now use the `hyp login` command to log in to the Hyp CLI.
    This links your project to the Hypermode platform, allowing you to leverage the model in your modus app.
  </Step>

  <Step title="Track local model inferences">
    When testing an AI app locally, Modus records the inference and related metadata
    in the `View Inferences` tab of the APIs explorer.

    <Note>
      Local model tracing is only supported on Linux and macOS. Windows support is
      coming soon.
    </Note>

    ![local model tracing](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/observe-functions/local-inference-history.png)
  </Step>
</Steps>

<Tip>
  For more inspiration, check out the [Modus
  recipes](https://github.com/hypermodeinc/modus-recipes).
</Tip>


# Roadmap
Source: https://docs.hypermode.com/modus/roadmap

Upcoming features and improvements for Modus.

<Warning>upcoming features and improvements</Warning>

## Roadmap

Welcome to the Modus roadmap! Here you'll find information about upcoming
features, improvements, and planned releases. Stay tuned to see what's coming
next for Modus.

### Q4 2023

#### New features

* **Real-time Collaboration**: Enable multiple developers to work on the same
  project simultaneously with real-time updates.
* **Enhanced Security**: Implement advanced security features, including
  role-based access control and audit logging.

#### Improvements

* **Performance Optimization**: Further optimize the framework to reduce latency
  and improve overall performance.
* **Expanded Language Support**: Add support for additional programming
  languages, including Ruby and Rust.

We're committed to continuously improving Modus and delivering new features that
help you build better apps. Your feedback is invaluable to us, so please share
your thoughts and suggestions on our
[Community Forum](https://community.hypermode.com) or
[GitHub Issues](https://github.com/hypermodeinc/modus/issues).

Stay tuned for more updates and exciting new features!

<Note>
  This roadmap is subject to change based on user feedback and evolving
  priorities. For the latest updates, please visit our [Roadmap
  Page](https://docs.hypermode.com/modus/roadmap).
</Note>


# Run Locally
Source: https://docs.hypermode.com/modus/run-locally

Test your Modus app and iterate quickly

Modus provides a local development environment that makes it easy to build and
test your app, with local access to models.

## Launching your app in development mode

To run your app, from the project root, run:

```sh
modus dev
```

The `modus dev` command compiles your app code, starts a local server, and
provides a URL to access your app's API. It also enables fast refresh, which
automatically recompiles and reloads any changed functions while preserving app
state during development.

Once your app is running, you can access the graphical interface for your API at
the URL located in your terminal.

```sh
View endpoint: http://localhost:8686/explorer
```

The API Explorer interface allows you to interact with your app's API and test
your functions.

<img className="block" src="https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/api-explorer.png" alt="API Graphical Interface." />

## Environment secrets

When you run your app locally using `modus dev`, the runtime replaces the
placeholders of the manifest with values from environment variables defined in
your operating system or in `.env` files.

The environment variables keys must be upper case and follow the naming
convention:

`MODUS_<CONNECTION NAME>_<PLACEHOLDER>`

For example, with the following manifest:

```json modus.json
{
  "connections": {
    "openai": {
      "type": "http",
      "baseUrl": "https://api.openai.com/",
      "headers": {
        "Authorization": "Bearer {{API_KEY}}"
      }
    }
  }
}
```

The Modus runtime substitutes `{{API_KEY}}` with the value of the environment
variable `MODUS_OPENAI_API_KEY`

An easy way to define the environment variables when working locally is to use
the file `.env.dev.local` located in your app folder.

For the previous manifest, we can set the key in the .env.dev.local file as
follow:

```text .env.dev.local
MODUS_OPENAI_API_KEY="your openai key"
```

<Warning>
  You should exclude `.env` files from source control. Projects created with
  `modus new` exclude these files automatically when creating your project.
</Warning>

## Using Hypermode-hosted models

To use Hypermode-hosted models in your local environment, first install the Hyp
CLI:

```sh
npm install -g @hypermode/hyp-cli
```

Then log in to your Hypermode account:

```sh
hyp login
```

After logging in, your app automatically connects to Hypermode-hosted models
when running locally. For more information on the models available to use, see
[hosted shared models](/hosted-models#shared-models).

## Working with Collections

Collections requires a PostgreSQL instance for local development. While
Hypermode manages this database in production, you'll need to set up PostgreSQL
locally when developing outside the Hypermode platform. For detailed setup
instructions, see
[Develop locally with Collections](/modus/search#develop-locally-with-collections).


# Collections APIs
Source: https://docs.hypermode.com/modus/sdk/assemblyscript/collections

Add storage and vector search capabilities to your functions.

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="AssemblyScript" feature="Collections" />

The Modus Collection APIs allow you to add vector search within your functions.

## Import

To begin, import the `collections` namespace from the SDK:

```ts
import { collections } from "@hypermode/modus-sdk-as"
```

## Collections APIs

{/* vale Google.Headings = NO */}

The APIs in the `collections` namespace are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Mutation Functions

#### upsert

Inserts or updates an item in a collection.

<Note>
  If the item already exists, the function overwrites the previous value. If
  not, it creates a new one.
</Note>

```ts
function upsert(
  collection: string,
  key: string | null,
  text: string,
  labels: string[] = [],
  namespace: string = "",
): CollectionMutationResult
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="key" type="string">
  The unique identifier for the item in the namespace. If `null`, the function
  generates a unique identifier.
</ResponseField>

<ResponseField name="text" type="string" required>
  The text of the item to add to the collection.
</ResponseField>

<ResponseField name="labels" type="string[]">
  An optional array of labels to associate with the item.
</ResponseField>

<ResponseField name="namespace" type="string">
  Associates the item with a specific namespace. Defaults to an empty namespace
  if not provided.
</ResponseField>

#### upsertBatch

Inserts or updates a batch of items into a collection.

<Note>
  If an item with the same key already exists, the original text is overwritten
  with the new text.
</Note>

```ts
function upsertBatch(
  collection: string,
  keys: string[] | null,
  texts: string[],
  labelsArr: string[][] = [],
  namespace: string = "",
): CollectionMutationResult
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="keys" type="string[]">
  Array of keys for the item to add to the collection. If you pass `null` for
  any key, Hypermode assigns a new UUID as the key for the item.
</ResponseField>

<ResponseField name="texts" type="string[]" required>
  Array of texts for the items to add to the collection.
</ResponseField>

<ResponseField name="labelsArr" type="string[][]">
  An optional array of arrays of labels to associate with the items.
</ResponseField>

<ResponseField name="namespace" type="string">
  Associates the item with a specific namespace. Defaults to an empty namespace
  if not provided.
</ResponseField>

#### remove

Removes an item from the collection.

```ts
function remove(
  collection: string,
  key: string,
  namespace: string = "",
): CollectionMutationResult
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="key" type="string" required>
  The key of the item to delete from the collection.
</ResponseField>

<ResponseField name="namespace" type="string">
  The namespace to remove the item from. Defaults to the default namespace if
  not provided.
</ResponseField>

### Search and Retrieval Functions

#### computeDistance

Computes distance between two keys in a collection using a search method's
embedder.

```ts
function computeDistance(
  collection: string,
  searchMethod: string,
  key1: string,
  key2: string,
  namespace: string = "",
): CollectionSearchResultObject
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="searchMethod" type="string" required>
  The search method used to calculate embedding for key's texts.
</ResponseField>

<ResponseField name="key1, key2" type="string" required>
  Keys to compute similarity on.
</ResponseField>

<ResponseField name="namespace" type="string">
  The namespace to search the items from. Defaults to the default namespace if
  not provided.
</ResponseField>

#### getLabels

Get the labels for an item in a collection.

```ts
function getLabels(
  collection: string,
  key: string,
  namespace: string = "",
): string[]
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="key" type="string" required>
  The key of the item to retrieve.
</ResponseField>

<ResponseField name="namespace" type="string">
  The namespace to get the item from. Defaults to the default namespace if not
  provided.
</ResponseField>

#### getNamespaces

Get all namespaces in a collection.

```ts
function getNamespaces(collection: string): string[]
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

#### getText

Gets an item's text from a collection, give the item's key.

```ts
function getText(
  collection: string,
  key: string,
  namespace: string = "",
): string
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="key" type="string" required>
  The key of the item to retrieve.
</ResponseField>

<ResponseField name="namespace" type="string">
  The namespace to get the item from. Defaults to the default namespace if not
  provided.
</ResponseField>

#### getTexts

Get all items from a collection. The result is a map of key to text for all
items in the collection.

```ts
function getTexts(
  collection: string,
  namespace: string = "",
): Map<string, string>
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="namespace" type="string">
  The namespace to get the items from. Defaults to the default namespace if not
  provided.
</ResponseField>

#### getVector

Get the vector for an item in a collection.

```ts
function getVector(
  collection: string,
  searchMethod: string,
  key: string,
  namespace: string = "",
): f32[]
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="searchMethod" type="string" required>
  The search method used to calculate embedding for key's texts.
</ResponseField>

<ResponseField name="key" type="string" required>
  The key of the item to retrieve.
</ResponseField>

<ResponseField name="namespace" type="string">
  The namespace to get the item from. Defaults to the default namespace if not
  provided.
</ResponseField>

#### nnClassify

Classify an item in the collection using previous vectors' labels.

```ts
function nnClassify(
  collection: string,
  searchMethod: string,
  text: string,
  namespace: string = "",
): CollectionClassificationResult
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="searchMethod" type="string" required>
  The search method used to calculate embedding for text & search against.
</ResponseField>

<ResponseField name="text" type="string" required>
  The text to compute natural language search on.
</ResponseField>

<ResponseField name="namespace" type="string">
  The namespace to search the items from. Defaults to the default namespace if
  not provided.
</ResponseField>

#### search

Perform a natural language search on items within a collection. This method is
useful for finding items that match a search query based on semantic meaning.

<Note>
  Modus uses the same embedder for both inserting text into the collection, and
  for the text used when searching the collection.
</Note>

```ts
function search(
  collection: string,
  searchMethod: string,
  text: string,
  limit: i32,
  returnText: bool = false,
  namespaces: string[] = [],
): CollectionSearchResult
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="searchMethod" type="string" required>
  The search method used to calculate embedding for text & search against.
</ResponseField>

<ResponseField name="text" type="string" required>
  The text to compute natural language search on.
</ResponseField>

<ResponseField name="limit" type="i32" required>
  The number of result objects to return.
</ResponseField>

<ResponseField name="returnText" type="bool">
  A flag to return the texts in the response.
</ResponseField>

<ResponseField name="namespaces" type="string[]">
  A list of namespaces to search the item from. Defaults to the default
  namespace if not provided.
</ResponseField>

#### searchByVector

Perform a vector-based search on a collection, which is helpful for scenarios
requiring precise similarity calculations between pre-computed embeddings.

<Note>
  Modus uses the same embedder for both inserting text into the collection, and
  for the vector used when searching the collection.
</Note>

```ts
function searchByVector(
  collection: string,
  searchMethod: string,
  vector: f32[],
  limit: i32,
  returnText: bool = false,
  namespaces: string[] = [],
): CollectionSearchResult
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="searchMethod" type="string" required>
  The search method used to calculate embedding for vector & search against.
</ResponseField>

<ResponseField name="vector" type="f32[]" required>
  The vector to compute search on.
</ResponseField>

<ResponseField name="limit" type="i32" required>
  The number of result objects to return.
</ResponseField>

<ResponseField name="returnText" type="bool">
  A flag to return the texts in the response.
</ResponseField>

<ResponseField name="namespaces" type="string[]">
  An optional array of namespaces to search within.
</ResponseField>

### Maintenance Functions

#### recomputeSearchMethod

Recalculates the embeddings for all items in a collection. It can be
resource-intensive, use it when necessary, for example after you have updated
the method for embedding calculation and want to re-compute the embeddings for
existing data in the collection.

```ts
function recomputeSearchMethod(
  collection: string,
  searchMethod: string,
  namespace: string = "",
): SearchMethodMutationResult
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="searchMethod" type="string" required>
  The search method to recompute embeddings for.
</ResponseField>

<ResponseField name="namespace" type="string">
  The namespace to use. Defaults to the default namespace if not provided.
</ResponseField>

### Types

#### CollectionMutationResult

Represents the result of a mutation operation on a collection.

```ts
class CollectionMutationResult {
  collection: string
  status: CollectionStatus
  error: string
  isSuccessful: bool
  operation: string
  keys: string[]
}
```

<ResponseField name="collection" type="string">
  Name of the collection.
</ResponseField>

<ResponseField name="status" type="CollectionStatus">
  The status of the operation.
</ResponseField>

<ResponseField name="error" type="string">
  Error message, if any.
</ResponseField>

<ResponseField name="isSuccessful" type="bool">
  A boolean indicating whether the operation completed successfully. Use this to
  confirm success before handling the result.
</ResponseField>

<ResponseField name="operation" type="string">
  The operation performed.
</ResponseField>

<ResponseField name="keys" type="string[]">
  The keys of the items affected by the operation.
</ResponseField>

#### CollectionClassificationLabelObject

Represents a classification label.

```ts
class CollectionClassificationLabelObject {
  label: string
  confidence: f64
}
```

<ResponseField name="label" type="string">
  The classification label.
</ResponseField>

<ResponseField name="confidence" type="f64">
  The confidence score of the classification label.
</ResponseField>

#### CollectionClassificationResult

Represents the result of a classification operation on a collection.

```ts
class CollectionClassificationResult {
  collection: string
  status: CollectionStatus
  error: string
  isSuccessful: bool
  searchMethod: string
  labelsResult: CollectionClassificationLabelObject[]
  cluster: CollectionClassificationResultObject[]
}
```

<ResponseField name="collection" type="string">
  Name of the collection.
</ResponseField>

<ResponseField name="status" type="CollectionStatus">
  The status of the operation.
</ResponseField>

<ResponseField name="error" type="string">
  Error message, if any.
</ResponseField>

<ResponseField name="isSuccessful" type="bool">
  A boolean indicating whether the operation completed successfully. Use this to
  confirm success before handling the result.
</ResponseField>

<ResponseField name="searchMethod" type="string">
  The search method used in the operation.
</ResponseField>

<ResponseField name="labelsResult" type="CollectionClassificationLabelObject[]">
  The classification labels.
</ResponseField>

<ResponseField name="cluster" type="CollectionClassificationResultObject[]">
  The classification results.
</ResponseField>

#### CollectionClassificationResultObject

Represents an object in the classification results.

```ts
class CollectionClassificationResultObject {
  key: string
  labels: string[]
  distance: f64
  score: f64
}
```

<ResponseField name="key" type="string">
  The key of the item classified.
</ResponseField>

<ResponseField name="labels" type="string[]">
  The classification labels.
</ResponseField>

<ResponseField name="distance" type="f64">
  The distance of the item from the classification labels.
</ResponseField>

<ResponseField name="score" type="f64">
  The similarity score of the item classified.
</ResponseField>

#### CollectionSearchResult

Represents the result of a search operation on a collection.

```ts
class CollectionSearchResult {
  collection: string
  status: CollectionStatus
  error: string
  isSuccessful: bool
  searchMethod: string
  objects: CollectionSearchResultObject[]
}
```

<ResponseField name="collection" type="string">
  Name of the collection.
</ResponseField>

<ResponseField name="status" type="CollectionStatus">
  The status of the operation.
</ResponseField>

<ResponseField name="error" type="string">
  Error message, if any.
</ResponseField>

<ResponseField name="isSuccessful" type="bool">
  A boolean indicating whether the operation completed successfully. Use this to
  confirm success before handling the result.
</ResponseField>

<ResponseField name="searchMethod" type="string">
  The search method used in the operation.
</ResponseField>

<ResponseField name="objects" type="CollectionSearchResultObject[]">
  The search results.
</ResponseField>

#### CollectionSearchResultObject

Represents an object in the search results.

```ts
class CollectionSearchResultObject {
  namespace: string
  key: string
  text: string
  labels: string[]
  distance: f64
  score: f64
}
```

<ResponseField name="namespace" type="string">
  The namespace of the item found as part of the search.
</ResponseField>

<ResponseField name="key" type="string">
  The key of the item found as part of the search.
</ResponseField>

<ResponseField name="text" type="string">
  The text of the item found as part of the search.
</ResponseField>

<ResponseField name="distance" type="f64">
  The distance of the item from the search text.
</ResponseField>

<ResponseField name="score" type="f64">
  The similarity score of the item found, as it pertains to the search.
</ResponseField>

#### CollectionStatus

The status of a collection operation.

```ts
enum CollectionStatus {
  Success = "success"
  Error = "error"
}
```

<ResponseField name="Success" type="string">
  The operation was successful.
</ResponseField>

<ResponseField name="Error" type="string">
  The operation encountered an error.
</ResponseField>

#### SearchMethodMutationResult

Represents the result of a mutation operation on a search method.

```ts
class SearchMethodMutationResult {
  collection: string
  status: CollectionStatus
  error: string
  isSuccessful: bool
  operation: string
  searchMethod: string
}
```

<ResponseField name="collection" type="string">
  Name of the collection.
</ResponseField>

<ResponseField name="status" type="CollectionStatus">
  The status of the operation.
</ResponseField>

<ResponseField name="error" type="string">
  Error message, if any.
</ResponseField>

<ResponseField name="isSuccessful" type="bool">
  A boolean indicating whether the operation completed successfully. Use this to
  confirm success before handling the result.
</ResponseField>

<ResponseField name="operation" type="string">
  The operation performed.
</ResponseField>

<ResponseField name="searchMethod" type="string">
  The search method affected by the operation.
</ResponseField>


# Console APIs
Source: https://docs.hypermode.com/modus/sdk/assemblyscript/console

Capture errors and debugging information in your functions

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="AssemblyScript" feature="Console" />

The Modus Console APIs allow you to capture information from your functions,
such as errors and other information that can help you debug your functions.

<Info>
  Unlike other APIs, the `console` namespace is globally available by default in
  all AssemblyScript functions, you don't need to import it.
</Info>

The Console APIs mimic the behavior of the AssemblyScript
[`console`](https://www.assemblyscript.org/stdlib/console.html) object, but
directs the output to Hypermode as follows:

* All console output is available in the runtime logs. When hosting on
  Hypermode, the logs are available in the "Function Runs" section of the
  Console UI.
* Output from `console.error` is also sent to the GraphQL response.
* Errors thrown with the `throw` keyword are also sent to the GraphQL response.

## Console APIs

{/* vale Google.Headings = NO */}

The APIs in the `console` namespace are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Assertion Functions

#### assert

Asserts that a condition is true, and logs an error if it's not.

```ts
function assert<T>(assertion: T, message?: string): void
```

<ResponseField name="assertion" required>
  The condition to assert. Typically a boolean value. In the case of an object,
  asserts that the object isn't `null`.
</ResponseField>

<ResponseField name="message" type="string">
  An error message to log, if the assertion is false.
</ResponseField>

### Logging Functions

#### log

Generate a log message, with no particular logging level.

```ts
function log(message?: string): void
```

<ResponseField name="message" type="string">
  A message you want to log.
</ResponseField>

#### debug

Generate a log message with the "debug" logging level.

```ts
function debug(message?: string): void
```

<ResponseField name="message" type="string">
  A debug message you want to log.
</ResponseField>

#### info

Generate a log message with the "info" logging level.

```ts
function info(message?: string): void
```

<ResponseField name="message" type="string">
  An informational message you want to log.
</ResponseField>

#### warn

Generate a log message with the "warning" logging level.

```ts
function warn(message?: string): void
```

<ResponseField name="message" type="string">
  A warning message you want to log.
</ResponseField>

#### error

Generate a log message with the "error" logging level.

```ts
function error(message?: string): void
```

<ResponseField name="message" type="string">
  An error message you want to log.
</ResponseField>

### Timing Functions

#### time

Starts a new timer using the specified label.

```ts
function time(label?: string): void
```

<ResponseField name="label" type="string">
  An optional label for the timer.
</ResponseField>

#### timeLog

Logs the current value of a timer previously started with `console.time`.

```ts
function timeLog(label?: string): void
```

<ResponseField name="label" type="string">
  An optional label for the timer.
</ResponseField>

#### timeEnd

Logs the current value of a timer previously started with `console.time`, and
discards the timer.

```ts
function timeEnd(label?: string): void
```

<ResponseField name="label" type="string">
  An optional label for the timer.
</ResponseField>


# Dgraph APIs
Source: https://docs.hypermode.com/modus/sdk/assemblyscript/dgraph

Execute queries and mutations against a Dgraph database

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="AssemblyScript" feature="Dgraph" />

The Modus Dgraph APIs allow you to run queries and mutations against a Dgraph
database.

## Import

To begin, import the `dgraph` namespace from the SDK:

```ts
import { dgraph } from "@hypermode/modus-sdk-as"
```

## Dgraph APIs

{/* vale Google.Headings = NO */}

The APIs in the `dgraph` namespace are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### alterSchema

Alter the schema of a Dgraph database.

```ts
function alterSchema(connection: string, schema: string): string
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="schema" type="string" required>
  The schema to apply to the Dgraph database.
</ResponseField>

#### dropAll

Drop all data from a Dgraph database.

```ts
function dropAll(connection: string): string
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

#### dropAttr

Drop an attribute from a Dgraph schema.

```ts
function dropAttr(connection: string, attr: string): string
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="attr" type="string" required>
  The attribute to drop from the Dgraph schema.
</ResponseField>

#### escapeRDF

Ensures proper escaping of RDF string literals.

```ts
function escapeRDF(value: string): string
```

<ResponseField name="value" type="string" required>
  The RDF string literal to escape.
</ResponseField>

#### executeMutations

Execute one or more mutations, without a filtering query.

<Tip>
  If you need to filter the mutations based on a query, use the
  [`executeQuery`](#executequery) function instead, and pass the mutations after
  the query.
</Tip>

```ts
function executeMutations(
  connection: string,
  ...mutations: Mutation[]
): Response
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="mutations" type="...Mutation[]" required>
  One or more [`Mutation`](#mutation) objects to execute.
</ResponseField>

#### executeQuery

Execute a DQL query to retrieve data from a Dgraph database.

Also used to execute a filtering query and apply one or more mutations to the
result of the query.

<Tip>
  If you need apply mutations *without* a filtering query, use the
  [`executeMutations`](#executemutations) function instead.
</Tip>

```ts
function executeQuery(
  connection: string,
  query: Query,
  ...mutations: Mutation[]
): Response
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="request" type="Query" required>
  A [`Query`](#query) object describing the DQL query to execute.
</ResponseField>

<ResponseField name="mutations" type="...Mutation[]">
  Optional parameters specifying one or more [`Mutation`](#mutation) objects to
  execute on the nodes matched by the query.
</ResponseField>

### Types

#### Mutation

A Dgraph mutation object, used to execute mutations.

```ts
class Mutation {
  setJson: string
  delJson: string
  setNquads: string
  delNquads: string
  condition: string
  withSetJson(json: string): Mutation
  withDelJson(json: string): Mutation
  withSetNquads(nquads: string): Mutation
  withDelNquads(nquads: string): Mutation
  withCondition(cond: string): Mutation
}
```

<ResponseField name="new dgraph.Mutation(setJson, delJson, setNquads, delNquads, condition)">
  Creates a new `Mutation` object with the given `setJson`, `delJson`,
  `setNquads`, `delNquads`, and `condition` fields.
</ResponseField>

<ResponseField name="setJson" type="string">
  A JSON string representing the data to set in the mutation.
</ResponseField>

<ResponseField name="delJson" type="string">
  A JSON string representing the data to delete in the mutation.
</ResponseField>

<ResponseField name="setNquads" type="string">
  A string representing the data to set in the mutation in RDF N-Quads format.
</ResponseField>

<ResponseField name="delNquads" type="string">
  A string representing the data to delete in the mutation in RDF N-Quads
  format.
</ResponseField>

<ResponseField name="condition" type="string">
  A string representing the condition query for the mutation, as a DQL `@if`
  directive.
</ResponseField>

<ResponseField name="withSetJson(json)">
  Sets the `setJson` field of the mutation to the given `json` string. Returns
  the `Mutation` object to allow for method chaining.
</ResponseField>

<ResponseField name="withDelJson(json)">
  Sets the `delJson` field of the mutation to the given `json` string. Returns
  the `Mutation` object to allow for method chaining.
</ResponseField>

<ResponseField name="withSetNquads(nquads)">
  Sets the `setNquads` field of the mutation to the given `nquads` string.
  Returns the `Mutation` object to allow for method chaining.
</ResponseField>

<ResponseField name="withDelNquads(nquads)">
  Sets the `delNquads` field of the mutation to the given `nquads` string.
  Returns the `Mutation` object to allow for method chaining.
</ResponseField>

<ResponseField name="withCondition(cond)">
  Sets the `condition` field of the mutation to the given `cond` string, which
  should be a DQL `@if` directive. Returns the `Mutation` object to allow for
  method chaining.
</ResponseField>

#### Query

A Dgraph query object, used to execute queries.

```ts
class Query {
  query: string = ""
  variables: Map<string, string> = new Map<string, string>()
  withVariable<T>(name: string, value: T): this
}
```

<ResponseField name="new dgraph.Query(query, variables)">
  Creates a new `Query` object with the given `query` and `variables`. `query`
  is a Dgraph Query Language (DQL) query string, and `variables` is an optional
  [`Variables`](#variables) object.
</ResponseField>

<ResponseField name="query" type="string">
  The DQL query to execute.
</ResponseField>

<ResponseField name="variables" type="Map<string, string>">
  A map of query variables, with values encoded as JSON strings.
</ResponseField>

<ResponseField name="withVariable(name, value)">
  Sets a query variable with the given `name` and `value`. `name` is of type
  `string`, and `value` can be a string, number, or boolean.

  Returns the `Query` object to allow for method chaining.

  <Tip>
    The `withVariable` method is the preferred way to set query variables in a
    `Query` object, and allows for fluent method chaining to add multiple variables.
    For example:

    ```ts
    const query = new Query(`
      query all($name: string, $age: int) {
        all(func: eq(name, $name)) {
          name
          age
        }
      }
    `)
      .withVariable("name", "Alice")
      .withVariable("age", 30)

    const response = dgraph.executeQuery("my-dgraph-connection", query)
    ```
  </Tip>
</ResponseField>

#### Request

A Dgraph request object, used to execute queries and mutations.

<Info>
  {/* vale Google.Passive = NO */}

  This object was used by the `execute` function, which has been replaced by the
  [`executeQuery`](#executequery) and [`executeMutations`](#executemutations)
  functions. You should no longer need to create a `Request` object directly.

  {/* vale Google.Passive = YES */}
</Info>

```ts
class Request {
  query: Query = new Query()
  mutations: Mutation[] = []
}
```

<ResponseField name="new dgraph.Request(query, mutations)">
  Creates a new `Request` object with the given `query` and `mutations`.

  The [`query`](#query) and [`mutations`](#mutation) fields are optional and
  default to `null`.
</ResponseField>

<ResponseField name="query" type="Query">
  A Dgraph [`query`](#query) object.
</ResponseField>

<ResponseField name="mutations" type="Mutation[]">
  An array of Dgraph [`mutation`](#mutation) objects.
</ResponseField>

#### Variables

A Variables object used to set query variables in a Dgraph query.

<Info>
  As of SDK version 0.17.1, it's no longer necessary to explicitly create a
  `Variables` object. Instead, use the `withVariable` method on the
  [`Query`](#query) object to set query variables.
</Info>

```ts
class Variables {
  set<T>(name: string, value: T): void
  toMap(): Map<string, string>
}
```

<ResponseField name="Variables.set(name, value)">
  Sets a query variable with the given `name` and `value`. `name` is of type
  `string`, and `value` can be a string, number, or boolean.
</ResponseField>

<ResponseField name="Variables.toMap()">
  Returns a map of all query variables set in the `Variables` object, with
  values encoded as strings.
</ResponseField>


# GraphQL APIs
Source: https://docs.hypermode.com/modus/sdk/assemblyscript/graphql

Access external GraphQL data sources from your functions

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="AssemblyScript" feature="GraphQL" />

The Modus GraphQL APIs allow you to securely call and fetch data from any
GraphQL endpoint.

## Import

To begin, import the `graphql` namespace from the SDK:

```ts
import { graphql } from "@hypermode/modus-sdk-as"
```

## GraphQL APIs

{/* vale Google.Headings = NO */}

The APIs in the `graphql` namespace are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### execute

Execute a GraphQL statement to call a query or apply mutation against a GraphQL
endpoint.

```ts
function execute<T>(
  connection: string,
  statement: string,
  variables?: Variables,
): Response<T>
```

<ResponseField name="T" required>
  Type of object to use for the data returned from the query. This can be any
  type, including a custom type defined in your project. It should match the shape
  of the data returned from the GraphQL query.

  <Tip>
    Define custom types in the project's source code. In AssemblyScript, create
    classes decorated with `@json`.

    All types, including classes, base classes, and field types must be JSON
    serializable. You can also use built-in types such as strings, numbers, arrays,
    and maps.
  </Tip>
</ResponseField>

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connection).
</ResponseField>

<ResponseField name="statement" type="string" required>
  GraphQL statement containing the query or mutation operation to execute.

  <Warning>
    While it's possible to directly include parameter values into your GraphQL
    statement, it's highly recommended to pass a [`Variables`](#variables) object
    instead. This can help to prevent against injection attacks and other security
    vulnerabilities.
  </Warning>
</ResponseField>

<ResponseField name="variables" type="Variables">
  Optional variables to include with the query.

  See the details of the [`Variables`](#variables) object for more information.
</ResponseField>

### Types

#### Variables

A container for variables to include with a GraphQL operation.

To use this feature, create a new `Variables` object and call the `set` method
for each variable you want to include. Then pass the object to the `execute`
function.

```ts
class Variables {
  set<T>(name: string, value: T): void
  toJSON(): string
}
```

<ResponseField name="set(name, value)">
  Set a variable with a name and value to include with the GraphQL operation.

  <Expandable title="parameters">
    {/* markdownlint-disable MD046 */}

    <ResponseField name="name" type="string" required>
      The name of the variable to include in the GraphQL operation.
    </ResponseField>

    <ResponseField name="value" required>
      The value of the variable to include in the GraphQL operation.

      The value can be of any type that's JSON serializable, including strings,
      numbers, boolean values, arrays, maps, and custom objects decorated with
      `@json`.
    </ResponseField>

    {/* markdownlint-restore MD046 */}
  </Expandable>
</ResponseField>

<ResponseField name="toJSON()" type="string">
  Serializes the variables to a JSON string for inclusion in the GraphQL
  operation. The `execute` function calls this automatically when you pass a
  `Variables` object. You typically don't need to call it directly.
</ResponseField>

#### Response

A response object from the GraphQL query.

Either `errors` or `data` is present, depending on the result of the query.

```ts
class Response<T> {
  errors: ErrorResult[] | null
  data: T | null
}
```

<ResponseField name="errors" type="ErrorResult[]">
  An array of errors incurred as part of your GraphQL request, if any.

  Each error in the array is an [`ErrorResult`](#errorresult) object. If there are
  no errors, this field is `null`.
</ResponseField>

<ResponseField name="data" type="T">
  The resulting data selected by the GraphQL operation.

  The data has the type specified in the call to the `execute` function. If data
  is absent due to errors, this field is `null`.
</ResponseField>

#### ErrorResult

The details of an error incurred as part of a GraphQL operation.

```ts
class ErrorResult {
  message: string
  locations: CodeLocation[] | null
  path: string[] | null
}
```

<ResponseField name="message" type="string">
  Description of the error incurred.
</ResponseField>

<ResponseField name="locations" type="CodeLocation[]">
  An array of [`CodeLocation`](#codelocation) objects that point to the specific
  location of the error in the GraphQL statement.
</ResponseField>

<ResponseField name="path" type="string[]">
  Path to the area of the GraphQL statement related to the error.

  Each item in the array represents a segment of the path.
</ResponseField>

#### CodeLocation

The location of specific code within a GraphQL statement.

```ts
class CodeLocation {
  line: u32
  column: u32
}
```

<ResponseField name="line" type="u32">
  Line number within the GraphQL statement for the code.
</ResponseField>

<ResponseField name="column" type="u32">
  Column number within the GraphQL statement for the code.
</ResponseField>


# HTTP APIs
Source: https://docs.hypermode.com/modus/sdk/assemblyscript/http

Access external HTTP endpoints from your functions

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="AssemblyScript" feature="HTTP" />

The Modus HTTP APIs allow you to securely call and fetch data from an HTTP
endpoint. It is similar to the HTTP
[`fetch`](https://developer.mozilla.org/docs/Web/API/Fetch_API) API used in
JavaScript, but with some modifications to work with Modus.

<Warning>
  As a security measure, you can only call HTTP endpoints that you
  [defined in your app's manifest](/modus/app-manifest#connections). Any attempt
  to access an arbitrary URL, for a connection not defined in your app's manifest,
  results in an error.

  Additionally, you should use placeholders for connection secrets in the
  manifest, rather than hardcoding them in your functions. Enter the values for
  each connections' secrets via environment variables or the Hypermode UI. This
  ensures that your secrets are securely stored, aren't committed to your
  repository, and aren't visible or accessible from your functions code.
</Warning>

## Import

To begin, import the `http` namespace from the SDK:

```ts
import { http } from "@hypermode/modus-sdk-as"
```

## HTTP APIs

{/* vale Google.Headings = NO */}

The APIs in the `http` namespace are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### fetch

Invoke an HTTP endpoint to retrieve data or trigger external action.

Returns a [`Response`](#response) object with the HTTP response data.

```ts
function fetch(
  requestOrUrl: string | Request,
  options: RequestOptions = new RequestOptions(),
): Response
```

<ResponseField name="requestOrUrl" type="string | Request" required>
  Either a URL `string` or a [`Request`](#request) object, describing the HTTP
  request to make.

  If a `string`, the operation uses the `GET` HTTP method with no headers other
  than those defined in the manifest entry of the connection.

  <Note>
    Each request must match to a connection entry in the manifest, using the
    `baseUrl` field. The request URL passed to the `fetch` function (or via a
    `Request` object) must start with the manifest entry's `baseUrl` value to
    match.
  </Note>
</ResponseField>

<ResponseField name="options" type="RequestOptions">
  A [`RequestOptions`](#requestoptions) object with additional options for the
  request, such as the HTTP method, headers, and body.
</ResponseField>

### Types

#### Content

Represents content used in the body of an HTTP request or response.

```ts
class Content {
  static from<T>(value: T): Content
  readonly data: ArrayBuffer
  bytes(): Uint8Array
  text(): string
  json<T>(): T
}
```

<ResponseField name="Content.from(value)">
  Creates a new `Content` object from the given value.

  The value can be of any type that's JSON serializable, including strings,
  numbers, boolean values, arrays, maps, and custom objects decorated with
  `@json`.

  If the value is a `string`, an `ArrayBuffer`, or a `Uint8Array` it's sent as-is,
  without JSON serialization.
</ResponseField>

<ResponseField name="data" type="ArrayBuffer">
  The raw binary content data buffer.
</ResponseField>

<ResponseField name="bytes()">
  Returns the binary content data as a `Uint8Array`.
</ResponseField>

<ResponseField name="text()">
  Interprets the content as a UTF-8 encoded string, and returns it as a `string`
  value.
</ResponseField>

<ResponseField name="json<T>()">
  Interprets the content as a UTF-8 encoded string containing JSON in the shape
  of type `T`, and returns it as a value of type `T`.
</ResponseField>

#### Header

Represents an HTTP request or response header.

```ts
class Header {
  name: string
  values: string[]
}
```

<ResponseField name="name" type="string">
  The name of the header.
</ResponseField>

<ResponseField name="values" type="string[]">
  An array of values for the header. Typically a header has a single value, but
  some headers can have multiple values.
</ResponseField>

#### Headers

Represents a collection of HTTP headers.

```ts
class Headers {
  static from(
    value: string[][] | Map<string, string> | Map<string, string[]>,
  ): Headers
  append(name: string, value: string): void
  entries(): string[][]
  get(name: string): string | null
}
```

<ResponseField name="Headers.from(value)">
  Creates a new `Headers` object from the given `value`.

  The `value` must be one of the following types:

  * A `string[][]`, where each inner array contains a header name and value.
  * A `Map<string, string>`, where the keys are header names and the values are
    header values.
  * A `Map<string, string[]>`, where the keys are header names and the values are
    arrays of header values.
</ResponseField>

<ResponseField name="append(name, value)">
  Appends a new header with the given `name` and `value` to the collection.
</ResponseField>

<ResponseField name="entries()">
  Returns a `string[][]`, where each inner array contains a header name and
  value.
</ResponseField>

<ResponseField name="get(name)">
  Returns the value of the header with the given `name`, or `null` if the header
  doesn't exist. If there are multiple values for the header, this function
  concatenates them with a comma to form a single string.
</ResponseField>

#### Request

Represents an HTTP request to make.

```ts
class Request {
  constructor(url: string, options?: RequestOptions)
  static clone(request: Request, options: RequestOptions): Request
  readonly url: string
  readonly method: string
  readonly headers: Headers
  readonly body: ArrayBuffer
  bytes(): Uint8Array
  text(): string
  json<T>(): T
}
```

<ResponseField name="new http.Request(url, options?)">
  Creates a new `Request` object with the given `url` and `options`.

  The required `url` parameter must be a fully qualified URL of the request,
  including the protocol. For example, `"https://example.com"`.

  The optional `options` parameter is a [`RequestOptions`](#requestoptions) object
  that's used to set the HTTP method, headers, and body if needed.
</ResponseField>

<ResponseField name="Request.clone(request, options)">
  Creates a new `Request` object by cloning the given `request` and applying the
  `options` to it.
</ResponseField>

<ResponseField name="url" type="string">
  The fully qualified URL of the request, including the protocol. For example,
  `"https://example.com"`.
</ResponseField>

<ResponseField name="method" type="string">
  The HTTP method of the request. For example, `"GET"`, `"POST"`, `"PUT"`, or
  `"DELETE"`.
</ResponseField>

<ResponseField name="headers" type="Headers">
  The HTTP headers of the request, as a [`Headers`](#headers) object.
</ResponseField>

<ResponseField name="body" type="ArrayBuffer">
  The raw binary content data buffer of the request body.

  <Tip>
    The request body isn't normally read directly. Instead, use the `bytes`,
    `text` or `json` functions.
  </Tip>
</ResponseField>

<ResponseField name="bytes()">
  Returns the binary content data of the request body as a `Uint8Array`.
</ResponseField>

<ResponseField name="text()">
  Interprets the request body as a UTF-8 encoded string, and returns it as a
  `string` value.
</ResponseField>

<ResponseField name="json<T>()">
  Interprets the request body as a UTF-8 encoded string containing JSON in the
  shape of type `T`, and returns it as a value of type `T`.
</ResponseField>

#### RequestOptions

Options for the HTTP request.

```ts
class RequestOptions {
  method: string | null
  headers: Headers
  body: Content | null
}
```

<ResponseField name="method" type="string | null">
  The HTTP method of the request. For example, `"GET"`, `"POST"`, `"PUT"`, or
  `"DELETE"`. If `null` (the default), the request uses the `GET` method.
</ResponseField>

<ResponseField name="headers" type="Headers">
  The HTTP headers of the request, as a [`Headers`](#headers) object.

  <Tip>
    By default, the `RequestOptions` contains an empty `Headers` object which you
    can add headers to using the `append` method.
  </Tip>
</ResponseField>

<ResponseField name="body" type="Content | null">
  Content to pass in the request body, as a [`Content`](#content) object, or
  `null` (the default) if there is no body to pass.

  <Tip>
    It is generally recommended to supply a `Content-Type` header for any requests
    that have a body.
  </Tip>
</ResponseField>

#### Response

Represents the response received from the HTTP server.

```ts
class Response {
  readonly status: u16
  readonly statusText: string
  readonly headers: Headers
  readonly body: ArrayBuffer
  readonly ok: bool
  bytes(): Uint8Array
  text(): string
  json<T>(): T
}
```

<ResponseField name="status" type="u16">
  The HTTP response status code, such as `200` for success.
</ResponseField>

<ResponseField name="statusText" type="string">
  The HTTP response status text associated with the status code, such as `"OK"`
  for success.
</ResponseField>

<ResponseField name="headers" type="Headers">
  The HTTP headers received with the response, as a [`Headers`](#headers)
  object.
</ResponseField>

<ResponseField name="body" type="ArrayBuffer">
  The raw binary content data buffer of the response body.

  <Tip>
    The response body isn't normally read directly. Instead, use the `bytes`,
    `text` or `json` functions.
  </Tip>
</ResponseField>

<ResponseField name="ok" type="bool">
  A boolean value indicating whether the response was successful. It is `true`
  if the status code is in the range `200-299`, and `false` otherwise.
</ResponseField>

<ResponseField name="bytes()">
  Returns the binary content data of the response body as a `Uint8Array`.
</ResponseField>

<ResponseField name="text()">
  Interprets the response body content as a UTF-8 encoded string, and returns it
  as a `string` value.
</ResponseField>

<ResponseField name="json<T>()">
  Interprets the response body content as a UTF-8 encoded string containing JSON
  in the shape of type `T`, and returns it as a value of type `T`.
</ResponseField>


# Local Time APIs
Source: https://docs.hypermode.com/modus/sdk/assemblyscript/localtime

Access the user's local time and time zone in your functions

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="AssemblyScript" feature="Local Time" />

The Modus Local Time APIs allow you to access the user's local time and time
zone from your functions.

## Import

To begin, import the `localtime` namespace from the SDK:

```ts
import { localtime } from "@hypermode/modus-sdk-as"
```

{/* vale Google.Headings = NO */}

## Local Time APIs

The APIs in the `localtime` namespace are below.

All time zones use the IANA time zone database format. For example,
`"America/New_York"`. You can find a list of valid time zones
[here](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).

<Info>
  {/* vale Google.Passive = NO */}

  For APIs that work with the user's local time, the time zone is determined in
  the following order of precedence:

  * If the `X-Time-Zone` header is present in the request, the time zone is set to
    the value of the header.
  * If the `TZ` environment variable is set on the host, the time zone is set to
    the value of the variable.
  * Otherwise, the time zone is set to the host's local time zone.

  {/* vale Google.Passive = YES */}
</Info>

<Tip>
  When working locally with `modus dev`, Modus uses the host's local time zone by
  default. You can override this by setting the `TZ` environment variable in your
  `.env.local` file.
</Tip>

<Tip>
  In a browser-based web app, you can get the user's time zone with the following
  JavaScript code:

  ```js
  const timeZone = Intl.DateTimeFormat().resolvedOptions().timeZone
  ```

  Assign that value to a `"X-Time-Zone"` request header when calling Modus, to use
  it for all local time calculations.
</Tip>

<Tip>
  If you just need the current UTC time, you can use AssemblyScript's built-in
  support:

  ```ts
  const now = new Date(Date.now())

  // if you need a string
  const utcTime = now.toISOString()
  ```
</Tip>

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### getTimeZone

Returns the user's time zone in IANA format.

```ts
function getTimeZone(): string
```

#### isValidTimeZone

Determines whether the specified time zone is a valid IANA time zone and
recognized by the system as such.

```ts
function isValidTimeZone(tz: string): bool
```

<ResponseField name="tz" type="string" required>
  An IANA time zone identifier, such as `"America/New_York"`.
</ResponseField>

#### now

Returns the current local time in the user's time zone, as a string in ISO 8601
extended format, including the applicable time zone offset.

For example, `"2025-12-31T12:34:56.789-04:00"`.

```ts
function now(): string
```

#### nowInZone

Returns the current local time in a specified time zone, as a string in ISO 8601
extended format, including the applicable time zone offset.

For example, `"2025-12-31T12:34:56.789-04:00"`.

```ts
function nowInZone(tz: string): string
```

<ResponseField name="tz" type="string" required>
  An IANA time zone identifier, such as `"America/New_York"`.
</ResponseField>


# AI Model APIs
Source: https://docs.hypermode.com/modus/sdk/assemblyscript/models

Invoke AI models from your functions

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="AssemblyScript" feature="Models" />

The Modus Models APIs allow you to invoke AI models directly from your
functions, irrespective of the model's host.

Since many models have unique interfaces, the design of the Models APIs are
extremely flexible. A common base class forms the core of the APIs, which
extends to conform to any model's required schema.

The SDK contains both the base types and pre-defined implementations for many
commonly used models. You can either use one of the pre-defined model types, or
can create custom types for any model you like, by following the same pattern as
implemented in the pre-defined models.

<Tip>
  For your reference, several complete examples for using the Models APIs are available in
  [Model Invoking](/modus/model-invoking).

  Each example demonstrates using different types of AI models for different
  purposes. However, the Models interface isn't limited to these purposes. You can
  use it for any task that an AI model can perform.
</Tip>

## Import

To begin, import the `models` namespace from the SDK:

```ts
import { models } from "@hypermode/modus-sdk-as"
```

You'll also need to import one or more classes for the model you are working
with. For example:

```ts
import { OpenAIChatModel } from "@hypermode/modus-sdk-as/models/openai"
```

If you would like to request a new model, please
[open an issue](https://github.com/hypermodeinc/modus/issues). You can also send
a pull request, if you'd like to contribute a new model yourself.

## Models APIs

{/* vale Google.Headings = NO */}

The APIs in the `models` namespace are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### getModel

Get a model instance by name and type.

```ts
function getModel<T>(modelName: string): T
```

<ResponseField name="T" required>
  The type of model to return. This can be any class that extends the `Model`
  base class.
</ResponseField>

<ResponseField name="modelName" type="string" required>
  The name of the model to retrieve. This must match the name of a model defined
  in your project's manifest file.
</ResponseField>

### Types

#### Model

The base class for all models that Modus functions can invoke.

<Tip>
  If you are implementing a custom model, you should extend this class. You'll
  also need classes to represent the input and output types for your model. See
  the implementations of the pre-defined models in the Modus GitHub repository
  for examples.
</Tip>

```ts
abstract class Model<TInput, TOutput> {
  debug: bool
  info: ModelInfo
  invoke(input: TInput): TOutput
}
```

<ResponseField name="TInput" required>
  The type of the input data for the model. This can be any type, including a
  custom type defined in your project. It should match the shape of the data
  expected by the model. Usually a class.
</ResponseField>

<ResponseField name="TOutput" required>
  The type of the output data from the model. This can be any type, including a
  custom type defined in your project. It should match the shape of the data
  returned by the model. Usually a class.
</ResponseField>

<ResponseField name="debug" type="bool">
  A flag to enable debug mode for the model. When enabled, Modus automatically
  logs the full request and response data to the console. Implementations can
  also use this flag to enable additional debug logging. Defaults to `false`.
</ResponseField>

<ResponseField name="info" type="ModelInfo">
  Information about the model set by the Modus Runtime when creating the
  instance. See the [`ModelInfo`](#modelinfo) object for more information.
</ResponseField>

<ResponseField name="invoke(input)" type="method">
  Invokes the model with input data and returns the output data.
</ResponseField>

#### ModelInfo

Information about a model that's used to construct a `Model` instance. It is
also available as a property on the `Model` class.

<Info>
  This class relays information from the Modus runtime to the model
  implementation. Generally, you don't need to create `ModelInfo` instances
  directly.

  However, if you are implementing a custom model, you may wish to use a property
  from this class, such as `fullName`, for model providers that require the model
  name in the input request body.
</Info>

```ts
class ModelInfo {
  readonly name: string
  readonly fullName: string
}
```

<ResponseField name="name" type="string">
  The name of the model from the app manifest.
</ResponseField>

<ResponseField name="fullName" type="string">
  The full name or identifier of the model, as defined by the model provider.
</ResponseField>


# MySQL APIs
Source: https://docs.hypermode.com/modus/sdk/assemblyscript/mysql

Execute queries against a MySQL database

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="AssemblyScript" feature="MySQL" />

The Modus MySQL APIs allow you to run queries against MySQL or any
MySQL-compatible database platform.

## Import

To begin, import the `mysql` namespace from the SDK:

```ts
import { mysql } from "@hypermode/modus-sdk-as"
```

## MySQL APIs

{/* vale Google.Headings = NO */}

The APIs in the `mysql` namespace are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### execute

Execute a SQL statement against a MySQL database, without any data returned. Use
this for insert, update, or delete operations, or for other SQL statements that
don't return data.

<Note>
  The `execute` function is for operations that don't return data. However, some
  insert/update/delete operations may still return data. In these cases, you can
  use the `queryScalar` or `query` functions instead.
</Note>

```ts
function execute(
  connection: string,
  statement: string,
  params?: Params,
): Response
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="statement" type="string" required>
  SQL statement containing the query or mutation operation to execute.

  <Warning>
    While it's possible to directly include parameter values into your SQL
    statement, it's highly recommended to pass a [`Params`](#params) object
    instead. This can help to protect against injection attacks and other security
    vulnerabilities.
  </Warning>
</ResponseField>

<ResponseField name="params" type="Params">
  Optional parameters to include with the query.

  See the details of the [`Params`](#params) object for more information.
</ResponseField>

#### query

Execute a SQL statement against a MySQL database, returning a set of rows. In
the results, each row converts to an object of type `T`, with fields matching
the column names.

```ts
function query<T>(
  connection: string,
  statement: string,
  params?: Params,
): QueryResponse<T>
```

<ResponseField name="T" required>
  Type of object to use for the data returned from the query. This can be any
  type, including a custom type defined in your project. It should match the shape
  of the row returned from the SQL query.

  <Tip>
    Define custom types in the app's source code. In AssemblyScript, create classes
    decorated with `@json`.

    All types, including classes, base classes, and field types must be JSON
    serializable. You can also use built-in types such as strings, numbers, arrays,
    and maps.

    If working with MySQL's `point` data type, you can use a [`Point`](#point) or
    [`Location`](#location) object to represent the data.
  </Tip>
</ResponseField>

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="statement" type="string" required>
  SQL statement containing the query or mutation operation to execute.

  <Warning>
    While it's possible to directly include parameter values into your SQL
    statement, it's highly recommended to pass a [`Params`](#params) object
    instead. This can help to protect against injection attacks and other security
    vulnerabilities.
  </Warning>
</ResponseField>

<ResponseField name="params" type="Params">
  Optional parameters to include with the query.

  See the details of the [`Params`](#params) object for more information.
</ResponseField>

#### queryScalar

Execute a SQL statement against a MySQL database, returning a single scalar
value. For example, the result could be a count, sum, or average, or it could be
an identifier.

```ts
function queryScalar<T>(
  connection: string,
  statement: string,
  params?: Params,
): ScalarResponse<T>
```

<ResponseField name="T" required>
  Type of object to use for the data returned from the query. This should
  generally be a scalar data type, such as a number or string. It should match
  the type of the data returned from the SQL query.
</ResponseField>

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="statement" type="string" required>
  SQL statement containing the query or mutation operation to execute.

  <Warning>
    While it's possible to directly include parameter values into your SQL
    statement, it's highly recommended to pass a [`Params`](#params) object
    instead. This can help to protect against injection attacks and other security
    vulnerabilities.
  </Warning>
</ResponseField>

<ResponseField name="params" type="Params">
  Optional parameters to include with the query.

  See the details of the [`Params`](#params) object for more information.
</ResponseField>

### Types

#### Location

Represents a location on Earth, having `longitude` and `latitude` coordinates.

Correctly serializes to and from MySQL's point type, in (longitude, latitude)
order.

<Info>
  This class is identical to the [Point](#point) class, but uses different field
  names.
</Info>

```ts
class Location {
 longitude: f64,
 latitude: f64,
}
```

<ResponseField name="longitude" type="f64" required>
  The longitude coordinate of the location, in degrees.
</ResponseField>

<ResponseField name="latitude" type="f64" required>
  The latitude coordinate of the location, in degrees.
</ResponseField>

#### Params

A container for parameters to include with a SQL operation.

To use this feature, create a new `Params` object and call the `push` method for
each parameter you want to include. Then pass the object to the `execute`,
`query`, or `queryScalar` function along with your SQL statement.

```ts
class Params {
  push<T>(value: T): void
  toJSON(): string
}
```

<ResponseField name="push(value)">
  Push a parameter value into the list included with the SQL operation. The
  sequence of calls to `push` determines the order of the parameters in the SQL
  statement. This corresponds to the order of the `?` placeholders or `$1`, `$2`,
  etc.

  <Expandable title="parameters">
    {/* markdownlint-disable MD046 */}

    <ResponseField name="value" required>
      The value of the parameter to include in the SQL operation.

      The value can be of any type that's JSON serializable, including strings,
      numbers, boolean values, arrays, maps, and custom objects decorated with
      `@json`, as long as the database supports it.

      <Tip>
        If working with MySQL's `Point` data type, you can either pass separate
        parameters for the coordinates and use a `point()` function in the SQL
        statement, or you can pass a [`Point`](#point) or [`Location`](#location)
        object as a single parameter.
      </Tip>
    </ResponseField>

    {/* markdownlint-restore MD046 */}
  </Expandable>
</ResponseField>

<ResponseField name="toJSON()" type="string">
  Serializes the parameters to a JSON string for inclusion in the SQL operation.
  The SDK functions call this automatically when you pass a `Params` object. You
  typically don't need to call it directly.
</ResponseField>

#### Point

Represents a point in 2D space, having `x` and `y` coordinates. Correctly
serializes to and from MySQL's point type, in (x, y) order.

<Info>
  This class is identical to the [Location](#location) class, but uses different
  field names.
</Info>

```ts
class Point {
 x: f64,
 y: f64,
}
```

<ResponseField name="x" type="f64" required>
  The x coordinate of the point.
</ResponseField>

<ResponseField name="y" type="f64" required>
  The y coordinate of the point.
</ResponseField>

#### QueryResponse

Represents the response from a [`query`](#query) operation.

```ts
class QueryResponse<T> {
  error: string | null
  rowsAffected: u32
  lastInsertId: u64
  rows: T[]
}
```

<ResponseField name="error" type="string | null">
  An error message, if an error occurred during the operation. Otherwise, this
  field is `null`.
</ResponseField>

<ResponseField name="rowsAffected" type="u32">
  The number of rows affected by the operation, which typically corresponds to
  the number of rows returned.
</ResponseField>

<ResponseField name="lastInsertId" type="u64">
  When inserting a row, this field contains the ID of the last inserted row.
  This is useful for tables with auto-incrementing primary keys.
</ResponseField>

<ResponseField name="rows" type="T[]">
  An array of objects, each representing a row returned from the query. Each
  object has fields corresponding to the columns in the result set.
</ResponseField>

#### Response

Represents the response from an [`execute`](#execute) operation. Also serves as
the base class for `QueryResponse<T>` and `ScalarResponse<T>`.

```ts
class Response {
  error: string | null
  rowsAffected: u32
  lastInsertId: u64
}
```

<ResponseField name="error" type="string | null">
  An error message, if an error occurred during the operation. Otherwise, this
  field is `null`.
</ResponseField>

<ResponseField name="rowsAffected" type="u32">
  The number of rows affected by the operation.
</ResponseField>

<ResponseField name="lastInsertId" type="u64">
  When inserting a row, this field contains the ID of the last inserted row.
  This is useful for tables with auto-incrementing primary keys.
</ResponseField>

#### ScalarResponse

Represents the response from a [`queryScalar`](#queryscalar) operation.

```ts
class ScalarResponse<T> {
  error: string | null
  rowsAffected: u32
  lastInsertId: u64
  value: T
}
```

<ResponseField name="error" type="string | null">
  An error message, if an error occurred during the operation. Otherwise, this
  field is `null`.
</ResponseField>

<ResponseField name="rowsAffected" type="u32">
  The number of rows affected by the operation, which is typically 1 for a
  scalar query.
</ResponseField>

<ResponseField name="lastInsertId" type="u64">
  When inserting a row, this field contains the ID of the last inserted row.
  This is useful for tables with auto-incrementing primary keys.
</ResponseField>

<ResponseField name="value" type="T">
  The scalar value returned from the query.
</ResponseField>


# Neo4j APIs
Source: https://docs.hypermode.com/modus/sdk/assemblyscript/neo4j

Execute queries and mutations against a Neo4j database

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="AssemblyScript" feature="Neo4j" />

The Modus Neo4j APIs allow you to run queries and mutations against a Neo4j
database.

## Import

To begin, import the `neo4j` namespace from the SDK:

```ts
import { neo4j } from "@hypermode/modus-sdk-as"
```

## Neo4j APIs

{/* vale Google.Headings = NO */}

The APIs in the `neo4j` namespace are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### executeQuery

Executes a Cypher query on the Neo4j database.

```ts
function executeQuery(
  connection: string,
  query: string,
  parameters: Variables = new Variables(),
  dbName: string = "neo4j",
): EagerResult
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="query" type="string" required>
  A Neo4j Cypher query to execute.
</ResponseField>

<ResponseField name="parameters" type="Variables">
  The parameters to pass to the query.
</ResponseField>

<ResponseField name="dbName" type="string">
  An optional database name to use. Defaults to "neo4j" if not provided.
</ResponseField>

### Types

#### EagerResult

The result of a Neo4j query.

```ts
class EagerResult {
  Keys: string[]
  Records: Record[]
}
```

<ResponseField name="keys" type="string[]">
  The keys of the result.
</ResponseField>

<ResponseField name="records" type="Record[]">
  The records of the result.
</ResponseField>

#### Entity

An abstract class representing possible entities in a Neo4j query result.

```ts
abstract class Entity {
  elementId: string
  props: DynamicMap
}
```

<ResponseField name="elementId" type="string">
  The ID of the entity.
</ResponseField>

<ResponseField name="props" type="DynamicMap">
  The properties of the entity.
</ResponseField>

#### Node

A node in a Neo4j query result.

```ts
class Node extends Entity {
  elementId: string // from base class
  props: DynamicMap // from base class
  labels: string[]
}
```

<ResponseField name="elementId" type="string">
  The ID of the node.
</ResponseField>

<ResponseField name="props" type="DynamicMap">
  The properties of the node.
</ResponseField>

<ResponseField name="labels" type="string[]">
  The labels of the node.
</ResponseField>

#### Path

A path in a Neo4j query result.

```ts
class Path {
  nodes: Node[]
  relationships: Relationship[]
}
```

<ResponseField name="nodes" type="Node[]">
  The nodes in the path.
</ResponseField>

<ResponseField name="relationships" type="Relationship[]">
  The relationships in the path.
</ResponseField>

#### Point2D

A 2D point in a Neo4j query result.

```ts
class Point2D {
  x: f64
  y: f64
  spatialRefId: u32
}
```

<ResponseField name="x" type="f64">
  The X coordinate of the point.
</ResponseField>

<ResponseField name="y" type="f64">
  The Y coordinate of the point.
</ResponseField>

<ResponseField name="spatialRefId" type="u32">
  The spatial reference ID of the point.
</ResponseField>

#### Point3D

A 3D point in a Neo4j query result.

```ts
class Point3D {
  x: f64
  y: f64
  z: f64
  spatialRefId: u32
}
```

<ResponseField name="x" type="f64">
  The X coordinate of the point.
</ResponseField>

<ResponseField name="y" type="f64">
  The Y coordinate of the point.
</ResponseField>

<ResponseField name="z" type="f64">
  The Z coordinate of the point.
</ResponseField>

<ResponseField name="spatialRefId" type="u32">
  The spatial reference ID of the point.
</ResponseField>

#### Record

A record in a Neo4j query result.

```ts
class Record {
  keys: string[]
  values: string[]
  get(key: string): string
  getValue<T>(key: string): T
  asMap(): Map<string, string>
}
```

<ResponseField name="keys" type="string[]">
  The keys of the record.
</ResponseField>

<ResponseField name="values" type="string[]">
  The values of the record.
</ResponseField>

<ResponseField name="get(key)" type="method">
  Get a value from a record at a given key as a JSON encoded string.

  <Info>
    Usually, you should use `getValue<T>(key)` instead of this method.
  </Info>
</ResponseField>

<ResponseField name="getValue<T>(key)" type="method">
  Get a value from a record at a given key and cast or decode it to a specific
  type.
</ResponseField>

<ResponseField name="asMap()" type="method">
  Convert the record to a map of keys and JSON encoded string values.
</ResponseField>

#### Relationship

A relationship in a Neo4j query result.

```ts
class Relationship extends Entity {
  elementId: string // from base class
  props: DynamicMap // from base class
  startElementId: string
  endElementId: string
  type: string
}
```

<ResponseField name="elementId" type="string">
  The ID of the relationship.
</ResponseField>

<ResponseField name="props" type="DynamicMap">
  The properties of the relationship.
</ResponseField>

<ResponseField name="startElementId" type="string">
  The ID of the start node.
</ResponseField>

<ResponseField name="endElementId" type="string">
  The ID of the end node.
</ResponseField>

<ResponseField name="type" type="string">
  The type of the relationship.
</ResponseField>


# Modus AssemblyScript SDK
Source: https://docs.hypermode.com/modus/sdk/assemblyscript/overview

Learn how to use the Modus AssemblyScript SDK

export const language_0 = "AssemblyScript"

The Modus {language_0} SDK provides a set of APIs that allow you to interact with
various databases and services in Modus apps written in {language_0}. It is
designed to work seamlessly with the Modus platform, enabling you to build
powerful apps with ease.

<Info>
  The Modus SDKs come in multiple languages, each following the conventions for
  that language, including its capabilities and limitations. While each SDK
  provides similar features, the APIs may differ slightly between languages. Be
  sure to refer to the documentation for the specific SDK you're using.

  Wherever possible, we've tried to keep the APIs consistent across languages.
  However, you may find differences in the APIs due to language-specific
  constraints.

  If you have any questions or need help, please reach out to us via the `#modus`
  channel on the [Hypermode Discord server](https://discord.hypermode.com/).
</Info>

## Importing the APIs

The Modus AssemblyScript SDK organizes its APIs into namespaces based on their
purpose. In your AssemblyScript code, you can import each namespace you'd like
to use like this:

```ts
import { <namespace> } from "@hypermode/modus-sdk-as"
```

## Available APIs

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

The following namespaces are available. They're grouped into categories below
for your convenience. Click on a namespace to view more information about its
APIs.

### Client APIs

| Namespace              | Purpose                                                                            |
| :--------------------- | :--------------------------------------------------------------------------------- |
| [`http`](./http)       | Provides access to external HTTP endpoints, including REST APIs and other services |
| [`graphql`](./graphql) | Allows you to securely call and fetch data from any GraphQL endpoint               |

### Database APIs

| Namespace                    | Purpose                                         |
| :--------------------------- | :---------------------------------------------- |
| [`dgraph`](./dgraph)         | Access and modify data in a Dgraph database     |
| [`mysql`](./mysql)           | Access and modify data in a MySQL database      |
| [`neo4j`](./neo4j)           | Access and modify data in a Neo4j database      |
| [`postgresql`](./postgresql) | Access and modify data in a PostgreSQL database |

### Inference APIs

| Namespace            | Purpose                                                                                        |
| :------------------- | :--------------------------------------------------------------------------------------------- |
| [`models`](./models) | Invoke AI models, including large language models, embedding models, and classification models |

### Storage APIs

| Namespace                      | Purpose                                                    |
| :----------------------------- | :--------------------------------------------------------- |
| [`collections`](./collections) | Provides integrated storage and vector search capabilities |

### System APIs

| Namespace                  | Purpose                                                  |
| :------------------------- | :------------------------------------------------------- |
| [`console`](./console)     | Provides logging, assertion, and timing functions        |
| [`localtime`](./localtime) | Allows you to access the user's local time and time zone |


# PostgreSQL APIs
Source: https://docs.hypermode.com/modus/sdk/assemblyscript/postgresql

Execute queries against a PostgreSQL database

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="AssemblyScript" feature="PostgreSQL" />

The Modus PostgreSQL APIs allow you to run queries against PostgreSQL or any
PostgreSQL-compatible database platform.

## Import

To begin, import the `postgresql` namespace from the SDK:

```ts
import { postgresql } from "@hypermode/modus-sdk-as"
```

## PostgreSQL APIs

{/* vale Google.Headings = NO */}

The APIs in the `postgresql` namespace are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### execute

Execute a SQL statement against a PostgreSQL database, without any data
returned. Use this for insert, update, or delete operations, or for other SQL
statements that don't return data.

<Note>
  The `execute` function is for operations that don't return data. However, some
  insert/update/delete operations may still return data. In these cases, you can
  use the `queryScalar` or `query` functions instead.
</Note>

```ts
function execute(
  connection: string,
  statement: string,
  params?: Params,
): Response
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="statement" type="string" required>
  SQL statement containing the query or mutation operation to execute.

  <Warning>
    While it's possible to directly include parameter values into your SQL
    statement, it's highly recommended to pass a [`Params`](#params) object
    instead. This can help to protect against injection attacks and other security
    vulnerabilities.
  </Warning>
</ResponseField>

<ResponseField name="params" type="Params">
  Optional parameters to include with the query.

  See the details of the [`Params`](#params) object for more information.
</ResponseField>

#### query

Execute a SQL statement against a PostgreSQL database, returning a set of rows.
In the results, each row converts to an object of type `T`, with fields matching
the column names.

```ts
function query<T>(
  connection: string,
  statement: string,
  params?: Params,
): QueryResponse<T>
```

<ResponseField name="T" required>
  Type of object to use for the data returned from the query. This can be any
  type, including a custom type defined in your project. It should match the shape
  of the row returned from the SQL query.

  <Tip>
    Define custom types in the app's source code. In AssemblyScript, create classes
    decorated with `@json`.

    All types, including classes, base classes, and field types must be JSON
    serializable. You can also use built-in types such as strings, numbers, arrays,
    and maps.

    If working with PostgreSQL's `point` data type, you can use a [`Point`](#point)
    or [`Location`](#location) object to represent the data.
  </Tip>
</ResponseField>

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="statement" type="string" required>
  SQL statement containing the query or mutation operation to execute.

  <Warning>
    While it's possible to directly include parameter values into your SQL
    statement, it's highly recommended to pass a [`Params`](#params) object
    instead. This can help to protect against injection attacks and other security
    vulnerabilities.
  </Warning>
</ResponseField>

<ResponseField name="params" type="Params">
  Optional parameters to include with the query.

  See the details of the [`Params`](#params) object for more information.
</ResponseField>

#### queryScalar

Execute a SQL statement against a PostgreSQL database, returning a single scalar
value. For example, the result could be a count, sum, or average, or it could be
an identifier.

```ts
function queryScalar<T>(
  connection: string,
  statement: string,
  params?: Params,
): ScalarResponse<T>
```

<ResponseField name="T" required>
  Type of object to use for the data returned from the query. This should
  generally be a scalar data type, such as a number or string. It should match
  the type of the data returned from the SQL query.
</ResponseField>

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="statement" type="string" required>
  SQL statement containing the query or mutation operation to execute.

  <Warning>
    While it's possible to directly include parameter values into your SQL
    statement, it's highly recommended to pass a [`Params`](#params) object
    instead. This can help to protect against injection attacks and other security
    vulnerabilities.
  </Warning>
</ResponseField>

<ResponseField name="params" type="Params">
  Optional parameters to include with the query.

  See the details of the [`Params`](#params) object for more information.
</ResponseField>

### Types

#### Location

Represents a location on Earth, having `longitude` and `latitude` coordinates.

Correctly serializes to and from PostgreSQL's point type, in (longitude,
latitude) order.

<Info>
  This class is identical to the [Point](#point) class, but uses different field
  names.
</Info>

```ts
class Location {
 longitude: f64,
 latitude: f64,
}
```

<ResponseField name="longitude" type="f64" required>
  The longitude coordinate of the location, in degrees.
</ResponseField>

<ResponseField name="latitude" type="f64" required>
  The latitude coordinate of the location, in degrees.
</ResponseField>

#### Params

A container for parameters to include with a SQL operation.

To use this feature, create a new `Params` object and call the `push` method for
each parameter you want to include. Then pass the object to the `execute`,
`query`, or `queryScalar` function along with your SQL statement.

```ts
class Params {
  push<T>(value: T): void
  toJSON(): string
}
```

<ResponseField name="push(value)">
  Push a parameter value into the list included with the SQL operation. The
  sequence of calls to `push` determines the order of the parameters in the SQL
  statement. This corresponds to the order of the `?` placeholders or `$1`, `$2`,
  etc.

  <Expandable title="parameters">
    {/* markdownlint-disable MD046 */}

    <ResponseField name="value" required>
      The value of the parameter to include in the SQL operation.

      The value can be of any type that's JSON serializable, including strings,
      numbers, boolean values, arrays, maps, and custom objects decorated with
      `@json`, as long as the database supports it.

      <Tip>
        If working with PostgreSQL's `Point` data type, you can either pass separate
        parameters for the coordinates and use a `point()` function in the SQL
        statement, or you can pass a [`Point`](#point) or [`Location`](#location)
        object as a single parameter.
      </Tip>
    </ResponseField>

    {/* markdownlint-restore MD046 */}
  </Expandable>
</ResponseField>

<ResponseField name="toJSON()" type="string">
  Serializes the parameters to a JSON string for inclusion in the SQL operation.
  The SDK functions call this automatically when you pass a `Params` object. You
  typically don't need to call it directly.
</ResponseField>

#### Point

Represents a point in 2D space, having `x` and `y` coordinates. Correctly
serializes to and from PostgreSQL's point type, in (x, y) order.

<Info>
  This class is identical to the [Location](#location) class, but uses different
  field names.
</Info>

```ts
class Point {
 x: f64,
 y: f64,
}
```

<ResponseField name="x" type="f64" required>
  The x coordinate of the point.
</ResponseField>

<ResponseField name="y" type="f64" required>
  The y coordinate of the point.
</ResponseField>

#### QueryResponse

Represents the response from a [`query`](#query) operation.

```ts
class QueryResponse<T> {
  error: string | null
  rowsAffected: u32
  lastInsertId: u64
  rows: T[]
}
```

<ResponseField name="error" type="string | null">
  An error message, if an error occurred during the operation. Otherwise, this
  field is `null`.
</ResponseField>

<ResponseField name="rowsAffected" type="u32">
  The number of rows affected by the operation, which typically corresponds to
  the number of rows returned.
</ResponseField>

<ResponseField name="lastInsertId" type="u64">
  This field is available for other database types, but isn't populated for
  PostgreSQL. Instead, use `query` or `queryScalar` with a `RETURNING` clause to
  get the last inserted ID.
</ResponseField>

<ResponseField name="rows" type="T[]">
  An array of objects, each representing a row returned from the query. Each
  object has fields corresponding to the columns in the result set.
</ResponseField>

#### Response

Represents the response from an [`execute`](#execute) operation. Also serves as
the base class for `QueryResponse<T>` and `ScalarResponse<T>`.

```ts
class Response {
  error: string | null
  rowsAffected: u32
  lastInsertId: u64
}
```

<ResponseField name="error" type="string | null">
  An error message, if an error occurred during the operation. Otherwise, this
  field is `null`.
</ResponseField>

<ResponseField name="rowsAffected" type="u32">
  The number of rows affected by the operation.
</ResponseField>

<ResponseField name="lastInsertId" type="u64">
  This field is available for other database types, but isn't populated for
  PostgreSQL. Instead, use `query` or `queryScalar` with a `RETURNING` clause to
  get the last inserted ID.
</ResponseField>

#### ScalarResponse

Represents the response from a [`queryScalar`](#queryscalar) operation.

```ts
class ScalarResponse<T> {
  error: string | null
  rowsAffected: u32
  lastInsertId: u64
  value: T
}
```

<ResponseField name="error" type="string | null">
  An error message, if an error occurred during the operation. Otherwise, this
  field is `null`.
</ResponseField>

<ResponseField name="rowsAffected" type="u32">
  The number of rows affected by the operation, which is typically 1 for a
  scalar query.
</ResponseField>

<ResponseField name="lastInsertId" type="u64">
  This field is available for other database types, but isn't populated for
  PostgreSQL. Instead, use `query` or `queryScalar` with a `RETURNING` clause to
  get the last inserted ID.
</ResponseField>

<ResponseField name="value" type="T">
  The scalar value returned from the query.
</ResponseField>


# Collections APIs
Source: https://docs.hypermode.com/modus/sdk/go/collections

Add storage and vector search capabilities to your functions.

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="Go" feature="Collections" />

The Modus Collection APIs allow you to add vector search within your functions.

## Import

To begin, import the `collections` package from the SDK:

```go
import "github.com/hypermodeinc/modus/sdk/go/pkg/collections"
```

## Collections APIs

{/* vale Google.Headings = NO */}

The APIs in the `collections` package are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Mutation Functions

#### Upsert

Inserts or updates an item in a collection.

<Note>
  If the item already exists, the function overwrites the previous value. If
  not, it creates a new one.
</Note>

```go
func Upsert(
  collection string,
  key *string,
  text string,
  labels []string,
  opts ...NamespaceOption
) (*CollectionMutationResult, error)
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="key" type="*string">
  The unique identifier for the item in the namespace. If `nil`, the function
  generates a unique identifier.
</ResponseField>

<ResponseField name="text" type="string" required>
  The text of the item to add to the collection.
</ResponseField>

<ResponseField name="labels" type="[]string">
  An optional slice of labels to associate with the item.
</ResponseField>

<ResponseField name="opts" type="...NamespaceOption">
  Associates the item with a specific namespace. Defaults to an empty namespace
  if not provided.

  Pass `collections.WithNamespace("namespace")` to specify a namespace.
</ResponseField>

#### UpsertBatch

Inserts or updates a batch of items into a collection.

<Note>
  If an item with the same key already exists, the original text is overwritten
  with the new text.
</Note>

```go
func UpsertBatch(
  collection string,
  keys []string,
  texts []string,
  labelsArr [][]string,
  opts ...NamespaceOption
) (*CollectionMutationResult, error)
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="keys" type="[]string">
  Slice of keys for the item to add to the collection. If you pass `nil` for any
  key, Hypermode assigns a new UUID as the key for the item.
</ResponseField>

<ResponseField name="texts" type="[]string" required>
  Slice of texts for the items to add to the collection.
</ResponseField>

<ResponseField name="labelsArr" type="[][]string">
  An optional slice of slices of labels to associate with the items.
</ResponseField>

<ResponseField name="opts" type="...NamespaceOption">
  Associates the item with a specific namespace. Defaults to an empty namespace
  if not provided.

  Pass `collections.WithNamespace("namespace")` to specify a namespace.
</ResponseField>

#### Remove

Removes an item from the collection.

```go
func Remove(
  collection string,
  key string,
  opts ...NamespaceOption
) (*CollectionMutationResult, error)
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="key" type="string" required>
  The key of the item to delete from the collection.
</ResponseField>

<ResponseField name="opts" type="...NamespaceOption">
  The namespace to remove the item from. Defaults to the default namespace if
  not provided.

  Pass `collections.WithNamespace("namespace")` to specify a namespace.
</ResponseField>

### Search and Retrieval Functions

#### ComputeDistance

Computes distance between two keys in a collection using a search method's
embedder.

```go
func ComputeDistance(
  collection string,
  searchMethod string,
  key1 string,
  key2 string,
  opts ...NamespaceOption
) (*CollectionSearchResultObject, error)
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="searchMethod" type="string" required>
  The search method used to calculate embedding for key's texts.
</ResponseField>

<ResponseField name="key1, key2" type="string" required>
  Keys to compute similarity on.
</ResponseField>

<ResponseField name="opts" type="...NamespaceOption">
  The namespace to search the items from. Defaults to the default namespace if
  not provided.

  Pass `collections.WithNamespace("namespace")` to specify a namespace.
</ResponseField>

#### NnClassify

Classify an item in the collection using previous vectors' labels.

```go
func NnClassify(
  collection string,
  searchMethod string,
  text string,
  opts ...NamespaceOption
) (*CollectionClassificationResult, error)
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="searchMethod" type="string" required>
  The search method used to calculate embedding for text & search against.
</ResponseField>

<ResponseField name="text" type="string" required>
  The text to compute natural language search on.
</ResponseField>

<ResponseField name="opts" type="...NamespaceOption">
  The namespace to search the items from. Defaults to the default namespace if
  not provided.

  Pass `collections.WithNamespace("namespace")` to specify a namespace.
</ResponseField>

#### GetLabels

Get the labels for an item in a collection.

```go
func GetLabels(
  collection string,
  key string,
  opts ...NamespaceOption
) ([]string, error)
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="key" type="string" required>
  The key of the item to retrieve.
</ResponseField>

<ResponseField name="opts" type="...NamespaceOption">
  The namespace to get the item from. Defaults to the default namespace if not
  provided.

  Pass `collections.WithNamespace("namespace")` to specify a namespace.
</ResponseField>

#### GetNamespaces

Get all namespaces in a collection.

```go
func GetNamespaces(collection string) ([]string, error)
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

#### GetText

Gets an item's text from a collection, give the item's key.

```go
func GetText(
  collection string,
  key string,
  opts ...NamespaceOption
) (string, error)
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="key" type="string" required>
  The key of the item to retrieve.
</ResponseField>

<ResponseField name="opts" type="...NamespaceOption">
  The namespace to get the item from. Defaults to the default namespace if not
  provided.

  Pass `collections.WithNamespace("namespace")` to specify a namespace.
</ResponseField>

#### GetTexts

Get all items from a collection. The result is a map of key to text for all
items in the collection.

```go
func GetTexts(
  collection string,
  opts ...NamespaceOption
) (map[string]string, error)
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="opts" type="...NamespaceOption">
  The namespace to get the items from. Defaults to the default namespace if not
  provided.

  Pass `collections.WithNamespace("namespace")` to specify a namespace.
</ResponseField>

#### GetVector

Get the vector for an item in a collection.

```go
func GetVector(
  collection string,
  searchMethod string,
  key string,
  opts ...NamespaceOption
) ([]float32, error)
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="searchMethod" type="string" required>
  The search method used to calculate embedding for key's texts.
</ResponseField>

<ResponseField name="key" type="string" required>
  The key of the item to retrieve.
</ResponseField>

<ResponseField name="opts" type="...NamespaceOption">
  The namespace to get the item from. Defaults to the default namespace if not
  provided.

  Pass `collections.WithNamespace("namespace")` to specify a namespace.
</ResponseField>

#### Search

Perform a natural language search on items within a collection. This method is
useful for finding items that match a search query based on semantic meaning.

<Note>
  Modus uses the same embedder for both inserting text into the collection, and
  for the text used when searching the collection.
</Note>

```go
func Search(
  collection string,
  searchMethod string,
  text string,
  opts ...SearchOption
) (*CollectionSearchResult, error)
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="searchMethod" type="string" required>
  The search method used to calculate embedding for text & search against.
</ResponseField>

<ResponseField name="text" type="string" required>
  The text to compute natural language search on.
</ResponseField>

<ResponseField name="opts" type="...SearchOption">
  Additional options for the search:

  * `collections.WithLimit(limit int)`: The number of result objects to return.
  * `collections.WithReturnText(returnText bool)`: A flag to return the texts in
    the response.
  * `collections.WithNamespaces(namespaces []string)`: A list of namespaces to
    search the item from. Defaults to the default namespace if not provided.
</ResponseField>

#### SearchByVector

Perform a vector-based search on a collection, which is helpful for scenarios
requiring precise similarity calculations between pre-computed embeddings.

<Note>
  Modus uses the same embedder for both inserting text into the collection, and
  for the vector used when searching the collection.
</Note>

```go
func SearchByVector(
  collection string,
  searchMethod string,
  vector []float32,
  opts ...SearchOption
) (*CollectionSearchResult, error)
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="searchMethod" type="string" required>
  The search method used to calculate embedding for vector & search against.
</ResponseField>

<ResponseField name="vector" type="[]float32" required>
  The vector to compute search on.
</ResponseField>

<ResponseField name="opts" type="...SearchOption">
  Additional options for the search:

  * `collections.WithLimit(limit int)`: The number of result objects to return.
  * `collections.WithReturnText(returnText bool)`: A flag to return the texts in
    the response.
  * `collections.WithNamespaces(namespaces []string)`: A list of namespaces to
    search the item from. Defaults to the default namespace if not provided.
</ResponseField>

### Maintenance Functions

#### RecomputeSearchMethod

Recalculates the embeddings for all items in a collection. It can be
resource-intensive, use it when necessary, for example after you have updated
the method for embedding calculation and want to re-compute the embeddings for
existing data in the collection.

```go
func RecomputeSearchMethod(
  collection string,
  searchMethod string,
  opts ...NamespaceOption
) (*SearchMethodMutationResult, error)
```

<ResponseField name="collection" type="string" required>
  Name of the collection, as [defined in the
  manifest](/modus/app-manifest#collections).
</ResponseField>

<ResponseField name="searchMethod" type="string" required>
  The search method to recompute embeddings for.
</ResponseField>

<ResponseField name="opts" type="...NamespaceOption">
  The namespace to use. Defaults to the default namespace if not provided.

  Pass `collections.WithNamespace("namespace")` to specify a namespace.
</ResponseField>

### Types

#### CollectionClassificationLabelObject

Represents a classification label.

```go
type CollectionClassificationLabelObject struct {
  Label string
  Confidence float64
}
```

<ResponseField name="Label" type="string">
  The classification label.
</ResponseField>

<ResponseField name="Confidence" type="float64">
  The confidence score of the classification label.
</ResponseField>

#### CollectionClassificationResult

Represents the result of a classification operation on a collection.

```go
type CollectionClassificationResult struct {
  Collection string
  Status string
  Error string
  SearchMethod string
  LabelsResult []*CollectionClassificationLabelObject
  Cluster []*CollectionClassificationResultObject
}
```

<ResponseField name="Collection" type="string">
  Name of the collection.
</ResponseField>

<ResponseField name="Status" type="string">
  The status of the operation.
</ResponseField>

<ResponseField name="Error" type="string">
  Error message, if any.
</ResponseField>

<ResponseField name="SearchMethod" type="string">
  The search method used in the operation.
</ResponseField>

<ResponseField name="LabelsResult" type="[]*CollectionClassificationLabelObject">
  The classification labels.
</ResponseField>

<ResponseField name="Cluster" type="[]*CollectionClassificationResultObject">
  The classification results.
</ResponseField>

#### CollectionClassificationResultObject

Represents an object in the classification results.

```go
type CollectionClassificationResultObject struct {
  Key string
  Labels []string
  Distance float64
  Score float64
}
```

<ResponseField name="Key" type="string">
  The key of the item classified.
</ResponseField>

<ResponseField name="Labels" type="[]string">
  The classification labels.
</ResponseField>

<ResponseField name="Distance" type="float64">
  The distance of the item from the classification labels.
</ResponseField>

<ResponseField name="Score" type="float64">
  The similarity score of the item classified.
</ResponseField>

#### CollectionMutationResult

Represents the result of a mutation operation on a collection.

```go
type CollectionMutationResult struct {
  Collection string
  Status string
  Error string
  Operation string
  Keys []string
}
```

<ResponseField name="Collection" type="string">
  Name of the collection.
</ResponseField>

<ResponseField name="Status" type="string">
  The status of the operation.
</ResponseField>

<ResponseField name="Error" type="string">
  Error message, if any.
</ResponseField>

<ResponseField name="Operation" type="string">
  The operation performed.
</ResponseField>

<ResponseField name="Keys" type="[]string">
  The keys of the items affected by the operation.
</ResponseField>

#### CollectionSearchResult

Represents the result of a search operation on a collection.

```go
type CollectionSearchResult struct {
  Collection string
  Status string
  Error string
  SearchMethod string
  Objects []*CollectionSearchResultObject
}
```

<ResponseField name="Collection" type="string">
  Name of the collection.
</ResponseField>

<ResponseField name="Status" type="string">
  The status of the operation.
</ResponseField>

<ResponseField name="Error" type="string">
  Error message, if any.
</ResponseField>

<ResponseField name="SearchMethod" type="string">
  The search method used in the operation.
</ResponseField>

<ResponseField name="Objects" type="[]*CollectionSearchResultObject">
  The search results.
</ResponseField>

#### CollectionSearchResultObject

Represents an object in the search results.

```go
type CollectionSearchResultObject struct {
  Namespace string
  Key string
  Text string
  Labels []string
  Distance float64
  Score float64
}
```

<ResponseField name="Namespace" type="string">
  The namespace of the item found as part of the search.
</ResponseField>

<ResponseField name="Key" type="string">
  The key of the item found as part of the search.
</ResponseField>

<ResponseField name="Text" type="string">
  The text of the item found as part of the search.
</ResponseField>

<ResponseField name="Distance" type="float64">
  The distance of the item from the search text.
</ResponseField>

<ResponseField name="Score" type="float64">
  The similarity score of the item found, as it pertains to the search.
</ResponseField>

#### CollectionStatus

The status of a collection operation.

```go
type CollectionStatus = string

const (
  Success CollectionStatus = "success"
  Error   CollectionStatus = "error"
)
```

<ResponseField name="Success">The operation was successful.</ResponseField>

<ResponseField name="Error">The operation encountered an error.</ResponseField>

#### SearchMethodMutationResult

Represents the result of a mutation operation on a search method.

```go
type SearchMethodMutationResult struct {
  Collection string
  Status string
  Error string
  Operation string
  SearchMethod string
}
```

<ResponseField name="Collection" type="string">
  Name of the collection.
</ResponseField>

<ResponseField name="Status" type="string">
  The status of the operation.
</ResponseField>

<ResponseField name="Error" type="string">
  Error message, if any.
</ResponseField>

<ResponseField name="Operation" type="string">
  The operation performed.
</ResponseField>

<ResponseField name="SearchMethod" type="string">
  The search method affected by the operation.
</ResponseField>


# Console APIs
Source: https://docs.hypermode.com/modus/sdk/go/console

Capture errors and debugging information in your functions

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="Go" feature="Console" />

The Modus Console APIs allow you to capture information from your functions,
such as errors and other information that can help you debug your functions.

<Info>
  In addition to the Console APIs, you can also use any standard Go function that
  emits output to `stdout` or `stderr`. The output from these functions are
  available in the runtime logs.

  For example, you may use:

  * `fmt.Println` to write to informational messages to the logs
  * `fmt.Fprintf(os.Stderr, ...)` to write error messages to the logs

  You may also *return* `error` objects from your function, whose messages get
  captured and returned in the GraphQL response.
</Info>

## Import

To begin, import the `console` package from the SDK:

```go
import "github.com/hypermodeinc/modus/sdk/go/pkg/console"
```

## Console APIs

{/* vale Google.Headings = NO */}

The APIs in the `console` package are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Assertion Functions

#### Assert

Asserts that a condition is true, and logs an error if it's not.

```go
func Assert(condition bool, message string)
```

<ResponseField name="condition" type="bool" required>
  The boolean condition to assert.
</ResponseField>

<ResponseField name="message" type="string" required>
  An error message to log, if the assertion is false.
</ResponseField>

### Logging Functions

<Tip>
  For Go, typically you use these logging APIs when you need detailed control of
  the logging level, or when you need to log multi-line messages as a single log
  entry. Otherwise, you can use the standard Go functions from the `fmt` package
  to emit messages to `stdout` or `stderr`.
</Tip>

#### Log

Generate a log message, with no particular logging level.

```go
func Log(message string)
```

<ResponseField name="message" type="string" required>
  A message you want to log.
</ResponseField>

#### Logf

Generate a message using Go format specifiers and logs the message, with no
particular logging level.

```go
func Logf(message string, args ...any)
```

<ResponseField name="message" type="string" required>
  A message you want to log.
</ResponseField>

<ResponseField name="args" type="...any">
  Arguments that correspond to the format specifiers in the message.
</ResponseField>

#### Debug

Generate a log message with the "debug" logging level.

```go
func Debug(message string)
```

<ResponseField name="message" type="string" required>
  A debug message you want to log.
</ResponseField>

#### Debugf

Generate a message using Go format specifiers and logs the message with the
"debug" logging level.

```go
func Debugf(message string, args ...any)
```

<ResponseField name="message" type="string" required>
  A debug message you want to log.
</ResponseField>

<ResponseField name="args" type="...any">
  Arguments that correspond to the format specifiers in the message.
</ResponseField>

#### Info

Generate a log message with the "info" logging level.

```go
func Info(message string)
```

<ResponseField name="message" type="string" required>
  An informational message you want to log.
</ResponseField>

#### Infof

Generate a message using Go format specifiers and logs the message with the
"info" logging level.

```go
func Infof(message string, args ...any)
```

<ResponseField name="message" type="string" required>
  An informational message you want to log.
</ResponseField>

<ResponseField name="args" type="...any">
  Arguments that correspond to the format specifiers in the message.
</ResponseField>

#### Warn

Generate a log message with the "warning" logging level.

```go
func Warn(message string)
```

<ResponseField name="message" type="string" required>
  A warning message you want to log.
</ResponseField>

#### Warnf

Generate a message using Go format specifiers and logs the message with the
"warning" logging level.

```go
func Warnf(message string, args ...any)
```

<ResponseField name="message" type="string" required>
  A warning message you want to log.
</ResponseField>

<ResponseField name="args" type="...any">
  Arguments that correspond to the format specifiers in the message.
</ResponseField>

#### Error

Generate a log message with the "error" logging level.

```go
func Error(message string)
```

<ResponseField name="message" type="string" required>
  An error message you want to log.
</ResponseField>

#### Errorf

Generate a message using Go format specifiers and logs the message with the
"error" logging level.

```go
func Errorf(message string, args ...any)
```

<ResponseField name="message" type="string" required>
  An error message you want to log.
</ResponseField>

<ResponseField name="args" type="...any">
  Arguments that correspond to the format specifiers in the message.
</ResponseField>

### Timing Functions

#### Time

Starts a new timer using the specified label.

```go
func Time(label ...string)
```

<ResponseField name="label" type="string">
  An optional label for the timer.
</ResponseField>

#### TimeLog

Logs the current value of a timer previously started with `console.Time`.

```go
func TimeLog(label string)
```

<ResponseField name="label" type="string">
  An optional label for the timer.
</ResponseField>

#### TimeEnd

Logs the current value of a timer previously started with `console.Time`, and
discards the timer.

```go
func TimeEnd(label string)
```

<ResponseField name="label" type="string">
  An optional label for the timer.
</ResponseField>


# Dgraph APIs
Source: https://docs.hypermode.com/modus/sdk/go/dgraph

Execute queries and mutations against a Dgraph database

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="Go" feature="Dgraph" />

The Modus Dgraph APIs allow you to run queries and mutations against a Dgraph
database.

## Import

To begin, import the `dgraph` package from the SDK:

```go
import "github.com/hypermodeinc/modus/sdk/go/pkg/dgraph"
```

## Dgraph APIs

{/* vale Google.Headings = NO */}

The APIs in the `dgraph` package are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### AlterSchema

Alter the schema of a Dgraph database.

```go
func AlterSchema(connection, schema string) error
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="schema" type="string" required>
  The schema to apply to the Dgraph database.
</ResponseField>

#### DropAll

Drop all data from a Dgraph database.

```go
func DropAll(connection string) error
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

#### DropAttr

Drop an attribute from a Dgraph schema.

```go
func DropAttr(connection, attr string) error
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="attr" type="string" required>
  The attribute to drop from the Dgraph schema.
</ResponseField>

#### EscapeRDF

Ensures proper escaping of RDF string literals.

```go
func EscapeRDF(value string) string
```

<ResponseField name="value" type="string" required>
  The RDF string literal to escape.
</ResponseField>

#### ExecuteMutations

Execute one or more mutations, without a filtering query.

<Tip>
  If you need to filter the mutations based on a query, use the
  [`ExecuteQuery`](#executequery) function instead, and pass the mutations after
  the query.
</Tip>

```go
func ExecuteMutations(
  connection string,
  mutations ...*Mutation
) (*Response, error)
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="mutations" type="...*Mutation" required>
  One or more pointers to [`Mutation`](#mutation) objects to execute.
</ResponseField>

#### ExecuteQuery

Execute a DQL query to retrieve data from a Dgraph database.

Also used to execute a filtering query and apply one or more mutations to the
result of the query.

<Tip>
  If you need apply mutations *without* a filtering query, use the
  [`ExecuteMutations`](#executemutations) function instead.
</Tip>

```go
func ExecuteQuery(
  connection string,
  query *Query,
  mutations ...*Mutation
) (*Response, error)
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="request" type="*Query*" required>
  A pointer to a [`Query`](#query) object describing the DQL query to execute.
</ResponseField>

<ResponseField name="mutations" type="...*Mutation">
  Optional parameters specifying pointers to one or more [`Mutation`](#mutation)
  objects to execute on the nodes matched by the query.
</ResponseField>

#### NewMutation

Creates a new [`Mutation`](#mutation) object.

```go
func NewMutation() *Mutation
```

#### NewQuery

Creates a new [`Query`](#query) object from a DQL query string.

```go
func NewQuery(query string) *Query
```

<ResponseField name="query" type="string" required>
  The DQL query to execute.
</ResponseField>

### Types

#### Mutation

A Dgraph mutation object, used to execute mutations.

```go
type Mutation struct {
  SetJson   string
  DelJson   string
  SetNquads string
  DelNquads string
  Condition string
}

// methods
func (*Mutation) WithSetJson(json string) *Mutation
func (*Mutation) WithDelJson(json string) *Mutation
func (*Mutation) WithSetNquads(nquads string) *Mutation
func (*Mutation) WithDelNquads(nquads string) *Mutation
func (*Mutation) WithCondition(cond string) *Mutation
```

<ResponseField name="SetJson" type="string">
  A JSON string representing the data to set in the mutation.
</ResponseField>

<ResponseField name="DelJson" type="string">
  A JSON string representing the data to delete in the mutation.
</ResponseField>

<ResponseField name="SetNquads" type="string">
  A string representing the data to set in the mutation in RDF N-Quads format.
</ResponseField>

<ResponseField name="DelNquads" type="string">
  A string representing the data to delete in the mutation in RDF N-Quads
  format.
</ResponseField>

<ResponseField name="Condition" type="string">
  A string representing the condition query for the mutation, as a DQL `@if`
  directive.
</ResponseField>

<ResponseField name="condition" type="string">
  A string representing the condition query for the mutation, as a DQL `@if`
  directive.
</ResponseField>

<ResponseField name="WithSetJson(json)">
  Sets the `SetJson` field of the mutation to the given `json` string. Returns
  the `Mutation` object to allow for method chaining.
</ResponseField>

<ResponseField name="WithDelJson(json)">
  Sets the `DelJson` field of the mutation to the given `json` string. Returns
  the `Mutation` object to allow for method chaining.
</ResponseField>

<ResponseField name="WithSetNquads(nquads)">
  Sets the `SetNquads` field of the mutation to the given `nquads` string.
  Returns the `Mutation` object to allow for method chaining.
</ResponseField>

<ResponseField name="WithDelNquads(nquads)">
  Sets the `DelNquads` field of the mutation to the given `nquads` string.
  Returns the `Mutation` object to allow for method chaining.
</ResponseField>

<ResponseField name="WithCondition(cond)">
  Sets the `condition` field of the mutation to the given `cond` string, which
  should be a DQL `@if` directive. Returns the `Mutation` object to allow for
  method chaining.
</ResponseField>

#### Query

A Dgraph query object, used to execute queries.

```go
type Query struct {
  Query     string
  Variables map[string]string
}

// methods
func (*Query) WithVariable(key string, value any) *Query
```

<ResponseField name="Query" type="string">
  The DQL query to execute.
</ResponseField>

<ResponseField name="Variables" type="map[string]string">
  A map of query variables, with values encoded as strings.
</ResponseField>

<ResponseField name="WithVariable(name, value)">
  Sets a query variable with the given `name` and `value`. `name` is of type
  `string`, and `value` can be a string, number, or boolean.

  Returns the `*Query` object to allow for method chaining.

  <Tip>
    The `WithVariable` method is the preferred way to set query variables in a
    `Query` object, and allows for fluent method chaining to add multiple variables.
    For example:

    ```go
    query := NewQuery(`
      query all($name: string, $age: int) {
        all(func: eq(name, $name)) {
          name
          age
        }
      }
    `).
      WithVariable("name", "Alice").
      WithVariable("age", 30)

    response := dgraph.ExecuteQuery("my-dgraph-connection", query)
    ```
  </Tip>
</ResponseField>

#### Request

A Dgraph request object, used to execute queries and mutations.

<Info>
  {/* vale Google.Passive = NO */}

  This object was used by the `Execute` function, which has been replaced by the
  [`ExecuteQuery`](#executequery) and [`ExecuteMutations`](#executemutations)
  functions. You should no longer need to create a `Request` object directly.

  {/* vale Google.Passive = YES */}
</Info>

```go
type Request struct {
  Query     *Query
  Mutations []*Mutation
}
```

<ResponseField name="Query" type="*Query">
  A pointer to a Dgraph [`query`](#query) object.
</ResponseField>

<ResponseField name="Mutations" type="[]*Mutation">
  An slice of pointers to Dgraph [`mutation`](#mutation) objects.
</ResponseField>


# GraphQL APIs
Source: https://docs.hypermode.com/modus/sdk/go/graphql

Access external GraphQL data sources from your functions

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="Go" feature="GraphQL" />

The Modus GraphQL APIs allow you to securely call and fetch data from any
GraphQL endpoint.

## Import

To begin, import the `graphql` package from the SDK:

```go
import "github.com/hypermodeinc/modus/sdk/go/pkg/graphql"
```

## GraphQL APIs

{/* vale Google.Headings = NO */}

The APIs in the `graphql` package are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### Execute

Execute a GraphQL statement to call a query or apply mutation against a GraphQL
endpoint.

```go
Execute[T any](
  connection,
  statement string,
  variables map[string]any
) (*Response[T], error)
```

<ResponseField name="T" required>
  Type of object to use for the data returned from the query. This can be any
  type, including a custom type defined in your project. It should match the shape
  of the data returned from the GraphQL query.

  <Tip>
    Define custom types in the project's source code. All types must be JSON
    serializable. You can also use built-in types such as strings, numbers,
    slices, and maps.
  </Tip>
</ResponseField>

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connection).
</ResponseField>

<ResponseField name="statement" type="string" required>
  GraphQL statement containing the query or mutation operation to execute.

  <Warning>
    While it's possible to directly include parameter values into your GraphQL
    statement, it's highly recommended to pass a variables map instead. This can
    help to prevent against injection attacks and other security vulnerabilities.
  </Warning>
</ResponseField>

<ResponseField name="variables" type="map[string]any">
  Optional variables to include with the query.

  The key should be the name of the GraphQL variable. The value can be of any type
  that's JSON serializable, including strings, numbers, boolean values, slices,
  maps, and custom structs.
</ResponseField>

### Types

#### Response

```go
type Response[T any] struct {
  Errors []ErrorResult
  Data   *T
}
```

A response object from the GraphQL query.

Either `Errors` or `Data` is present, depending on the result of the query.

<ResponseField name="Errors" type="[]ErrorResult">
  An slice of errors incurred as part of your GraphQL request, if any.

  Each error in the slice is an [`ErrorResult`](#errorresult) object. If there are
  no errors, this field is `nil`.
</ResponseField>

<ResponseField name="Data" type="*T">
  The resulting data selected by the GraphQL operation.

  The data is a pointer to an object the type specified in the call to the
  `Execute` function. If data is absent due to errors, this field is `nil`.
</ResponseField>

#### ErrorResult

```go
type ErrorResult struct {
  Message   string
  Locations []CodeLocation
  Path      []string
}
```

The details of an error incurred as part of a GraphQL operation.

<ResponseField name="Message" type="string">
  Description of the error incurred.
</ResponseField>

<ResponseField name="Path" type="[]string">
  Path to the area of the GraphQL statement related to the error.

  Each item in the slice represents a segment of the path.
</ResponseField>

<ResponseField name="Locations" type="[]CodeLocation">
  An slice of [`CodeLocation`](#codelocation) objects that point to the specific
  location of the error in the GraphQL statement.
</ResponseField>

#### CodeLocation

```go
type CodeLocation struct {
  Line   uint32
  Column uint32
}
```

The location of specific code within a GraphQL statement.

<ResponseField name="Line" type="uint32">
  Line number within the GraphQL statement for the code.
</ResponseField>

<ResponseField name="Column" type="uint32">
  Column number within the GraphQL statement for the code.
</ResponseField>


# HTTP APIs
Source: https://docs.hypermode.com/modus/sdk/go/http

Access external HTTP endpoints from your functions

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="Go" feature="HTTP" />

The Modus HTTP APIs allow you to securely call and fetch data from an HTTP
endpoint. It is similar to the HTTP
[`fetch`](https://developer.mozilla.org/docs/Web/API/Fetch_API) API used in
JavaScript, but with some modifications to work with Modus.

In a future version of Modus, we'll support Go's standard `net/http` package for
making HTTP requests. However, that's currently not supported due to technical
limitations.

<Warning>
  As a security measure, you can only call HTTP endpoints that you
  [defined in your app's manifest](/modus/app-manifest#connections). Any attempt
  to access an arbitrary URL, for a connection not defined in your app's manifest,
  results in an error.

  Additionally, you should use placeholders for connection secrets in the
  manifest, rather than hardcoding them in your functions. Enter the values for
  each connections' secrets via environment variables or the Hypermode UI. This
  ensures that your secrets are securely stored, aren't committed to your
  repository, and aren't visible or accessible from your functions code.
</Warning>

## Import

To begin, import the `http` package from the SDK:

```go
import "github.com/hypermodeinc/modus/sdk/go/pkg/http"
```

## HTTP APIs

{/* vale Google.Headings = NO */}

The APIs in the `http` package are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### Fetch

Invoke an HTTP endpoint to retrieve data or trigger external action.

Returns a [`Response`](#response) object with the HTTP response data.

```go
func Fetch[T *Request | string](
  requestOrUrl T,
  options ...*RequestOptions
) (*Response, error)
```

<ResponseField name="requestOrUrl" type="*Request | string" required>
  Either a URL `string` or a pointer to a [`Request`](#request) object, describing
  the HTTP request to make.

  If a `string`, the operation uses the `GET` HTTP method with no headers other
  than those defined in the manifest entry of the connection.

  <Note>
    Each request must match to a connection entry in the manifest, using the
    `baseUrl` field. The request URL passed to the `Fetch` function (or via a
    `Request` object) must start with the manifest entry's `baseUrl` value to
    match.
  </Note>
</ResponseField>

<ResponseField name="options" type="*RequestOptions">
  An optional pointer to a [`RequestOptions`](#requestoptions) object with
  additional options for the request, such as the HTTP method, headers, and
  body.
</ResponseField>

#### NewContent

Creates a new [`Content`](#content) object from the given `value`, and returns a
pointer to it.

```go
func NewContent(value any) *Content
```

<ResponseField name="value" required>
  The value to create the `Content` object from. Must be one of the following
  types:

  * A [`Content`](#content) object, or a pointer to one.
  * A `[]byte` of binary content, or a pointer to one.
  * A `string` of text content.
  * Any other object that's JSON serializable. The content then becomes the JSON
    representation of the object.
</ResponseField>

#### NewHeaders

Creates a new [`Headers`](#headers) object from the given `value`, and returns a
pointer to it.

```go
func NewHeaders[T [][]string | map[string]string | map[string][]string](
  value T
) *Headers
```

<ResponseField name="value" type="[][]string | map[string]string | map[string][]string" required>
  The value object to create the [`Headers`](#headers) object from. Must be one of
  the following types:

  * A `[][]string`, where each inner slice contains a header name and value.
  * A `map[string]string`, where the keys are header names and the values are
    header values.
  * A `map[string][]string`, where the keys are header names and the values are
    slices of header values.
</ResponseField>

#### NewRequest

Creates a new [`Request`](#request) object with the given `url` and `options`,
and returns a pointer to it.

```go
func NewRequest(url string, options ...*RequestOptions) *Request
```

<ResponseField name="url" type="string" required>
  The fully qualified URL of the request, including the protocol. For example,
  `"https://example.com"`.
</ResponseField>

<ResponseField name="options" type="*RequestOptions">
  An optional pointer to a [`RequestOptions`](#requestoptions) object that's
  used to set the HTTP method, headers, and body if needed.
</ResponseField>

### Types

#### Content

Represents content used in the body of an HTTP request or response.

<Tip>
  Use the [`NewContent`](#newcontent) function to create a new `Content` object.
</Tip>

```go
type Content struct {
  // no exported fields
}

// methods
func (*Content) Bytes() []byte
func (*Content) Text() string
func (*Content) JSON(result any) error
```

<ResponseField name="Bytes()">
  Returns the binary content as a byte slice.
</ResponseField>

<ResponseField name="Text()">
  Interprets the content as a UTF-8 encoded string, and returns it as a `string`
  value.
</ResponseField>

<ResponseField name="JSON(result any) error">
  Interprets the content as a UTF-8 encoded string containing JSON, and attempts
  to deserialize it into the `result` provided.

  Pass the `result` as a pointer to an object matching the shape of the JSON. For
  example:

  ```go
  var data MyData
  if err := content.JSON(&data); err != nil {
    // handle error
  }
  ```
</ResponseField>

#### Header

Represents an HTTP request or response header.

```go
type Header struct {
  Name   string
  Values []string
}
```

<ResponseField name="Name" type="string">
  The name of the header.
</ResponseField>

<ResponseField name="Values" type="[]string">
  An slice of values for the header. Typically a header has a single value, but
  some headers can have multiple values.
</ResponseField>

#### Headers

Represents a collection of HTTP headers.

<Tip>
  Use the [`NewHeaders`](#newheaders) function to create a new `Headers` object.
</Tip>

```go
type Headers struct {
  // no exported fields
}

// methods
func (*Headers) Append(name, value string)
func (*Headers) Entries() [][]string
func (*Headers) Get(name string) *string
```

<ResponseField name="Append(name, value)">
  Appends a new header with the given `name` and `value` to the collection.
</ResponseField>

<ResponseField name="Entries()">
  Returns a `[][]string`, where each inner slice contains a header name and
  value.
</ResponseField>

<ResponseField name="Get(name)">
  Returns the value of the header with the given `name`, or `nil` if the header
  doesn't exist. If there are multiple values for the header, this function
  concatenates them with a comma to form a single string.
</ResponseField>

#### Request

Represents an HTTP request to make.

<Tip>
  Use the [`NewRequest`](#newrequest) function to create a new `Request` object.
</Tip>

```go
type Request struct {
  Url     string
  Method  string
  Headers *Headers
  Body    []byte
}

// methods
func (*Request) Clone(options ...*RequestOptions) *Request
func (*Request) Bytes() []byte
func (*Request) Text() string
func (*Request) JSON(result any) error
```

<ResponseField name="Url" type="string">
  The fully qualified URL of the request, including the protocol. For example,
  `"https://example.com"`.
</ResponseField>

<ResponseField name="Method" type="string">
  The HTTP method of the request. For example, `"GET"`, `"POST"`, `"PUT"`, or
  `"DELETE"`.
</ResponseField>

<ResponseField name="Headers" type="Headers">
  The HTTP headers of the request, as a pointer to a [`Headers`](#headers)
  object.
</ResponseField>

<ResponseField name="Body" type="[]byte">
  The raw binary content data of the request body.

  <Tip>
    The request body isn't normally read directly. Instead, use the `Bytes`,
    `Text`, or `JSON` methods.
  </Tip>
</ResponseField>

<ResponseField name="Clone(options)">
  Clones the `Request` object, applies the `options` to the new object, and
  returns it.
</ResponseField>

<ResponseField name="Bytes()">
  Returns the binary content of the request body as a byte slice.
</ResponseField>

<ResponseField name="Text()">
  Interprets the content of the request body as a UTF-8 encoded string, and
  returns it as a `string` value.
</ResponseField>

<ResponseField name="JSON(result any) error">
  Interprets the content of the request body as a UTF-8 encoded string containing
  JSON, and attempts to deserialize it into the `result` provided.

  Pass the `result` as a pointer to an object matching the shape of the JSON. For
  example:

  ```go
  var data MyData
  if err := request.JSON(&data); err != nil {
    // handle error
  }
  ```
</ResponseField>

#### RequestOptions

Options for the HTTP request.

```go
type RequestOptions struct {
  Method  string
  Headers any
  Body    any
}
```

<ResponseField name="Method" type="string">
  The HTTP method of the request. For example, `"GET"`, `"POST"`, `"PUT"`, or
  `"DELETE"`. If empty, the request uses the `GET` method.
</ResponseField>

<ResponseField name="Headers">
  The HTTP headers of the request, which must be of one of the following types:

  * A [`Headers`](#headers) object, or a pointer to one.
  * A `[][]string`, where each inner slice contains a header name and value.
  * A `map[string]string`, where the keys are header names and the values are
    header values.
  * A `map[string][]string`, where the keys are header names and the values are
    slices of header values.
</ResponseField>

<ResponseField name="Body" type="any">
  Content to pass in the request body. Must be one of the following types:

  * A [`Content`](#content) object, or a pointer to one.
  * A `[]byte` of binary content, or a pointer to one.
  * A `string` of text content.
  * `nil` if there is no body to pass.

  <Tip>
    It is generally recommended to supply a `Content-Type` header for any requests
    that have a body.
  </Tip>
</ResponseField>

#### Response

Represents the response received from the HTTP server.

```go
type Response struct {
  Status     uint16
  StatusText string
  Headers    *Headers
  Body       []byte
}

// methods
func (*Response) Ok() bool
func (*Response) Bytes() []byte
func (*Response) Text() string
func (*Response) JSON(result any) error
```

<ResponseField name="Status" type="uint16">
  The HTTP response status code, such as `200` for success.
</ResponseField>

<ResponseField name="StatusText" type="string">
  The HTTP response status text associated with the status code, such as `"OK"`
  for success.
</ResponseField>

<ResponseField name="Headers" type="*Headers">
  The HTTP headers received with the response, as a pointer to a
  [`Headers`](#headers) object.
</ResponseField>

<ResponseField name="Body" type="[]byte">
  The raw binary content data of the response body.

  <Tip>
    The response body isn't normally read directly. Instead, use the `Bytes`,
    `Text`, or `JSON` methods.
  </Tip>
</ResponseField>

<ResponseField name="Bytes()">
  Returns the binary content of the response body as a byte slice.
</ResponseField>

<ResponseField name="Text()">
  Interprets the content of the response body as a UTF-8 encoded string, and
  returns it as a `string` value.
</ResponseField>

<ResponseField name="JSON(result any) error">
  Interprets the content of the response body as a UTF-8 encoded string containing
  JSON, and attempts to deserialize it into the `result` provided.

  Pass the `result` as a pointer to an object matching the shape of the JSON. For
  example:

  ```go
  var data MyData
  if err := response.JSON(&data); err != nil {
    // handle error
  }
  ```
</ResponseField>


# Local Time APIs
Source: https://docs.hypermode.com/modus/sdk/go/localtime

Access the user's local time and time zone in your functions

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="Go" feature="Local Time" />

The Modus Local Time APIs allow you to access the user's local time and time
zone from your functions.

## Import

To begin, import the `localtime` package from the SDK:

```go
import "github.com/hypermodeinc/modus/sdk/go/pkg/localtime"
```

{/* vale Google.Headings = NO */}

## Local Time APIs

The APIs in the `localtime` package are below.

All time zones use the IANA time zone database format. For example,
`"America/New_York"`. You can find a list of valid time zones
[here](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).

<Info>
  {/* vale Google.Passive = NO */}

  For APIs that work with the user's local time, the time zone is determined in
  the following order of precedence:

  * If the `X-Time-Zone` header is present in the request, the time zone is set to
    the value of the header.
  * If the `TZ` environment variable is set on the host, the time zone is set to
    the value of the variable.
  * Otherwise, the time zone is set to the host's local time zone.

  {/* vale Google.Passive = YES */}
</Info>

<Tip>
  When working locally with `modus dev`, Modus uses the host's local time zone by
  default. You can override this by setting the `TZ` environment variable in your
  `.env.local` file.
</Tip>

<Tip>
  In a browser-based web app, you can get the user's time zone with the following
  JavaScript code:

  ```js
  const timeZone = Intl.DateTimeFormat().resolvedOptions().timeZone
  ```

  Assign that value to a `"X-Time-Zone"` request header when calling Modus, to use
  it for all local time calculations.
</Tip>

<Tip>
  In many cases, you can use Go's built-in time support:

  ```go
  // to get the current time in UTC
  now := time.Now().UTC()

  // if you need a string
  s := now.Format(time.RFC3339)
  ```
</Tip>

<Warning>
  Due to Go's WASM implementation, the standard `time.Now()` function always
  returns the UTC time, not the user's local time like it usually does in Go. If
  you need the user's local time, use `localtime.Now()` instead.
</Warning>

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### GetLocation

Returns a pointer to a Go `time.Location` object for a specific time zone.
Errors if the time zone provided is invalid.

```go
func GetLocation(tz string) (*time.Location, error)
```

#### GetTimeZone

Returns the user's time zone in IANA format.

```go
func GetTimeZone() string
```

#### IsValidTimeZone

Determines whether the specified time zone is a valid IANA time zone and
recognized by the system as such.

```go
func IsValidTimeZone(tz string) bool
```

<ResponseField name="tz" type="string" required>
  An IANA time zone identifier, such as `"America/New_York"`.
</ResponseField>

#### Now

Returns the current time as a `time.Time` object, with the location set to the
user's local time zone. Errors if the time zone passed to the host is invalid.

```go
func Now() (time.Time, error)
```

#### NowInZone

Returns the current time as a `time.Time` object, with the location set to a
specific time zone. Errors if the time zone provided is invalid.

```go
func NowInZone(tz string) (time.Time, error)
```

<ResponseField name="tz" type="string" required>
  An IANA time zone identifier, such as `"America/New_York"`.
</ResponseField>


# AI Model APIs
Source: https://docs.hypermode.com/modus/sdk/go/models

Invoke AI models from your functions

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="Go" feature="Models" />

The Modus Models APIs allow you to invoke AI models directly from your
functions, irrespective of the model's host.

Since many models have unique interfaces, the design of the Models APIs are
extremely flexible. An interface and common base type forms the core of the
APIs, which extends to conform to any model's required schema.

The SDK contains both the base types and pre-defined implementations for many
commonly used models. You can either use one of the pre-defined model types, or
can create custom types for any model you like, by following the same pattern as
implemented in the pre-defined models.

<Tip>
  For your reference, several complete examples for using the Models APIs are available in
  [Model Invoking](/modus/model-invoking).

  Each example demonstrates using different types of AI models for different
  purposes. However, the Models interface isn't limited to these purposes. You can
  use it for any task that an AI model can perform.
</Tip>

## Import

To begin, import the `models` package from the SDK:

```go
import "github.com/hypermodeinc/modus/sdk/go/pkg/models"
```

You'll also need to import one or more packages for the model you are working
with. For example:

```go
import "github.com/hypermodeinc/modus/sdk/go/pkg/models/openai"
```

If you would like to request a new model, please
[open an issue](https://github.com/hypermodeinc/modus/issues). You can also send
a pull request, if you'd like to contribute a new model yourself.

## Models APIs

{/* vale Google.Headings = NO */}

The APIs in the `models` package are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### GetModel

Get a model instance by name and type.

```go
func GetModel[TModel](modelName string) (*TModel, error)
```

<ResponseField name="TModel" required>
  The type of model to return. This can be any struct that implements the
  `Model` interface.
</ResponseField>

<ResponseField name="modelName" type="string" required>
  The name of the model to retrieve. This must match the name of a model defined
  in your project's manifest file.
</ResponseField>

### Types

#### Model

The interface that all Modus models must implement.

```go
type Model[TIn, TOut any] interface {
  Info() *ModelInfo
  Invoke(input *TIn) (*TOut, error)
}
```

<ResponseField name="TIn" required>
  The type of the input data for the model. This can be any type, including a
  custom type defined in your project. It should match the shape of the data
  expected by the model. It is usually a struct.
</ResponseField>

<ResponseField name="TOut" required>
  The type of the output data from the model. This can be any type, including a
  custom type defined in your project. It should match the shape of the data
  returned by the model. It is usually a struct.
</ResponseField>

<ResponseField name="Info()" type="method">
  Returns information about the model set by the Modus Runtime when creating the
  instance. See the [`ModelInfo`](#modelinfo) object for more information.
</ResponseField>

<ResponseField name="Invoke(input)" type="method">
  Invokes the model with input data and returns the output data.
</ResponseField>

#### ModelBase

The base type for all models that Modus functions can invoke.

<Tip>
  If you are implementing a custom model, you'll need to create a type alias
  based on this struct. You'll also need structs to represent the input and
  output types for your model. See the implementations of the pre-defined models
  in the Modus GitHub repository for examples.
</Tip>

```go
type ModelBase[TIn, TOut any] struct {
  Debug bool
}

// methods
func (ModelBase[TIn, TOut]) Info() *ModelInfo
func (ModelBase[TIn, TOut]) Invoke(input *TIn) (*TOut, error)
```

<ResponseField name="TIn" required>
  The type of the input data for the model. This can be any type, including a
  custom type defined in your project. It should match the shape of the data
  expected by the model. It is usually a struct.
</ResponseField>

<ResponseField name="TOut" required>
  The type of the output data from the model. This can be any type, including a
  custom type defined in your project. It should match the shape of the data
  returned by the model. It is usually a struct.
</ResponseField>

<ResponseField name="Debug" type="bool">
  A flag to enable debug mode for the model. When enabled, Modus automatically
  logs the full request and response data to the console. Implementations can
  also use this flag to enable additional debug logging. Defaults to `false`.
</ResponseField>

<ResponseField name="Info()" type="method">
  Returns information about the model set by the Modus Runtime when creating the
  instance. See the [`ModelInfo`](#modelinfo) object for more information.
</ResponseField>

<ResponseField name="Invoke(input)" type="method">
  Invokes the model with input data and returns the output data.
</ResponseField>

#### ModelInfo

Information about a model that's used to construct a `Model` instance. It is
also available from a method on the `Model` interface.

<Info>
  This struct relays information from the Modus runtime to the model
  implementation. Generally, you don't need to create `ModelInfo` instances
  directly.

  However, if you are implementing a custom model, you may wish to use a field
  from this struct, such as `FullName`, for model providers that require the model
  name in the input request body.
</Info>

```go
type ModelInfo struct {
  Name string
  FullName string
}
```

<ResponseField name="Name" type="string">
  The name of the model from the app manifest.
</ResponseField>

<ResponseField name="FullName" type="string">
  The full name or identifier of the model, as defined by the model provider.
</ResponseField>


# MySQL APIs
Source: https://docs.hypermode.com/modus/sdk/go/mysql

Execute queries against a MySQL database

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="Go" feature="MySQL" />

The Modus MySQL APIs allow you to run queries against MySQL or any
MySQL-compatible database platform.

## Import

To begin, import the `mysql` package from the SDK:

```go
import "github.com/hypermodeinc/modus/sdk/go/pkg/mysql"
```

## MySQL APIs

{/* vale Google.Headings = NO */}

The APIs in the `mysql` package are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### Execute

Execute a SQL statement against a MySQL database, without any data returned. Use
this for insert, update, or delete operations, or for other SQL statements that
don't return data.

<Note>
  The `Execute` function is for operations that don't return data. However, some
  insert/update/delete operations may still return data. In these cases, you can
  use the `QueryScalar` or `Query` functions instead.
</Note>

```go
func Execute(connection, statement string, params ...any) (*db.Response, error)
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="statement" type="string" required>
  SQL statement containing the query or mutation operation to execute.

  <Warning>
    While it's possible to directly include parameter values into your SQL
    statement, it's highly recommended to pass parameters as arguments instead.
    This can help to protect injection attacks and other security vulnerabilities.
  </Warning>
</ResponseField>

<ResponseField name="params" type="...any">
  Optional parameters to include with the query.
</ResponseField>

#### Query

Execute a SQL statement against a MySQL database, returning a set of rows. In
the results, each row converts to an object of type `T`, with fields matching
the column names.

```go
func Query[T any](connection, statement string, params ...any) (*db.QueryResponse[T], error)
```

<ResponseField name="T" required>
  Type of object to use for the data returned from the query. This can be any
  type, including a custom type defined in your project. It should match the shape
  of the row returned from the SQL query.

  <Tip>
    Define custom types in the project's source code. All types must be JSON
    serializable. You can also use built-in types such as strings, numbers, slices,
    and maps.

    If working with MySQL's `point` data type, you can use a [`Point`](#point) or
    [`Location`](#location) object to represent the data.
  </Tip>
</ResponseField>

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="statement" type="string" required>
  SQL statement containing the query or mutation operation to execute.

  <Warning>
    While it's possible to directly include parameter values into your SQL
    statement, it's highly recommended to pass parameters as arguments instead.
    This can help to protect injection attacks and other security vulnerabilities.
  </Warning>
</ResponseField>

<ResponseField name="params" type="...any">
  Optional parameters to include with the query.
</ResponseField>

#### QueryScalar

Execute a SQL statement against a MySQL database, returning a single scalar
value. For example, the result could be a count, sum, or average, or it could be
an identifier.

```go
func QueryScalar[T any](connection, statement string, params ...any) (*db.ScalarResponse[T], error)
```

<ResponseField name="T" required>
  Type of object to use for the data returned from the query. This should
  generally be a scalar data type, such as a number or string. It should match
  the type of the data returned from the SQL query.
</ResponseField>

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="statement" type="string" required>
  SQL statement containing the query or mutation operation to execute.

  <Warning>
    While it's possible to directly include parameter values into your SQL
    statement, it's highly recommended to pass parameters as arguments instead.
    This can help to protect injection attacks and other security vulnerabilities.
  </Warning>
</ResponseField>

<ResponseField name="params" type="...any">
  Optional parameters to include with the query.
</ResponseField>

### Types

#### Location

Represents a location on Earth, having `Longitude` and `Latitude` coordinates.

Correctly serializes to and from MySQL's point type, in (longitude, latitude)
order.

<Info>
  This struct is identical to the [Point](#point) struct, but uses different
  field names.
</Info>

```go
type Location struct {
  Longitude float64
  Latitude float64
}
```

<ResponseField name="Longitude" type="float64" required>
  The longitude coordinate of the location, in degrees.
</ResponseField>

<ResponseField name="Latitude" type="float64" required>
  The latitude coordinate of the location, in degrees.
</ResponseField>

#### Point

Represents a point in 2D space, having `X` and `Y` coordinates. Correctly
serializes to and from MySQL's point type, in (x, y) order.

<Info>
  This struct is identical to the [Location](#location) struct, but uses
  different field names.
</Info>

```go
type Point struct {
  X float64
  Y float64
}
```

<ResponseField name="X" type="float64" required>
  The X coordinate of the point.
</ResponseField>

<ResponseField name="Y" type="float64" required>
  The Y coordinate of the point.
</ResponseField>

#### QueryResponse

Represents the response from a [`Query`](#query) operation.

```go
type QueryResponse[T any] struct {
  RowsAffected uint32
  LastInsertId uint64
  Rows []T
}
```

<ResponseField name="RowsAffected" type="uint32">
  The number of rows affected by the operation, which typically corresponds to
  the number of rows returned.
</ResponseField>

<ResponseField name="lastInsertId" type="uint64">
  When inserting a row, this field contains the ID of the last inserted row.
  This is useful for tables with auto-incrementing primary keys.
</ResponseField>

<ResponseField name="Rows" type="[]T">
  An slice of objects, each representing a row returned from the query. Each
  object has fields corresponding to the columns in the result set.
</ResponseField>

#### Response

Represents the response from an [`Execute`](#execute) operation.

```go
type Response struct {
  RowsAffected uint32
  LastInsertId uint64
}
```

<ResponseField name="RowsAffected" type="uint32">
  The number of rows affected by the operation, which typically corresponds to
  the number of rows returned.
</ResponseField>

<ResponseField name="lastInsertId" type="uint64">
  When inserting a row, this field contains the ID of the last inserted row.
  This is useful for tables with auto-incrementing primary keys.
</ResponseField>

#### ScalarResponse

Represents the response from a [`QueryScalar`](#queryscalar) operation.

```go
type ScalarResponse[T any] struct {
  RowsAffected uint32
  LastInsertId uint64
  Value T
}
```

<ResponseField name="RowsAffected" type="uint32">
  The number of rows affected by the operation, which typically corresponds to
  the number of rows returned.
</ResponseField>

<ResponseField name="lastInsertId" type="uint64">
  When inserting a row, this field contains the ID of the last inserted row.
  This is useful for tables with auto-incrementing primary keys.
</ResponseField>

<ResponseField name="Value" type="T">
  The scalar value returned from the query.
</ResponseField>


# Neo4j APIs
Source: https://docs.hypermode.com/modus/sdk/go/neo4j

Execute queries and mutations against a Neo4j database

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="Go" feature="Neo4j" />

The Modus Neo4j APIs allow you to run queries and mutations against a Neo4j
database.

## Import

To begin, import the `neo4j` package from the SDK:

```go
import "github.com/hypermodeinc/modus/sdk/go/pkg/neo4j"
```

## Neo4j APIs

{/* vale Google.Headings = NO */}

The APIs in the `neo4j` package are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### ExecuteQuery

Executes a Cypher query on the Neo4j database.

```go
func ExecuteQuery(
  connection,
  query string,
  parameters map[string]any,
  opts ...Neo4jOption
) (*EagerResult, error)
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="query" type="string" required>
  A Neo4j Cypher query to execute.
</ResponseField>

<ResponseField name="parameters" type="map[string]any" required>
  A map of parameters to pass to the query.
</ResponseField>

<ResponseField name="opts" type="Neo4jOption">
  Optional arguments to pass to the query. Specify the database name using the
  `WithDbName` option.
</ResponseField>

#### GetProperty

Get a property from an entity at a given key and cast or decode it to a specific
type.

```go
func GetProperty[T PropertyValue](e Entity, key string) (T, error)
```

<ResponseField name="T" type="PropertyValue" required>
  The type to cast or decode the value to.
</ResponseField>

<ResponseField name="e" type="Entity" required>
  The entity to get the value from.
</ResponseField>

<ResponseField name="key" type="string" required>
  The key of the value to get.
</ResponseField>

#### GetRecordValue

Get a value from a record at a given key and cast or decode it to a specific
type.

```go
func GetRecordValue[T RecordValue](record *Record, key string) (T, error)
```

<ResponseField name="T" type="RecordValue" required>
  The type to cast or decode the value to.
</ResponseField>

<ResponseField name="record" type="*Record" required>
  The record to get the value from.
</ResponseField>

<ResponseField name="key" type="string" required>
  The key of the value to get.
</ResponseField>

### Types

#### EagerResult

The result of a Neo4j query or mutation.

```go
type EagerResult struct {
  Keys    []string
  Records []*Record
}
```

<ResponseField name="Keys" type="[]string">
  The keys of the result.
</ResponseField>

<ResponseField name="Records" type="[]*Record">
  The records of the result.
</ResponseField>

#### Entity

An interface representing possible entities in a Neo4j query result.

```go
type Entity interface {
  GetElementId() string
  GetProperties() map[string]any
}
```

<ResponseField name="GetElementId()" type="method">
  Returns the ID of the entity.
</ResponseField>

<ResponseField name="GetProperties()" type="method">
  Returns the properties of the entity.
</ResponseField>

#### Node

A node in a Neo4j query result.

```go
type Node struct {
  ElementId string
  Labels    []string
  Props     map[string]any
}
```

<ResponseField name="ElementId" type="string">
  The ID of the node.
</ResponseField>

<ResponseField name="Labels" type="[]string">
  The labels of the node.
</ResponseField>

<ResponseField name="Props" type="map[string]any">
  The properties of the node.
</ResponseField>

<ResponseField name="GetElementId()" type="method">
  Returns the ID of the node.
</ResponseField>

<ResponseField name="GetProperties()" type="method">
  Returns the properties of the node.
</ResponseField>

#### Path

A path in a Neo4j query result.

```go
type Path struct {
  Nodes         []Node
  Relationships []Relationship
}
```

<ResponseField name="Nodes" type="[]Node">
  The nodes in the path.
</ResponseField>

<ResponseField name="Relationships" type="[]Relationship">
  The relationships in the path.
</ResponseField>

#### Point2D

A 2D point in a Neo4j query result.

```go
type Point2D struct {
  X            float64
  Y            float64
  SpatialRefId uint32
}
```

<ResponseField name="X" type="float64">
  The X coordinate of the point.
</ResponseField>

<ResponseField name="Y" type="float64">
  The Y coordinate of the point.
</ResponseField>

<ResponseField name="SpatialRefId" type="uint32">
  The spatial reference ID of the point.
</ResponseField>

#### Point3D

A 3D point in a Neo4j query result.

```go
type Point3D struct {
  X            float64
  Y            float64
  Z            float64
  SpatialRefId uint32
}
```

<ResponseField name="X" type="float64">
  The X coordinate of the point.
</ResponseField>

<ResponseField name="Y" type="float64">
  The Y coordinate of the point.
</ResponseField>

<ResponseField name="Z" type="float64">
  The Z coordinate of the point.
</ResponseField>

<ResponseField name="SpatialRefId" type="uint32">
  The spatial reference ID of the point.
</ResponseField>

#### PropertyValue

A type constraint for retrieving property values from a Neo4j entity.

```go
type PropertyValue interface {
  bool | int64 | float64 | string |
    time.Time | []byte | []any | Point2D | Point3D
}
```

#### Record

A record in a Neo4j query result.

```go
type Record struct {
  Keys   []string
  Values []string
}
```

<ResponseField name="Values" type="[]string">
  The values of the record.
</ResponseField>

<ResponseField name="Keys" type="[]string">
  The keys of the record.
</ResponseField>

<ResponseField name="Get(key)" type="method">
  Get the value of a record at a given key as a JSON encoded string.

  <Info>
    Usually, you should use the `GetRecordValue[T](key)` function instead of this method.
  </Info>
</ResponseField>

<ResponseField name="AsMap()" type="method">
  Convert the record to a map of keys and JSON encoded string values.
</ResponseField>

#### RecordValue

A type constraint for retrieving values from a Neo4j record.

```go
type RecordValue interface {
  bool | int64 | float64 | string | time.Time |
  []byte | []any | map[string]any |
  Node | Relationship | Path | Point2D | Point3D
}
```

#### Relationship

A relationship in a Neo4j query result.

```go
type Relationship struct {
  ElementId      string
  StartElementId string
  EndElementId   string
  Type           string
  Props          map[string]any
}
```

<ResponseField name="ElementId" type="string">
  The ID of the relationship.
</ResponseField>

<ResponseField name="StartElementId" type="string">
  The ID of the start node.
</ResponseField>

<ResponseField name="EndElementId" type="string">
  The ID of the end node.
</ResponseField>

<ResponseField name="Type" type="string">
  The type of the relationship.
</ResponseField>

<ResponseField name="Props" type="map[string]any">
  The properties of the relationship.
</ResponseField>

<ResponseField name="GetElementId()" type="method">
  Returns the ID of the node.
</ResponseField>

<ResponseField name="GetProperties()" type="method">
  Returns the properties of the node.
</ResponseField>


# Modus Go SDK
Source: https://docs.hypermode.com/modus/sdk/go/overview

Learn how to use the Modus Go SDK

export const language_0 = "Go"

The Modus {language_0} SDK provides a set of APIs that allow you to interact with
various databases and services in Modus apps written in {language_0}. It is
designed to work seamlessly with the Modus platform, enabling you to build
powerful apps with ease.

<Info>
  The Modus SDKs come in multiple languages, each following the conventions for
  that language, including its capabilities and limitations. While each SDK
  provides similar features, the APIs may differ slightly between languages. Be
  sure to refer to the documentation for the specific SDK you're using.

  Wherever possible, we've tried to keep the APIs consistent across languages.
  However, you may find differences in the APIs due to language-specific
  constraints.

  If you have any questions or need help, please reach out to us via the `#modus`
  channel on the [Hypermode Discord server](https://discord.hypermode.com/).
</Info>

## Importing the APIs

The Modus Go SDK organizes its APIs into packages based on their purpose. In
your Go code, you can import each package you'd like to use like this:

```go
import "github.com/hypermodeinc/modus/sdk/go/pkg/<package>"
```

## Available APIs

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

The following packages are available. They're grouped into categories below for
your convenience. Click on a package to view more information about its APIs.

### Client APIs

| Package                | Purpose                                                                            |
| :--------------------- | :--------------------------------------------------------------------------------- |
| [`http`](./http)       | Provides access to external HTTP endpoints, including REST APIs and other services |
| [`graphql`](./graphql) | Allows you to securely call and fetch data from any GraphQL endpoint               |

### Database APIs

| Package                      | Purpose                                         |
| :--------------------------- | :---------------------------------------------- |
| [`dgraph`](./dgraph)         | Access and modify data in a Dgraph database     |
| [`mysql`](./mysql)           | Access and modify data in a MySQL database      |
| [`neo4j`](./neo4j)           | Access and modify data in a Neo4j database      |
| [`postgresql`](./postgresql) | Access and modify data in a PostgreSQL database |

### Inference APIs

| Package              | Purpose                                                                                        |
| :------------------- | :--------------------------------------------------------------------------------------------- |
| [`models`](./models) | Invoke AI models, including large language models, embedding models, and classification models |

### Storage APIs

| Package                        | Purpose                                                    |
| :----------------------------- | :--------------------------------------------------------- |
| [`collections`](./collections) | Provides integrated storage and vector search capabilities |

### System APIs

| Package                    | Purpose                                                  |
| :------------------------- | :------------------------------------------------------- |
| [`console`](./console)     | Provides logging, assertion, and timing functions        |
| [`localtime`](./localtime) | Allows you to access the user's local time and time zone |


# PostgreSQL APIs
Source: https://docs.hypermode.com/modus/sdk/go/postgresql

Execute queries against a PostgreSQL database

export const SdkHeader = ({language, feature}) => <Note>
    <p>
      While each Modus SDK offers similar capabilities, the APIs and usage may
      vary between languages.
    </p>
    <p>
      Modus {feature} APIs documentation is available on the following pages:
    </p>
    <ul>
      {(() => {
  const languages = ["AssemblyScript", "Go"];
  const page = feature.toLowerCase().replace(/\W/g, "");
  return languages.map(lang => {
    if (lang === language) {
      return <li>
                <b>
                  {lang} {feature} APIs
                </b>
                (this page)
              </li>;
    } else {
      return <li>
                <a href={`../${lang.toLowerCase()}/${page}`}>
                  {lang} {feature} APIs
                </a>
              </li>;
    }
  });
})()}
    </ul>
  </Note>;

<SdkHeader language="Go" feature="PostgreSQL" />

The Modus PostgreSQL APIs allow you to run queries against PostgreSQL or any
PostgreSQL-compatible database platform.

## Import

To begin, import the `postgresql` package from the SDK:

```go
import "github.com/hypermodeinc/modus/sdk/go/pkg/postgresql"
```

## PostgreSQL APIs

{/* vale Google.Headings = NO */}

The APIs in the `postgresql` package are below, organized by category.

<Tip>
  We're constantly introducing new APIs through ongoing development with early
  users. Please [open an issue](https://github.com/hypermodeinc/modus/issues) if
  you have ideas on what would make Modus even more powerful for your next app!
</Tip>

### Functions

#### Execute

Execute a SQL statement against a PostgreSQL database, without any data
returned. Use this for insert, update, or delete operations, or for other SQL
statements that don't return data.

<Note>
  The `Execute` function is for operations that don't return data. However, some
  insert/update/delete operations may still return data. In these cases, you can
  use the `QueryScalar` or `Query` functions instead.
</Note>

```go
func Execute(connection, statement string, params ...any) (*db.Response, error)
```

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="statement" type="string" required>
  SQL statement containing the query or mutation operation to execute.

  <Warning>
    While it's possible to directly include parameter values into your SQL
    statement, it's highly recommended to pass parameters as arguments instead.
    This can help to protect injection attacks and other security vulnerabilities.
  </Warning>
</ResponseField>

<ResponseField name="params" type="...any">
  Optional parameters to include with the query.
</ResponseField>

#### Query

Execute a SQL statement against a PostgreSQL database, returning a set of rows.
In the results, each row converts to an object of type `T`, with fields matching
the column names.

```go
func Query[T any](connection, statement string, params ...any) (*db.QueryResponse[T], error)
```

<ResponseField name="T" required>
  Type of object to use for the data returned from the query. This can be any
  type, including a custom type defined in your project. It should match the shape
  of the row returned from the SQL query.

  <Tip>
    Define custom types in the project's source code. All types must be JSON
    serializable. You can also use built-in types such as strings, numbers, slices,
    and maps.

    If working with PostgreSQL's `point` data type, you can use a [`Point`](#point)
    or [`Location`](#location) object to represent the data.
  </Tip>
</ResponseField>

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="statement" type="string" required>
  SQL statement containing the query or mutation operation to execute.

  <Warning>
    While it's possible to directly include parameter values into your SQL
    statement, it's highly recommended to pass parameters as arguments instead.
    This can help to protect injection attacks and other security vulnerabilities.
  </Warning>
</ResponseField>

<ResponseField name="params" type="...any">
  Optional parameters to include with the query.
</ResponseField>

#### QueryScalar

Execute a SQL statement against a PostgreSQL database, returning a single scalar
value. For example, the result could be a count, sum, or average, or it could be
an identifier.

```go
func QueryScalar[T any](connection, statement string, params ...any) (*db.ScalarResponse[T], error)
```

<ResponseField name="T" required>
  Type of object to use for the data returned from the query. This should
  generally be a scalar data type, such as a number or string. It should match
  the type of the data returned from the SQL query.
</ResponseField>

<ResponseField name="connection" type="string" required>
  Name of the connection, as [defined in the
  manifest](/modus/app-manifest#connections).
</ResponseField>

<ResponseField name="statement" type="string" required>
  SQL statement containing the query or mutation operation to execute.

  <Warning>
    While it's possible to directly include parameter values into your SQL
    statement, it's highly recommended to pass parameters as arguments instead.
    This can help to protect injection attacks and other security vulnerabilities.
  </Warning>
</ResponseField>

<ResponseField name="params" type="...any">
  Optional parameters to include with the query.
</ResponseField>

### Types

#### Location

Represents a location on Earth, having `Longitude` and `Latitude` coordinates.

Correctly serializes to and from PostgreSQL's point type, in (longitude,
latitude) order.

<Info>
  This struct is identical to the [Point](#point) struct, but uses different
  field names.
</Info>

```go
type Location struct {
  Longitude float64
  Latitude float64
}
```

<ResponseField name="Longitude" type="float64" required>
  The longitude coordinate of the location, in degrees.
</ResponseField>

<ResponseField name="Latitude" type="float64" required>
  The latitude coordinate of the location, in degrees.
</ResponseField>

#### Point

Represents a point in 2D space, having `X` and `Y` coordinates. Correctly
serializes to and from PostgreSQL's point type, in (x, y) order.

<Info>
  This struct is identical to the [Location](#location) struct, but uses
  different field names.
</Info>

```go
type Point struct {
  X float64
  Y float64
}
```

<ResponseField name="X" type="float64" required>
  The X coordinate of the point.
</ResponseField>

<ResponseField name="Y" type="float64" required>
  The Y coordinate of the point.
</ResponseField>

#### QueryResponse

Represents the response from a [`Query`](#query) operation.

```go
type QueryResponse[T any] struct {
  RowsAffected uint32
  LastInsertId uint64
  Rows []T
}
```

<ResponseField name="RowsAffected" type="uint32">
  The number of rows affected by the operation, which typically corresponds to
  the number of rows returned.
</ResponseField>

<ResponseField name="LastInsertId" type="uint64">
  This field is available for other database types, but isn't populated for
  PostgreSQL. Instead, use `Query` or `QueryScalar` with a `RETURNING` clause to
  get the last inserted ID.
</ResponseField>

<ResponseField name="Rows" type="[]T">
  An slice of objects, each representing a row returned from the query. Each
  object has fields corresponding to the columns in the result set.
</ResponseField>

#### Response

Represents the response from an [`Execute`](#execute) operation.

```go
type Response struct {
  RowsAffected uint32
  LastInsertId uint64
}
```

<ResponseField name="RowsAffected" type="uint32">
  The number of rows affected by the operation, which typically corresponds to
  the number of rows returned.
</ResponseField>

<ResponseField name="LastInsertId" type="uint64">
  This field is available for other database types, but isn't populated for
  PostgreSQL. Instead, use `Query` or `QueryScalar` with a `RETURNING` clause to
  get the last inserted ID.
</ResponseField>

#### ScalarResponse

Represents the response from a [`QueryScalar`](#queryscalar) operation.

```go
type ScalarResponse[T any] struct {
  RowsAffected uint32
  LastInsertId uint64
  Value T
}
```

<ResponseField name="RowsAffected" type="uint32">
  The number of rows affected by the operation, which typically corresponds to
  the number of rows returned.
</ResponseField>

<ResponseField name="LastInsertId" type="uint64">
  This field is available for other database types, but isn't populated for
  PostgreSQL. Instead, use `Query` or `QueryScalar` with a `RETURNING` clause to
  get the last inserted ID.
</ResponseField>

<ResponseField name="Value" type="T">
  The scalar value returned from the query.
</ResponseField>


# Search
Source: https://docs.hypermode.com/modus/search

Add natural language search to your app using AI embeddings

The Modus Collections API provides a robust way to store, retrieve, and search
through data using both natural language and vector-based search methods. By
leveraging embeddings, developers can enable semantic and similarity-based
searches, improving the relevance of search results within their apps.

For example, with natural language similarity, if you search for a product
description like 'sleek red sports car', the search method returns similar
product descriptions such as "luxury sports car in red" or 'high-speed car with
sleek design'.

## Understanding key components

**Collections**: a collection is a structured storage that organizes and stores
textual data and associated metadata. Collections enable sophisticated search,
retrieval, and classification tasks using vector embeddings.

**Search Methods**: a search method associated with a collection, defines how to
convert collection items into a vector representation and provides indexing
parameters.

**Vector embeddings**: for vector-based search and comparison, Modus converts
each item in the collection into a vector representation called **embedding**.
By embedding data, you enable powerful natural language and similarity-based
searches.

<Note>
  Modus runtime automatically compute the embeddings, according to your
  configuration, when you add or update items.
</Note>

## Initializing your collection

Before implementing search, ensure you have
[defined a collection in the app manifest](./app-manifest#collections). In this
example, `myProducts` is the collection used to store product descriptions.

First, we need to populate the collection with items (for example, product
descriptions). You can insert individual or multiple items using the `upsert`
and `upsertBatch` methods, respectively.

Use `upsert` to insert a product description into the collection. If you don't
specify a key, Modus generates a unique key for you.

<CodeGroup>
  ```go Go
  func AddProduct(description string) ([]string, error) {
    res, err := collections.Upsert(
      "myProducts",  // Collection name defined in the manifest
      nil,           // using nil to let Modus generate a unique ID
      description,   // the text to store
      nil            // we don't have labels for this item
      )
    if err != nil {
      return nil, err
    }
    return res.Keys, nil
  }
  ```

  ```ts AssemblyScript
  export function addProduct(description: string): string {
    const response = collections.upsert(
      "myProducts", // Collection name defined in the manifest
      null, // using null to let Modus generate a unique ID
      description, // the text to store
      // no labels for this item
      // no namespace provided, use defautl namespace
    )
    return response.keys[0] // return the identifier of the item
  }
  ```
</CodeGroup>

## Configure your search method

The search capability relies on a search method and embedding function. To
configure your search method.

### Create an embedding function

An embedding function is any API function that transforms text into vectors that
represent their meaning in a high-dimensional space.

Embeddings functions must have the following signature:

<CodeGroup>
  ```go Go
  package main

  func Embed(text []string) ([][]float32, error) {
    ...
  }

  ```

  ```ts AssemblyScript
  export function embed(text: string[]): f32[][] {
    ...
  }
  ```
</CodeGroup>

Modus computes vectors using embedding models. Here are a few examples:

<Tabs>
  <Tab title="all-MiniLM-L6-v2">
    [Declare the model](./app-manifest#models) in the app manifest

    ```json model.json
      "models": {
        // model card: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
        "minilm": {
          "sourceModel": "sentence-transformers/all-MiniLM-L6-v2", // model name on the provider
          "provider": "hugging-face", // provider for this model
          "connection": "hypermode" // host where the model is running
        }
      }
    ```

    Create the embedding function using the embedding model:

    <CodeGroup>
      ```go Go
      package main

      import (
        "github.com/hypermodeinc/modus/sdk/go/models"
        "github.com/hypermodeinc/modus/sdk/go/models/experimental"
      )

      func Embed(text []string) ([][]float32, error) {
        // "minilm" is the model name declared in the app manifest
        model, err := models.GetModel[experimental.EmbeddingsModel]("minilm")
        if err != nil {
            return nil, err
        }

        input, err := model.CreateInput(text...)
        if err != nil {
            return nil, err
        }
        output, err := model.Invoke(input)
        if err != nil {
            return nil, err
        }
        return output.Predictions, nil
      }

      ```

      ```ts AssemblyScript
      import { models } from "@hypermode/modus-sdk-as"
      import { EmbeddingsModel } from "@hypermode/modus-sdk-as/models/experimental/embeddings"

      export function embed(texts: string[]): f32[][] {
        // "minilm" is the model name declared in the app manifest
        const model = models.getModel<EmbeddingsModel>("minilm")
        const input = model.createInput(texts)
        const output = model.invoke(input)
        return output.predictions
      }
      ```
    </CodeGroup>
  </Tab>

  <Tab title="OpenAI">
    [Declare the model](./app-manifest#models) in the app manifest

    ```json modus.json
      "models": {
        // model docs: https://platform.openai.com/docs/models/embeddings
        "openai-embeddings": {
          "sourceModel": "text-embedding-3-small",
          "connection": "openai",
          "path": "v1/embeddings"
        }
      },
      "connections": {
        "openai": {
          "type": "http",
          "baseUrl": "https://api.openai.com/",
          "headers": {
            "Authorization": "Bearer {{API_KEY}}"
          }
        }
      }
    ```

    Create the embedding function using the embedding model:

    <CodeGroup>
      ```go Go
      import (
        "github.com/hypermodeinc/modus/sdk/go/models"
        "github.com/hypermodeinc/modus/sdk/go/models/experimental"
      )

      func Embed(texts ...string) ([][]float32, error) {
        // retrieve the model for OpenAI embeddings
        // "openai-embeddings" is the model name declared in the app manifest
        model, err := models.GetModel[openai.EmbeddingsModel]("openai-embeddings")
        if err != nil {
          return nil, fmt.Errorf("failed to get OpenAI embeddings model: %w", err)
        }

        // create input for the model using the provided texts
        input, err := model.CreateInput(texts)
        if err != nil {
          return nil, fmt.Errorf("failed to create input for OpenAI embeddings: %w", err)
        }

        // invoke the model with the generated input
        output, err := model.Invoke(input)
        if err != nil {
          return nil, fmt.Errorf("failed to invoke OpenAI embeddings model: %w", err)
        }

        // prepare the result slice based on the size of the output data
        results := make([][]float32, len(output.Data))

        // copy embeddings from output into the result slice
        for i, d := range output.Data {
          results[i] = d.Embedding
        }

        return results, nil
      }
      ```

      ```ts AssemblyScript
      export function embed(text: string[]): f32[][] {
        const model = models.getModel<OpenAIEmbeddingsModel>("openai-embeddings")
        // "openai-embeddings" is the model name declared in the app manifest
        const input = model.createInput(text)
        const output = model.invoke(input)
        return output.data.map<f32[]>((d) => d.embedding)
      }
      ```
    </CodeGroup>
  </Tab>
</Tabs>

### Declare the search method

With an embedding function in place, declare a search method in the
[collection properties](/modus/app-manifest#collections).

```json modus.json
  "collections": {
    "myProducts": {
        "searchMethods": {
            "searchMethod1": {
                "embedder": "embed" // embedding function name
            }
        }
    }
  }

```

## Implement semantic similarity search

With the products stored, you can now search the collection by semantic
similarity. The search] API computes an embedding for the provided text,
compares it with the embeddings of the items in the collection, and returns the
most similar items.

<CodeGroup>
  ```go Go
  func SearchProducts(productDescription string, maxItems int) (*collections.CollectionSearchResult, error) {
    return collections.Search(myProducts, searchMethods[0], productDescription, collections.WithLimit(maxItems), collections.WithReturnText(true))
  }
  ```

  ```ts AssemblyScript
  export function searchProducts(
    product_description: string,
    maxItems: i32,
  ): collections.CollectionSearchResult {
    const response = collections.search(
      "myProducts", // collection name declared in the app manifest
      "searchMethod1", // search method declared for this collection in the manifest
      product_description, // text to search for
      maxItems,
      true, //  returnText: bool, true to return the items text.
      // no namespace provide, use the default namespace
    )
    return response
  }
  ```
</CodeGroup>

### Search result format

The search response is a CollectionSearchResult containing the following fields:

* `collection`: the name of the collection.
* `status`: the status of the operation.
* `objects`: the search result items with their text, distance, and score
  values.
  * `distance`: a lower value indicates a closer match between the search query
    and the item in the collection
  * `score`: a higher value (closer to 1) represents a better match

```json
{
  "collection": "myProducts",
  "status": "success",
  "objects": [
    {
      "key": "item-key-123",
      "text": "Sample product description",
      "distance": 0.05,
      "score": 0.95
    }
  ]
}
```

## Search for similar Items

When you need to search similar items to a given item, use the `searchByVector`
API. Retrieve the vector associated with the given item by its key, then perform
a search using that vector.

<CodeGroup>
  ```go Go
  func SearchSimilarProduct(productKey string, maxItems int) (*collections.CollectionSearchResult, error) {
    vec, err := collections.GetVector(
      "myProducts",
      "searchMethod1",
      productKey)

    if err != nil {
      return nil, err
    }
    return collections.SearchByVector(
      "myProducts",
      "searchMethod1",
      vec,
      collections.WithLimit(maxItems),
      collections.WithReturnText(true)
    )
  }
  ```

  ```ts AssemblyScript
  export function searchSimilarProducts(
    productId: string,
    maxItems: i32,
  ): collections.CollectionSearchResult {
    const embedding_vector = collections.getVector(
      "myProducts", // Collection name defined in the manifest
      "searchMethod1", // search method declared for the collection
      productId, // key of the collection item to retrieve
    )
    // search for similar products using the embedding vector
    const response = collections.searchByVector(
      "myProducts",
      "searchMethod1",
      embedding_vector,
      maxItems,
      true, // get the product description
    )

    return response
  }
  ```
</CodeGroup>

## Develop locally with Collections

While Collections expose a key-value interface for working with data, a
PostgreSQL database instance persists the data. When using Collections in a
Modus app deployed to the Hypermode platform, Hypermode manages this PostgreSQL
database and users don't need to perform any additional setup. However, when
developing with Modus locally or outside of the Hypermode platform, you need a
PostgreSQL instance for the Collections persistence layer.

<Note>
  The dependency on PostgreSQL is temporary and we're working to replace it with
  ModusDB, an embedded multi-model database, in an upcoming release.
</Note>

Any hosting method for this PostgreSQL database is sufficient, so feel free to
use your favorite method of installing and running PostgreSQL locally or in the
cloud. You must apply a database migration after you start the PostgreSQL
database (Step 3 below) and set the `MODUS_DB` environment variable with your
PostgreSQL database connection string (Step 4 below). The steps below describe
using [Docker](https://www.docker.com/products/docker-desktop/) to start and
configure a PostgreSQL instance for local development with Collections.

To start and configure a local PostgreSQL instance for working with Collections
locally using [Docker Compose](https://docs.docker.com/compose/), follow these
steps:

<Steps>
  <Step title="Clone Modus locally">
    Clone the [Modus GitHub repository](https://github.com/hypermodeinc/modus):

    ```sh
    git clone https://github.com/hypermodeinc/modus.git
    ```
  </Step>

  <Step title="Start a PostgreSQL instance">
    Start the Collections PostgreSQL database using the local Docker Compose script:

    ```sh
    cd modus/runtime/tools/local
    docker compose up
    ```
  </Step>

  <Step title="Apply database migration">
    Next, apply the database schema using the
    [golang-migrate](https://github.com/golang-migrate/migrate) utility.

    On MacOS, you can install this utility with the following:

    ```sh
    brew install golang-migrate
    ```

    Then, you can apply the migration as follows:

    ```sh
    export POSTGRESQL_URL='postgresql://postgres:postgres@localhost:5433/my-runtime-db?sslmode=disable'
    migrate -database ${POSTGRESQL_URL} -path ../../db/migrations up
    ```
  </Step>

  <Step title="Set environment variable">
    Set the `MODUS_DB` environment variable:

    ```sh
    export MODUS_DB=postgresql://postgres:postgres@localhost:5433/my-runtime-db?sslmode=disable
    ```
  </Step>
</Steps>

You can now use Collections locally in your Modus app.


# Testing
Source: https://docs.hypermode.com/modus/testing



<Warning>
  * writing unit and integration tests - test frameworks and tools supported
</Warning>


# Troubleshooting
Source: https://docs.hypermode.com/modus/troubleshooting

Common issues, resolutions, and best practices for triaging issues in Modus.

<Warning>common issues, resolutions, and best practices</Warning>

## Troubleshooting

Welcome to the Modus troubleshooting guide! This page covers common issues you
might encounter, their resolutions, and best practices for triaging issues
effectively.

### Common issues and resolutions

#### Issue: CLI hangs during project initialization

**Symptoms**: the Modus CLI hangs indefinitely when running `modus init`.

**Resolution**:

1. Ensure you have the latest version of the Modus CLI installed.
2. Check your internet connection.
3. Run the command with the `--verbose` flag to get more detailed output:

   ```sh
   modus init --verbose
   If the issue persists, try deleting the .modus directory in your home folder and re-running the command.
   Issue: Environment Variables Not Loading
   Symptoms: Environment variables defined in .env are not being recognized by your Modus functions.
   ```


# Upgrading
Source: https://docs.hypermode.com/modus/upgrading





# Observe Functions
Source: https://docs.hypermode.com/observe-functions

Understand what's happening within your functions

When you invoke a function or model within your app, Modus records the
execution. The Hypermode Console makes debugging simple with easy to understand
logs and metrics.

## Function runs

From your project, navigate to the Function Runs tab to view execution logs of
your functions.

![function runs](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/observe-functions/function-runs.png)

For more information on recording info, warnings, and errors in your Modus app,
see [Error Handling](/modus/error-handling).

## Model tracing

For each model invocation, Hypermode records the model inference and related
metadata. This includes the input and output of the model, as well as the
timestamp and duration of the inference.

From your project, select the Inferences tab to view the inferences from your
app's models.

![model tracing](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/observe-functions/inference-history.png)


# Quickstart
Source: https://docs.hypermode.com/quickstart

Start building AI features in under five minutes

In this quickstart we'll show you how to get set up with Hypermode and build an
intelligent API that you can integrate into your app. You'll learn how to use
the basic components of a Modus app and how to deploy it to Hypermode.

## Prerequisites

* [Node.js](https://nodejs.org/en/download/package-manager) - v22 or higher
* Text editor - we recommend [VS Code](https://code.visualstudio.com/)
* Terminal - access Modus through a command-line interface (CLI)
* [GitHub Account](https://github.com/join)

## Deploying your first Hypermode project

<Steps>
  <Step title="Create Modus app">
    We built Hypermode on top of Modus, an open source, serverless framework for crafting
    intelligent functions and APIs, powered by WebAssembly. With Hypermode, you can deploy,
    secure, and observe your Modus apps.

    To get started, [create your first Modus app](/modus/quickstart). You can import this app into
    Hypermode in the next step.
  </Step>

  <Step title="Import Modus app">
    You can import your Modus app via the Hypermode Console or through the terminal with Hyp CLI.

    <Tabs>
      <Tab title="Hypermode Console">
        Navigate to the [Hypermode Console](https://hypermode.com/sign-in) and click **New Project**.
        When prompted, connect your GitHub account and select the repository you want to import.
        Once you've selected your repository, click "Import" to deploy your app.
      </Tab>

      <Tab title="Hyp CLI">
        Install the Hyp CLI via npm.

        ```sh
        npm install -g @hypermode/hyp-cli
        ```

        From the terminal, run the following command to import your Modus app into Hypermode. This command
        creates your Hypermode project and deploys your app.

        ```sh
        hyp link
        ```
      </Tab>
    </Tabs>

    When Hypermode creates your project, a runtime is initiated for your app as well as connections to
    any [Hypermode-hosted models](/hosted-models).
  </Step>

  <Step title="Explore API endpoint">
    After deploying your app, Hypermode lands you in your project home. You can see the status of your
    project and the API endpoint generated for your app.

    From the **Query** page, you can run a sample query to verify it's working as expected. In the following
    query, we're going to use the `generateText` function to generate text from the shared Meta Llama
    3.1 model based on the prompt "How are black holes created?"

    ```GraphQL
    query myPrompt {
      generateText(text:"How are black holes created?")
    }
    ```

    <img className="block" src="https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/hyp-quickstart/graphiql-blackhole.png" alt="Hypermode's console showing results of query 'how are black holes created'." />
  </Step>

  <Step title="Observe function execution">
    Let's dig deeper into the behavior of our AI service when we ran the query by looking at the
    **Inferences** page. You can see the step-by-step inference process and the inputs and outputs of
    the model at each step of your function.
    After invoking the Meta Llama model from the function code, we can see the function execution. In this case, it took Llama 4.4 seconds to
    reply to the prompt. We can also see the parameters on both the inputs and outputs.

    ```json
    {
      "model": "meta-llama/Llama-3.2-3B-Instruct",
      "messages": [
        {
          "role": "system",
          "content": "You are a helpful assistant. Limit your answers to 150 words."
        },
        {
          "role": "user",
          "content": "How are black holes created?"
        }
      ],
      "max_tokens": 200,
      "temperature": 0.7
    }
    ```

    <img className="block" src="https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/hyp-quickstart/inference-history.png" alt="Hypermode's console showing the inputs and outputs of the last model inference." />
  </Step>

  <Step title="Customize your function">
    Hypermode makes it simple to iterate quickly. Let's make a few changes to your app
    to explore how easy customizing your API is.

    Our API is responding using language that is more formal than we want. Let's update our
    `generateText` function to respond using exclusively surfing analogies.

    <Tabs>
      <Tab title="Go">
        Go to the `main.go` file and locate the `generateText` function. Modify the function to only respond like a surfer, like this:

        ```ts main.go
        func GenerateText(text string) (string, error) {
          model, err := models.GetModel[openai.ChatModel]("text-generator")
          if err != nil {
              return "", err
          }

          input, err := model.CreateInput(
              openai.NewSystemMessage("You are a helpful assistant. Only respond using surfing analogies and metaphors."),
              openai.NewUserMessage(text),
          )
          if err != nil {
              return "", err
          }

          output, err := model.Invoke(input)
          if err != nil {
              return "", err
          }

          return strings.TrimSpace(output.Choices[0].Message.Content), nil
        }
        ```
      </Tab>

      <Tab title="AssemblyScript">
        Go to the `index.ts` file and locate the `generateText` function. Modify the function to only respond like a surfer, like this:

        ```ts index.ts
        export function generateText(text: string): string {
          const model = models.getModel<OpenAIChatModel>("text-generator")

          const input = model.createInput([
            new SystemMessage(
              "You are a helpful assistant. Only respond using surfing analogies and metaphors.",
            ),
            new UserMessage(text),
          ])

          const output = model.invoke(input)

          return output.choices[0].message.content.trim()
        }
        ```
      </Tab>
    </Tabs>

    Save the file and push an update to your Git repo. Hypermode automatically redeploys
    whenever you push an update to the target branch in your Git repo. Go back to the Hypermode Console
    and run the same query as before. You should see the response now uses surfing analogies!

    <img className="block" src="https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/hyp-quickstart/graphiql-surfing.png" alt="Hypermode's console showing results of new query." />
  </Step>
</Steps>

## Next steps

Hypermode and Modus provide a powerful platform for building and hosting AI
models, data, and logic. You now know the basics of Hypermode. There's no limit
to what you can build.

And when you're ready to [integrate Hypermode into your app](/integrate-api),
that's as simple as calling a GraphQL endpoint.


# Security
Source: https://docs.hypermode.com/security

Data handling, privacy, and security best practices

At Hypermode, we take security seriously. This document outlines our approach to
data handling, privacy, and security best practices to ensure your projects are
safe and secure.

## System description

Hypermode is a fully managed AI development platform that's designed to offer
easy integration of data and AI models, code-first agents and inferences, and
end-to-end visibility. The platform includes production hosting and
automatically created branch-based environments for apps and graphs, powered by
Modus and Dgraph.

Hypermode Inc. is responsible for the development and maintenance of the
service. Services leverage cloud-hosted infrastructure and we make some
components available as open source software under the Apache 2.0 License.
Relevant code repositories are available publicly through GitHub, and we welcome
contributions by the community, subject to a public code of conduct and GitHub
terms of service.

## Data handling and privacy

Hypermode maintains a strong commitment to protecting your data and privacy. For
detailed information on our data handling and privacy policies, please refer to
our [Privacy Policy](https://hypermode.com/privacy).

## Security best practices

Security is a shared responsibility. To help you keep your projects secure, we
recommend the following best practices:

* **Use strong passwords**: ensure that all user accounts use strong, unique
  passwords
* **Develop secure apps**: implement security and quality coding best practices
  in the code you write and deploy
* **Regularly update dependencies**: keep your project dependencies up-to-date
  to avoid vulnerabilities
* **Monitor for security alerts**: use tools to monitor your codebase for
  security vulnerabilities and address them promptly
* **Manage roles and permissions**: add and remove users from your organization
  as your team members change

## Reporting security issues

We take the security of Hypermode and our open source projects very seriously.
If you believe you have found a security vulnerability, we encourage you to let
us know right away.

We investigate all legitimate reports and do our best to quickly fix the
problem. Please report any issues or vulnerabilities via GitHub Security
Advisories instead of posting a public issue in GitHub. You can also send
security communications to [security@hypermode.com](mailto:security@hypermode.com).

Please include the version identifier and details on how the you can exploit the
vulnerability.

We appreciate your help in keeping Hypermode secure.


# Semantic Search With Dgraph and Modus
Source: https://docs.hypermode.com/semantic-search

Add natural language search to your app with Dgraph, Modus, and AI embeddings

By leveraging embeddings and similarity search backed by a scalable vector index
developers can enable semantic and similarity-based searches, improving the
relevance of search results within their applications. This tutorial covers
implementing semantic search using Modus, Dgraph, and Hypermode hosted AI models
to add natural language or semantic search to your app using an example of
ecommerce product data.

<iframe
  src="https://www.youtube.com/embed/Z2fB-nBf4Wo"
  title="Natural Language Search With Dgraph, Modus, and Hypermode"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write;
encrypted-media; gyroscope; picture-in-picture"
  allowfullscreen
  style={{ aspectRatio: "16 / 9", width: "100%" }}
/>

## Semantic search overview

Semantic search focuses on understanding the meaning and context behind queries
rather than just matching keywords, using embeddings to capture semantic
relationships between concepts. Vector search serves as the technical
implementation method, converting text into numerical vector embeddings and
finding similar content through mathematical distance calculations in
multidimensional space.

Vector search is a powerful technique that transforms data (like text, images,
or audio) into numerical representations called embeddings. These embeddings
capture the semantic meaning of the content in a multi-dimensional space and
position similar items closer together. When performing a search, the query is
also converted into an embedding, and the system finds items whose embeddings
are closest to the query embedding.

This approach offers significant benefits over traditional keyword-based search,
including improved relevance by capturing context and semantics, enhanced
precision by understanding user intent, and the ability to handle complex
queries with higher accuracy. Vector search is particularly effective for
applications like semantic search, recommendation systems, and retrieval
augmented generation (RAG), optimizing both efficiency and accuracy in finding
and retrieving data based on meaningful similarity rather than exact matches.
When combined with graph traversals, vector search can enable complex GraphRAG
retrieval patterns.

## The components

* **Dgraph** is a scalable graph database capable of near real-time graph
  traversals and vector search
* **Modus** is the serverless API framework for building AI apps
* **Hypermode** is the AI development platform, hosting our app, graph, and
  supporting models

## Prerequisites

This tutorial assumes you have:

1. Created a Dgraph instance, either hosted in the cloud or locally via Docker
   or installed via the Dgraph binary
2. Installed the Modus CLI and created a Modus app. See the
   [Modus Quickstart](modus/quickstart) to get started with Modus.

## Natural language search with Dgraph and Modus

The steps to implement natural language or semantic search with Dgraph include
defining the Dgraph connection in your Modus app manifest, selecting and
configuring an embedding model, declaring a vector index in the Dgraph DQL
schema, and using the `similar_to` DQL function to search for similar text in
vector space.

Our example app uses ecommerce product data consisting of product details to
enable semantic product search based on natural language terms.

## Declare Dgraph connection and Hypermode embedding model

First, update the Modus app manifest file `modus.json` to define the connection
to your Dgraph instance and the embedding model used to generate embeddings.
Here we're using the MiniLM model hosted by Hypermode and connecting to a
locally running Dgraph instance.

```json
{
  "$schema": "https://schema.hypermode.com/modus.json",
  "endpoints": {
    "default": {
      "type": "graphql",
      "path": "/graphql",
      "auth": "bearer-token"
    }
  },
  "models": {
    "minilm": {
      "sourceModel": "sentence-transformers/all-MiniLM-L6-v2",
      "connection": "hypermode",
      "provider": "hugging-face"
    }
  },
  "connections": {
    "dgraph-grpc": {
      "type": "dgraph",
      "grpcTarget": "localhost:9080"
    }
  }
}
```

<Note>
  In order to use Hypermode hosted models in the local Modus development
  environment you'll need to use the `hyp` CLI to connect your local environment
  with your Hypermode account. See the [Using Hypermode-hosted
  models](work-locally#using-hypermode-hosted-models) docs page for more
  information.
</Note>

## Type definitions

Next, in our Modus app we define our data model using classes with decorators
for automatic serialization/deserialization. The `@json` decorator enables JSON
serialization, while `@alias` maps property names to Dgraph convention friendly
formats.

We'll be using ecommerce data so we'll create simple types defining products and
their categories.

```ts
@json
export class Product {
  @alias("Product.id")
  id!: string

  @alias("Product.title")
  title: string = ""

  @alias("Product.description")
  description: string = ""

  @alias("Product.category")
  @omitnull()
  category: Category | null = null
}

@json
export class Category {
  @alias("Category.name")
  name: string = ""
}
```

## Embedding model integration

Next, we create an embedding function that uses a transformer model (MiniLM in
this case) to convert product descriptions and search queries into vectors:

```ts
import { models } from "@hypermode/modus-sdk-as"
import { EmbeddingsModel } from "@hypermode/modus-sdk-as/models/experimental/embeddings"

const EMBEDDING_MODEL = "minilm"

export function embedText(content: string[]): f32[][] {
  const model = models.getModel<EmbeddingsModel>(EMBEDDING_MODEL)
  const input = model.createInput(content)
  const output = model.invoke(input)
  return output.predictions
}
```

## Define Dgraph schema

We declare a schema to use Dgraph's vector search capability and create an index
on the `Product.embedding` property, even though Dgraph can function without a
schema.

To define our Dgraph schema with vector indexing support we add the
`@index(hnsw)` directive to the property storing the embedding value, in this
case `Product.embedding`. We also define the other property types and node
labels.

```rdf
<Category.name>: string @index(hash) .
<Product.category>: uid @reverse .
<Product.description>: string .
<Product.id>: string @index(hash) .
<Product.embedding>: float32vector @index(hnsw) .
```

To apply this schema to our Dgraph instance we can make a POST request to the
`/alter` endpoint of our Dgraph instance:

```bash
curl -X POST localhost:8080/alter --silent --data-binary '@dqlschema.txt'
```

or use the schema tab of the Ratel interface to apply the schema.

## Define Modus mutation function

Now we're ready to create a Modus function to create data in Dgraph. Here we
create an upsert mutation that creates a product and related category in Dgraph,
without creating duplicate nodes.

This function uses the embedding model we configured in previous steps to create
an embedding of the product description and save to the `Product.embedding`
property.

```ts
const DGRAPH_CONNECTION = "dgraph-grpc"

/**
 * Add or update a new product to the database
 */
export function upsertProduct(product: Product): Map<string, string> | null {
  let payload = buildProductMutationJson(DGRAPH_CONNECTION, product)

  const embedding = embedText([product.description])[0]
  payload = addEmbeddingToJson(payload, "Product.embedding", embedding)

  const mutation = new dgraph.Mutation(payload)
  const response = dgraph.executeMutations(DGRAPH_CONNECTION, mutation)

  return response.Uids
}
```

<Note>
  Refer to the full code
  [here](https://github.com/hypermodeinc/modus-recipes/tree/main/dgraph-101) for
  how to implement other Dgraph mutation and query functions and associated
  Dgraph helpers.
</Note>

## Dgraph `similar_to` query function

Next, we create a Modus function that uses Dgraph's `similar_to` query function
that leverages the vector index to find semantically similar products by
computing an embedding of the search term and searching for nearby product
descriptions in vector space.

```ts
/**
 * Search products by similarity to a given text
 */
export function searchProducts(search: string): Product[] {
  const embedding = embedText([search])[0]
  const topK = 3
  const body = `
    Product.id
    Product.description
    Product.title
    Product.category {
      Category.name
    }
  `
  return searchBySimilarity<Product>(
    DGRAPH_CONNECTION,
    embedding,
    "Product.embedding",
    body,
    topK,
  )
}

export function searchBySimilarity<T>(
  connection: string,
  embedding: f32[],
  predicate: string,
  body: string,
  topK: i32,
): T[] {
  const query = new dgraph.Query(`
    query search($vector: float32vector) {
        var(func: similar_to(${predicate},${topK},$vector))  {
            vemb as Product.embedding
            dist as math((vemb - $vector) dot (vemb - $vector))
            score as math(1 - (dist / 2.0))
        }

        list(func:uid(score),orderdesc:val(score))  @filter(gt(val(score),0.25)){
            ${body}
        }
    }`).withVariable("$vector", embedding)

  const response = dgraph.executeQuery(connection, query)
  console.log(response.Json)
  return JSON.parse<ListOf<T>>(response.Json).list
}
```

## Query Modus endpoint

We can run our Modus app using the `modus dev` command which generates a GraphQL
schema from the functions we've defined and start a local GraphQL endpoint for
testing and development.

Navigate to `http://localhost:8686/explorer` in your browser and use the Modus
API Explorer to first insert sample data into Dgraph using the upsert mutation
function we defined previously and then search for similar products using vector
search.

First, to create product and category nodes:

```graphql
mutation ($product: ProductInput!) {
  upsertProduct(product: $product) {
    key
    value
  }
}
```

We'll use the following values for the `product` variable creating three product
nodes ad their associated category nodes in Dgraph:

| Product ID | Title                   | Description                                                                                        | Category           |
| ---------- | ----------------------- | -------------------------------------------------------------------------------------------------- | ------------------ |
| P001       | Solar-Powered Umbrella  | A stylish umbrella with solar panels that charge your devices while you walk.                      | Outdoor Gear       |
| P002       | Self-Warming Coffee Mug | A mug that keeps your coffee at the perfect temperature using smart heating technology.            | Kitchen Appliances |
| P003       | Smart Pillow 2.0        | A pillow that tracks your sleep patterns and plays soothing sounds to help you fall asleep faster. | Smart Home         |

![Creating products in Dgraph](https://mintlify.s3.us-west-1.amazonaws.com/hypermode/images/tutorials/semantic-search/upsert.png)

And then to search for semantically similar products based on a search string we
can execute the following query, using the value of our search string for the
`$search` variable.

```graphql
query ($search: String!) {
  searchProducts(search: $search) {
    id
    title
    description
    category {
      name
    }
  }
}
```

For example, if we search using the search term "rain":

```graphql
query {
  searchProducts(search: "rain") {
    id
    title
    description
    category {
      name
    }
  }
}
```

the product search results returns our solar powered umbrella.

```json
{
  "data": {
    "searchProducts": [
      {
        "id": "P001",
        "title": "Solar-Powered Umbrella",
        "description": "A stylish umbrella with solar panels that charge your devices while you walk.",
        "category": {
          "name": "Outdoor Gear"
        }
      }
    ]
  }
}
```

Even though the description of the solar powered umbrella doesn't include the
word "rain" thanks to the meaning encoded into the embedding our semantic search
process understands the association between rain and umbrella.

## Next steps

Now that we've implemented semantic search using Dgraph, Modus, and Hypermode
hosted models using the local development experience we're ready to take the
next step and deploy our project to Hypermode. See the [Deploy Project](deploy)
section for a walk through of this process.

## Resources

* You can find the full code for this example in the
  [Modus Recipes](https://github.com/hypermodeinc/modus-recipes) GitHub
  repository:
  [https://github.com/hypermodeinc/modus-recipes/tree/main/dgraph-101](https://github.com/hypermodeinc/modus-recipes/tree/main/dgraph-101)
* Watch a video overview of this tutorial in the
  [Hypermode YouTube channel](https://www.youtube.com/@hypermodeinc):
  [https://www.youtube.com/watch?v=Z2fB-nBf4Wo](https://www.youtube.com/watch?v=Z2fB-nBf4Wo)


# User Management
Source: https://docs.hypermode.com/user-management

Add and remove users from your organization

Invite team members to your organization in Hypermode. To see who currently has
access to your organization and make changes, select the target organization
from the top navigation and then click **Users**.

## Inviting a user

You can add a new user to join your organization by sending an email invitation.
From the Users tab, click **Invite User**. In the side panel, enter the email
address of the user you wish to invite and click **Invite**.

The user is now listed as `Pending Invitation` in the `Date Joined` column of
the user list.

## Managing invitations

After sending an invitation, the user appears as `Pending Invitation` in the
organization user list.

If the user lost or didn't receive the initial invitation, click **Resend
Invite** from the `...` edit menu within the target user’s row.

If you need to cancel a sent invitation, click **Revoke Invite** from the `...`
edit menu within the target user’s row.

## Removing a user

To remove a user from your organization, identify the user you would like to
remove in the Users tab. Click the `...` to reveal the **Remove User** button.

Removing a user from your organization doesn't delete their Hypermode user
account or impact their access to other organizations.


# Work Locally
Source: https://docs.hypermode.com/work-locally

Seamless local-to-cloud experience for rapid experimentation

By using the Hyp CLI, you can access Hypermode-hosted models defined in your
[app's manifest](/modus/app-manifest). This allows for seamless and rapid
experimentation with new AI models.

## Hyp CLI setup

The Hyp CLI helps you invoke your Hypermode-hosted models from your local Modus
development environment. You can install the Hyp CLI to augment Modus and access
your Hypermode-hosted models.

```sh
npm install -g @hypermode/hyp-cli
```

## Log into Hypermode

Before running `modus dev` to run your Modus app locally, ensure you're logged
into the Hyp CLI with the `hyp login` command.


