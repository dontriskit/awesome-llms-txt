title: Maxim AI Documentation - Simulate, evaluate, and observe your AI agents

===

url: https://getmaxim.ai/docs/analyze/overview
meta: {
  "title": "Overview",
  "description": "Explore powerful analysis tools in Maxim for generating comparison reports, creating live dashboards, and gaining actionable insights from your AI application data."
}

The Analyze section provides powerful tools to help you gain insights from your data.

## Analyze your data

![Analyze main page](/images/docs/analyze/analyze.png)

Here are the key features available:

## Generate comparison reports

Create detailed comparison reports between existing runs to easily analyze, share, and export data as needed. This feature enables you to make data-driven decisions by comparing different test runs side by side.

## Create live dashboards (Coming soon)

Create dynamic dashboards based on a filter logic of recent runs to monitor progress over time. This upcoming feature will allow you to visualize and track your data in real-time through customizable dashboards.

These analysis tools are designed to help you make the most of your test data and provide actionable insights for your team.


url: https://getmaxim.ai/docs/api/overview
meta: {
  "title": "Overview",
  "description": "Welcome to the Maxim API documentation. This guide provides comprehensive information about our available APIs, their endpoints, and how to use them."
}

## Available APIs

Maxim offers several specialized APIs to help you integrate with our platform:

| API                           | Description                                                |
| ----------------------------- | ---------------------------------------------------------- |
| **Prompts API**               | Manage AI prompts and templates                            |
| **Alerts API**                | Configure and manage alert notifications                   |
| **Integrations API**          | Connect Maxim with external services like Slack, Pagerduty |
| **Log Repositories API**      | Manage log storage and organization                        |
| **Test Runs API**             | Create and manage test execution workflows                 |
| **Test Run Share Report API** | Generate and share test results                            |
| **Test Run Entries API**      | Access detailed test execution data                        |
| **Logs API**                  | Query and analyze log data                                 |
| **Datasets API**              | Manage training and evaluation datasets                    |
| **Evaluators API**            | Get evaluator details and execute evaluators               |

## Authentication

All API requests require authentication using API keys.

Include your API key in the request headers using the `x-maxim-api-key` header:


url: https://getmaxim.ai/docs/evaluate/concepts
meta: {
  "title": "Concepts",
  "description": "Learn about the key concepts in Maxim"
}

## Prompts

Prompts are text-based inputs provided to AI models to guide their responses and influence the behaviour of the model. The structure and complexity of prompts can vary based on the specific AI model and the intended use case. Prompts may range from simple questions to detailed instructions or multi-turn conversations. They can be optimised or fine-tuned using a range of configuration options, such as variables and other model parameters like temperature, max tokens, etc, to achieve the desired output.

Here's an example of a multi-turn prompt structure:

<table>
  <thead>
    <tr>
      <th>Turn</th>
      <th>Content</th>
      <th>Purpose</th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>Initial prompt</td>
      <td>You are a helpful AI assistant specialized in geography.</td>
      <td>Sets the context for the interaction (optional, model-dependent)</td>
    </tr>

    <tr>
      <td>User input</td>
      <td>What's the capital of France?</td>
      <td>The first query for the AI to respond to</td>
    </tr>

    <tr>
      <td>Model response</td>
      <td>The capital of France is Paris.</td>
      <td>The model's response to the first query</td>
    </tr>

    <tr>
      <td>User input</td>
      <td>What's its population?</td>
      <td>A follow-up question, building on the previous context</td>
    </tr>

    <tr>
      <td>Model response</td>
      <td>As of 2023, the estimated population of Paris is about 2.2 million people in the city proper.</td>
      <td>The model's response to the follow-up question</td>
    </tr>
  </tbody>
</table>

You can find more about prompts [here](/docs/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground).

## Prompt comparisons

Prompt comparisons help evaluate different prompts side-by-side to determine which ones produce the best results for a given task. They allow for easy comparison of prompt structures, outputs, and performance metrics across multiple models or configurations.

You can find more about prompt comparisons [here](/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground).

## Prompt chains

Prompt chains are structured sequences of AI interactions designed to tackle complex tasks through a series of interconnected steps. Prompt chains provide a visual representation of the workflow, support agentic behavior, allow for code-based and API configuration.

You can find more about prompt chains [here](/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains).

## Workflows

Workflows enable end-to-end testing of AI applications via HTTP endpoints. They allow seamless integration of existing AI services without code changes, featuring payload configuration with dynamic variables, playground testing, and output mapping for evaluation.

You can find more about workflows [here](/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-your-ai-outputs-using-application-endpoint).

## Test runs

Test runs are controlled executions of prompts, prompt-chains or workflows to evaluate their performance, accuracy, and behavior under various conditions. They can be single or comparison runs providing detailed summaries, performance metrics, and debug information for every entry to assess AI model performance.

Tests can be run on prompts, chains, workflows or datasets directly.

## Evaluators

Evaluators are tools or metrics used to assess the quality, accuracy, and effectiveness of AI model outputs. We have various types of evaluators that can be customized and integrated into workflows and test runs. See below for more details.

You can find more about Maxim's [pre-built evaluators](/docs/library/how-to/evaluators/use-pre-built-evaluators) or [create your own](/docs/library/how-to/evaluators/create-custom-ai-evaluator)

<table>
  <thead>
    <tr>
      <th>Evaluator type</th>
      <th>Description</th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>AI</td>
      <td>Uses AI models to assess outputs</td>
    </tr>

    <tr>
      <td>Programmatic</td>
      <td>Applies predefined rules or algorithms</td>
    </tr>

    <tr>
      <td>Statistical</td>
      <td>Utilizes statistical methods for evaluation</td>
    </tr>

    <tr>
      <td>Human</td>
      <td>Involves human judgment and feedback</td>
    </tr>

    <tr>
      <td>API-based</td>
      <td>Leverages external APIs for assessment</td>
    </tr>

    <tr>
      <td colspan="2">You can find more about our evaluators [here](/docs/library/how-to/evaluators/use-pre-built-evaluators).</td>
    </tr>
  </tbody>
</table>

## Datasets

Datasets are collections of data used for training, testing, and evaluating AI models within workflows and evaluations. They allow users to test their prompts and AI systems against their own data, and include features for data structure management, integration with AI workflows, and privacy controls.

You can find more about datasets [here](/docs/library/how-to/datasets/use-dataset-templates).

## Context sources

Context sources handle and organize contextual information that AI models use to understand and respond to queries more effectively. They support Retrieval-Augmented Generation (RAG) and include API integration, sample input testing, and seamless incorporation into AI workflows. Context sources enable developers to enhance their AI models' performance by providing relevant background information for more accurate and contextually appropriate responses.

You can find more about context sources [here](/docs/library/how-to/context-sources/bring-your-rag-via-an-api-endpoint).

## Prompt tools

Prompt tools are utilities that assist in creating, refining, and managing prompts, enhancing the efficiency of working with AI prompts. They feature custom function creation, a playground environment, and integration with workflows.

You can find more about prompt tools [here](/docs/library/how-to/prompt-tools/create-a-code-tool).

## Simulation

Maxim's AI powered simulations help mimic multi-turn interactions with your AI application. Based on the given scenario and user persona, the simulation agent sends back and forth messages to your application via the HTTP endpoint. Providing context to the simulation makes sure it acts as close to a real user as possible. Specific multi-turn evaluators can be used to measure the quality of your agent in responding for every scenario.

You can find more about running simulations [here](/docs/evaluate/quickstart/simulate-and-evaluate-multi-turn-conversations).


url: https://getmaxim.ai/docs/evaluate/overview
meta: {
  "title": "Overview",
  "description": "Learn how to evaluate AI application performance through prompt testing, workflow automation, and continuous log monitoring. Streamline your AI testing pipeline with comprehensive evaluation tools."
}

Experimentation and evaluation forms the cornerstone of our platform. This includes Prompts experimentation and testing, end to end testing of your application using Workflows, ability to view and manage all your test run reports, and set up continuous evaluation on logs.

Evaluation can be run on multiple elements - prompts, workflows or directly on datasets with output data.

The below diagrams clarify how the different elements sit together to run evaluations right from experimentation and pre-release to continuous quality check in production.

![Evaluate](/images/docs/evaluate/overview/evaluate.png)


url: https://getmaxim.ai/docs/introduction/overview
meta: {
  "title": "Overview",
  "description": "Maxim streamlines AI application development and deployment by applying traditional software best practices to non-deterministic AI workflows."
}

<video url="https://drive.google.com/file/d/1-h9AwW2FRM5dZWHyvFS6BLO9chSIDxLD/preview" />

Our advanced evaluation and observability tools help teams maintain quality, reliability, and speed throughout the AI application lifecycle.

![Overview](/images/docs/introduction/overview.png)

## 1. Experiment

We have a Playground++ built for advanced prompt engineering, enabling rapid iteration, deployment, and experimentation.

* Organize and version their prompts effectively
* Deploy prompts with different deployment variables and experimentation strategies without code changes
* Connect with databases, RAG pipelines, and prompt tools seamlessly
* Simplify decision-making by comparing output quality, cost, and latency across various combinations of prompts, models, and parameters

## 2. Evaluate

Our unified framework for machine and human evaluations allows you to quantify improvements or regressions and deploy with confidence.

* Access a variety of off-the-shelf evaluators through the evaluator store
* Create custom evaluators suited to specific application needs
* Measure quality of prompts or workflows quantitatively using AI, programmatic, or statistical evaluators
* Visualize evaluation runs on large test suites across multiple versions of prompts or workflows
* Human evaluations can be conducted for last-mile quality checks and nuanced assessments

## 3. Observe

The observability suite empowers you to monitor real-time production logs and run them through periodic quality checks to ensure production quality.

* Create multiple repositories for multiple apps for your production data that can be logged and analyzed using distributed tracing
* Live issues can be tracked, debugged, and resolved quickly
* In-production quality can be measured using automated evaluations based on custom rules
* Datasets can be curated with ease for evaluation and fine-tuning needs

## 4. Data engine

Seamless data management for AI applications allows users to curate and enrich multi-modal datasets easily for evaluation and fine-tuning needs.

* Import datasets, including images, with a few clicks
* Continuously curate and evolve datasets from production data
* Enrich data using in-house or Maxim-managed data labeling and feedback
* Create data splits for targeted evaluations and experiments


url: https://getmaxim.ai/docs/library/concepts
meta: {
  "title": "Concepts",
  "description": "Explore key concepts in AI evaluation, including evaluators, datasets, and custom tools for assessing model performance and output quality."
}

## Evaluators

Evaluators are tools or metrics used to assess the quality, accuracy, and effectiveness of AI model outputs. We have various types of evaluators that can be customized and integrated into workflows and test runs. See below for more details.

You can find more about evaluators [here](/docs/library/how-to/evaluators/use-pre-built-evaluators).

<table>
  <thead>
    <tr>
      <th>Evaluator type</th>
      <th>Description</th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>AI</td>
      <td>Uses AI models to assess outputs</td>
    </tr>

    <tr>
      <td>Programmatic</td>
      <td>Applies predefined rules or algorithms</td>
    </tr>

    <tr>
      <td>Statistical</td>
      <td>Utilizes statistical methods for evaluation</td>
    </tr>

    <tr>
      <td>Human</td>
      <td>Involves human judgment and feedback</td>
    </tr>

    <tr>
      <td>API-based</td>
      <td>Leverages external APIs for assessment</td>
    </tr>
  </tbody>
</table>

## Evaluator Store

A large set of pre-built evaluators are available for you to use directly. These can be found in the evaluator store and added to your workspace with a single click.

At Maxim, our pre-built evaluators fall into two categories:

1. **Maxim-created Evaluators**: These are evaluators created, benchmarked, and managed by Maxim. There are three kinds of Maxim-created evaluators:
   1. **AI Evaluators:** These evaluators use other large language models to evaluate your application (LLM-as-a-Judge).
   2. **Statistical Evaluators:** Traditional ML metrics such as BLEU, ROUGE, WER, TER, etc.
   3. **Programmatic Evaluators:** JavaScript functions for common use cases like `validJson`, `validURL`, etc., that help validate your responses.

2. **Third-party Evaluators**: We have also enabled popular third-party libraries for evaluation, such as RAGAS, so you can use them in your evaluation workflows with just a few clicks. If you have any custom integration requests, please feel free to drop us a note.

Within the store, you can search for an evaluator or filter by type tags. Simply click the **"Add to workspace"** button to make it available for use by your team.

<video url="https://drive.google.com/file/d/1XvoC_7cUM1kF0Off0u_9g4qxMlaSpp74/preview" />

If you want us to build a specific evaluator for your needs, please drop a line at [contact@getmaxim.ai](mailto:contact@getmaxim.ai).

## Custom Evaluators

While we provide many evaluators for common use cases out of the box, we understand that some applications have specific requirements. Keeping that in mind, the platform allows for easy creation of custom evaluators of the following types:

### AI Evaluators

These evaluators use other LLMs to evaluate your application. You can configure different prompts, models, and scoring strategies depending on your use case. Once tested in the playground, you can start using the evaluators in your workflows.

### Programmatic Evaluators

These are JavaScript functions where you can write your own custom logic. You can use the `{{input}}`, `{{output}}`, and `{{expectedOutput}}` variables, which pull relevant data from the dataset column or the response of the run to execute the evaluator.

### API-based Evaluators

If you have built your own evaluation model for specific use cases, you can expose the model using an HTTP endpoint and integrate it within Maxim for evaluation.

### Human Evaluators

This allows for the last mile of evaluation with human annotators in the loop. You can create a Human Evaluator for specific criteria that you want annotators to assess. During a test run, simply attach the evaluators, add details of the raters, and choose the sample set for human annotation. Learn more about the human evaluation lifecycle [here](/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline).

<Callout type="info">
  Every evaluator should return a score and reasoning, which are then analyzed and used to summarize results according to your criteria.
</Callout>

## Evaluator Grading

Every evaluator's grading configuration has two parts:

1. **Type of scale** – Yes/No, Scale of 1-5, etc.
   * For AI evaluators, this can be chosen, and an explanation is needed for grading logic.
   * For programmatic evaluators, the relevant response type can be configured.
   * For API-based evaluators, you can map the field to be used for scoring.

2. **Pass criteria** – This includes configuration for two levels:
   * The score at which an evaluator should pass for a given query.
   * The percentage of queries that need to pass for the evaluator to pass at the run level across all dataset entries.

For custom evaluators, both of these are configurable, while for pre-built evaluators, you can define your pass criteria.

The evaluator below assigns a score between 0 and 1. The pass criteria are defined such that a query passes if it scores **≥ 0.8**, and for the entire report, the evaluator **"Clarity"** passes if **80% of the queries score ≥ 0.8**.

<Callout type="info" id="evaluator-variables">
  Maxim uses reserved variables with specific meanings:

  * `{{input}}`: Input from the dataset
  * `{{expectedOutput}}`: Expected output from the dataset
  * `{{expectedToolCalls}}`: Expected tool calls from the dataset
  * `{{scenario}}`: Scenario from the dataset
  * `{{expectedSteps}}`: Expected steps from the dataset
  * `{{output}}`: Generated output of the workflow/prompt/prompt chain
  * `{{context}}`: Context to evaluate
</Callout>

<video url="https://drive.google.com/file/d/1toRqrwyKNB8Kpw4_MRlpojKTNdzBHzS1/preview" />

## Evaluator Reasoning

To help you analyze why certain cases perform well or underperform, we provide clear reasoning for each evaluator score. This can be viewed for each entry within the evaluation tab on its details sheet.

## Multimodal Datasets

Datasets in Maxim are multimodal and can be created directly on the platform or uploaded as existing CSV files.

In Maxim, datasets can have columns of the following types (entities):

* **Input (Compulsory entity):** A column associated with this type is treated as an input query to test your application.
* **Expected Output:** Data representing the desired response that the application should generate for the corresponding input.
* **Output:** This is for cases where you have run your queries elsewhere and have the outputs within your CSV that you want to evaluate directly.
* **Image:** You can upload images or provide an image URL.
* **Variables:** Any data that you want to dynamically change in prompts/workflows during runtime.
* **Expected Tool Calls:** Prompt tools expected to be triggered for the corresponding input.

Learn about creating multimodal datasets [here](/docs/library/how-to/datasets/use-dataset-templates).

## Data Curation

We understand that having high-quality datasets from the start is challenging and that datasets need to evolve continuously. Maxim’s platform ensures data curation possibilities at every stage of the lifecycle, allowing datasets to improve constantly.

Learn more about data curation [here](/docs/library/how-to/datasets/curate-data-from-production).

## Splits

Dataset splits allow you to create subsets of a larger dataset for different use cases or logical groupings. Creating a split helps manage datasets more easily and facilitates testing of specific prompts or workflows. You can create splits within a dataset, add elements to them, and view them side by side under different tabs. Learn more about dataset splits [here](/docs/library/how-to/datasets/use-splits-within-a-dataset).

## Prompt partials

Prompt partials act as reusable snippets that you can include across different Prompts. They help eliminate repetitive content by maintaining common prompt elements in a single place. Learn more about prompt partials [here](/docs/library/how-to/prompt-partials/create-prompt-partial).


url: https://getmaxim.ai/docs/library/overview
meta: {
  "title": "Overview",
  "index": 2
}

At Maxim, we have all the supporting components to aid you in your journey to ship high-quality AI reliably and with confidence. While Pre-release Tests and Post-release Observation are our hero flows for ensuring a smooth testing experience, we have added several crucial pieces under **Library** in Maxim to assist you with testing.

![Library](/images/docs/library/overview/library.png)

## Evaluators

You will get access to all the evaluators under the **Evaluators** tab in the left-side menu. You will also have access to the **Evaluator Store**, where you can browse evaluators and add them to your workspace. Read more about evaluators [here](/docs/library/how-to/evaluators/use-pre-built-evaluators).

## Datasets

You can create robust multimodal datasets on Maxim, which you can then use for your testing workflow. Read more about datasets [here](/docs/library/how-to/datasets/use-dataset-templates).

## Context Sources

To test your RAG pipeline, it’s very important to evaluate the retrieved context along with the final generated output. We allow you to bring your retrieved context using an HTTP workflow. Read more about context sources [here](/docs/library/how-to/context-sources/ingest-files-as-a-context-source).

## Prompt Tools

Being able to attach function calling to prompts ensures you can test your actual application flow, mimicking agentic workflows in your real application. Read more about prompt tools [here](/docs/library/how-to/prompt-tools/create-a-code-tool).

## Custom Models

At Maxim, you can create and update datasets, which continue evolving as you navigate through the application lifecycle. We allow you to use these datasets for your fine-tuning needs by partnering with fine-tuning providers. If you have such a need, please feel free to drop us a line at [contact@getmaxim.ai](mailto:contact@getmaxim.ai).


url: https://getmaxim.ai/docs/observe/concepts
meta: {
  "title": "Concepts",
  "description": "Learn about the key concepts of Maxim's AI Observability."
}

Maxim's observability platform builds upon established distributed tracing principles while extending them for GenAI-specific monitoring. Our architecture leverages proven concepts and enhances them with specialized components for AI applications.

Our observability features provide a comprehensive view of your AI applications through monitoring, troubleshooting, and alerting capabilities.

## Log Repository

The most central component of Maxim's observability platform is the Log Repository, as it is where logs are ingested. It allows for ease of searching and analyzing.

<Callout title="How should you be structuring log repositories?">
  Split your logs across multiple repositories in your workspace based on your needs. For example, you might have **one repository for production logs** and **another for development logs**; or you could have **a single repository for all logs**, differentiated by tags indicating the environment.

  We recommend implementing **separate log repositories for separate applications**, but also separate log repositories for separate services that managed by distinct teams. Trying to keep logs that are related to different applications or services in the same repository can lead to difficulties in analyzing and troubleshooting.
</Callout>

A Log Repository contains three main components:

### Overview Tab

The Overview tab provides a comprehensive snapshot of activity within your Log Repository for your specified timeframe. The metrics that are displayed in **the overview** include:

* Total traces
* Total usage
* Average user feedback
* Traces over time (graph)
* Total usage over time (graph)
* Average user feedback over time (graph)
* Latency over time (graph)
* Error rate (graph)

!\[Screenshot of overview metrics]

There is also an overview for **Evaluation results**, which includes:

* No of traces evaluated
* Evaluation summary
* Mean score over time (graph)
* Pass rate over time (graph)

!\[Screenshot of evaluation results overview metrics]

<Callout>
  We talk more about evaluation logs in [How to evaluate logs](/docs/observe/how-to/evaluate-logs/auto-evaluation) section of the documentation.
</Callout>

### Logs Tab

Logs Tab is where you'll find all the ingested logs in a tabular format, it is a detailed view where each log is displayed separately with it's own metrics.

!\[Screenshot of the logs tab]

For each log entry we display the following in the table:

| Field         | Description                                        |
| ------------- | -------------------------------------------------- |
| Timestamp     | The start time of the log                          |
| Input         | The user's prompt or query                         |
| Output        | The final response                                 |
| Model         | The AI models seen within the trace (e.g., gpt-4o) |
| Tokens        | Token count for the whole trace                    |
| Cost          | The cost for the whole trace in USD                |
| Latency       | Response time in milliseconds                      |
| User feedback | User feedback for a trace (if available)           |
| Tags          | Tags associated with the log entry                 |

Apart from the above fields, there are Evaluation score fields per evaluator too, these display any evaluation done on the trace and what score was obtained for that evaluation.

Each trace can be clicked to see an expanded view of the trace in a sheet. It displays detailed information on each component of the trace and also the evaluations done on each component.

<Callout>
  Learn more about logging in the [How to log your application](/docs/observe/how-to/log-your-application/setting-up-trace) section of the documentation.
</Callout>

### Alerts Tab

Once you start logging, you can set up alerts to monitor specific metrics. Alerts are triggered when the metric exceeds a certain threshold, and they can be configured to notify you via slack, pagerduty or opsgenie.

!\[Screenshot of the alerts tab]

<Callout>
  You can learn more on configuring alerts in the [How to set up alerts](/docs/observe/how-to/set-up-alerts/create-a-slack-integration) section of the documentation.
</Callout>

## Components of a log

Now that you know how to navigate through a Log Repository, let us discuss what each log contains and what each component of the log and their properties represent.

### Session

Session is a top level entity that captures all the multi-turn interactions of your system. For example, if you are building a chatbot, a session in Maxim logger is an entire conversation of a user with your chatbot.

Sessions persist across multiple interactions and remain active until explicitly closed - you can keep on adding different traces to it over the course of time unless you want to explicitly close the session.

### Trace

![trace](/images/docs/observe/trace.png)

In distributed tracing, a trace is the complete processing of a request through a distributed system, including all the actions between the request and the response.

| Property          | Description                                                                                                                                                                                                                                 |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| id                | Unique identifier for the trace; this usually can be your request id.                                                                                                                                                                       |
| name (optional)   | Name of the trace; you can keep it same as your API call. i.e. chatQuery                                                                                                                                                                    |
| tags (optional)   | Key-value pairs which you can use for filtering on the dashboard. <br /><br />There is no limit on the number of tags or the size of the string, although lower amounts are better and faster for search performance specific to your repo. |
| input (optional)  | The user's prompt or query.                                                                                                                                                                                                                 |
| output (optional) | This is the final response your API sends back.                                                                                                                                                                                             |

### Span

![span](/images/docs/observe/span.png)

Spans are fundamental building blocks of distributed tracing. A single trace in distributed tracing consists of a series of tagged time intervals known as spans. Spans represent a logical unit of work in completing a user request or transaction.

<Callout type="info" title="Sub-spans">
  A span can have other spans as children. You can create as many subspans as you want to logically group flows within a span.
</Callout>

| Property | Description                                                                                                                                                                    |
| -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| id       | Unique identifier for the span; it can be uuid() for each new trace. This has to be unique across all elements in a trace. If you these are duplicated, data gets overwritten. |
| name     | Name of the span; you can keep it same as your API call. i.e. chatQuery                                                                                                        |
| tags     | These are span specific tags.                                                                                                                                                  |
| spanId   | Parent span id; these are automatically generated when you call `span.addSpan()`.                                                                                              |

### Event

![event](/images/docs/observe/event.png)

Events mark significant points within a span or a trace recording instantaneous occurrences that provide additional context for understanding system behavior.

| Property | Description                                                                           |
| -------- | ------------------------------------------------------------------------------------- |
| id       | Unique identifier for the event; this has to be unique in a trace across all elements |
| name     | Name of the event. you can keep it same as your API call. i.e. chatQuery              |
| tags     | These are event specific tags.                                                        |

### Generation

A Generation represents a single Large Language Model (LLM) call within a trace or span. Multiple generations can exist within a single trace/span.

#### Structure

* Maxim SDK uses OpenAI's LLM call structure as the standard format.
* All incoming LLM calls are automatically converted to match OpenAI's structure via SDK.
* This standardization ensures consistent handling across different LLM providers.

| Property          | Description                                                                                                       |
| ----------------- | ----------------------------------------------------------------------------------------------------------------- |
| id                | Unique identifier for the generation; this has to be unique in a trace.                                           |
| name              | Name of the generation; it can be specific to your workflow intent detection or final summarization call.         |
| tags              | These are generation specific tags.                                                                               |
| messages          | The messages you are sending to LLM as input.                                                                     |
| model             | Model that is being used for this LLM call.                                                                       |
| model\_parameters | The model parameters that you are setting up. This is a key-value pair; you can pass any model parameter.         |
| error             | You can pass LLM error if it has occurred. You can filter all logs with LLM error on the dashboard using filters. |
| result            | Result object coming from the LLM.                                                                                |

### Retrieval

A Retrieval represents a query operation to fetch relevant context or information from a knowledge base or vector database within a trace or span. It is commonly used in RAG (Retrieval Augmented Generation) workflows, where context needs to be fetched before making LLM calls.

| Property | Description                                                                                               |
| -------- | --------------------------------------------------------------------------------------------------------- |
| id       | Unique identifier for the retrieval; this has to be unique in a trace.                                    |
| name     | Name of the retrieval; it can be specific to your workflow intent detection or final summarization call . |
| tags     | These are retrieval specific tags.                                                                        |
| input    | Input used to fetch relevant chunks from your knowledge base                                              |
| output   | Array of chunks returned by the knowledge base                                                            |

### Tool Call

Tool Call represents an external system or service call done based on an LLM response. Each tool call can be logged separately to track its input, output and latency.

| Property    | Description                                                                                                               |
| ----------- | ------------------------------------------------------------------------------------------------------------------------- |
| id          | Unique identifier for the tool call; this has to be unique in a trace (can be found in the tool call reponse of the LLM). |
| name        | Name of the tool call.                                                                                                    |
| description | Description of the tool call.                                                                                             |
| args        | Arguments passed to the tool call, these are tool specific arguments.                                                     |
| result      | Result returned by the tool call.                                                                                         |


url: https://getmaxim.ai/docs/observe/overview
meta: {
  "title": "Overview",
  "description": "Monitor AI applications in real-time with Maxim's enterprise-grade LLM observability platform."
}

## Improve your AI application outcomes

Build and monitor reliable AI applications for consistent results:

* Monitor AI model performance in production
* Detect and resolve issues proactively
* Improve response quality through data-driven insights
* Reduce costs with optimized resource usage

## Understanding LLM observability challenges

Why traditional monitoring fails LLM applications:

* Cannot track prompt and completion correlations
* Cannot monitor critical LLM metrics (token usage, model parameters, response quality)
* Struggles to process structured and unstructured data effectively
* Cannot trace reasoning or debug black-box LLM failures
* Fail to track complex workflows with RAG, tools, and multi-step reasoning
* Provide limited support for human feedback and preference models
* Lack subjective metric tracking for user ratings and A/B tests

## Maxim's solution

![Maxim platform architecture overview](/images/docs/observe/overview/observe.png)

Maxim platform leverages three core architectural principles:

### 1. Comprehensive distributed tracing

Track the complete request lifecycle, including LLM requests and responses. Debug precisely with end-to-end application flow visibility.

### 2. Zero-state SDK architecture

Maintain robust observability across functions, classes, and microservices without state management complexity.

### 3. Open source compatibility

Maxim logging is inspired by (and highly compatible with) open telemetry:

* Generate idempotent commit logs for every function call
* Support high concurrency and network instability
* Maintain accurate trace timelines regardless of log arrival order
* Production-proven reliability with over one billion indexed logs

## Key Features

### Real-time monitoring and alerting

Track GenAI metrics through distributed tracing and receive instant alerts via:

* Slack
* PagerDuty
* OpsGenie

Monitor critical thresholds for:

* Cost per trace
* Token usage
* User feedback patterns

<video url="https://drive.google.com/file/d/1Rqqt9WdUBgM8OtsX9ciKEOSOQpKT72CM/preview" />

### Saved views

Find common search patterns instantly:

* Store common search patterns
* Create debugging shortcuts
* Speed up issue resolution

<video url="https://drive.google.com/file/d/1HMc0wNdZdgz2SvZbYgfeD3CAeFLyDH1R/preview" />

### Online evaluation

Monitor application performance with:

* Custom filters and rules
* Automated reports
* Threshold-based alerts

<video url="https://drive.google.com/file/d/1e0zAhfpwUpbdKv7xkAjb-42xGVY3zUJr/preview" />

### Data curation

Transform logs into valuable datasets:

* Create datasets with one click
* Filter incoming logs
* Build targeted training data
* Update datasets for prompt improvements

<video url="https://drive.google.com/file/d/1r3eO0f0yuzZ3rPZUUw9k0HGfpCVePiNH/preview" />


url: https://getmaxim.ai/docs/observe/quickstart
meta: {
  "title": "Quickstart",
  "description": "Set up distributed tracing for your GenAI applications to monitor performance and debug issues across services."
}

This guide demonstrates distributed tracing setup using an enterprise search chatbot (similar to Glean) example that:

* Connects to company data sources (Google Drive, Dropbox)
* Enables natural language search across data via Slack or web interface

## System architecture

The application uses 5 microservices:

![System architecture showing microservice components](/images/docs/observe/qs-system-arch.png)

1. **API Gateway**: Authenticates the users and routes API requests

2. **Planner**: Creates execution plans for queries

3. **Intent detector**: Analyzes query intent

4. **Answer generator**: Creates prompts based on planner instructions and RAG context

5. **RAG pipeline**: Retrieves relevant information from vector database

## Setting up the Maxim dashboard

### 1. Create Maxim repository

Create a new repository called "Chatbot production":

<video url="https://drive.google.com/file/d/1HtbKsAiNaNUafEfSPONar8PeacAGTr2F/preview" />

### 2. Generate API key

<Steps>
  <Step>
    Navigate to Settings → API Keys
  </Step>

  <Step>
    Generate and save new API key
  </Step>
</Steps>

<video url="https://drive.google.com/file/d/12wOcrVxptlpCn2qIaBNTXPS1LxgfDSWL/preview" />

### 3. Install SDK

```package-install
npm install @maximai/maxim-js
```

```bash title="Python"
pip install maxim-py
```

```bash title="Go"
go get github.com/maximhq/maxim-go
```

```groovy title="Java"
compileOnly("ai.getmaxim:sdk:0.1.3")
```

### 4. Initialize logger

Add this code to initialize the logger in each service:

<Tabs groupId="language" items={["JS/TS", "Python", "Go","Java"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  const maxim = new Maxim({ apiKey: "api-key" });
  const logger = await maxim.logger({ id: "log-repository-id" });
  ```

  ```python tab="Python"
  from maxim.maxim import Config, Maxim
  from maxim.logger.logger import LoggerConfig

  maxim = Maxim(Config(apiKey=apiKey))

  logger = maxim.logger(LoggerConfig(id="log-repository-id"))
  ```

  ```go tab="Go"
  import "github.com/maximhq/maxim-go"
  import "github.com/maximhq/maxim-go/logging"

  m := maxim.Init(&maxim.MaximSDKConfig{
      ApiKey: "api-key"
  })

  logger, err := m.GetLogger(&logging.LoggerConfig{Id: "log-repository-id"})
  ```

  ```java tab="Java"
  import ai.getmaxim.sdk.Maxim;

  Maxim maxim = new Maxim(new Config(null, "api-key", null, false));

  Logger logger = maxim.logger(new LoggerConfig("log-repository-id"));
  ```
</Tabs>

### 5. Create trace in API gateway

Use `cf-request-id` as trace identifier:

<Tabs groupId="language" items={["JS/TS", "Python","Go","Java"]}>
  ```typescript tab="JS/TS"
  const trace = logger.trace({
      id: req.headers["cf-request-id"],
      name: "user-query",
      tags: {
          userId: req.body.userId,
          accountId: req.body.accountId
      },
  });

  trace.input("Hello, how are you?");
  trace.output("I'm fine, thank you!");

  trace.end();
  ```

  ```python tab="Python"
  from maxim.logger.components.trace import TraceConfig

  trace = logger.trace(TraceConfig(
      id=request.headers.get("cf-request-id"),
      name="user-query",
      tags={
          "userId": request.json.get("userId"),
          "accountId": request.json.get("accountId")
      }
  ))

  trace.set_input("Hello, how are you?")
  trace.set_output("I'm fine, thank you!")

  trace.end()
  ```

  ```go tab="Go"
  trace := logger.Trace(&logging.TraceConfig{
      Id: r.Header.Get("Cf-Request-Id"),
      Name: "user-query",
      Tags: map[string]string{
          "userId": r.Body.UserId,
          "accountId": r.Body.AccountId,
      },
  })

  trace.SetInput("Hello, how are you?")
  trace.SetOutput("I'm fine, thank you!")

  trace.End()
  ```

  ```java tab="Java"
  Trace trace = logger.trace(new TraceConfig(
      req.getHeader("cf-request-id"),
      "user-query",
      Map.of("userId", req.getBody().getUserId(), "accountId", req.getBody().getAccountId())
  ));

  trace.setInput("Hello, how are you?");
  trace.setOutput("I'm fine, thank you!");

  trace.end();
  ```
</Tabs>

You can get a hold of a trace in two ways:

<Tabs groupId="language" items={["JS/TS", "Python","Go","Java"]}>
  ```typescript tab="JS/TS"
  // Method 1: Using logger and trace ID
  logger.traceTag("trace-id", "newTag", "newValue");
  logger.traceEnd("trace-id");

  // Method 2: Using trace object
  const trace = logger.trace({ id: "trace-id" });
  trace.addTag("newTag", "newValue");
  trace.end();
  ```

  ```python tab="Python"
  from maxim.logger.components.trace import TraceConfig

  # Method 1: Using logger and trace ID
  trace = logger.trace_add_tag("trace-id", "newTag", "newValue")
  logger.trace_end("trace-id")

  # Method 2: Using trace object
  trace = logger.trace(TraceConfig(id="trace-id"))
  trace.add_tag("newTag", "newValue")
  trace.end()
  ```

  ```go tab="Go"
  // Method 1: Using logger and trace ID
  logger.AddTagToTrace("trace-id", "newTag", "newValue")
  logger.EndTrace("trace-id")

  // Method 2: Using trace object
  trace := logger.Trace(&logging.TraceConfig{
      Id: "trace-id",
  })
  trace.AddTag("newTag", "newValue")
  trace.End()
  ```

  ```java tab="Java"
  // Method 1: Using logger and trace ID
  logger.traceAddTag("trace-id", "newTag", "newValue");
  logger.traceEnd("trace-id");

  // Method 2: Using trace object
  Trace trace = logger.trace(new TraceConfig("trace-id"));
  trace.addTag("newTag", "newValue");
  trace.end();
  ```
</Tabs>

<Callout>
  You can manipulate every entity of Maxim observability framework (Span, Generation, Retrieval, Event) in the same way.
</Callout>

### 6. Add spans in services

Create spans to track operations in each service:

<Tabs groupId="language" items={["JS/TS", "Python","Go","Java"]}>
  ```typescript tab="JS/TS"
  // Getting hold of trace using request id / trace id
  const trace = logger.trace({id: req.headers["cf-request-id"]});
  // Creating a new span
  const span = trace.span({
      id: uuid(),
      name: "plan-query",
      tags: {
          userId: req.body.userId,
          accountId: req.body.accountId
      },
  });
  ```

  ```python tab="Python"
  from maxim.logger.components.span import SpanConfig

  trace = logger.trace({"id": request.headers.get("cf-request-id")})
  span = trace.span(SpanConfig(
      id=uuid4(),
      name="plan-query",
      tags={
          "userId": request.json.get("userId"),
          "accountId": request.json.get("accountId")
      }
  ))
  ```

  ```go tab="Go"
  trace := logger.Trace(&logging.TraceConfig{
      Id: r.Header.Get("cf-Request-Id"),
  })
  span := trace.Span(&logging.SpanConfig{
      Id: r.Header.Get("cf-Request-Id"),
      Name: "plan-query",
      Tags: map[string]string{
          "userId": r.Body.UserId,
          "accountId": r.Body.AccountId,
      },
  })
  ```

  ```java tab="Java"
  Trace trace = logger.trace(new TraceConfig(
      req.getHeader("cf-request-id"),
  ));
  Span span = trace.span(new SpanConfig(
      req.getHeader("cf-request-id"),
      "plan-query",
      Map.of("userId", req.getBody().getUserId(), "accountId", req.getBody().getAccountId())
  ));
  ```
</Tabs>

<Callout>
  When creating spans, consider adding relevant tags that provide context about the operation being performed. These tags help in filtering and analyzing traces later. Remember to end each span once its operation completes to ensure accurate timing measurements.
</Callout>

### 7. Log LLM calls

<Steps>
  <Step>
    Track LLM interactions using generations:

    <Tabs groupId="language" items={["JS/TS", "Python","Go","Java"]}>
      ```typescript tab="JS/TS"
      // Creating a new generation
      const generation = span.generation({
          id: uuid(),
          name: "plan-query",
          provider: "openai",
          model: "gpt-3.5-turbo-16k",
          modelParameters: { temperature: 0.7 },
          tags: {
              userId: req.body.userId,
              accountId: req.body.accountId
          },
      });
      ```

      ```python tab="Python"
      from maxim.logger.components.generation import GenerationConfig

      generation = span.generation(GenerationConfig(
          id=uuid4(),
          name="plan-query",
          provider="openai",
          model="gpt-3.5-turbo-16k",
          messages: [{ role: "system", content: "SYSTEM PROMPT" }],
          model_parameters={"temperature": 0.7},
      ))
      ```

      ```go tab="Go"
      generation := span.Generation(&logging.GenerationConfig{
          Id: r.Header.Get("cf-Request-Id"),
          Name: "plan-query",
          Provider: "openai",
          Model: "gpt-3.5-turbo-16k",
          Messages: []logging.CompletionRequest {
              {
                  Role:    "system",
                  Content: "SYSTEM PROMPT",
              },
          },
          ModelParameters: map[string]interface{}{"temperature": 0.7},
      })
      ```

      ```java tab="Java"
      Generation generation = span.generation(new GenerationConfig(
          req.getHeader("cf-request-id"),
          "plan-query",
          "openai",
          "gpt-3.5-turbo-16k",
          Arrays.asList(new Message("system", "SYSTEM PROMPT")), // messages (optional)
          Map.of("temperature", 0.7),
      ));
      ```
    </Tabs>
  </Step>

  <Step>
    Log LLM responses:

    <Tabs groupId="language" items={["JS/TS", "Python","Go","Java"]}>
      ```typescript tab="JS/TS"
      generation.result({
          id: uuid(),
          object: "chat.completion",
          created: Date.now(),
          model: "gpt-3.5-turbo-16k",
          choices: [{
              index: 0,
              message: {
                  role: "assistant",
                  content: "response"
              },
              finish_reason: "stop"
          }],
          usage: {
              prompt_tokens: 100,
              completion_tokens: 50,
              total_tokens: 150
          }
      });
      ```

      ```python tab="Python"
      generation.result({
          "id": str(uuid4()),
          "object": "chat.completion",
          "created": int(time.time()),
          "model": "gpt-3.5-turbo-16k",
          "choices": [{
              "index": 0,
              "message": {
                  "role": "assistant",
                  "content": "response"
              },
              "finish_reason": "stop"
          }],
          "usage": {
              "prompt_tokens": 100,
              "completion_tokens": 50,
              "total_tokens": 150
          }
      })
      ```

      ```go tab="Go"
      generation.Result(&logging.GenerationResult{
          Id: r.Header.Get("cf-Request-Id"),
          Object: "chat.completion",
          Created: time.Now().Unix(),
          Model: "gpt-3.5-turbo-16k",
          Choices: []logging.Choice{
              {
                  Index: 0,
                  Message: &logging.Message{
                      Role: "assistant",
                      Content: "response",
                  },
                  FinishReason: "stop",
              },
          },
          Usage: &logging.Usage{
              PromptTokens: 100,
              CompletionTokens: 50,
              TotalTokens: 150,
          },
      })
      ```

      ```java tab="Java"
      generation.result(new GenerationResult(
          req.getHeader("cf-request-id"),
          "chat.completion",
          System.currentTimeMillis() / 1000L,
          "gpt-3.5-turbo-16k",
          Arrays.asList(new Choice(
              0,
              new Message("assistant", "response"),
              "stop"
          )),
          new Usage(100, 50, 150)
      ));
      ```
    </Tabs>
  </Step>
</Steps>

<Callout>
  Maxim currently supports OpenAI message format. to convert other messaging formats to OpenAI format in the SDK.
</Callout>

## View traces

Access your traces in the Maxim dashboard within seconds of logging. The dashboard shows:

* Complete request lifecycle
* Durations and relationships of the Entities (Span/Trace)
* LLM generation details
* Performance metrics


url: https://getmaxim.ai/docs/sdk/overview
meta: {
  "title": "Introduction",
  "slug": "sdk",
  "index": 2,
  "group": "SDK",
  "description": "Dive into the Maxim SDK to supercharge your AI application development"
}

import { ChevronRight } from "lucide-react";

Maxim is a comprehensive platform designed to streamline AI application evaluation and observability. It offers a suite of tools and services that help developers and teams apply traditional software best practices to non-deterministic AI workflows.

Maxim SDK exposes Maxim's most critical functionalities behind a simple set of function calls, allowing developers to integrate Maxim workflows into their own workflows seamlessly.

## Language and framework support

| Language             | SDK Link                                                              | Package installer command                 |
| -------------------- | --------------------------------------------------------------------- | ----------------------------------------- |
| Python               | [PyPI](https://pypi.org/project/maxim-py/)                            | `pip install maxim-py`                    |
| JavaScript           | [npm](https://www.npmjs.com/package/@maximai/maxim-js)                | `npm install @maximai/maxim-js`           |
| Javascript Langchain | [npm](https://www.npmjs.com/package/@maximai/maxim-js-langchain)      | `npm install @maximai/maxim-js-langchain` |
| Go                   | [Go SDK](https://github.com/maximhq/maxim-go)                         | `go get github.com/maximhq/maxim-go`      |
| Java/JVM             | [Java/JVM SDK](https://central.sonatype.com/artifact/ai.getmaxim/sdk) | `implementation("ai.getmaxim:sdk:0.1.3")` |

## Initializing SDK

<Tabs id="language" items={["JS/TS", "Python","Go", "Java/Kotlin"]} persist>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js"

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "" });
  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  # [!code word:api_key]
  maxim = Maxim(Config(api_key=""))
  ```

  ```go tab="Go"
  import "github.com/maximhq/maxim-go"

  // [!code word:ApiKey]
  mx := maxim.Init(&maxim.MaximSDKConfig{ApiKey: ""})
  ```

  ```kotlin tab="Java/Kotlin"
  import ai.getmaxim.sdk.Maxim;
  import ai.getmaxim.sdk.Config;

  // [!code word:apiKey]
  var maxim = Maxim(Config(apiKey = ""))
  ```
</Tabs>

## Whats next?

<Cards className="gap-4 sm:grid-cols-2 grid grid-cols-3 pt-2">
  <Card
    className="flex flex-col no-underline"
    href="/docs/evaluate/how-to/evaluate-prompts/deploy-prompts"
    title={
  		<div className="flex flex-row items-center">
  			<h3 className="font-medium">Prompt management using Maxim</h3>
  			<ChevronRight className="ml-auto shrink-0"/>
  		</div>
  	}
  />

  <Card
    className="flex flex-col no-underline"
    href="/docs/library/how-to/datasets/add-new-entries-using-sdk"
    title={
  		<div className="flex flex-row items-center">
  			<h3 className="font-medium">Dataset management using Maxim</h3>
  			<ChevronRight className="ml-auto shrink-0"/>
  		</div>
  	}
  />

  <Card
    className="flex flex-col items-center justify-center no-underline"
    href="/docs/observe/overview"
    title={
  		<div className="flex flex-row items-center">
  			<h3 className="font-medium">Observe and evaluate your production logs realtime using Maxim.</h3>
  			<ChevronRight className="ml-auto shrink-0"/>
  		</div>
  	}
  />

  <Card
    className="flex flex-col justify-center no-underline w-full"
    href="/docs/evaluate/how-to/trigger-test-runs-using-sdk"
    title={
  		<div className="flex flex-row items-center w-full">
  			<h3 className="font-medium">Run tests using Maxim SDK.</h3>
  			<ChevronRight className="ml-auto shrink-0"/>
  		</div>
  	}
  />
</Cards>


url: https://getmaxim.ai/docs/sdk/upgrading-to-v3
meta: {
  "title": "Upgrading to v3",
  "description": "Changes in the Maxim SDK"
}

## Maxim SDK Initialization changes

* `apiKey` is now `api_key` in `Config`
* `baseUrl` is now `base_url` in `Config`

```python title="apiKey is now api_key in Config"
from maxim import Maxim,Config

maxim = Maxim(Config(api_key="maxim-api-key"))
```

## Import changes

We have pulled out most commonly used imports at the relevant places.

1. `from maxim.logger import Logger, LoggerConfig` instead of `from maxim.logger.logger import Logger, LoggerConfig`
2. `from maxim import Maxim, Config` instead of `from maxim.maxim import Maxim, Config`
3. `from maxim.logger import Trace, TraceConfig` instead of `from maxim.logger.trace import Trace, TraceConfig`

<Callout>
  Old imports will still work.
</Callout>

## Prompt management changes

1. `getPrompt` is now `get_prompt`
2. `getPromptChain` is now `get_prompt_chain`
3. `getPrompts` is now `get_prompts`
4. `getPromptChains` is now `get_prompt_chains`
5. `getFolder` is now `get_folder`
6. `getFolders` is now `get_folders`


url: https://getmaxim.ai/docs/self-hosting/dataplane
meta: {
  "title": "Data plane deployment",
  "description": "This guide details Maxim's data plane deployment process, outlining how to establish data processing infrastructure within your cloud environment. It emphasizes enhanced security, control, and data tenancy, ensuring compliance with data residency requirements while leveraging cloud-based services."
}

Data Plane Deployment is designed for organizations seeking a balance between enhanced security and the benefits of cloud-based services. This deployment option ensures that your sensitive data processing occurs within your own infrastructure while still allowing secure connections to Maxim's cloud-hosted application services. It's particularly beneficial for companies subject to data residency requirements, such as those mandated by GDPR, as it allows for precise control over data location and processing, helping to maintain compliance with regional data protection regulations.

With Data Plane Deployment:

* Your data remains within your controlled environment
* Only the data processing infrastructure is deployed in your VPC
* Secure VPC peering enables connection to Maxim's cloud-hosted application services
* You maintain control over data locality and processing
* You benefit from Maxim's managed application services and updates

This approach offers a hybrid solution, combining the security of on-premises data handling with the scalability and features of cloud-based applications.

## Setup requirements

* ✅ Google Cloud Project or AWS sub-account.
* ✅ Credentials attached to the [eng@getmaxim.ai](mailto:eng@getmaxim.ai) email address.
* ✅ Admin access to the Google Cloud Project or AWS sub-account.

## Deployment process

* We deploy the data plane in a VPC.
* We use secure VPC peering to connect to Maxim's cloud-hosted application plane.

## Release cadence

* All application plane updates are available at the same time as cloud offering updates.

## Security measures

* All service account keys will be rotated at least every 90 days.
* Access to the shared Google Cloud Project or AWS sub-account will be limited to the [eng@getmaxim.ai](mailto:eng@getmaxim.ai) email address.
* 2FA will be required for accessing the shared Google Cloud Project or AWS sub-account.

## SLAs

* 99.9% uptime.
* \< 5 mins response time (acknowledgment) in case of incidents.
* \< 48 hrs resolution time.

## Support

* We provide 24/7 support for any issues that may occur during the deployment process.
* We also offer 24/7 support for any issues that may arise during the operation of the service.
* We assign a dedicated support engineer to each account to address any issues that may occur during the deployment process and operation.


url: https://getmaxim.ai/docs/self-hosting/overview
meta: {
  "title": "Overview",
  "description": "Maxim offers self hosting and flexible enterprise deployment options with either full VPC isolation (Zero Touch) or hybrid setup with secure VPC peering (Data Plane), tailored to your security needs."
}

## Zero Touch Deployment

We set up both the data plane and control plane directly in your VPC.
This ensures that your data stays completely within your infrastructure, with no data exchange between your VPC and our cloud services.

## Data Plane Deployment

We deploy only the data plane in your VPC, which connects to our cloud-hosted application plane through secure VPC peering.
Each deployment type is designed to meet different security and integration needs. Let's explore the details of each option.

<div className="flex flex-col bg-background-highlight-primary rounded-md">
  <div className="px-4 text-md border-b border-border-strong rounded-tl-md rounded-tr-md">
    Maxim is designed for companies with a security mindset.
  </div>

  <div className="flex flex-row gap-4 w-full px-4">
    <img src="https://cdn.prod.website-files.com/665ab0daac869acad030a349/66fe99bae027e906828812ed_21972-312_SOC_NonCPA.png" width={92} height={92} />

    <img src="https://cdn.prod.website-files.com/665ab0daac869acad030a349/66fe9aa86579ed03ca44fde2_PNG_GDPR-e1672263252689-p-500.png" width={92} height={92} />

    <img src="https://cdn.prod.website-files.com/665ab0daac869acad030a349/6703769213e45e2379621c47_ISO%2027001.png" width={92} height={92} />

    <img src="https://cdn.prod.website-files.com/665ab0daac869acad030a349/674059445b7e5f0567d4aa54_image%20(15).png" width={92} height={92} />
  </div>
</div>

## Maxim infrastructure

To better understand these deployment options, let's examine the key components of our infrastructure:

![infrastructure](/images/docs/invpc/architecture.png)

### Control plane

The control plane encompasses all applications that handle the business logic and user experience. Web service and serverless functions are exposed to internet via a load balancer.

#### Components

| Component            | Description                                      | Language   |
| -------------------- | ------------------------------------------------ | ---------- |
| Web service          | Dashboard + APIs                                 | TypeScript |
| AI-Stack             | Proprietary evaluators                           | Python     |
| Workers              | Real-time log processing, evaluation, and alerts | Go         |
| Serverless functions | SDK APIs                                         | Go         |

### Data plane

The data plane encompasses all components that handle data at rest and in transit. We utilize secure VPC peering (where required) to connect to control plane.

#### Components

| Component            | Description                            |
| -------------------- | -------------------------------------- |
| Dragonfly            | In-memory key-value store              |
| Kafka                | Queue for real-time log processing     |
| BigQuery/Redshift    | Data lake                              |
| Clickhouse           | OLAP database for logs and evaluations |
| MySQL                | OLTP database                          |
| Firestore/DocumentDB | Used as a vector database              |

## Pillars of Maxim's Infrastructure

### Infra as code

* We deploy our infrastructure using Terraform.
* All the images are securely hosted and fingerprinted by customer-specific keys for every version.
* Our infra as code is available for review by the customer.

### Cloud provider support

* We manage the orchestration of the infrastructure using Kubernetes.
* We support GCP and AWS cloud providers.

### Security measures

* We attach a cloud-native security layer to ingress (Cloud Armor on GCP, AWS WAF, etc.).
* We have pre-defined configurations vetted by our security team for every cloud provider.
* The k8s cluster is deployed in a separate VPC.
* We use cloud-native MySQL for storage with encryption at rest.
* All outgoing traffic is routed through a cloud-native internet gateway.


url: https://getmaxim.ai/docs/self-hosting/zerotouch
meta: {
  "title": "Zero Touch Deployment",
  "description": "This guide outlines Maxim's zero-touch deployment process, covering infrastructure components, security protocols, and supported cloud providers."
}

Zero Touch Deployment is designed for organizations that require the highest level of security and privacy. This deployment option ensures that your data remains within your infrastructure, with no data exchange occurring with Maxim's cloud services.

## Setup Requirements

* ✅ Google Cloud Project or AWS sub-account
* ✅ Credentials attached to [eng@getmaxim.ai](mailto:eng@getmaxim.ai) email address
* ✅ Admin access to the Google Cloud Project or AWS sub-account
* ✅ Domain name/subdomain for generating SSL certificates and serving Maxim app
* ✅ If the service is going to be public, we will configure a Cloudflare Turnstile key for the subdomain used in the Maxim INVPC deployment.

## Infrastructure requirements

1. k8s
   1.1 GCP - Google Kubernetes Engine (GKE)
   1.2 AWS - Elastic Kubernetes Service (EKS)
   1.3 Azure - Azure Kubernetes Service (AKS)
2. 3 VMs, min: 2vCPU + 8 Gi RAM
3. 1 L7 load balancer
4. 1 Bucket as CDN backend
5. 1/2 Static IPs for egress
6. NAT gateway for internet access
7. Security (Optional)
   1.1. GCP - Cloud armor for DDoS protection and WAF capabilities
   1.2. AWS - AWS Shield and AWS WAF for threat protection
   1.3. Azure - Azure DDoS Protection and Azure Web Application Firewall
8. Data lake
   1.1 GCP - BigQuery
   1.2 AWS - Amazon Redshift
   1.3 Azure - Azure Synapse Analytics
9. Document DB
   1.1 GCP - Firestore
   1.2 AWS - Amazon DocumentDB
   1.3 Azure - Azure Cosmos DB
10. MySQL
    1.1. GCP - Cloud SQL for MySQL
    1.2. AWS - Amazon RDS for MySQL
    1.3. Azure - Azure Database for MySQL
11. Cloud native logging and error tracking

## Services we deploy

1. Dashboard: NextJS service (3 instances)
2. AIStack: Python(FastAPI) service (2 instances)
3. Workers: Go(workers) (3 instances)
4. Kafka cluster: 3 nodes
   4.1. Kafka UI: 1 node
   4.2. Kafka exporter (Observability)
5. Clickhouse cluster: 3 nodes
6. Redis sentinel: 1 master, 3 read-replica, 3 sentinel
7. k8s-prometheus (Observability)

## Deployment Process

* We deploy the data plane and application plane in the same VPC.
* We create a cloud provider-specific Docker image repository for storing all images.
* We use Tailscale for secure communication between the central CD pipeline and the application plane.

## Release Cadence

* We release new versions every week, combining all fixes and features released in the previous week's cloud offering.
* For security vulnerabilities, we release a patch within 24 hours.
* For critical vulnerabilities, we release a patch within 1 hour.

## Observability

* We use a shared Sentry instance to track errors and exceptions.
* We use a shared Prometheus + Grafana instance to track metrics.
* Customers receive access to Sentry projects and Grafana dashboards for audit and monitoring purposes.

## Security Measures

* All service account keys are rotated at least every 90 days.
* Access to the shared Google Cloud Project or AWS sub-account is limited to the [eng@getmaxim.ai](mailto:eng@getmaxim.ai) email address.
* 2FA is required for accessing the shared Google Cloud Project or AWS sub-account.
* We enable cloud provider-specific security features and share the audit every 60 days (example dashboard).

![security2](/images/docs/invpc/security-report-image-2.png)

## SLAs

* 99.9% uptime.
* \< 5 minutes response time (acknowledgment) for incidents.
* \< 48 hours resolution time.

## Support

* We provide 24/7 support for any issues that may occur during the deployment process.
* We also offer 24/7 support for any issues that may arise during service operation.
* We assign a dedicated support engineer to each account to address any issues that may occur during the deployment process and operation.


url: https://getmaxim.ai/docs/support/raising-an-incident
meta: {
  "title": "Raising an incident",
  "description": "Learn how to report bugs, issues, or incidents on the Maxim platform. This guide walks you through the process of submitting a report, ensuring your concerns are addressed promptly and efficiently."
}

import { Bug, Settings, SquareTerminal } from "lucide-react";

At Maxim, we're committed to providing you with a seamless experience. If you encounter any issues while using our platform, we're here to help. This guide will walk you through the process of reporting a bug or raising an incident.

## How to report an issue

Reporting an issue on Maxim is a straightforward process. Follow these steps to ensure your concern reaches our support team:

<Steps>
  <Step>
    ### Navigate to settings

    Go to the `Settings` page by clicking on the settings button at the bottom of your sidebar.

    ![Screenshot of Maxim sidebar with settings highlighted](/images/docs/support/raising-an-incident/sidebar-highlighted-settings.png)
  </Step>

  <Step>
    ### Locate the 'Report a bug' tab

    Find the `Report a bug` tab at the bottom of the settings menu and click on it.

    ![Screenshot of settings menu with 'Report a bug' tab highlighted](/images/docs/support/raising-an-incident/report-a-bug-tab-in-settings.png)
  </Step>

  <Step>
    ### Fill out the report form

    Complete the form with the following details:

    #### Title

    Provide a concise title that summarizes the issue you're experiencing.

    #### Description (optional)

    While optional, we recommend including:

    * Steps to reproduce the issue
    * Expected behavior
    * Actual behavior
    * Any error messages

    #### Priority level

    Select the appropriate priority level for your issue:

    <Cards className="sm:grid-cols-2 grid grid-cols-4 gap-4">
      <Card icon={<Bug className="h-6 w-6 text-red-500" />} title="Highest" description="Critical bugs or crashes preventing you from using the platform" />

      <Card icon={<Bug className="h-6 w-6 text-orange-500" />} title="High" description="Urgent issues in a workflow that block completion of task" />

      <Card icon={<Bug className="h-6 w-6 text-yellow-500" />} title="Medium" description="Hindrance to your workflow with a sub-optimal workaround" />

      <Card icon={<Bug className="h-6 w-6 text-green-500" />} title="Low" description="Unsure if the flow is behaving as expected" />
    </Cards>

    ![Screenshot of the Report a bug form](/images/docs/support/raising-an-incident/report-a-bug-form.png)
  </Step>

  <Step>
    ### Submit your report

    Click the 'Submit' button to send your report to our support team.
  </Step>
</Steps>

## What happens next?

After submitting your report, our dedicated support team will review the information provided and prioritize your issue based on its severity. Rest assured, we take every report seriously and are committed to resolving your concerns as quickly as possible.

<Callout type="info" title="Stay updated">
  You'll receive updates on the progress of your reported issue through your registered email address or dedicated slack channel. If we need additional information, we'll reach out to you promptly.
</Callout>

We value your trust in Maxim and are committed to providing you with the best possible support. Your feedback helps us improve and enhance our platform, ensuring a better experience for all our users. Thank you for helping us make Maxim better!

For any other queries, you may also reach out to us at [contact@getmaxim.ai](mailto:contact@getmaxim.ai) or the dedicated slack channel.


url: https://getmaxim.ai/docs/analyze/how-to/comparison-reports
meta: {
  "title": "Generate and share comparison reports",
  "description": "Learn how to create and analyze comparison reports to track improvements, identify trends, and make data-driven decisions across different test runs."
}

## Create a comparison report

<Steps>
  <Step>
    Name your comparison report something descriptive (e.g., "Co-pilot Nov Updates Comparison")
  </Step>

  <Step>
    Pick the runs you want to compare by clicking the add button next to each one

    ![Add runs](/images/docs/analyze/how-to/comparison-reports/add-runs.png)
  </Step>

  <Step>
    You can set any run as your base run to compare others against

    ![Mark base run](/images/docs/analyze/how-to/comparison-reports/mark-base.png)
  </Step>

  <Step>
    Use the search bar and filters to find specific runs
  </Step>

  <Step>
    Click "Create dashboard" and you're all set
  </Step>
</Steps>

## Understand your comparison report

You'll see several key metrics and visualizations:

* Summary by Evaluator
* Cost by Prompt
* Token usage
* Latency metrics

If you've set a base run, you'll see how metrics change compared to that baseline.

![Evaluator summary differences](/images/docs/analyze/how-to/comparison-reports/eval-summary.png)

## Update your report

Hover over the report title and click "Edit" to add new runs, remove existing ones, or change your base run.

![Updating run](/images/docs/analyze/how-to/comparison-reports/edit-run.png)

## Share your report

Just click the "Share report" button at the top of the page to share with your team.


url: https://getmaxim.ai/docs/api/alerts/delete
meta: {
  "title": "Delete Alert",
  "description": "Delete an alert",
  "full": true,
  "_openapi": {
    "method": "DELETE",
    "route": "/v1/alerts",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Delete an alert"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/alerts-4dea720b.yaml"} operations={[{"path":"/v1/alerts","method":"delete"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/alerts/get
meta: {
  "title": "Get Alerts",
  "description": "Get alerts for a workspace",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/alerts",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get alerts for a workspace"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/alerts-4dea720b.yaml"} operations={[{"path":"/v1/alerts","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/alerts/post
meta: {
  "title": "Create Alert",
  "description": "Create a new alert",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/alerts",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Create a new alert"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/alerts-4dea720b.yaml"} operations={[{"path":"/v1/alerts","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/alerts/put
meta: {
  "title": "Update Alert",
  "description": "Update an alert",
  "full": true,
  "_openapi": {
    "method": "PUT",
    "route": "/v1/alerts",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Update an alert"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/alerts-4dea720b.yaml"} operations={[{"path":"/v1/alerts","method":"put"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/delete
meta: {
  "title": "Delete Dataset",
  "description": "Delete a dataset",
  "full": true,
  "_openapi": {
    "method": "DELETE",
    "route": "/v1/datasets",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Delete a dataset"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets","method":"delete"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/get
meta: {
  "title": "Get Datasets",
  "description": "Get datasets or a specific dataset",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/datasets",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get datasets or a specific dataset"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/post
meta: {
  "title": "Create Dataset",
  "description": "Create a new dataset",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/datasets",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Create a new dataset"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/put
meta: {
  "title": "Update Dataset",
  "description": "Update a dataset",
  "full": true,
  "_openapi": {
    "method": "PUT",
    "route": "/v1/datasets",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Update a dataset"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets","method":"put"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/evaluators/get
meta: {
  "title": "Get evaluators",
  "description": "Get an evaluator by ID, name or fetch all evaluators for a workspace",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/evaluators",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get an evaluator by ID, name or fetch all evaluators for a workspace"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/evaluators-79496bae.yaml"} operations={[{"path":"/v1/evaluators","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/folders/get
meta: {
  "title": "Get Folders",
  "description": "Get folder details. If id or name is provided, returns a single folder object. Otherwise, lists sub-folders under the parentFolderId (or root).",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/folders",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get folder details. If id or name is provided, returns a single folder object. Otherwise, lists sub-folders under the parentFolderId (or root)."
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/folders-3d5e102b.yaml"} operations={[{"path":"/v1/folders","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/folders/post
meta: {
  "title": "Create Folder",
  "description": "Create a new folder for organizing entities",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/folders",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Create a new folder for organizing entities"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/folders-3d5e102b.yaml"} operations={[{"path":"/v1/folders","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/integrations/delete
meta: {
  "title": "Delete Integration",
  "description": "Delete an integration",
  "full": true,
  "_openapi": {
    "method": "DELETE",
    "route": "/v1/integrations",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Delete an integration"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/integrations-1525d92b.yaml"} operations={[{"path":"/v1/integrations","method":"delete"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/integrations/get
meta: {
  "title": "Get Integrations",
  "description": "Get integrations for a workspace",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/integrations",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get integrations for a workspace"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/integrations-1525d92b.yaml"} operations={[{"path":"/v1/integrations","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/integrations/post
meta: {
  "title": "Create Integration",
  "description": "Create a new integration for notification channels",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/integrations",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Create a new integration for notification channels"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/integrations-1525d92b.yaml"} operations={[{"path":"/v1/integrations","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/integrations/put
meta: {
  "title": "Update Integration",
  "description": "Update an integration",
  "full": true,
  "_openapi": {
    "method": "PUT",
    "route": "/v1/integrations",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Update an integration"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/integrations-1525d92b.yaml"} operations={[{"path":"/v1/integrations","method":"put"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/log-repositories/delete
meta: {
  "title": "Delete a log repository",
  "description": "Delete a log repository",
  "full": true,
  "_openapi": {
    "method": "DELETE",
    "route": "/v1/log-repositories",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Delete a log repository"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/log-repositories-6eadfb8f.yaml"} operations={[{"path":"/v1/log-repositories","method":"delete"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/log-repositories/get
meta: {
  "title": "Get log repositories",
  "description": "Get log repositories",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/log-repositories",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get log repositories"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/log-repositories-6eadfb8f.yaml"} operations={[{"path":"/v1/log-repositories","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/log-repositories/post
meta: {
  "title": "Create a new log repository",
  "description": "Create a new log repository",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/log-repositories",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Create a new log repository"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/log-repositories-6eadfb8f.yaml"} operations={[{"path":"/v1/log-repositories","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/log-repositories/put
meta: {
  "title": "Update log repository",
  "description": "Update log repository",
  "full": true,
  "_openapi": {
    "method": "PUT",
    "route": "/v1/log-repositories",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Update log repository"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/log-repositories-6eadfb8f.yaml"} operations={[{"path":"/v1/log-repositories","method":"put"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/prompts/delete
meta: {
  "title": "Delete Prompt",
  "description": "Delete a prompt",
  "full": true,
  "_openapi": {
    "method": "DELETE",
    "route": "/v1/prompts",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Delete a prompt"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/prompts-77f24507.yaml"} operations={[{"path":"/v1/prompts","method":"delete"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/prompts/get
meta: {
  "title": "Get Prompts",
  "description": "Get prompts for a workspace",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/prompts",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get prompts for a workspace"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/prompts-77f24507.yaml"} operations={[{"path":"/v1/prompts","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/prompts/post
meta: {
  "title": "Create Prompt",
  "description": "Create a new prompt",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/prompts",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Create a new prompt"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/prompts-77f24507.yaml"} operations={[{"path":"/v1/prompts","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/prompts/put
meta: {
  "title": "Update Prompt",
  "description": "Update an existing prompt",
  "full": true,
  "_openapi": {
    "method": "PUT",
    "route": "/v1/prompts",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Update an existing prompt"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/prompts-77f24507.yaml"} operations={[{"path":"/v1/prompts","method":"put"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/test-runs/delete
meta: {
  "title": "Delete test runs",
  "description": "Delete test runs from a workspace",
  "full": true,
  "_openapi": {
    "method": "DELETE",
    "route": "/v1/test-runs",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Delete test runs from a workspace"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/test-runs-46e87990.yaml"} operations={[{"path":"/v1/test-runs","method":"delete"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/test-runs/get
meta: {
  "title": "Get test runs",
  "description": "Get test runs for a workspace",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/test-runs",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get test runs for a workspace"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/test-runs-46e87990.yaml"} operations={[{"path":"/v1/test-runs","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-datasets
meta: {
  "title": "Evaluate Datasets",
  "description": "Learn how to evaluate your AI outputs against expected results using Maxim's Dataset evaluation tools"
}

## Get started with Dataset evaluation

Have a dataset ready directly for evaluation? Maxim lets you evaluate your AI's performance directly without setting up workflows.

<Steps>
  <Step>
    ### Prepare Your Dataset

    Include these columns in your dataset:

    * Input queries or prompts
    * Your AI's actual outputs
    * Expected outputs (ground truth)

    ![Configure Test Run for Datasets](/images/docs/evaluate/how-to/evaluate-datasets/evaluation-dataset.png)
  </Step>

  <Step>
    ### Configure the Test Run

    * On the Dataset page, click the "Test" button, on the top right corner
    * Your Dataset should be already be pre-selected in the drop-down. Attach the [Evaluators](/docs/library/how-to/evaluators/use-pre-built-evaluators) and [Context Sources](/docs/library/how-to/context-sources/bring-your-rag-via-an-api-endpoint) you want to use.
    * Click "Trigger Test Run"

    The Dataset appears pre-selected in the drop-down. You can attach the evaluators and context sources (if any) you want to use.

    ![Configure Test Run for Datasets](/images/docs/evaluate/how-to/evaluate-datasets/test-form-dataset.png)
  </Step>
</Steps>

<Callout type="info">
  If you want to test only certain entries from your Dataset, you can [create a Dataset split](/docs/library/how-to/datasets/use-splits-within-a-dataset) and run the test run on the split the same way as above.
</Callout>


url: https://getmaxim.ai/docs/evaluate/how-to/scheduled-test-runs
meta: {
  "title": "Scheduled test runs",
  "description": "Learn how to schedule test runs for your prompts, prompt chains and workflows at a regular interval."
}

import { CalendarClockIcon } from "lucide-react";

<Steps>
  <Step>
    ## Select Prompt or Workflow or Prompt chain to schedule runs for.

    <div className="flex flex-row flex-grow items-center gap-0.5">Click on **"Test"** and select <CalendarClockIcon className="w-4 h-4 mx-1" /> from the test run sheet header.</div>
  </Step>

  <Step>
    ## Pick version

    Add name of the schedule, version (applicable for Prompts and Prompt chains) to run this scheduled test on.

    <Callout type="info">
      If you select **Latest** version - then we will pick the latest session of Prompt or prompt chain at each instance of scheduled run.
    </Callout>
  </Step>

  <Step>
    ## Decide schedule of the run

    Select schedule (every hour, day etc) along with the starting date time for the run. The granularity of repetition is an hour at the moment. Please reach out to us if you want lower granularity than this.
  </Step>

  <Step>
    ## Select evaluators

    As a last step, select evaluators you want to run. You can update this config anytime by editing this schedule.
  </Step>

  <Step>
    Once you save, Maxim will run these jobs at the given cadence. You can pause and resume these runs anytime. All the scheduled runs will be
    available in the Runs tab. You can filter these runs by the schedule name on the test run report table.
  </Step>
</Steps>


url: https://getmaxim.ai/docs/evaluate/how-to/trigger-test-runs-using-sdk
meta: {
  "title": "Trigger Test Runs using SDK",
  "description": "Learn how to programmatically trigger test runs using Maxim's SDK with custom datasets, flexible output functions, and evaluations for your AI applications."
}

import { SquareActivity, SquareCheck, SquareTerminal, Table2 } from 'lucide-react'

While Maxim's web interface provides a powerful way to run tests, the SDK offers even more flexibility and control. With the SDK, you can:

* Use custom datasets directly from your code
* Control how outputs are generated
* Integrate testing into your CI/CD pipeline
* Get real-time feedback on test progress
* Handle errors programmatically

## Example of triggering test runs using the SDK

The SDK uses a builder pattern to configure and run tests. Follow this example to trigger test runs:

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "" });

  const result = await maxim
  .createTestRun("My First SDK Test", "your-workspace-id")
  .withDataStructure(/* your data structure here */)
  .withData(/* your data here */)
  .yieldsOutput(/* your output function here */)
  .withWorkflowId(/* or you can pass workflow ID from Maxim platform */)
  .withPromptVersionId(/* or you can pass prompt version ID from Maxim platform */)
  .withEvaluators(/* your evaluators here */)
  .run();
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.logger import Logger, LoggerConfig

  # [!code word:api_key]
  maxim = Maxim(Config(api_key=""))

  result = maxim
  .create_test_run("My First SDK Test", "your-workspace-id")
  .with_data_structure() # your data structure here
  .with_data() # your data here
  .yields_output() # your output function here
  .with_workflow_id() # or you can pass workflow ID from Maxim platform
  .with_prompt_version_id() # or you can pass prompt version ID from Maxim platform
  .with_evaluators() # your evaluators here
  .run();
  ```
</Tabs>

Copy your workspace ID from the workspace switcher in the left topbar

![Screenshot of copy workspace ID option](/images/docs/sdk/copy-workspace-id-option.png)

## Understanding the data structure

Understand the data structure to maintain type safety and validate data columns. It maps your data columns to specific types that Maxim understands.

### Basic structure

Define your data structure using an object that maps column names to specific types.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  const dataStructure = {
      myQuestionColumn: "INPUT",
      expectedAnswerColumn: "EXPECTED_OUTPUT",
      contextColumn: "CONTEXT_TO_EVALUATE",
      additionalDataColumn: "VARIABLE"
  }
  ```

  ```python tab="Python"
  data_structure = {
      myQuestionColumn: "INPUT",
      expectedAnswerColumn: "EXPECTED_OUTPUT",
      contextColumn: "CONTEXT_TO_EVALUATE",
      additionalDataColumn: "VARIABLE"
  }
  ```
</Tabs>

### Available types

* `INPUT` - Main input text (only one allowed)
* `EXPECTED_OUTPUT` - Expected response (only one allowed)
* `CONTEXT_TO_EVALUATE` - Context for evaluation (only one allowed)
* `VARIABLE` - Additional data columns (multiple allowed)
* `NULLABLE_VARIABLE` - Optional data columns (multiple allowed)

### Example

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "YOUR_API_KEY" });

  const result = maxim
      .createTestRun("Question Answering Test", workspaceId)
      .withDataStructure({
          question: "INPUT",
          answer: "EXPECTED_OUTPUT",
          context: "CONTEXT_TO_EVALUATE",
          metadata: "NULLABLE_VARIABLE"
      })
      // ... rest of the configuration
  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="YOUR_API_KEY"))

  result = maxim
      .create_test_run("Question Answering Test", workspace_id)
      .with_data_structure({
          question: "INPUT",
          answer: "EXPECTED_OUTPUT",
          context: "CONTEXT_TO_EVALUATE",
          metadata: "NULLABLE_VARIABLE"
      })
      # ... rest of the configuration
  ```
</Tabs>

## Working with data sources

Maxim's SDK supports multiple ways to provide test data:

### 1. Callable

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  <Tab value="JS/TS">
    ```typescript tab="JS/TS"
    import { CSVFile, Maxim } from '@maximai/maxim-js';

    const myCSVFile = new CSVFile('./test.csv', {
        question: 0, // column index in CSV
        answer: 1,
        context: 2
    });

    // [!code word:apiKey]
    const maxim = new Maxim({ apiKey: "YOUR_API_KEY" });

    const result = maxim
        .createTestRun("CSV Test Run", workspaceId)
        .withDataStructure({
            question: "INPUT",
            answer: "EXPECTED_OUTPUT",
            context: "CONTEXT_TO_EVALUATE"
        })
        .withData(myCSVFile)
        // ... rest of the configuration
    ```

    <Callout type="info">
      The `CSVFile` class automatically validates your CSV headers against the data structure and provides type-safe access to your data.
    </Callout>
  </Tab>

  <Tab value="Python">
    ```python tab="Python"
    from maxim import Maxim, Config

    # [!code word:api_key]
    maxim = Maxim(Config(api_key="YOUR_API_KEY"))

    index = 0

    def get_next_row() -> Optional[Dict[str, Any]]:
        index = index + 1
        return db.get_row(index)


    result = maxim
        .create_test_run("CSV Test Run", workspace_id)
        .with_data_structure({
            question: "INPUT",
            answer: "EXPECTED_OUTPUT",
            context: "CONTEXT_TO_EVALUATE"
        })
        .with_data(get_next_row)
        # ... rest of the configuration
    ```
  </Tab>
</Tabs>

### 2. Manual data array

For smaller datasets or programmatically generated data:

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "YOUR_API_KEY" });

  const manualData = [
      {
          question: "What is the capital of France?",
          answer: "Paris",
          context: "France is a country in Western Europe..."
      },
      {
          question: "Who wrote Romeo and Juliet?",
          answer: "William Shakespeare",
          context: "William Shakespeare was an English playwright..."
      }
  ];

  const result = maxim
      .createTestRun("Manual Data Test", workspaceId)
      .withDataStructure({
          question: "INPUT",
          answer: "EXPECTED_OUTPUT",
          context: "CONTEXT_TO_EVALUATE"
      })
      .withData(manualData)
      // ... rest of the configuration
  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="YOUR_API_KEY"))

  manual_data = [
      {
          question: "What is the capital of France?",
          answer: "Paris",
          context: "France is a country in Western Europe..."
      },
      {
          question: "Who wrote Romeo and Juliet?",
          answer: "William Shakespeare",
          context: "William Shakespeare was an English playwright..."
      }
  ];

  result = maxim
      .create_test_run("Manual Data Test", workspace_id)
      .with_data_structure({
          question: "INPUT",
          answer: "EXPECTED_OUTPUT",
          context: "CONTEXT_TO_EVALUATE"
      })
      .with_data(manual_data)
      # ... rest of the configuration
  ```
</Tabs>

### 3. Platform datasets

Use existing datasets from your Maxim workspace:

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "YOUR_API_KEY" });

  const result = maxim
      .createTestRun("Platform Dataset Test", workspaceId)
      .withDataStructure({
          question: "INPUT",
          answer: "EXPECTED_OUTPUT",
          context: "CONTEXT_TO_EVALUATE"
      })
      .withData("your-dataset-id")
      // ... rest of the configuration
  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="YOUR_API_KEY"))

  result = maxim
      .create_test_run("Platform Dataset Test", workspaceId)
      .with_data_structure({
          question: "INPUT",
          answer: "EXPECTED_OUTPUT",
          context: "CONTEXT_TO_EVALUATE"
      })
      .with_data("your-dataset-id")
      # ... rest of the configuration
  ```
</Tabs>

## Trigger a test on a workflow stored on Maxim platform

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "YOUR_API_KEY" });

  const result = maxim
      .createTestRun("Custom Output Test", workspaceId)
      .withDataStructure({
          question: "INPUT",
          answer: "EXPECTED_OUTPUT",
          context: "CONTEXT_TO_EVALUATE"
      })
      .withData(myData)
      .withWorkflowId(workflowIdFromDashboard, contextToEvaluate) // context to evaluate is optional; it can either be a variable used in the workflow or a column name present in the dataset
  ```

  ```python tab="Python"
  const result = maxim
      .create_test_run("Custom Output Test", workspaceId)
      .with_data_structure({
          question: "INPUT",
          answer: "EXPECTED_OUTPUT",
          context: "CONTEXT_TO_EVALUATE"
      })
      .with_data(myData)
      .with_workflow_id(workflowIdFromDashboard, contextToEvaluate) # context to evaluate is optional; it can either be a variable used in the workflow or a column name present in the dataset
  ```
</Tabs>

Find the workflow ID in the workflows tab and from menu click on copy ID.

![screenshot of copying ID workflow](/images/docs/sdk/copy-id-workflow.png)

## Trigger a test on a prompt version stored on Maxim platform

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "YOUR_API_KEY" });

  const result = maxim
      .createTestRun("Custom Output Test", workspaceId)
      .withDataStructure({
          question: "INPUT",
          answer: "EXPECTED_OUTPUT",
          context: "CONTEXT_TO_EVALUATE"
      })
      .withData(myData)
      .withPromptVersionId(promptVersionIdFromPlatform, contextToEvaluate) // context to evaluate is optional; it can either be a variable used in the prompt or a column name present in the dataset
  ```

  ```python tab="Python"
  const result = maxim
      .create_test_run("Custom Output Test", workspaceId)
      .with_data_structure({
          question: "INPUT",
          answer: "EXPECTED_OUTPUT",
          context: "CONTEXT_TO_EVALUATE"
      })
      .with_data(myData)
      .with_prompt_version_id(promptVersionIdFromPlatform, contextToEvaluate) # context to evaluate is optional; it can either be a variable used in the prompt or a column name present in the dataset
  ```
</Tabs>

To get prompt version ID, go to prompts tab, select the version you want to run tests on and from menu click on copy version id.

![topbar](/images/docs/sdk/copy-prompt-version-id.png)

## Custom output function

The output function is where you define how to generate responses for your test cases:

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "YOUR_API_KEY" });

  const result = maxim
      .createTestRun("Custom Output Test", workspaceId)
      .withDataStructure({
          question: "INPUT",
          answer: "EXPECTED_OUTPUT",
          context: "CONTEXT_TO_EVALUATE"
      })
      .withData(myData)
      .yieldsOutput(async (data) => {
          // Call your model or API
          const response = await yourModel.call(
              data.question,
              data.context
          );

          return {
              // Required: The actual output
              data: response.text,

              // Optional: Context used for evaluation
              // Returning a value here will utilize this context for
              // evaluation instead of the CONTEXT_TO_EVALUATE column (if provided)
              retrievedContextToEvaluate: response.relevantContext,

              // Optional: Performance metrics
              meta: {
                  usage: {
                      promptTokens: response.usage.prompt_tokens,
                      completionTokens: response.usage.completion_tokens,
                      totalTokens: response.usage.total_tokens,
                      latency: response.latency
                  },
                  cost: {
                      input: response.cost.input,
                      output: response.cost.output,
                      total: response.cost.input + response.cost.output
                  }
              }
          };
      })
  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="YOUR_API_KEY"))

  def run(data):
      # ======================================================#
      # REPLACE THIS WITH YOUR ACTUAL WORKFLOW / MODEL CALL
      # ======================================================#
      response = {
          "text": "dummy response",
          "usage":{
              "prompt_tokens":10,
              "completion_tokens":20,
              "total_tokens":30,
              "latency": 233
          },
          "cost": {
              "input_cost": 0.2,
              "output_cost": 0.002,
              "total_cost": 0.202
          }
      }
      # ======================================================#
      # END
      # ======================================================#
      return {
          # Required: The actual output
          data: response.text,

          # Optional: Context used for evaluation
          # Returning a value here will utilize this context for
          # evaluation instead of the CONTEXT_TO_EVALUATE column (if provided)
          retrieved_context_to_evaluate: response.relevantContext,

          # Optional: Performance metrics
          meta: {
              usage: {
                  prompt_tokens: response.usage.prompt_tokens,
                  completion_tokens: response.usage.completion_tokens,
                  total_tokens: response.usage.total_tokens,
                  latency: response.latency
              },
              cost: {
                  input_cost: response.cost.input,
                  output_cost: response.cost.output,
                  total_cost: response.cost.input + response.cost.output
              }
          }
      }

  result = maxim
      .create_test_run("Custom Output Test", workspaceId)
      .with_data_structure({
          question: "INPUT",
          answer: "EXPECTED_OUTPUT",
          context: "CONTEXT_TO_EVALUATE"
      })
      .with_data(myData)
      .yields_output(lambda data: run(data))

  ```
</Tabs>

<Callout type="warn">
  If your output function throws an error, the entry will be marked as failed and you'll receive the index in the `failed_entry_indices` array after the run completes.
</Callout>

## Adding evaluators

Choose which evaluators to use for your test run:

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "YOUR_API_KEY" });

  const result = maxim
      .createTestRun("Evaluated Test", workspaceId)
      // ... previous configuration
      .withEvaluators(
          "Faithfulness", // names of evaluators installed in your workspace
          "Semantic Similarity",
          "Answer Relevance"
      )
  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="YOUR_API_KEY"))

  result = maxim
      .create_test_run("Evaluated Test", workspace_id)
      # ... previous configuration
      .with_evaluators(
          "Faithfulness", # names of evaluators installed in your workspace
          "Semantic Similarity",
          "Answer Relevance"
      )
  ```
</Tabs>

### Human evaluation

For evaluators that require human input, setting up the human evaluation configuration is required and can be done as follows:

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "YOUR_API_KEY" });

  const result = maxim
      .createTestRun("Human Evaluated Test", workspaceId)
      // ... previous configuration
      .withEvaluators("Human Evaluator")
      .withHumanEvaluationConfig({
          emails: ["evaluator@company.com"],
          instructions: "Please evaluate the response according to the evaluation criteria"
      })
  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="YOUR_API_KEY"))

  result = maxim
      .create_test_run("Human Evaluated Test", workspace_id)
      # ... previous configuration
      .with_evaluators("Human Evaluator")
      .with_human_evaluation_config({
          emails: ["evaluator@company.com"],
          instructions: "Please evaluate the response according to the evaluation criteria"
      })
  ```
</Tabs>

## Custom evaluators

You can create custom evaluators to implement specific evaluation logic for your test runs:

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import {
      Maxim,
      createDataStructure,
      createCustomEvaluator,
      createCustomCombinedEvaluatorsFor,
  } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({
      apiKey: process.env.MAXIM_API_KEY
  });

  const dataStructure = createDataStructure({
      Input: 'INPUT',
      'Expected Output': 'EXPECTED_OUTPUT',
      stuff: 'CONTEXT_TO_EVALUATE',
  });

  // example of creating a custom evaluator
  const myCustomEvaluator = createCustomEvaluator<typeof dataStructure>(
      'apostrophe-checker',
      (result) => {
          if (result.output.includes("'")) {
              return {
                  score: true,
                  reasoning: 'The output contains an apostrophe',
              };
          } else {
              return {
                  score: false,
                  reasoning: 'The output does not contain an apostrophe',
              };
          }
      },
      {
          onEachEntry: {
              scoreShouldBe: '=',
              value: true,
          },
          forTestrunOverall: {
              overallShouldBe: '>=',
              value: 80,
              for: 'percentageOfPassedResults',
          },
      },
  );

  // example of creating a combined custom evaluator
  const myCombinedCustomEvaluator = createCustomCombinedEvaluatorsFor(
      'apostrophe-checker-2',
      'containsSpecialCharacters',
  ).build<typeof dataStructure>(
      (result) => {
          return {
              'apostrophe-checker-2': {
                  score: result.output.includes("'") ? true : false,
                  reasoning: result.output.includes("'")
                      ? 'The output contains an apostrophe'
                      : 'The output does not contain an apostrophe',
              },
              containsSpecialCharacters: {
                  score: result.output
                      .split('')
                      .filter((char) => /[!@#$%^&*(),.?"':{}|<>]/.test(char))
                      .length,
              },
          };
      },
      {
          'apostrophe-checker-2': {
              onEachEntry: {
                  scoreShouldBe: '=',
                  value: true,
              },
              forTestrunOverall: {
                  overallShouldBe: '>=',
                  value: 80,
                  for: 'percentageOfPassedResults',
              },
          },
          containsSpecialCharacters: {
              onEachEntry: {
                  scoreShouldBe: '>',
                  value: 3,
              },
              forTestrunOverall: {
                  overallShouldBe: '>=',
                  value: 80,
                  for: 'percentageOfPassedResults',
              },
          },
      },
  );
  ```

  ```python tab="Python"
  from maxim import Maxim
  from typing import Dict
  from maxim.evaluators import BaseEvaluator
  from maxim.models import LocalEvaluatorResultParameter, LocalEvaluatorReturn, PassFailCriteria, PassFailCriteriaForTestrunOverall, PassFailCriteriaOnEachEntry, ManualData, Data, TestRunLogger, YieldedOutput

  class MyCustomEvaluator(BaseEvaluator):
      # implement evaluate function
      def evaluate(
          self, result: LocalEvaluatorResultParameter, data: ManualData
      ) -> Dict[str, LocalEvaluatorReturn]:
          # You can pass as many scores as you want in this dict
          # All of these will show up in the test run report
          return {
              "apostrophe-checker-2": LocalEvaluatorReturn(
                  score="'" in result.output,
                  reasoning="The output contains an apostrophe" if "'" in result.output else "The output does not contain an apostrophe"
              ),
              "contains_special_characters": LocalEvaluatorReturn(
                  score=len([char for char in result.output if char in '!@#$%^&*(),.?":{}|<>']),
                  reasoning="The output contains special characters"
              )
          }
  ```
</Tabs>

### Using custom evaluators

Once created, custom evaluators can be used alongside built-in evaluators:

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "YOUR_API_KEY" });

  const result = await maxim
      .createTestRun(`sdk test run ${Date.now()}`, payload.workspaceId)
      .withEvaluators(
          // platform evaluators
          'Faithfulness',
          'Semantic Similarity',
          // custom evaluators
          myCustomEvaluator,
          myCombinedCustomEvaluator,
      )
      .run();
  ```

  ```python tab="Python"
  result = maxim
      .create_test_run("Custom Evaluated Test", workspace_id)
      # ... previous configuration
      .with_evaluators(
          # Platform evaluators
          "Faithfulness",
          "Semantic Similarity",
          # Custom evaluators
          MyCustomEvaluator(
              pass_fail_criteria={
                  "apostrophe-checker-2": PassFailCriteria(
                      on_each_entry_pass_if=PassFailCriteriaOnEachEntry(
                          score_should_be="=",
                          value=True
                      ),
                      for_testrun_overall_pass_if=PassFailCriteriaForTestrunOverall(
                          overall_should_be=">=",
                          value=80,
                          for_result="percentageOfPassedResults"
                      )
                  ),
                  "contains-special-characters": PassFailCriteria(
                      on_each_entry_pass_if=PassFailCriteriaOnEachEntry(
                          score_should_be=">",
                          value=3
                      ),
                      for_testrun_overall_pass_if=PassFailCriteriaForTestrunOverall(
                          overall_should_be=">=",
                          value=80,
                          for_result="percentageOfPassedResults"
                      )
                  )
              }
          )
      )
  ```
</Tabs>

## Advanced configuration

### Concurrency control

Manage how many entries are processed in parallel:

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "YOUR_API_KEY" });

  const result = await maxim
      .createTestRun("Long Test", workspaceId)
      // ... previous configuration
      .withConcurrency(5); // Process 5 entries at a time
  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="YOUR_API_KEY"))

  result = maxim
      .create_test_run("Concurrent Test", workspace_id)
      # ... previous configuration
      .with_concurrency(5) # Process 5 entries at a time
  ```
</Tabs>

### Timeout configuration

Set custom timeout for long-running tests:

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "YOUR_API_KEY" });

  const result = await maxim
      .createTestRun("Long Test", workspaceId)
      // ... previous configuration
      .run(120) // Wait up to 120 minutes
  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="YOUR_API_KEY"))

  result = maxim
      .create_test_run("Long Test", workspace_id)
      # ... previous configuration
      .run(120) # Wait up to 120 minutes
  ```
</Tabs>

## Complete example

Here's a complete example combining all the features:

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { CSVFile, Maxim } from '@maximai/maxim-js';

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "YOUR_API_KEY" });

  // Initialize your data source
  const testData = new CSVFile('./qa_dataset.csv', {
      question: 0,
      expected_answer: 1,
      context: 2,
      metadata: 3
  });

  try {
      const result = await maxim
          .createTestRun(`QA Evaluation ${new Date().toISOString()}`, 'your-workspace-id')
          .withDataStructure({
              question: "INPUT",
              expected_answer: "EXPECTED_OUTPUT",
              context: "CONTEXT_TO_EVALUATE",
              metadata: "NULLABLE_VARIABLE"
          })
          .withData(testData)
          .yieldsOutput(async (data) => {
              const startTime = Date.now();

              // Your model call here
              const response = await yourModel.generateAnswer(
                  data.question,
                  data.context
              );

              const latency = Date.now() - startTime;

              return {
                  data: response.answer,
                  // Returning a value here will utilize this context for
                  // evaluation instead of the CONTEXT_TO_EVALUATE column
                  // (in this case, the `context` column)
                  retrievedContextToEvaluate: response.retrievedContext,
                  meta: {
                      usage: {
                          promptTokens: response.tokens.prompt,
                          completionTokens: response.tokens.completion,
                          totalTokens: response.tokens.total,
                          latency
                      },
                      cost: {
                          input: response.cost.prompt,
                          output: response.cost.completion,
                          total: response.cost.total
                      }
                  }
              };
          })
          .withEvaluators(
              "Faithfulness",
              "Answer Relevance",
              "Human Evaluator"
          )
          .withHumanEvaluationConfig({
              emails: ["qa-team@company.com"],
              instructions: `Please evaluate the responses for accuracy and completeness. Consider both factual correctness and answer format.`
          })
          .withConcurrency(10)
          .run(30); // 30 minutes timeout

      console.log("Test Run Link:", result.testRunResult.link);
      console.log("Failed Entries:", result.failedEntryIndices);
      console.log("Evaluation Results:", result.testRunResult.result[0]);
      /*
      the result.testRunResult.result[0] object looks like this (values are mock data):
      {
          cost: {
              input: 1.905419538506091,
              completion: 2.010163610111029,
              total: 3.915583148617119
          },
          latency: {
              min: 6,
              max: 484.5761906393187,
              p50: 438,
              p90: 484,
              p95: 484,
              p99: 484,
              mean: 346.2,
              standardDeviation: 179.4284,
              total: 5
          },
          name: 'sdk test run 1734931207308',
          usage: { completion: 206, input: 150, total: 356 },
          individualEvaluatorMeanScore: {
              Faithfulness: { score: 0, outOf: 1 },
              'Answer Relevance': { score: 0.2, outOf: 1 },
          }
      }
      */
  } catch (error) {
      console.error("Test Run Failed:", error);
  } finally {
      await maxim.cleanup();
  }
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.evaluators.evaluators import create_custom_evaluator
  from maxim.models.dataset import ManualData
  from maxim.models.evaluator import LocalEvaluatorResultParameter, LocalEvaluatorReturn, PassFailCriteria, PassFailCriteriaForTestrunOverall, PassFailCriteriaOnEachEntry

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="YOUR_API_KEY"))

  def apostrophe_checker(result: LocalEvaluatorResultParameter, data: ManualData) -> LocalEvaluatorReturn:
      if "'" in result.output:
          return LocalEvaluatorReturn(
              score=True,
              reasoning="The output contains an apostrophe"
          )
      else:
          return LocalEvaluatorReturn(
              score=False,
              reasoning="The output does not contain an apostrophe"
          )

  custom_evaluator = create_custom_evaluator(
      "apostrophe-checker",
      apostrophe_checker,
      PassFailCriteria(
          on_each_entry=PassFailCriteriaOnEachEntry(
              score_should_be="=",
              value=True
          ),
          for_testrun_overall=PassFailCriteriaForTestrunOverall(
              overall_should_be=">=",
              value=80,
              for_result="percentageOfPassedResults"
          )
      )
  )

  def run(data):
      start_time = time.time()

      # Your model call here
      response = your_model.generate_answer(
          data.question,
          data.context
      )

      latency = (time.time() - start_time) * 1000  # Convert to milliseconds

      return {
          "data": response.answer,
          # Returning a value here will utilize this context for
          # evaluation instead of the CONTEXT_TO_EVALUATE column
          # (in this case, the `context` column)
          "retrieved_context_to_evaluate": response.retrieved_context,
          "meta": {
              "usage": {
                  "prompt_tokens": response.tokens.prompt,
                  "completion_tokens": response.tokens.completion,
                  "total_tokens": response.tokens.total,
                  "latency": latency
              },
              "cost": {
                  "input_cost": response.cost.prompt,
                  "output_cost": response.cost.completion,
                  "total_cost": response.cost.total
              }
          }
      }


  try:
      result = maxim
          .create_test_run(f"QA Evaluation {time.now()}", 'your-workspace-id')
          .with_data_structure({
              "question": "INPUT",
              "expected_answer": "EXPECTED_OUTPUT",
              "context": "CONTEXT_TO_EVALUATE",
              "metadata": "NULLABLE_VARIABLE"
          })
          .with_data(testData)
          .yields_output(lambda data : run(data))
          .with_evaluators(
              custom_evaluator,
              "Faithfulness",
              "Answer Relevance",
              "Human Evaluator"
          )
          .with_human_evaluation_config({
              "emails": ["qa-team@company.com"],
              "instructions": 'Please evaluate the responses for accuracy and completeness. Consider both factual correctness and answer format.'
          })
          .with_concurrency(10)
          .run(30); # 30 minutes timeout

      print("Test Run Link:", result.test_run_result.link);
      print("Failed Entries:", result.failed_entry_indices);
      print("Evaluation Results:", result.test_run_result.result[0]);
      """
      the result.test_run_result.result[0] object looks like this (values are mock data):
      {
          cost: {
              input: 1.905419538506091,
              completion: 2.010163610111029,
              total: 3.915583148617119
          },
          latency: {
              min: 6,
              max: 484.5761906393187,
              p50: 438,
              p90: 484,
              p95: 484,
              p99: 484,
              mean: 346.2,
              standard_deviation: 179.4284,
              total: 5
          },
          name: 'sdk test run 1734931207308',
          usage: { completion: 206, input: 150, total: 356 },
          individual_evaluator_mean_score: {
              Faithfulness: { score: 0, outOf: 1, pass: False },
              'Answer Relevance': { score: 0.2, outOf: 1, pass: True },
              'apostrophe-checker': { score: 0.7, pass: False },
          }
      }
      """
  except Exception as e:
      print("Test Run Failed:", e)
  finally:
      maxim.cleanup();
  ```
</Tabs>


url: https://getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-api-workflow
meta: {
  "title": "Test your AI application via an API endpoint",
  "description": "Run your first test on an AI application via HTTP endpoint with ease, no code changes needed."
}

<video url="https://www.youtube.com/embed/J1Jhkh8wzZ4" />

<Steps>
  <Step>
    ### Configure your HTTP endpoint

    Set up your API endpoint in the [Workflows](/docs/evaluate/concepts#workflows) section. Add request headers and body parameters as needed. Transform requests using optional pre/post scripts for advanced configurations.

    ![Configure your HTTP endpoint in Workflows](/images/docs/evaluate/quickstart/workflow/api-workflow-interface.png)
  </Step>

  <Step>
    ### Set up test parameters

    Select a [Dataset](/docs/evaluate/concepts#datasets) containing your test cases and add [Evaluators](/docs/evaluate/concepts#evaluators) to assess response quality. Combine evaluators to measure accuracy, toxicity, and other metrics that matter to you.

    ![Configure dataset and evaluators for your test](/images/docs/evaluate/quickstart/workflow/api-workflow-test-trigger-sheet.png)
  </Step>

  <Step>
    ### Analyze test results

    Review performance metrics and quality scores for each test case. Identify patterns, spot issues, and make data-driven improvements to your AI application.

    ![Review test run results and metrics](/images/docs/evaluate/quickstart/workflow/api-workflow-test-run-report.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-prompt-chains
meta: {
  "title": "Test your first Prompt Chain",
  "description": "Test your agentic workflows using Prompt Chains with Datasets and Evaluators in minutes. View results across your test cases to find areas where it works well or needs improvement."
}

<Steps>
  <Step>
    ### Build your chain

    Create a Prompt Chain by connecting Prompt, Code, and API nodes based on your data flow. Each node type handles a specific task in your AI workflow.

    ![Connect nodes to create a Prompt chain](/images/docs/evaluate/quickstart/prompt-chain/prompt-chain-playground-interface.png)
  </Step>

  <Step>
    ### Configure your test

    Test your chain against a [Dataset](/docs/library/how-to/datasets/use-dataset-templates) and add [Evaluators](/docs/library/how-to/evaluators/use-pre-built-evaluators) to measure the quality of outputs. Configure any additional parameters needed for your test run.

    ![Select dataset and evaluators for testing](/images/docs/evaluate/quickstart/prompt-chain/prompt-chain-testrun-trigger-sheet.png)
  </Step>

  <Step>
    ### Review results

    Analyze the test report for quality metrics like accuracy and performance metrics like latency and cost. Use these insights to iterate on your chain.

    ![Analyze test run results](/images/docs/evaluate/quickstart/prompt-chain/prompt-chain-test-run-report.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/evaluate/quickstart/run-your-first-test-on-prompt
meta: {
  "title": "Run your first Prompt test",
  "description": "Test your Prompts with Datasets and Evaluators in minutes. View results across your test cases to find areas where it works well or needs improvement."
}

<video url="https://www.youtube.com/embed/S3Rqb902cfg" />

<Steps>
  <Step>
    ### Create and publish a Prompt

    Start by creating a new [Prompt](/docs/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground) in the playground. Configure your messages and settings, then [publish the version](/docs/evaluate/how-to/evaluate-prompts/create-prompt-versions) when you're ready for testing.

    ![Prompt playground showing message configuration and publish option](/images/docs/evaluate/quickstart/prompt/prompt-playground-interface.png)
  </Step>

  <Step>
    ### Configure and trigger a test

    Choose a [Dataset](/docs/library/how-to/datasets/use-dataset-templates) with your test cases and add [Evaluators](/docs/library/how-to/evaluators/use-pre-built-evaluators) to measure response quality. You can mix and match evaluators to check for accuracy, toxicity, and more.

    ![Test configuration sheet showing dataset and evaluator selection](/images/docs/evaluate/quickstart/prompt/prompt-test-trigger-sheet.png)
  </Step>

  <Step>
    ### Review test results

    Once the test is complete, you'll get a comprehensive report to understand your Prompt's performance. You'll see:

    * Overall quality scores across your test cases
    * Which inputs performed best and worst
    * Side-by-side comparisons of expected vs. actual outputs
    * Detailed evaluator feedback on specific responses

    This helps you quickly identify where your Prompt shines and where it needs improvement.

    ![Test run report showing metrics and results](/images/docs/evaluate/quickstart/prompt/prompt-test-run-report.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/evaluate/quickstart/simulate-and-evaluate-multi-turn-conversations
meta: {
  "title": "Test multi-turn AI conversations",
  "description": "Evaluate AI chat interactions automatically using conversation simulation, without code changes"
}

Simulate and test multi-turn conversations with your AI agent using Maxim Workflows. Instead of manual testing, our simulation engine interacts with your agent based on predefined configurations.

<Steps>
  <Step>
    ### Configure your HTTP endpoint

    Add your API endpoint in [Workflows](/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-your-ai-outputs-using-application-endpoint). Configure request headers and body parameters as needed.

    ![Multi turn workflow interface](/images/docs/evaluate/quickstart/workflow/multi-turn/multi-turn-simulation-interface.png)
  </Step>

  <Step>
    ### Configure test settings

    Create a test run with:

    * A dataset containing test scenarios
    * Relevant evaluators for chat quality
    * Simulation settings for conversation flow
    * Optional columns for additional test parameters

    ![Test run trigger for multi turn simulation](/images/docs/evaluate/quickstart/workflow/multi-turn/multi-turn-simulation-testrun-trigger-sheet.png)
  </Step>

  <Step>
    ### Review simulation results

    Analyze the test report to understand conversation quality and performance metrics.

    <Callout type="info">Multi-turn test runs take longer to complete since each scenario involves multiple conversation steps</Callout>

    ![Test run report showing conversation metrics](/images/docs/evaluate/quickstart/workflow/multi-turn/multi-turn-simulation-testrun-report.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/introduction/quickstart/running-first-test
meta: {
  "title": "Running your first test",
  "description": "Learn how to get started with your first test run in Maxim"
}

## 1. Set up your environment

First, configure your AI model providers:

<Steps>
  <Step>
    Go to `Settings` → `Models`.
  </Step>

  <Step>
    Click on the tab of the provider for which you want to add an API key.
  </Step>

  <Step>
    Click on `Add New` and fill in the required details.
  </Step>
</Steps>

<Callout type="info">
  Maxim requires at least one provider with access to GPT-3.5 and GPT-4 models. We use industry-standard encryption to securely store your API keys.
</Callout>

To learn more about API keys, inviting users, and managing roles, refer to our [Workspace and roles](/docs/introduction/quickstart/setting-up-workspace) guide.

<video url="https://drive.google.com/file/d/1WzUCBIDewojn6r3Om0HUEmb_oI8OyM3W/preview" />

## 2. Create your first prompt or workflow

Create prompts to experiment and evaluate a call to a model with attached context or tools. Use workflows to easily test your complex AI agents using the HTTP endpoint for your application without any integration.

### Prompt

<Steps>
  <Step>
    Navigate to the `Prompts` tab under the `Evaluate` section and click on Single prompts.
  </Step>

  <Step>
    Click `Create prompt` or `Try sample` to get started.
  </Step>

  <Step>
    Write your system prompt and user prompt in the respective fields.
  </Step>

  <Step>
    Configure additional settings like model, temperature, and max tokens.
  </Step>

  <Step>
    Click `Run` to test your prompt and see the AI's response.
  </Step>

  <Step>
    Iterate on your prompt based on the results.
  </Step>

  <Step>
    When satisfied, click `Save` to create a new version of your prompt.
  </Step>
</Steps>

To learn more about prompts, refer to our [detailed guide on prompts](/docs/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground).

### Workflow

<Steps>
  <Step>
    Navigate to the `Workflows` tab under the `Evaluate` section.
  </Step>

  <Step>
    Click `Create Workflow` or `Try sample`.
  </Step>

  <Step>
    Enter your API endpoint URL in the `URL` field.
  </Step>

  <Step>
    Configure any necessary headers or parameters. You can use dynamic variables like `{input}` to reference static context easily in any part of your workflow using `{}`
  </Step>

  <Step>
    Click `Run` to test your endpoint in the playground.
  </Step>

  <Step>
    In the `Output Mapping` section, select the part of the response you want to evaluate (e.g., `data.response`).
  </Step>

  <Step>
    Click `Save` to create your workflow.
  </Step>

  To learn more about workflows, refer to our detailed guide on [Workflows](/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-your-ai-outputs-using-application-endpoint).

  ## 3. Prepare your dataset

  Organize and manage the data you'll use for testing and evaluation:

  <Steps>
    <Step>
      Navigate to the Datasets tab under the `Library` section.
    </Step>

    <Step>
      Click `Create New` or `Upload CSV`. We also have a sample dataset created for you. Click on `View our sample dataset` to get started.
    </Step>

    <Step>
      If creating a new dataset, enter a name and description for your dataset.
    </Step>

    <Step>
      Add columns to your dataset (e.g., 'input' and 'expected\_output').
    </Step>

    <Step>
      Add entries to your dataset, filling in the values for each column.
    </Step>

    <Step>
      Click `Save` to create your dataset.
    </Step>
  </Steps>

  To learn more about datasets, refer to our detailed guide on [Datasets](/docs/library/how-to/datasets/use-dataset-templates).
</Steps>

## 5. Add evaluators

Set up evaluators to assess your prompt or workflow's performance:

<Steps>
  <Step>
    Navigate to the `Evaluators` tab under the `Library` section.
  </Step>

  <Step>
    Click `Add Evaluator` to browse available evaluators.
  </Step>

  <Step>
    Choose an evaluator type (e.g., AI, Programmatic, API, or Human).
  </Step>

  <Step>
    Configure the evaluator settings as needed.
  </Step>

  <Step>
    Click `Save` to add the evaluator to your workspace.
  </Step>
</Steps>

To learn more about evaluators, refer to our detailed guide on [Evaluators](/docs/library/how-to/evaluators/use-pre-built-evaluators).

## 6. Run your first test

Execute a test run to evaluate your prompt or workflow:

<Steps>
  <Step>
    Navigate to your saved prompt or workflow.
  </Step>

  <Step>
    Click `Test` in the top right corner.
  </Step>

  <Step>
    Select the dataset you created earlier.
  </Step>

  <Step>
    Choose the evaluators you want to use for this test run.
  </Step>

  <Step>
    Click `Trigger Test Run` to start the evaluation process.
  </Step>
</Steps>

<Callout type="info">
  If you've added human evaluators, you'll be prompted to set up human annotation on the report or via email.
</Callout>

## 7. Analyze test results

Review and analyze the results of your test run:

<Steps>
  <Step>
    Navigate to the `Runs` tab in the left navigation menu.
  </Step>

  <Step>
    Find your recent test run and click on it to view details.
  </Step>

  <Step>
    Review the overall performance metrics and scores for each evaluator.
  </Step>

  <Step>
    Drill down into individual queries to see specific scores and reasoning.
  </Step>

  <Step>
    Use these insights to identify areas for improvement in your prompt or workflow.
  </Step>
</Steps>

## Next steps

Now that you've completed your first cycle on the Maxim platform, consider exploring these additional capabilities:

1. [Prompt comparisons](/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground): Evaluate different prompts side-by-side to determine which ones produce the best results for a given task.
2. [Prompt chains](/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains): Create complex, multi-step AI workflows. Learn how to connect prompts, code, and APIs to build powerful, real-world AI systems using our intuitive, no-code editor.
3. [Context sources](/docs/library/how-to/context-sources/bring-your-rag-via-an-api-endpoint): Integrate Retrieval-Augmented Generation (RAG) into your workflows.
4. [Prompt tools](/docs/library/how-to/prompt-tools/create-a-code-tool): Enhance your prompts with custom functions and agentic behaviors.
5. [Observability](/docs/observe/overview): Use our stateless SDK to monitor real-time production logs and run periodic quality checks.

By following this guide, you've learned how to set up your environment, create prompts, prepare datasets, set up workflows, add evaluators, run tests, and analyze results. This foundational knowledge will help you leverage Maxim's powerful features to develop and improve your AI applications efficiently.


url: https://getmaxim.ai/docs/introduction/quickstart/setting-up-workspace
meta: {
  "title": "Setting up your workspace",
  "description": "Learn how to set up workspaces, invite team members, and manage role-based access control (RBAC) in Maxim. Streamline your AI project organization and control user permission within your enterprise."
}

## Add model API keys

First, configure your model providers:

* Go to `Settings` → `Models`.
* Click on the tab of the provider for which you want to add an API key.
* Click on `Add new` and fill in the required details.

For easy onboaring, we offer 50 free model usage credits that can be used across the platform. You can use these credits to run your first test.

<Callout type="info">
  To set up local models from Ollama with Maxim, follow the same steps as above. Don't forget to run his command to forward Ollama's port via ngrok:

  ```bash
  ngrok http 11434 --host-header="localhost:11434"
  ```

  To download Ollama, follow this [link](https://ollama.com/). Learn more about setting up Ollama via ngrok [here](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-use-ollama-with-ngrok).
</Callout>

<video url="https://drive.google.com/file/d/1LgdYAUCymvVfj76gL1YJiy1KZZBPku6Y/preview" />

## Create Maxim API key

* Go to `Settings` → `API keys`.
* Click on `Generate new`, name your key, and copy the generated key.

![Create Maxim API key](/images/docs/introduction/create-maxim-api-key.png)

## Invite team members

To invite a team member to your workspace, follow these steps:

* Go to `Settings` → `Members`.
* Click on `Invite`, located at the top right of the page.
* Enter the email ID of the person you want to invite. To invite multiple people, separate their email IDs with commas.
* Select the role you want to assign to the invited people from the dropdown menu, along with the workspace.
* Click on `Send invites`.

The invited people will receive an email with a link to join the workspace.

<video url="https://drive.google.com/file/d/1feOFDpnfLYmtdGPleT7FMocsEHNN-vAL/preview" />

## Create roles

Maxim uses a role-based access control (RBAC) system to manage user permissions. You can create a role by following these steps:

* Go to `Settings` → `Roles`.
* Click on `Create role`.
* Enter a name for the role.
* Select the permissions you want to assign to the role.
* Click on `Create`.

<video url="https://drive.google.com/file/d/10s17EcrpWYVhVxDE-nVCL-KakdAA3u6X/preview" />

You can explore more about your workspace settings by heading to the workspace settings page. You can explore more about billing, proxy settings and more.


url: https://getmaxim.ai/docs/observe/integrations/openai-agents-sdk
meta: {
  "title": "OpenAI Agents SDK",
  "description": "How to integrate Maxim's observability and real-time evaluation capabilities with OpenAI Agents SDK."
}

<video url="https://www.youtube.com/embed/dn-QXc7JgsI" />

The [OpenAI Agents SDK](https://openai.github.io/openai-agents-python/) enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions. It's a production-ready upgrade of our previous experimentation for agents, Swarm. The Agents SDK has a very small set of primitives:

* **Agents**, which are LLMs equipped with instructions and tools
* **Handoffs**, which allow agents to delegate to other agents for specific tasks
* **Guardrails**, which enable the inputs to agents to be validated

## Integrating with Maxim

<Steps>
  <Step>
    Create a Maxim account and a Log repository. Follow the instructions in the [quickstart
    section](https://www.getmaxim.ai/docs/observe/quickstart).
  </Step>

  <Step>
    Install Maxim SDK

    ```bash
    pip install maxim-py
    ```
  </Step>

  <Step>
    **Set the following environment variables to configure the Maxim SDK**

    | Environment Variable | Description                     |
    | -------------------- | ------------------------------- |
    | `MAXIM_API_KEY`      | Your Maxim API key              |
    | `MAXIM_LOG_REPO_ID`  | ID of the log repository to use |

    And then Maxim SDK will automatically initialize using these env variables.

    ```python
    from maxim import Maxim,Config
    from maxim.logger.openai.agents import MaximOpenAIAgentsTracingProcessor
    # Creating a new logger instance
    # It automatically initializes using MAXIM_API_KEY and MAXIM_LOG_REPO_ID from env variables.
    logger = Maxim(Config()).logger()
    ```
  </Step>

  <Step>
    **OR you can manually initialize the SDK**

    ```python
    from maxim import Maxim,Config
    from maxim.logger import LoggerConfig

    logger = Maxim(Config(api_key="your_api_key")).logger(LoggerConfig(id="your_log_repo_id"))
    ```
  </Step>

  <Step>
    Add the `MaximOpenAIAgentsTracingProcessor` to your agent using `add_trace_processor` or `set_trace_processor`.

    ```python
    from agents import add_trace_processor
    from maxim.logger.openai.agents import MaximOpenAIAgentsTracingProcessor

    add_trace_processor(MaximOpenAIAgentsTracingProcessor(logger))

    # Your agent code
    ```
  </Step>
</Steps>

## Cookbooks

Here are a few cookbooks that you can use to get started with Maxim:

1. [Language Agent](https://getmax.im/openai-agents-language)
2. [Customer support agent](https://getmax.im/openai-agents-customer-support)


url: https://getmaxim.ai/docs/api/datasets/columns/delete
meta: {
  "title": "Delete Dataset Columns",
  "description": "Delete dataset columns",
  "full": true,
  "_openapi": {
    "method": "DELETE",
    "route": "/v1/datasets/columns",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Delete dataset columns"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets/columns","method":"delete"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/columns/get
meta: {
  "title": "Get Dataset Columns",
  "description": "Get dataset columns",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/datasets/columns",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get dataset columns"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets/columns","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/columns/post
meta: {
  "title": "Create Dataset Columns",
  "description": "Create dataset columns",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/datasets/columns",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Create dataset columns"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets/columns","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/columns/put
meta: {
  "title": "Update Dataset Columns",
  "description": "Update dataset columns",
  "full": true,
  "_openapi": {
    "method": "PUT",
    "route": "/v1/datasets/columns",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Update dataset columns"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets/columns","method":"put"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/entries/delete
meta: {
  "title": "Delete Dataset Entries",
  "description": "Delete dataset entries",
  "full": true,
  "_openapi": {
    "method": "DELETE",
    "route": "/v1/datasets/entries",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Delete dataset entries"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets/entries","method":"delete"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/entries/get
meta: {
  "title": "Get Dataset Entries",
  "description": "Get dataset entries",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/datasets/entries",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get dataset entries"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets/entries","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/entries/post
meta: {
  "title": "Create Dataset entries",
  "description": "Create dataset entries",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/datasets/entries",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Create dataset entries"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets/entries","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/entries/put
meta: {
  "title": "Update Dataset Entries",
  "description": "Update dataset entries",
  "full": true,
  "_openapi": {
    "method": "PUT",
    "route": "/v1/datasets/entries",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Update dataset entries"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets/entries","method":"put"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/splits/delete
meta: {
  "title": "Delete Dataset Split",
  "description": "Delete dataset split",
  "full": true,
  "_openapi": {
    "method": "DELETE",
    "route": "/v1/datasets/splits",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Delete dataset split"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets/splits","method":"delete"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/splits/get
meta: {
  "title": "Get Dataset Splits",
  "description": "Get dataset splits",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/datasets/splits",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get dataset splits"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets/splits","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/splits/post
meta: {
  "title": "Create Dataset Split",
  "description": "Create dataset split",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/datasets/splits",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Create dataset split"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets/splits","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/datasets/splits/put
meta: {
  "title": "Update Dataset Split",
  "description": "Update dataset split",
  "full": true,
  "_openapi": {
    "method": "PUT",
    "route": "/v1/datasets/splits",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Update dataset split"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/datasets-4ec8fc21.yaml"} operations={[{"path":"/v1/datasets/splits","method":"put"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/evaluators/execute/post
meta: {
  "title": "Execute an evaluator",
  "description": "Execute an evaluator to assess content based on predefined criteria and return grading results, reasoning, and execution logs",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/evaluators/execute",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Execute an evaluator to assess content based on predefined criteria and return grading results, reasoning, and execution logs"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/evaluators-79496bae.yaml"} operations={[{"path":"/v1/evaluators/execute","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/folders/contents/get
meta: {
  "title": "Get Folder Contents",
  "description": "Get the contents (entities) of a specific folder, identified by folderId or name+parentFolderId.",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/folders/contents",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get the contents (entities) of a specific folder, identified by folderId or name+parentFolderId."
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/folders-3d5e102b.yaml"} operations={[{"path":"/v1/folders/contents","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/log-repositories/search/post
meta: {
  "title": "Search logs in a log repository",
  "description": "Search logs in a log repository",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/log-repositories/logs/search",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Search logs in a log repository"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/log-repositories-6eadfb8f.yaml"} operations={[{"path":"/v1/log-repositories/logs/search","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/log-repositories/traces/get
meta: {
  "title": "Get trace by ID",
  "description": "Get a specific trace by ID",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/log-repositories/logs/traces/{id}",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get a specific trace by ID"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/log-repositories-6eadfb8f.yaml"} operations={[{"path":"/v1/log-repositories/logs/traces/{id}","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/prompts/config/get
meta: {
  "title": "Get Prompt Config",
  "description": "Get prompt configuration",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/prompts/config",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get prompt configuration"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/prompts-77f24507.yaml"} operations={[{"path":"/v1/prompts/config","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/prompts/config/put
meta: {
  "title": "Update Prompt Config",
  "description": "Update prompt configuration",
  "full": true,
  "_openapi": {
    "method": "PUT",
    "route": "/v1/prompts/config",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Update prompt configuration"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/prompts-77f24507.yaml"} operations={[{"path":"/v1/prompts/config","method":"put"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/prompts/deploy/post
meta: {
  "title": "Deploy Prompt Version",
  "description": "Deploy a prompt version",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/prompts/deploy",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Deploy a prompt version"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/prompts-77f24507.yaml"} operations={[{"path":"/v1/prompts/deploy","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/prompts/run/post
meta: {
  "title": "Run Prompt Version",
  "description": "Run a specific version of a prompt",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/prompts/run",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Run a specific version of a prompt"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/prompts-77f24507.yaml"} operations={[{"path":"/v1/prompts/run","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/prompts/versions/get
meta: {
  "title": "Get Prompt Versions",
  "description": "Get versions of a prompt",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/prompts/versions",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get versions of a prompt"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/prompts-77f24507.yaml"} operations={[{"path":"/v1/prompts/versions","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/prompts/versions/post
meta: {
  "title": "Create a prompt version",
  "description": "Create a prompt version",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/prompts/versions",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Create a prompt version"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/prompts-77f24507.yaml"} operations={[{"path":"/v1/prompts/versions","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/test-runs/entries/get
meta: {
  "title": "Get test run entries",
  "description": "Get test run entries",
  "full": true,
  "_openapi": {
    "method": "GET",
    "route": "/v1/test-runs/entries",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Get test run entries"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/test-run-entries-34bffbf1.yaml"} operations={[{"path":"/v1/test-runs/entries","method":"get"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/api/test-runs/share-report/post
meta: {
  "title": "Share test run report",
  "description": "Share a test run report",
  "full": true,
  "_openapi": {
    "method": "POST",
    "route": "/v1/test-runs/share-report",
    "toc": [],
    "structuredData": {
      "headings": [],
      "contents": [
        {
          "content": "Share a test run report"
        }
      ]
    }
  }
}

{/* This file was generated by Fumadocs.
  Do not edit this file directly. Any changes should be made by running the generation command again. */}

<APIPage document={"https://cdn.getmaxim.ai/public/openapi/test-run-share-report-4497ade2.yaml"} operations={[{"path":"/v1/test-runs/share-report","method":"post"}]} webhooks={[]} hasHead={false} />


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-chains/create-customer-support-agent
meta: {
  "title": "Build an AI-powered customer support email agent",
  "description": "Create a workflow that automatically categorizes support emails, creates help desk tickets, and sends responses"
}

Create an intelligent workflow to process customer emails by classifying their intent, setting priorities, and generating personalized responses.

![Email support agent workflow](/images/docs/evaluate/how-to/prompt-chains/tutorials/customer-support-agent.png)

<Steps>
  <Step>
    ### Set up email classification

    ![Priority scoring setup](/images/docs/evaluate/how-to/prompt-chains/tutorials/email-classifier-node.png)

    Create a [Prompt](/docs/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground) with these system instructions:

    ```plaintext title="Email classifier Prompt"
    Analyze support emails and classify them by:

    1. Category:

       - Technical Issue
       - Billing Question
       - Feature Request
       - Account Management
       - General Inquiry

    2. Priority:

       - P1: Critical - Service down, security issues
       - P2: High - Major functionality blocked
       - P3: Medium - Non-critical issues
       - P4: Low - General questions, feedback

    3. Sentiment:
       - Positive
       - Neutral
       - Negative
       - Urgent

    Output:
    {
    "category": "string",
    "priority": "number",
    "sentiment": "string",
    "key_points": ["array of main issues"]
    }
    ```
  </Step>

  <Step>
    ### Add priority scoring

    Create a Prompt to determine response handling:

    ![Priority scoring setup](/images/docs/evaluate/how-to/prompt-chains/tutorials/priority-scorer-node.png)

    ```plaintext title="Priority scorer"
    Determine:

    1. Response time SLA(Service Level Agreement)
    2. Team assignment
    3. Escalation needs
    4. Customer tier impact
    ```
  </Step>

  <Step>
    ### Create help desk ticket

    Add an [API node](/docs/evaluate/how-to/evaluate-chains/use-api-nodes-within-chains) to create tickets in your help desk system:

    ![Create help desk ticket](/images/docs/evaluate/how-to/prompt-chains/tutorials/create-ticket-in-helpdesk-node.png)
  </Step>

  <Step>
    ### Generate personalized response

    Create a Prompt for customized responses:

    ```plaintext title="Response generator Prompt"
    Create a response that:

    1. Uses customer name
    2. Acknowledges the issue
    3. Lists next steps
    4. Follows brand voice
    5. Includes helpful resources
    6. Sets clear expectations
    ```

    ![Response generator](/images/docs/evaluate/how-to/prompt-chains/tutorials/support-response-generator-node.png)
  </Step>

  <Step>
    ### Send email response

    Configure an [API node](/docs/evaluate/how-to/evaluate-chains/use-api-nodes-within-chains) to send the generated response:

    ![Email API setup](/images/docs/evaluate/how-to/prompt-chains/tutorials/send-email-response-api-block.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-chains/create-product-description-generator
meta: {
  "title": "Generate and translate product descriptions with AI",
  "description": "Build an AI workflow to generate product descriptions from images using Prompt Chains"
}

Let's take a scenario where you have an application in which you want to build an AI workflow that generates a product description from a product image. Now, if you want to add more functionalities like translation, data processing, or sending data to external services, you would have to create a separate translation service, write additional code, and build an API layer to communicate with those external services.

This is exactly where Maxim [Prompt Chains](/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains) can help you build the workflow, covering all these pieces. With different types of nodes, you can create the same functionality with ease. Let's go through it step by step.

![Product description generator workflow](/images/docs/evaluate/how-to/prompt-chains/tutorials/product-description-generator-and-translator-chain.png)

<Steps>
  <Step>
    ### Create Prompt that generates product description

    Create a new [Prompt](/docs/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground) with these system instructions:

    ```plaintext title="System instructions for product description generator"
    You are a highly skilled AI that generates compelling product descriptions. Your goal is to highlight key features, benefits, and unique selling points in an engaging and brand-aligned tone.

    Guidelines:

    - Understand the Product: Focus on essential attributes and appeal to the target audience.
    - Tone & Style: Adapt based on the brand (e.g., luxury, casual, tech-savvy).
    - SEO & Readability: Use relevant keywords naturally, keep it concise, and structure for easy reading.
    - Persuasive & Action-Driven: Highlight USPs and include a strong call to action.

    Ensure descriptions are clear, engaging, and tailored to different product categories.
    ```
  </Step>

  <Step>
    ### Add the Prompt to a chain

    Create a [Prompt chain](/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains) and add your product description generator Prompt as the first step.

    ![Add product description generator to chain](/images/docs/evaluate/how-to/prompt-chains/tutorials/product-description-generator-bot-prompt-node.png)
  </Step>

  <Step>
    ### Add translation support

    Add another AI node to translate the generated product description:

    1. Connect it after the product description generator Prompt
    2. Configure it to translate the product description output
    3. Choose your target languages

    ![Add product description translator](/images/docs/evaluate/how-to/prompt-chains/tutorials/translator-prompt-node.png)
  </Step>

  <Step>
    ### Process translations with code block node

    Add a Code block node to handle translated product descriptions:

    1. Split text by language
    2. Structure output as key-value pairs

    ![Split translations with Code](/images/docs/evaluate/how-to/prompt-chains/tutorials/code-block-node.png)
  </Step>

  <Step>
    ### Connect logging service

    Add an API node to integrate with external systems:

    * Log results
    * Send notifications
    * Connect to analytics
    * Trigger actions in other tools

    [Configure](/docs/evaluate/how-to/evaluate-chains/use-api-nodes-within-chains) your endpoint details and required payload format.

    ![Configure API integration](/images/docs/evaluate/how-to/prompt-chains/tutorials/api-block-node.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-chains/debug-errors-at-every-node
meta: {
  "title": "Debug AI agent errors step by step",
  "description": "Identify and fix errors at each step of your AI workflow with detailed diagnostics"
}

Find and resolve issues in your Prompt Chains with clear error diagnostics. When a step fails, you'll see:

* The exact step that failed
* A descriptive error message
* Contextual information for debugging

This targeted error handling helps you quickly fix issues before they cascade through your AI workflow.

![Error details shown for a failed step in a Prompt Chain](/images/docs/evaluate/how-to/prompt-chains/prompt-chain-error-state.png)

In this example, the chain stopped at the translation step due to an incorrect max token configuration.


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-chains/deploy-prompt-chains
meta: {
  "title": "Deploy Prompt Chains",
  "description": "Quick iterations on prompt chains should not require code deployments every time. With more and more stakeholders working on prompt engineering, its critical to keep deployments of prompt chains as easy as possible without much overhead. Prompt chain deployments on Maxim allow conditional deployment of prompt chain changes that can be used via the SDK."
}

import { Rocket } from "lucide-react";
import { Button } from "ui";

## Why deploy Prompt Chains via Maxim

* Prompt experimentation - Create multiple versions of your prompt chains, and use a wide variety of models available on Maxim to test and compare their performance using your custom data.
* Deploy without code changes - Deploy the final version directly from UI—no code changes required. Use Maxim’s RBAC support to limit deployment permission to key stakeholders.
* Custom variables - Use custom variables to create rules to control which environments or user groups should receive the updates. This helps in setting up A/B tests or testing prompt variations internally before pushing to users.

### Deploying a Prompt Chain

<Steps>
  <Step>
    Navigate to Evaluation > Prompts > Prompt Chains and open the prompt chain you want to deploy.
  </Step>

  <Step>
    Click the <Button size="icon" variant="outline" className="inline-flex cursor-auto"><Rocket className="h-4 w-4" /></Button> button in the header and choose to deploy the present version.
  </Step>

  <Step>
    Add one or more rules for deployment e.g. Environment = prod.

    ![Deployment rules selection](/images/docs/evaluate/how-to/evaluate-prompts/deployments/deployment-rule-selection.png)
  </Step>

  <Step>
    Edit or define new variables by clicking <Button variant={"outline"} className="cursor-auto">Edit deployment variables</Button>
  </Step>

  <Step>
    Define the name and type of any variable. For variables of type `select` provide possible options. e.g. Environment: Beta, Staging, Prod.

    ![Add new deployment variable](/images/docs/evaluate/how-to/evaluate-prompts/deployments/add-new-deployment-variable.png)
  </Step>

  <Step>
    Every time you have a new version to deploy, use the variable based rules to deploy conditionally.
  </Step>

  <Step>
    View existing deployments for any prompt from the deploy button in the header.

    ![Deployments list](/images/docs/evaluate/how-to/evaluate-prompts/deployments/all-deployments-list.png)
  </Step>
</Steps>

## Fetching prompt chains via SDK

For building query to get prompt chain with specific deployment variables, you can use `QueryBuilder`.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true });

  const prompt = await maxim.getPromptChain("prompt-chain-id",
  					new QueryBuilder()
  						.and()
  						.deploymentVar("Environment", "prod")
  						.build());
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  prompt = maxim.get_prompt_chain("prompt-chain-id",
  	QueryBuilder()
  	.and_()
  	.deployment_var("Environment", "prod")
  	.build())
  ```
</Tabs>

Adding multiple queries

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true });

  const prompt = await maxim.getPromptChain(
  	"prompt-chain-id",
  	new QueryBuilder().and().deploymentVar("Environment", "prod").deploymentVar("CustomerId", "123").build(),
  );
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  prompt = maxim.get_prompt_chain("prompt-id",
  	QueryBuilder()
  		.and_()
  		.deployment_var("Environment", "prod")
  		.deployment_var("CustomerId", "123")
  		.build())
  ```
</Tabs>

Adding filters based on Tags

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true});

  const prompt = await maxim.getPromptChain(
  	"prompt-chain-id",
  	new QueryBuilder().and().deploymentVar("Environment", "prod").tag("TenantId", "3000").build(),
  );
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  prompt = maxim.get_prompt_chain("prompt-chain-id",
  	QueryBuilder()
  		.and_()
  		.deployment_var("Environment", "prod")
  		.tag("TenantId", "3000")
  		.build())
  ```
</Tabs>

<Callout>Learn more about advanced [prompt chain querying](/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains) techniques.</Callout>


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-chains/experiment-with-prompt-chains
meta: {
  "title": "Build complex AI workflows with Prompt Chains",
  "description": "Connect prompts, code, and APIs to create sophisticated AI systems using our visual editor - no coding required"
}

import { FileClock } from "lucide-react";

When building AI applications, you often need multiple steps working together - like chaining multiple AI models, processing data, or calling external services. While single Prompts work well for simple tasks, complex workflows need a more structured approach.

## What are Prompt Chains?

Prompt Chains let you connect three types of nodes to build your AI workflow:

1. **Prompt**: Run LLM interactions
2. **Code**: Add custom logic
3. **API**: Connect external services via HTTP API

Each node's output feeds into the next, creating a seamless flow of data.

![Visual editor showing connected Prompt Chain nodes](/images/docs/evaluate/how-to/prompt-chains/product-description-generator-and-translator-chain.png)

## Available nodes

<Accordions type="multiple">
  <Accordion title="Prompt">
    Run any [Prompt](/docs/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground) as part of your workflow.

    * Select from your existing prompts and versions
    * Support for variables and context
    * Input passed as user message
    * Output is the assistant's response

    Perfect for natural language tasks like classification, generation or analysis.

    <Img src="/images/docs/evaluate/how-to/prompt-chains/prompt-node.png" liftImageZIndex alt="Example of a Prompt node in the editor" className="mb-0" />
  </Accordion>

  <Accordion title="Code">
    Add JavaScript logic to process data between nodes.

    * Write custom code to transform data
    * Automatic JSON parsing of input
    * Automatic stringification of output
    * Seamless connection with other nodes

    Ideal for data manipulation, validation or custom processing.

    <Img src="/images/docs/evaluate/how-to/prompt-chains/code-node.png" liftImageZIndex alt="Example of a Code node with JavaScript editor" className="mb-0" />
  </Accordion>

  <Accordion title="API">
    Connect external services into your workflow.

    * Support for GET, POST and other methods
    * Variable substitution in headers, params and body
    * Automatic data format handling
    * Easy integration with other nodes

    Essential for retrieving external data or triggering services.

    <Img src="/images/docs/evaluate/how-to/prompt-chains/api-node.png" liftImageZIndex alt="Example of an API node configuration" className="mb-0" />
  </Accordion>
</Accordions>

## Create your first chain

<Steps>
  <Step>
    ### Initialize your chain

    Start from the `Start` node by dragging to create your first connection.
  </Step>

  <Step>
    ### Choose node functionality

    Configure the type and settings for your new node based on your requirements.

    ![Node type selection](/images/docs/evaluate/how-to/prompt-chains/new-node-state.png)
  </Step>

  <Step>
    ### Expand the workflow

    Connect additional nodes by dragging from the output handles to create new steps.

    ![Add more nodes](/images/docs/evaluate/how-to/prompt-chains/add-more-nodes.png)
  </Step>

  <Step>
    ### Optimize connections

    Modify the flow by adjusting node connections to achieve your desired logic.

    ![Restructure nodes](/images/docs/evaluate/how-to/prompt-chains/restructure-edges.png)
  </Step>

  <Step>
    ### Run a test

    Try out your chain in the playground to verify its behavior

    ![Input](/images/docs/evaluate/how-to/prompt-chains/prompt-chain-input.png)
  </Step>
</Steps>

<Callout type="warn">
  Ensure each node's output matches the input requirements of connected nodes.
</Callout>

## Version and deploy

Create versions to test or deploy your chain:

1. Click `Save` in the top right
2. Add an optional description
3. Click `Update`

![Version creation dialog](/images/docs/evaluate/how-to/prompt-chains/create-version-dialog.png)

<Callout type="info">
  View all versions using the <FileClock className="w-4 h-4 inline-block" /> icon
</Callout>

Next steps:

* [Run tests](/docs/evaluate/how-to/evaluate-chains/test-prompt-chains)
* [Deploy to production](/docs/evaluate/how-to/evaluate-chains/deploy-prompt-chains)
* [Access via SDK](/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains)

Learn more about [testing prompts](/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases) or check our [SDK documentation](/docs/sdk/overview).


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-chains/querying-prompt-chains
meta: {
  "title": "Query Prompt Chains via SDK",
  "description": "Learn how to efficiently query and retrieve prompt chains using the Maxim SDK, enabling advanced AI workflow management and customization"
}

<Callout>All Python code snippets in this document are for version 3.4.0+.</Callout>

### Initializing the SDK

<Tabs groupId="language" items={["JS/TS", "Python"]} persist>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "" });

  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  # [!code word:api_key]
  maxim = Maxim(Config(api_key=""))
  ```
</Tabs>

### For a prompt chain with specific deployment variables

For building query to get prompt chain with specific deployment variables, you can use `QueryBuilder`.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "" });

  const promptChain = await maxim.getPromptChain("prompt-chain-id",
  					new QueryBuilder()
  						.and()
  						.deploymentVar("Environment", "prod")
  						.build()});
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key=""))

  prompt_chain = maxim.get_prompt_chain("prompt-chain-id",
  	QueryBuilder()
  	.and_()
  	.deployment_var("Environment", "prod")
  	.build())
  ```
</Tabs>

Adding multiple queries

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "" });

  const promptChain = await maxim.getPromptChain(
  	"prompt-chain-id",
  	new QueryBuilder().and().deploymentVar("Environment", "prod").deploymentVar("CustomerId", "123").build(),
  );
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key=""))

  prompt_chain = maxim.get_prompt_chain("prompt-chain-id",
  	QueryBuilder()
  		.and_()
  		.deployment_var("Environment", "prod")
  		.deployment_var("CustomerId", "123")
  		.build())
  ```
</Tabs>

## Querying Prompts Chains

Sometimes you have usescases where you need to fetch multiple deployed prompt chains at once using a single query. For example, you might want to fetch all prompts for a specific customer or specific workflow. You can use `getPromptChains` function for this purpose.

<Callout>
  You will need to query using at-least one `deploymentVar` as a filter. Hence you will need to deploy prompt chain versions before querying
  them.
</Callout>

### Query deployed prompt chains using folder

To get all prompts from a folder, you can use `getPromptChains` function with `folderId` as a query parameter.

#### First capture folder id

There are multiple ways to capture folder id. You can use Maxim dashboard to get folder id.

1. Right click/click on three dots on the folder you want to get id for.
2. Select `Edit Folder` option.
3. You will see folder id in the form.

![Settings Page](/images/docs/folder-id.png)

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "" });

  const folder = await maxim.getFolderById("folder-id");
  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  # [!code word:api_key]
  maxim = Maxim(Config(api_key=""))

  folder = maxim.get_folder_by_id("folder-id")
  ```
</Tabs>

**To get folders using tags attached to the folder.**

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "" });

  const folders = await maxim.getFolders(new QueryBuilder().and().tag("CustomerId", "123").build());
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key=""))

  folders = maxim.get_folders(QueryBuilder().and_().tag("CustomerId", "123").build())
  ```
</Tabs>

<Callout>All the rules of prompt chain matching algorithm apply here. You can use same overriding techniques as explained above.</Callout>

#### Get all deployed prompts from a folder

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "" });

  const folder = await maxim.getFolderById("folder-id");
  const promptChains = await maxim.getPromptChains(
  	new QueryBuilder()
  	.and()
  	.folder(folder.id)
  	.deploymentVar("Environment", "prod")
  	.build(),
  );
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key=""))

  folder = maxim.get_folder_by_id("folder-id")
  promptChains = maxim.get_folders(
  	QueryBuilder()
  	.and_()
  	.folder(folder.id)
  	.deployment_var("Environment", "prod")
  	.build())
  ```
</Tabs>

You can filter prompt chains based on deploymentVars.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "" });

  const folder = await maxim.getFolderById("folder-id");
  const promptChains = await maxim.getPromptChains(
  		new QueryBuilder()
  		.and()
  		.folder(folder.id)
  		.deploymentVar("Environment", "prod")
  		.build());
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key=""))

  folder = maxim.get_folder_by_id("folder-id")
  prompt_chains = maxim.get_prompt_chain(
  	QueryBuilder()
  	.and_()
  	.folder(folder.id)
  	.deployment_var("Environment", "prod")
  	.build())
  ```
</Tabs>

<Callout>You have to pass at-least one filter along with `folder()`.</Callout>

## Prompt chain Structure

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  export type PromptChain = {
  	promptChainId: string;
  	version: number;
  	versionId: string;
  	nodes: ({ order: number } & PromptNode)[];
  };
  // Prompt node
  export type PromptNode = {
  	prompt: Prompt;
  };
  // Prompt
  export type Prompt = {
  	promptId: string;
  	version: number;
  	versionId: string;
  	messages: { role: string; content: string | CompletionRequestContent[] }[];
  	modelParameters: { [key: string]: any };
  	model: string;
  	tags: PromptTags;
  };
  ```

  ```python tab="Python"
  @dataclass
  class PromptChain():
  	prompt_chain_id: str
  	version: int
  	version_id: str
  	nodes: List[Dict[int, PromptNode]]

  @dataclass
  class PromptNode():
  	prompt: Prompt

  @dataclass
  class Prompt():
  	prompt_id: str
  	version: int
  	version_id: str
  	messages: List[Message]
  	model_parameters: Dict[str, Union[str, int, bool, Dict, None]]

  @dataclass
  class Message():
  	role: str
  	content: str
  ```
</Tabs>

## Folder Structure

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  export type Folder = {
  	id: string;
  	name: string;
  	parentFolderId?: string;
  	tags: { [key: string]: string };
  };
  ```

  ```python tab="Python"
  @dataclass
  class Folder():
  	id: str
  	name: str
  	parentFolderId: str
  	tags: Optional[Dict[str, Union[str, int, bool, None]]] = None
  ```
</Tabs>

### Using your own cache for prompts

Maxim SDK uses in-memory caching by default. You can use your own caching implementation by passing a custom cache object to the SDK. This allows you to remove complete dependency on our backend.

#### Interface for custom cache

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  export interface MaximCache {
  	getAllKeys(): Promise<string[]>;
  	get(key: string): Promise<string | null>;
  	set(key: string, value: string): Promise<void>;
  	delete(key: string): Promise<void>;
  }
  ```

  ```python tab="Python"
  class MaximInMemoryCache():
  	def getAllKeys(self) -> List[str]:
  		pass

  	def get(self, key: str) -> Optional[str]:
  		pass

  	def set(self, key: str, value: str) -> None:
  		pass

  	def delete(self, key: str) -> None:
  		pass
  ```
</Tabs>

You will have to pass this custom cache object to the SDK while initializing it.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  const maxim = new Maxim({ apiKey: "api-key", cache: new CustomCache() });
  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  maxim = Maxim(Config(apiKey=apiKey, baseUrl=baseUrl, cache=CustomCache()))
  ```
</Tabs>

#### Example

Here is the default in-memory cache implementation used by the SDK.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { MaximCache } from "@maximai/maxim-js";

  export class MaximInMemoryCache implements MaximCache {
  	private cache: Map<string, string> = new Map();

  	getAllKeys(): Promise<string[]> {
  		return Promise.resolve(Array.from(this.cache.keys()));
  	}

  	get(key: string): Promise<string | null> {
  		return Promise.resolve(this.cache.get(key) || null);
  	}
  	set(key: string, value: string): Promise<void> {
  		this.cache.set(key, value);
  		return Promise.resolve();
  	}
  }
  ```

  ```python tab="Python"
  class MaximInMemoryCache():
  	def __init__(self):
  		self.cache = {}

  	def getAllKeys(self) -> List[str]:
  		return list(self.cache.keys())

  	def get(self, key: str) -> Optional[str]:
  		return self.cache.get(key)

  	def set(self, key: str, value: str) -> None:
  		self.cache[key] = value

  	def delete(self, key: str) -> None:
  		if key in self.cache:
  			del self.cache[key]
  ```
</Tabs>

## Matching algorithm

Before going into the details of how to use the SDK, let's understand how the matching algorithm works. Maxim SDK uses best matching entity algorithm.

1. Let's assume that, you have asked for a prompt chain with deployment var `env` as `prod`, `customerId` as `"123"` and a tag, `tenantId` as `456` for `promptId` - `"abc"`.
2. SDK will first try to find a prompt chain matching all conditions.
3. **If we don't find any matching entity, we enforce only `deploymentVar` conditions (you can override this behaviour, as explained in the next section) and match as many tags as possible.**
4. If we still don't find any prompt chain, we check for a prompt chain version marked as fallback.
5. If we still don't find any prompt chain, we return `null`.

## Overriding fallback algorithm

1. You can override fallback algorithm by calling `.exactMatch()` on `QueryBuilder` object. That will enforce all conditions to be matched.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { QueryBuilder } from "@maximai/maxim-js;

  const promptChain = await maxim.getPromptChain(
  	"prompt-chain-id",
  	new QueryBuilder().and().deploymentVar("Environment", "prod").exactMatch().build(),
  );
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key=""))

  prompt_chain = maxim.get_prompt_chain(
      "prompt-chain-id",
      QueryBuilder()
          .and_()
      		.deployment_var("Environment", "prod")
      		.exact_match()
      		.build()
  )
  ```
</Tabs>

2. You can override fallback algorithm at each variable level. The third optional parameter in `deploymentVar` & `tag` function is `enforce`. If you pass `true` to this parameter, it will enforce exact match for that variable.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "" });

  const promptChain = await maxim.getPromptChain("prompt-id", new QueryBuilder().and().deploymentVar("Environment", "prod").build());
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key=""))

  prompt_chain = maxim.get_prompt_chain(
      "prompt-id",
      QueryBuilder()
          .and_()
      		.deployment_var("Environment", "prod")
      		.exact_match()
      		.build()
  )
  ```
</Tabs>


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-chains/test-prompt-chains
meta: {
  "title": "Test your agentic workflows using chains",
  "description": "Test Prompt Chains using datasets to evaluate performance across examples"
}

import { Button } from "ui";
import { ActivityIcon } from "lucide-react";

After testing in the playground, evaluate your Prompt Chains across multiple test cases to ensure consistent performance. You can do the same via test run.

<Steps>
  <Step>
    ### Create a Dataset

    Add test cases by creating a [Dataset](/docs/library/how-to/datasets/use-dataset-templates). For this example, we'll use a Dataset of product images to generate descriptions.

    ![Dataset with product images for testing](/images/docs/evaluate/how-to/prompt-chains/product-images-dataset.png)
  </Step>

  <Step>
    ### Build your Prompt Chain

    Create a Prompt Chain that processes your test examples. In this case, the chain generates product descriptions, translates them to multiple languages, and formats them to match specific requirements.

    ![Prompt chain for product description generation](/images/docs/evaluate/how-to/prompt-chains/product-description-generator-and-translator-chain.png)
  </Step>

  <Step>
    ### Start a test run

    Open the test configuration by clicking <Button><ActivityIcon className="h-3.5 w-3.5" /> Test</Button> in the top right corner.
  </Step>

  <Step>
    ### Configure your test

    Select your dataset and add [Evaluators](/docs/library/how-to/evaluators/use-pre-built-evaluators) to measure the quality of outputs.

    ![Test configuration with dataset and evaluator options](/images/docs/evaluate/how-to/prompt-chains/prompt-chain-test-run-trigger-sheet.png)
  </Step>

  <Step>
    ### Review results

    Monitor the [test run](/docs/evaluate/concepts#test-runs) to analyze the performance of your Prompt Chain across all inputs.

    ![Test run results showing performance metrics](/images/docs/evaluate/how-to/prompt-chains/prompt-chain-test-run-report.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-chains/use-api-nodes-within-chains
meta: {
  "title": "Use API nodes within chains",
  "description": "Make external API calls at any point in your Prompt Chain to integrate with third-party services. The API node lets you validate data, log events, fetch information, or perform any HTTP request without leaving your chain. Simply configure the endpoint, method, and payload to connect your AI workflow with external systems."
}

## Configuring API nodes

<Steps>
  <Step>
    ### Create an API node

    1. Drag from the start node or any existing node to create a connection
    2. Select API from the node type menu

    ![Add API node](/images/docs/evaluate/how-to/prompt-chains/add-api-node.png)
  </Step>

  <Step>
    ### Edit API node

    * Click the `more menu (3 dots)` in the top-right corner of the node.

    ![Edit API node](/images/docs/evaluate/how-to/prompt-chains/edit-api-node.png)
  </Step>

  <Step>
    ### API node editor

    Configure HTTP API requests with standard parameters and custom scripts. The editor provides a complete interface to set up your API endpoints.

    Request configuration

    * Select HTTP methods (GET, POST, PUT, DELETE)
    * Add request headers
    * Configure query parameters
    * Define request body

    Advanced options

    * Write pre-request scripts to modify request parameters.
    * Add post-response scripts to transform API responses.
    * Test API calls directly from the editor.

    ![Edit API node](/images/docs/evaluate/how-to/prompt-chains/api-node-editor.png)
  </Step>

  <Step>
    ### Select output field (optional)

    Click `run` to test your API endpoint. By default, the entire response body is set as the node output. To use a specific field from the response use the `Select output field` dropdown to choose the desired response field.

    ![Select output field](/images/docs/evaluate/how-to/prompt-chains/output-field-selector.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd
meta: {
  "title": "Automate Prompt evaluation via CI/CD",
  "description": "Trigger test runs in CI/CD pipelines to evaluate prompts automatically."
}

## Ensuring quality with every deployment

AI applications today are shipping lightning fast and there are a lot of iterations / changes being made to the system. Moving as fast means having to break a lot; but it doesn't necessarily have to. A good practice is to ensure that none of previous functionality breaks with the new changes.

This is where Maxim's CI/CD integration can help you out. By integrating a step to run a **Test run** into your deployment workflow, you can ensure that every new deployment is meeting a certain baseline quality.

## Before we start

Triggering a test run from CI/CD requires you have the following setup and ready:

1. [An API key from Maxim](/docs/observe/quickstart#2-generate-api-key)
2. [**A prompt version to test upon**](/docs/evaluate/how-to/evaluate-prompts/create-prompt-versions)
3. [A dataset to test against](/docs/library/how-to/datasets/use-dataset-templates)
4. [Evaluators to evaluate the prompt version against the dataset](/docs/library/how-to/evaluators/use-pre-built-evaluators)

Now that we have all the prerequisites, test runs can be triggered via:

* [CLI (executable binary)](#test-runs-via-cli)
* [GitHub Action](#test-runs-via-github-action)

## Test runs via CLI

Maxim offers a CLI tool that can be used to run test runs. It is an executable binary that can be run from the command line.

### Installation

Use the following command template to install the CLI tool (if you are using Windows, please refer to the Windows example as well):

<Tabs items={["Command template", "Linux amd64 example", "Windows example"]}>
  ```bash tab="Command template"
  wget https://downloads.getmaxim.ai/cli/<VERSION>/<OS>/<ARCH>/maxim
  ```

  ```bash tab="Linux amd64 example"
  wget https://downloads.getmaxim.ai/cli/v1/linux/amd64/maxim
  ```

  ```powershell tab="Windows example"
  # Use `.exe` extension for windows downloads.
  wget https://downloads.getmaxim.ai/cli/v1/windows/amd64/maxim.exe
  ```
</Tabs>

<Callout>
  We currently only have `v1` of the CLI tool. So please replace `<VERSION>` with `v1`.
</Callout>

#### Supported OS + ARCH

The command template can have the following OS + Architectures values,

| OS      | ARCH  |
| ------- | ----- |
| Linux   | amd64 |
| Linux   | 386   |
| Darwin  | arm64 |
| Darwin  | 386   |
| Windows | amd64 |
| Windows | 386   |

### Env Variables

We require you to set these environment variables before using the CLI, **these values cannot be passed via arguments/flags**.

| Name            | Value                                                                     |
| --------------- | ------------------------------------------------------------------------- |
| MAXIM\_API\_KEY | [API key obtained via Maxim](/docs/observe/quickstart#2-generate-api-key) |

### Triggering a test run

Use this template to trigger a test run:

```bash title="Shell command to trigger a test run"
# If you haven't added the binary to your PATH,
# replace `maxim` with the path to the binary you just downloaded
maxim test -p <prompt_version_id> -d <dataset_id> -e <comma_separated_evaluator_names>
```

Here are the arguments/flags that you can pass to the CLI to configure your test run

| Argument / Flag | Description                                                                                                 |
| --------------- | ----------------------------------------------------------------------------------------------------------- |
| -p              | Prompt version ID or IDs; in case you send multiple IDs (comma separated), it will create a comparison run. |
| -d              | Dataset ID                                                                                                  |
| -e              | Comma separated evaluator names <br /> Ex. `bias,clarity`                                                   |
| --json          | (optional) Output the result in JSON format                                                                 |

## Test runs via GitHub Action

GitHub actions provide a powerful way to run tests, build, and deploy your application. Our GitHub Action can seamlessly integrate with your existing deployment workflows, allowing you to ensure that your LLM is functioning as you expect.

### Quick Start

In order to add the GitHub Action to your workflow, you can start by adding a step that uses `maximhq/actions/test-runs@v1` as follows:

<Callout type="warn">
  Please ensure that you have the following setup:

  * in GitHub action **secrets**
    * MAXIM\_API\_KEY
  * in GitHub action **variables**
    * WORKSPACE\_ID
    * DATASET\_ID
    * **PROMPT\_VERSION\_ID**
</Callout>

```yaml title=".github/workflows/test-runs.yml"
name: Run Test Runs with Maxim

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  TEST_RUN_NAME: "Test Run via GitHub Action"
  CONTEXT_TO_EVALUATE: "context"
  EVALUATORS: "bias, clarity, faithfulness"

jobs:
  test_run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v2
      - name: Running Test Run
        id: test_run
        uses: maximhq/actions/test-runs@v1
        with:
          api_key: ${{ secrets.MAXIM_API_KEY }}
          prompt_version_id: ${{ vars.PROMPT_VERSION_ID }}
          test_run_name: ${{ env.TEST_RUN_NAME }}
          dataset_id: ${{ vars.DATASET_ID }}
          workflow_id: ${{ vars.WORKFLOW_ID }}
          context_to_evaluate: ${{ env.CONTEXT_TO_EVALUATE }}
          evaluators: ${{ env.EVALUATORS }}
      - name: Display Test Run Results
        if: success()
        run: |
          printf '%s\n' '${{ steps.test_run.outputs.test_run_result }}'
          printf '%s\n' '${{ steps.test_run.outputs.test_run_failed_indices }}'
          echo 'Test Run Report URL: ${{ steps.test_run.outputs.test_run_report_url }}'
```

This will trigger a test run on the platform and wait for it to complete before proceeding. The progress of the test run will be displayed in the **Running Test Run** section of the GitHub Action's logs as displayed below:

![GitHub Action Running Test Run Logs](/images/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd/github-action-running-test-run-logs.png)

### Inputs

The following are the inputs that can be used to configure the GitHub Action:

| Name                            | Description                                                                                                                                                                                                                                        | Required                                                         |
| ------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| `api_key`                       | Maxim API key                                                                                                                                                                                                                                      | Yes                                                              |
| `workspace_id`                  | Workspace ID to run the test run in                                                                                                                                                                                                                | Yes                                                              |
| `test_run_name`                 | Name of the test run                                                                                                                                                                                                                               | Yes                                                              |
| `dataset_id`                    | Dataset ID for the test run                                                                                                                                                                                                                        | Yes                                                              |
| `prompt_version_id`             | Prompt version ID to run for the test run (do not use with `workflow_id`)                                                                                                                                                                          | Yes (No if `workflow_id` is provided)                            |
| `workflow_id`                   | Workflow ID to run for the test run <br />(discussed in [Evaluate Workflows via API -> Automate workflow evaluation via CI/CD](/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd), do not use with `prompt_version_id`) | Yes (No if `prompt_version_id` is provided)                      |
| `context_to_evaluate`           | Variable name to evaluate; could be any variable used in the workflow / prompt or a column name                                                                                                                                                    | No                                                               |
| `evaluators`                    | Comma separated list of evaluator names                                                                                                                                                                                                            | No                                                               |
| `human_evaluation_emails`       | Comma separated list of emails to send human evaluations to                                                                                                                                                                                        | No (required in case there is a human evaluator in `evaluators`) |
| `human_evaluation_instructions` | Overall instructions for human evaluators                                                                                                                                                                                                          | No                                                               |
| `concurrency`                   | Maximum number of concurrent test run entries running                                                                                                                                                                                              | No (defaults to 10)                                              |
| `timeout_in_minutes`            | Fail if test run overall takes longer than this many minutes                                                                                                                                                                                       | No (defaults to 15 minutes)                                      |

### Outputs

The outputs that are provided by the GitHub Action in case it doesn't fail are:

| Name                      | Description                        |
| ------------------------- | ---------------------------------- |
| `test_run_result`         | Result of the test run             |
| `test_run_report_url`     | URL of the test run report         |
| `test_run_failed_indices` | Indices of failed test run entries |

## Evaluating Workflows

Please refer to [Evaluate Workflows via API -> Automate workflow evaluation via CI/CD](/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd)


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases
meta: {
  "title": "Run bulk comparisons across test cases",
  "description": "Experimenting across prompt versions at scale helps you compare results for performance and quality scores. By running experiments across datasets of test cases, you can make more informed decisions, prevent regressions and push to production with confidence and speed."
}

## Why run comparison experiments

* Make decisions between Prompt versions and models by comparing output differences.
* Analyze scores across all test cases in your Dataset for the evaluation metrics that you choose.
* Side by side comparison views for easy decision making and detailed view for every entry.

### Run a comparison report

<Steps>
  <Step>
    Open the Prompt playground for one of the Prompts you want to compare.

    ![Prompt playground](/images/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases/prompt-to-compare.png)
  </Step>

  <Step>
    Click the `test` button to start configuring your experiment.

    ![Open test configuration](/images/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases/test-config.png)
  </Step>

  <Step>
    Select the Prompt versions you want to compare it to. These could be totally different Prompts or another version of the same Prompt.

    ![Select versions to compare](/images/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases/select-prompts-to-compare.png)
  </Step>

  <Step>
    Select your Dataset to test it against.

    ![Select dataset](/images/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases/select-dataset.png)
  </Step>

  <Step>
    Optionally, select the context you want to evaluate if there is a difference in retrieval pipeline that needs comparison.

    ![Select context to evaluate](/images/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases/select-context-evaluate.png)
  </Step>

  <Step>
    Select existing Evaluators or add new ones from the store, then run your test.

    ![Select evaluators](/images/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases/select-evaluators.png)
  </Step>

  <Step>
    Once the run is completed, you will see summary details for each Evaluator. Below that, charts show the comparison data for latency, cost and tokens used.

    ![Report summary](/images/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases/report-summary.png)
  </Step>

  <Step>
    Each entry has 2 rows one below the other showing the outputs, latency and scores for the entities or versions compared. Deep dive into any entry by clicking the row and looking into the particular messages, evaluation details and logs.

    ![Report table](/images/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases/report-table.png)
  </Step>
</Steps>

If you want to compare Prompt versions over time (e.g. Last month's scores and this month's scores post a Prompt iteration), you can instead [generate a comparison report retrospectively](/docs/analyze/how-to/comparison-reports) under the analyze section.

## Next steps

* [Create presets to re-use your test configurations](/docs/evaluate/how-to/optimize-evaluation-processes/re-use-configuration-via-presets)


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/compare-prompt-versions
meta: {
  "title": "Compare Prompt versions",
  "description": "Track changes between different Prompt versions to understand what led to improvements or drops in quality."
}

![Side by side comparison of two Prompt versions](/images/docs/evaluate/how-to/evaluate-prompts/compare-prompt-versions/compare-prompt-versions.png)

<Steps>
  <Step>
    ## Access Prompt versions

    Open the Prompt you want to analyze

    Click "View all versions" from the versions dropdown

    ![List of all Prompt versions](/images/docs/evaluate/how-to/evaluate-prompts/compare-prompt-versions/versions-list-dropdown.png)
  </Step>

  <Step>
    ## Enter comparison mode

    Select "Comparison view" from the versions list

    ![Comparison view button in versions list](/images/docs/evaluate/how-to/evaluate-prompts/compare-prompt-versions/versions-list-sheet.png)
  </Step>

  <Step>
    ## Choose versions

    Select two versions to compare:

    * First version is auto-selected
    * Pick a second version to compare against
    * Click "Compare versions" to proceed
    * Change selection anytime during comparison

    ![Version selection interface](/images/docs/evaluate/how-to/evaluate-prompts/compare-prompt-versions/select-versions-to-compare.png)
  </Step>

  <Step>
    ## Review differences

    See changes between versions in a diff view that highlights:

    * Configuration changes
    * Message content updates
    * Parameter modifications

    **Pro tips:**

    * Switch version order using the swap button
    * Navigate between changes using the counter in header
    * Share comparison URL with team members

    ![Comparison interface with highlighted features](/images/docs/evaluate/how-to/evaluate-prompts/compare-prompt-versions/prompt-version-compare-quick-tips.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground
meta: {
  "title": "Compare Prompts in the playground",
  "description": "Iterating on Prompts as you evolve your AI application would need experiments across models, prompt structures, etc. In order to compare versions and make informed decisions about changes, the comparison playground allows a side by side view of results."
}

<video url="https://www.youtube.com/embed/ms-0zXuo0ks" />

## Why use Prompt comparison?

Prompt comparison combines multiple single Prompts into one view, enabling a streamlined approach for various workflows:

1. **Model comparison**: Evaluate the performance of different models on the same Prompt.
2. **Prompt optimization**: Compare different versions of a Prompt to identify the most effective formulation.
3. **Cross-Model consistency**: Ensure consistent outputs across various models for the same Prompt.
4. **Performance benchmarking**: Analyze metrics like latency, cost, and token count across different models and Prompts.

### Create a new comparison

<Steps>
  <Step>
    Navigate to the `Prompt` tab on the left side panel.
  </Step>

  <Step>
    Click on `Prompt` and choose `Prompt comparison`.

    ![Prompt comparison tab in navigation](/images/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground/prompt-comparison-tab-navigation.png)
  </Step>

  <Step>
    Click the `+` icon in the left sidebar.
  </Step>

  <Step>
    Choose to create a new Prompt comparison directly or create a folder for better organization.

    ![Dropdown menu for creating a new Prompt comparison](/images/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground/create-prompt-comparison-dropdown.png)
  </Step>

  <Step>
    Name your new Prompt comparison and optionally assign it to an existing folder.

    ![Creating a new Prompt comparison in Maxim](/images/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground/create-prompt-comparison-dialog.png)
  </Step>
</Steps>

### Configure Prompts

<Steps>
  <Step>
    Choose Prompts from your existing [Prompts](/docs/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground) or just select a model from the dropdown menu directly.

    <Img
      src={
              <div className="grid grid-cols-2 gap-2 items-center">
                  <img
                      src="/images/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground/selecting-prompts-for-comparison.png"
                      aria-disabled
                      className="rounded-md my-0 border"
                  />
                  <img
                      src="/images/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground/selecting-models-for-comparison.png"
                      aria-disabled
                      className="rounded-md my-0 border"
                  />
              </div>
          }
      alt="Selecting prompts and models for comparison"
    />
  </Step>

  <Step>
    Add more Prompts to compare using the "+" icon.
  </Step>

  <Step>
    Customize each Prompt independently. This won't affect the base Prompt in any manner so experiment freely.
  </Step>
</Steps>

### Run your comparison

You can choose to have the `Multi input` option either enabled or disabled.

* If enabled, provide input to each entry in the comparison individually.
* If disabled, the same input is taken for all the Prompts in the comparison.

<Img
  src={
  	<div className="flex flex-col gap-2">
  		<img
  			src="/images/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground/prompt-comparison-multi-input-enabled.png"
  			aria-disabled
  			className="rounded-md my-0 border"
  		/>
  		<img
  			src="/images/docs/evaluate/how-to/evaluate-prompts/compare-prompts-playground/prompt-comparison-multi-input-disabled.png"
  			aria-disabled
  			className="rounded-md my-0 border"
  		/>
  	</div>
  }
  alt="Multi input enabled vs disabled"
/>

<Callout type="info">
  You can compare up to **five** different Prompts side by side in a single comparison.
</Callout>

## Next steps

* [Compare results across test cases](/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases)


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/create-prompt-versions
meta: {
  "title": "Create and manage Prompt versions",
  "description": "As teams build their AI applications, a big part of experimentation is iterating on the prompt structure. In order to collaborate effectively and organize your changes clearly, Maxim allows prompt versioning and comparison runs across versions."
}

## Create prompt versions

A prompt version is a set of messages and configurations that is published to mark a particular state of the prompt. Versions are used to run tests, compare results and make deployments.

<Steps>
  <Step>
    If a prompt has changes that are not published, a badge showing `unpublished changes` will show near its name in the header.

    ![Unpublished changes](/images/docs/evaluate/how-to/evaluate-prompts/create-versions/unpublished-changes.png)
  </Step>

  <Step>
    To publish a version, click on the `publish version` button in the header and select which messages you want to add to this version. Optionally add a description for easy reference of other team members.

    ![Publish version](/images/docs/evaluate/how-to/evaluate-prompts/create-versions/publish-version.png)
  </Step>

  <Step>
    View recent versions by clicking on the arrow adjoining the `publish version` button and view the complete list using the button at the bottom of this list. Each version includes details about publisher and date of publishing for easy reference. Open any version by clicking on it.

    ![Version list](/images/docs/evaluate/how-to/evaluate-prompts/create-versions/version-list.png)
  </Step>
</Steps>

## Next steps

* [Run experiments on prompt versions](/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases)
* [Deploy prompt versions](/docs/evaluate/how-to/evaluate-prompts/deploy-prompts)


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/deploy-prompts
meta: {
  "title": "Deploy Prompts",
  "description": "Quick iterations on Prompts should not require code deployments every time. With more and more stakeholders working on prompt engineering, its critical to keep deployments of Prompts as easy as possible without much overhead. Prompt deployments on Maxim allow conditional deployment of prompt changes that can be used via the SDK."
}

import { Rocket } from "lucide-react";
import { Button } from "ui";

## Why deploy Prompts via Maxim

* Prompt experimentation - Create multiple versions of your Prompts, and use a wide variety of models available on Maxim to test and compare their performance using your custom data.
* Deploy without code changes - Deploy the final version directly from UI—no code changes required. Use Maxim’s RBAC support to limit deployment permission to key stakeholders.
* Custom variables - Use custom variables to create rules to control which environments or user groups should receive the updates. This helps in setting up A/B tests or testing prompt variations internally before pushing to users.

### Deploying a prompt

<Steps>
  <Step>
    Open the prompt version you want to deploy.

    ![Prompt version list](/images/docs/evaluate/how-to/evaluate-prompts/deployments/prompt-versions-list.png)
  </Step>

  <Step>
    Click the <Button size="icon" variant="outline" className="inline-flex cursor-auto"><Rocket className="h-4 w-4" /></Button> button in the header and choose to deploy the present version.
  </Step>

  <Step>
    Add one or more rules for deployment e.g. Environment = prod.

    ![Deployment rules selection](/images/docs/evaluate/how-to/evaluate-prompts/deployments/deployment-rule-selection.png)
  </Step>

  <Step>
    Edit or define new variables by clicking <Button variant={"outline"} className="cursor-auto">Edit deployment variables</Button>
  </Step>

  <Step>
    Define the name and type of any variable. For variables of type `select` provide possible options. e.g. Environment: Beta, Staging, Prod.

    ![Add new deployment variable](/images/docs/evaluate/how-to/evaluate-prompts/deployments/add-new-deployment-variable.png)
  </Step>

  <Step>
    Every time you have a new version to deploy, use the variable based rules to deploy conditionally.
  </Step>

  <Step>
    View existing deployments for any prompt from the deploy button in the header.

    ![Deployments list](/images/docs/evaluate/how-to/evaluate-prompts/deployments/deployment-list.png)
  </Step>
</Steps>

## Fetching Prompts via SDK

For building query to get prompt with specific deployment variables, you can use `QueryBuilder`.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true });

  const prompt = await maxim.getPrompt("prompt-id",
  					new QueryBuilder()
  						.and()
  						.deploymentVar("Environment", "prod")
  						.build());
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  prompt = maxim.get_prompt("prompt-id",
  	QueryBuilder()
  	.and_()
  	.deployment_var("Environment", "prod")
  	.build())
  ```
</Tabs>

Add multiple queries

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true });

  const prompt = await maxim.getPrompt(
  	"prompt-id",
  	new QueryBuilder().and().deploymentVar("Environment", "prod").deploymentVar("CustomerId", "123").build(),
  );
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  prompt = maxim.get_prompt("prompt-id",
  	QueryBuilder()
  		.and_()
  		.deployment_var("Environment", "prod")
  		.deployment_var("CustomerId", "123")
  		.build())
  ```
</Tabs>

Add filters based on tags

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true});

  const prompt = await maxim.getPrompt(
  	"prompt-id",
  	new QueryBuilder().and().deploymentVar("Environment", "prod").tag("TenantId", "3000").build(),
  );
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  prompt = maxim.get_prompt("prompt-id",
  	QueryBuilder()
  		.and_()
  		.deployment_var("Environment", "prod")
  		.tag("TenantId", "3000")
  		.build())
  ```
</Tabs>

<Callout>Learn more about advanced [prompt querying](/docs/evaluate/how-to/evaluate-prompts/querying-prompts) techniques.</Callout>


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/experiment-in-prompt-playground
meta: {
  "title": "Experiment in the Prompt playground",
  "description": "Create, refine, experiment and deploy your prompts via the playground. Organize of your prompts using folders and versions, experimenting with the real world cases by linking tools and context, and deploying based on custom logic."
}

Prompts in Maxim provide a powerful way to experiment with prompt structures, models and configurations. Maxim's playground allows you to iterate over prompts, test their effectiveness, and ensure they work well before integrating them into more complex workflows for your application.

## Selecting a model

Maxim supports a wide range of models, including:

* Open-source models
* Closed models
* Custom models

Easily experiment across models by [configuring models](/docs/introduction/quickstart/setting-up-workspace#add-model-api-keys) and selecting the relevant model from the dropdown at the top of the prompt playground.

![Model selection interface](/images/docs/evaluate/how-to/evaluate-prompts/experiment-playground/model-selection-interface.png)

## Adding system and user prompts

In the prompt editor, add your system and user prompts. The system prompt sets the context or instructions for the AI, while the user prompt represents the input you want the AI to respond to. Use the `Add message` button to append messages in the conversations before running it. Mimic assistant responses for debugging using the `assistant` type message.

![Prompt editor interface showing system and user prompt fields](/images/docs/evaluate/how-to/evaluate-prompts/experiment-playground/prompt-editor-interface.png)

<Callout type="info" title="Experimenting with tools">
  If your prompts require tool usage, you can attach tools and experiment using `tool` type messages. [Learn about using tools in playground](/docs/evaluate/how-to/evaluate-prompts/run-prompt-tool-calls).
</Callout>

## Configuring parameters

Each prompt has a set of parameters that you can configure to control the behavior of the model. Find details about the different parameters for each model in the model's documentation. Here are some examples of common parameters:

* Temperature
* Max tokens
* topP
* Logit bias
* Prompt tools (for function calls)
* Custom stop sequences

![Parameter configuration panel](/images/docs/evaluate/how-to/evaluate-prompts/experiment-playground/parameter-configuration-panel.png)

<Callout type="info" title="Select response formats">
  Experiment using the right response format like structured output, or JSON for models that allow it.
</Callout>

## Using variables

Maxim allows you to include variables in your prompts using double curly braces `{{ }}`. You can use this to reference dynamic data and add the values within the variable section on the right side.

Variable values can be static or dynamic where its connected to a [context source](/docs/library/how-to/context-sources/bring-your-rag-via-an-api-endpoint).

![Variables](/images/docs/evaluate/how-to/evaluate-prompts/experiment-playground/variables.png)

## Next steps

* For RAG applications using retrieved context, learn about [attaching context to your prompt](/docs/evaluate/how-to/evaluate-prompts/rag-quality).
* For agentic systems in which you want to test out correct tool usage by your prompt, learn about [running a prompt with tool calls](/docs/evaluate/how-to/evaluate-prompts/run-prompt-tool-calls).
* For better collaborative management of your prompts, learn about [versioning prompts](/docs/evaluate/how-to/evaluate-prompts/create-prompt-versions).


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline
meta: {
  "title": "Set up a human annotation pipeline",
  "description": "Human annotation is critical to improve your AI quality. Getting human raters to provide feedback on various dimensions can help measure the present status and be used to improve the system over time. Maxim's human-in-the-loop pipeline allows team members as well as external raters like subject matter experts to annotate AI outputs."
}

The Maxim platform allows you to integrate your human annotation pipeline alongside other forms of auto evaluation throughout the development lifecycle.

Add human evaluators to test runs using the following steps:

<Steps>
  <Step>
    **Creating human evaluators**

    Add instructions, score type and pass criteria.
  </Step>

  <Step>
    **Select the relevant human evaluators while triggering a test run**

    Switch these on while configuring test run for a Prompt or Workflow.
  </Step>

  <Step>
    **Set up human evaluation configurations for this run**

    Choose method of annotation, add general instructions and emails of raters if applicable and configure sampling rate.
  </Step>

  <Step>
    **Collect ratings via test report columns or via email**

    Based on method chosen, annotators can add their ratings on the run report or external dashboard link sent on their email.
  </Step>

  <Step>
    **See summary of human ratings and deep dive into particular cases**

    As a part of the test report, you can view status of rater inputs, rating details and add corrected outputs to dataset.
  </Step>
</Steps>

## Create human evaluators

Create custom human evaluators with specific criteria for rating. You can add instructions that will be sent alongside the evaluator so that human annotators or subject matter experts are aware of the logic for rating. You can also define the evaluation score type and pass criteria.

![Create human evaluator](/images/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline/create-evaluators.png)

## Select human evaluators while triggering a test run

On the test run configuration panel for Prompt (or Workflow), you can switch on the relevant human evaluators from the list. When you click on the `Trigger test run` button, if any human evaluators were chosen, you will see a popover to set up the human evaluation.

![Create human evaluator](/images/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline/select-human-evaluators.png)

## Set up human evaluation for this run

The human evaluation set requires the following choices

1. Method
   * Annotate on report - Columns will be added to existing report for all editors to add ratings
   * Send via email - People within or outside your organization can submit ratings. The link sent is accessible separately and does not need a paid seat on your Maxim organization.
2. If you choose to send evaluation requests via email, you need to provide the emails of the raters and instructions to be sent.
3. For email based evaluation requests to SMEs or external annotators, we make it easy to send only required entries using a sampling rate. Sampling rate can be defined in 2 ways:
   * Percentage of total entries - This is relevant for large datasets where in it’s not possible to manually rate all entries
   * Custom logic - This helps send entries of a particular type to raters. Eg. Ratings which have a low score on the Bias metric (auto eval). By defining these rules, you can make sure to use your SME’s time on the most relevant cases.

![Human annotation set up](/images/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline/set-up-human-annotation.png)

## Collect ratings via test report columns

All editors can add human annotations to the test report directly. Clicking on `select rating` button in the relevant evaluator column. A popover will show with all the evaluators that need ratings. Add comments for each rating. In case the output is not upto the mark, submit a re-written output.

![Annotate on report](/images/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline/annotate-on-report.png)

If one rater has already provided ratings, a different rater can still add their inputs. Hover on the row to reveal a button near the previous value. Add ratings via the popover as mentioned above. Average rating across raters will be shown for that evaluator and considered for the overall results calculations.

![Add rating](/images/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline/add-rating.png)

## Collect ratings via email

On completion of the test run, emails are sent to all raters provided during set up. This email will contain the requester name and instructions along with the link to the rater dashboard.

![email](/images/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline/email.png)

<Callout>
  The human rater external dashboard is accessible externally without a paid slot on Maxim.
</Callout>

You can send this to external annotation teams or SMEs who might be helping with annotation needs. As soon as a rater has started evaluating via the dashboard, you will see the status of evaluation change from `Pending` to `In-progress` on the test run summary.

Human raters can go through the query, retrieved context, output and expected output (if applicable) for each entry and then provide their ratings for each evaluation metric. They can also add comments or re-write the output for a particular entry. On completion of a rating for a particular entry they can save and proceed and these values will start reflecting on the Maxim test run report.

![Human rater dashboard](/images/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline/rater-dashboard.png)

## Analyze human ratings

Once all entries are completed by a rater, the summary scores and pass/fail results for the human ratings are shown along side all other auto evaluation results in the test run report. The human annotation section will show a `Completed` status next to this rater's email. To view the detailed ratings by a particular individual, click the `View details` button and go through the table provided.

![Human review details](/images/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline/review-details.png)

If there are particular cases where you would like to use the human corrected output to build ground truth in your datasets, you can use the [data curation flows.](/docs/library/how-to/datasets/curate-golden-dataset-for-human-annotation)


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/organize-prompts
meta: {
  "title": "Organize Prompts",
  "description": "Building AI applications collaboratively needs Prompts to be organized well for easy reference and access. Adding Prompts to folders, tagging them, and versioning on Maxim helps you maintain a holistic Prompt CMS."
}

## Organize using folders

All Prompts can be grouped under folders that map to your applications, projects or teams. This way, even with a large number of single Prompts, finding and iterating on Prompts is easy for any team member who joins your Maxim organization.

![Folders](/images/docs/evaluate/how-to/evaluate-prompts/organize-prompts/prompt-folders.png)

Create a new folder by clicking on the `+` icon on the Prompts sidebar. Give it a name. Start adding new Prompts to it via drag and drop or select the folder when creating a new Prompt.

![Adding to folder](/images/docs/evaluate/how-to/evaluate-prompts/organize-prompts/add-to-folder.png)

## Tag prompts

Tags act as custom metadata that can be used to identify and retrieve Prompts through the Maxim SDK. Add tags to a Prompt via the configuration section on the right side of the Prompt playground. Tags are simple key value pairs that can be defined and edited easily.

![Adding tags](/images/docs/evaluate/how-to/evaluate-prompts/organize-prompts/add-tags.png)

Fetch relevant Prompts in your code using the SDK and querying as per tag values as shown below:

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  <Tab value="JS/TS">
    ```typescript
    import { Maxim, QueryBuilder } from "@maximai/maxim-js";

    // [!code word:apiKey]
    const maxim = new Maxim({
    	apiKey: "",
    });

    const prompt = await maxim.getPrompt(
    	promptId,
    	new QueryBuilder()
    		.and()
    		.deploymentVar("Environment", "test")
    		.tag("CustomerId", 1234)
    		.tag("grade", "A")
    		.tag("test", true)
    		.exactMatch()
    		.build(),
    );
    ```
  </Tab>

  <Tab value="Python">
    ```python
    from maxim import Maxim, Config
    from maxim.models import QueryBuilder

    # [!code word:api_key]
    maxim = Maxim(Config(api_key=""))

    prompt = maxim.get_prompt(
        prompt_id,
        QueryBuilder()
            .and_()
            .deploymentVar("Environment", "test")
            .tag("CustomerId", 1234)
            .tag("grade", "A")
            .tag("test", True)
            .exactMatch()
            .build()
    )
    ```
  </Tab>
</Tabs>

## Prompt versions and sessions

Outside of folders or tags, iterations on your Prompts should also be organized effectively for insights on the impact of your changes. View details on [how to version Prompts](/docs/evaluate/how-to/evaluate-prompts/create-prompt-versions) and use them for testing.


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/querying-prompts
meta: {
  "title": "Query Prompts via SDK",
  "description": "Learn how to efficiently query and retrieve prompts using Maxim AI's SDK, including deployment-specific and tag-based queries for streamlined prompt management."
}

## Deployments

You can create prompts with versions and create their deployments that can be used via the SDK.

<video url="https://drive.google.com/file/d/1QwBA48CXoWRHrbDSwrXZQxGBxohi63gV/preview" />

<Callout>All Python code snippets in this document are for version 3.4.0+.</Callout>

### Initializing the SDK

<Tabs groupId="language" items={["JS/TS", "Python"]} persist>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true });

  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))
  ```
</Tabs>

### For a prompt with specific deployment variables

For building query to get prompt with specific deployment variables, you can use `QueryBuilder`.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true });

  const prompt = await maxim.getPrompt("prompt-id",
  					new QueryBuilder()
  						.and()
  						.deploymentVar("Environment", "prod")
  						.build());
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  prompt = maxim.get_prompt("prompt-id",
  	QueryBuilder()
  	.and_()
  	.deployment_var("Environment", "prod")
  	.build())
  ```
</Tabs>

Adding multiple queries

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true });

  const prompt = await maxim.getPrompt(
  	"prompt-id",
  	new QueryBuilder().and().deploymentVar("Environment", "prod").deploymentVar("CustomerId", "123").build(),
  );
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  prompt = maxim.get_prompt("prompt-id",
  	QueryBuilder()
  		.and_()
  		.deployment_var("Environment", "prod")
  		.deployment_var("CustomerId", "123")
  		.build())
  ```
</Tabs>

Adding filters based on Tags

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true});

  const prompt = await maxim.getPrompt(
  	"prompt-id",
  	new QueryBuilder().and().deploymentVar("Environment", "prod").tag("TenantId", "3000").build(),
  );
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  prompt = maxim.get_prompt("prompt-id",
  	QueryBuilder()
  		.and_()
  		.deployment_var("Environment", "prod")
  		.tag("TenantId", "3000")
  		.build())
  ```
</Tabs>

## Querying Prompts

Sometimes you have usescases where you need to fetch multiple deployed prompts at once using a single query. For example, you might want to fetch all prompts for a specific customer or specific workflow. You can use `getPrompts` function for this purpose.

<Callout>
  You will need to query using at-least one `deploymentVar` as a filter. Hence you will need to deploy prompt versions before querying them.
</Callout>

### Query deployed prompts using tags

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true });

  const prompts = await maxim.getPrompts(new QueryBuilder().and().deploymentVar("Environment", "prod").tag("CustomerId", "123").build());
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  prompts = maxim.get_prompts(
  	QueryBuilder()
  		.and_()
  		.deployment_var("Environment", "prod")
  		.tag("CustomerId", "123")
  		.build())
  ```
</Tabs>

### Query deployed prompts using folder

To get all prompts from a folder, you can use `getPrompts` function with `folderId` as a query parameter.

#### First capture folder id

There are multiple ways to capture folder id. You can use Maxim dashboard to get folder id.

1. Right click/click on three dots on the folder you want to get id for.
2. Select `Edit Folder` option.
3. You will see folder id in the form.

![Settings Page](/images/docs/folder-id.png)

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true });

  const folder = await maxim.getFolderById("folder-id");
  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  folder = maxim.get_folder_by_id("folder-id")
  ```
</Tabs>

**To get folders using tags attached to the folder.**

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true });

  const folders = await maxim.getFolders(new QueryBuilder().and().tag("CustomerId", "123").build());
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="",prompt_management=True))

  folders = maxim.get_folders(QueryBuilder().and_().tag("CustomerId", "123").build())
  ```
</Tabs>

<Callout>All the rules of prompt matching algorithm apply here. You can use same overriding techniques as explained above.</Callout>

#### Get all deployed prompts from a folder

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true });

  const folder = maxim.getFolderById("folder-id")
  const prompts = await maxim.getPrompts(
  	new QueryBuilder()
  	.and()
  	.folder(folder.id)
  	.deploymentVar("Environment", "prod")
  	.build(),
  );
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  folder = await maxim.get_folder_by_id("folder-id");
  prompts = maxim.get_prompts(
  	QueryBuilder()
  	.and_()
  	.folder(folder.id)
  	.deployment_var("Environment", "prod")
  	.build())
  ```
</Tabs>

You can filter prompts based on deploymentVars or tags attached to the prompt.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true });

  const folder = await maxim.getFolderById("folder-id");
  const prompts = await maxim.getPrompts(
  		new QueryBuilder()
  		.and()
  		.folder(folder.id)
  		.deploymentVar("Environment", "prod")
  		.tag("CustomerId","123")
  		.build());
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  folder = maxim.get_folder_by_id("folder-id")
  prompts = maxim.get_prompts(
  	QueryBuilder()
  	.and_()
  	.folder(folder.id)
  	.deployment_var("Environment", "prod")
  	.tag("CustomerId","123")
  	.build())
  ```
</Tabs>

<Callout>You have to pass at-least one filter along with `folder()`. Either a `deploymentVar` or a `tag`.</Callout>

## Prompt Structure

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  export type Prompt = {
  	promptId: string;
  	version: number;
  	versionId: string;
  	messages: { role: string; content: string }[];
  	modelParameters: { [key: string]: string };
  	provider: string;
  	model: string;
  	tags: { [key: string]: string };
  };
  ```

  ```python tab="Python"
  @dataclass
  class Message():
  	role: str
  	content: str

  @dataclass
  class Prompt():
  	prompt_id: str
  	version: int
  	version_id: str
  	messages: List[Message]
  	model_parameters: Dict[str, Union[str, int, bool, Dict, None]]
  ```
</Tabs>

## Folder Structure

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  export type Folder = {
  	id: string;
  	name: string;
  	parentFolderId?: string;
  	tags: { [key: string]: string };
  };
  ```

  ```python tab="Python"
  @dataclass
  class Folder():
  	id: str
  	name: str
  	parent_folder_id: str
  	tags: Optional[Dict[str, Union[str, int, bool, None]]] = None
  ```
</Tabs>

### Using your own cache for prompts

Maxim SDK uses in-memory caching by default. You can use your own caching implementation by passing a custom cache object to the SDK. This allows you to remove complete dependency on our backend.

#### Interface for custom cache

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  export interface MaximCache {
  	getAllKeys(): Promise<string[]>;
  	get(key: string): Promise<string | null>;
  	set(key: string, value: string): Promise<void>;
  	delete(key: string): Promise<void>;
  }
  ```

  ```python tab="Python"
  class MaximInMemoryCache():
  	def get_all_keys(self) -> List[str]:
  		pass

  	def get(self, key: str) -> Optional[str]:
  		pass

  	def set(self, key: str, value: str) -> None:
  		pass

  	def delete(self, key: str) -> None:
  		pass
  ```
</Tabs>

You will have to pass this custom cache object to the SDK while initializing it.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim } from "@maximai/maxim-js";

  const maxim = new Maxim({ apiKey: "api-key", promptManagement: true, cache: new CustomCache() });
  ```

  ```python tab="Python"
  from maxim import Maxim, Config

  maxim = Maxim(Config(api_key=apiKey, prompt_management=True, cache=CustomCache()))
  ```
</Tabs>

#### Example

Here is the default in-memory cache implementation used by the SDK.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  export class MaximInMemoryCache implements MaximCache {
  	private cache: Map<string, string> = new Map();

  	getAllKeys(): Promise<string[]> {
  		return Promise.resolve(Array.from(this.cache.keys()));
  	}

  	get(key: string): Promise<string | null> {
  		return Promise.resolve(this.cache.get(key) || null);
  	}
  	set(key: string, value: string): Promise<void> {
  		this.cache.set(key, value);
  		return Promise.resolve();
  	}
  }
  ```

  ```python tab="Python"
  class MaximInMemoryCache():
  	def __init__(self):
  		self.cache = {}

  	def get_all_keys(self) -> List[str]:
  		return list(self.cache.keys())

  	def get(self, key: str) -> Optional[str]:
  		return self.cache.get(key)

  	def set(self, key: str, value: str) -> None:
  		self.cache[key] = value

  	def delete(self, key: str) -> None:
  		if key in self.cache:
  			del self.cache[key]
  ```
</Tabs>

## Matching algorithm

Before going into the details of how to use the SDK, let's understand how the matching algorithm works. Maxim SDK uses best matching entity algorithm.

1. Let's assume that, you have asked for a prompt with deployment var `env` as `prod`, `customerId` as `"123"` and a tag, `tenantId` as `456` for `promptId` - `"abc"`.
2. SDK will first try to find a prompt matching all conditions.
3. **If we don't find any matching entity, we enforce only `deploymentVar` conditions (you can override this behaviour, as explained in the next section) and match as many tags as possible.**
4. If we still don't find any prompt, we check for a prompt version marked as fallback.
5. If we still don't find any prompt, we return `null`.

## Overriding fallback algorithm

1. You can override fallback algorithm by calling `.exactMatch()` on `QueryBuilder` object. That will enforce all conditions to be matched.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true });

  const prompt = await maxim.getPrompt(
  	"prompt-id",
  	new QueryBuilder().and().deploymentVar("Environment", "prod").tag("CustomerId", "123").exactMatch().build(),
  );
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  prompt = maxim.get_prompt("prompt-id", QueryBuilder().and_()
  		.deployment_var("Environment", "prod")
  		.tag("CustomerId","123")
  		.exact_match()
  		.build())
  ```
</Tabs>

2. You can override fallback algorithm at each variable level. The third optional parameter in `deploymentVar` & `tag` function is `enforce`. If you pass `true` to this parameter, it will enforce exact match for that variable.

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  import { Maxim, QueryBuilder } from "@maximai/maxim-js;

  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "", promptManagement: true });

  const prompt = await maxim.getPrompt(
  	"prompt-id",
  	new QueryBuilder().and().deploymentVar("Environment", "prod").tag("CustomerId", "123", true).build(),
  );
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.models import QueryBuilder

  # [!code word:api_key]
  maxim = Maxim(Config(api_key="", prompt_management=True))

  prompt = maxim.get_prompt("prompt-id", QueryBuilder().and_()
  		.deployment_var("Environment", "prod")
  		.tag("CustomerId", "123", true)
  		.exact_match()
  		.build())
  ```
</Tabs>


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/rag-quality
meta: {
  "title": "Measure the quality of your RAG pipeline",
  "description": "Retrieval quality directly impacts the quality of output from your AI application. While testing prompts, Maxim allows you to connect your RAG pipeline via a simple API endpoint and evaluates the retrieved context for every run. Context specific evaluators for precision, recall and relevance make it easy to see where retrieval quality is low."
}

## Fetch retrieved context while running prompts

To mimic the real output that your users would see when sending a query, it is necessary to consider what context is being retrieved and fed to the LLM. To make this easier in Maxim's playground, we allow you to attach the Context Source and fetch the relevant chunks. Follow the steps given below to use context in the prompt playground.

<Steps>
  <Step>
    Create a new [Context Source](/docs/library/how-to/context-sources/bring-your-rag-via-an-api-endpoint) in the Library of type API.

    ![Create context source](/images/docs/evaluate/how-to/evaluate-prompts/rag-quality/create-context-source.png)
  </Step>

  <Step>
    Set up the API endpoint of your RAG pipeline that provides the response of the final chunks for any given input.

    ![RAG API endpoint](/images/docs/evaluate/how-to/evaluate-prompts/rag-quality/api-endpoint.png)
  </Step>

  <Step>
    Reference a variable `{{context}}` in  your prompt to provide instructions on using this dynamic data.

    ![Variable usage](/images/docs/evaluate/how-to/evaluate-prompts/rag-quality/prompt-with-context-variable.png)
  </Step>

  <Step>
    Connect the Context Source as the dynamic value of the context variable in the variables table.

    ![Variable linking](/images/docs/evaluate/how-to/evaluate-prompts/rag-quality/dynamic-context-link.png)
  </Step>

  <Step>
    Run your prompt to see the retrieved context that is fetched for that input.

    ![Retrieved context](/images/docs/evaluate/how-to/evaluate-prompts/rag-quality/retrieved-context.png)
  </Step>
</Steps>

Test different inputs iteratively and make improvements to your RAG pipeline's performance.

## Evaluate retrieval at scale

While the playground experience allows you to experiment and debug when retrieval is not working well, it is important to do this at scale across multiple inputs and with a set of defined metrics. Follow the steps given below to run a test and evaluate context retrieval.

<Steps>
  <Step>
    Click on test for a prompt that has an attached context (as explained in the previous section).

    ![Test button](/images/docs/evaluate/how-to/evaluate-prompts/rag-quality/test-prompt-with-context.png)
  </Step>

  <Step>
    Select your dataset which has the required inputs.

    ![Dataset selection](/images/docs/evaluate/how-to/evaluate-prompts/rag-quality/select-dataset-context.png)
  </Step>

  <Step>
    For the `context to evaluate`, select the dynamic Context Source

    ![Dataset selection](/images/docs/evaluate/how-to/evaluate-prompts/rag-quality/select-context-evaluate.png)
  </Step>

  <Step>
    Select context specific evaluators - e.g. Context recall, context precision or context relevance and trigger the test

    ![Context evaluators](/images/docs/evaluate/how-to/evaluate-prompts/rag-quality/context-evaluation.png)
  </Step>

  <Step>
    Once the run is complete, the retrieved context column will be filled for all inputs.

    ![Variable linking](/images/docs/evaluate/how-to/evaluate-prompts/rag-quality/run-retrieved-context-column.png)
  </Step>

  <Step>
    View complete details of retrieved chunks by clicking on any entry.

    ![Retrieval details](/images/docs/evaluate/how-to/evaluate-prompts/rag-quality/retrieved-chunks-details.png)
  </Step>

  <Step>
    Evaluator scores and reasoning for every entry can be checked under the `evaluation` tab. Use this to debug retrieval issues.

    ![Evaluator reasoning](/images/docs/evaluate/how-to/evaluate-prompts/rag-quality/context-evaluation-reasoning.png)
  </Step>
</Steps>

By running experiments iteratively as you are making changes to your AI application, you can check for any regressions in the retrieval pipeline and continue to test for new test cases.


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/run-prompt-tool-calls
meta: {
  "title": "Run a Prompt with tool calls",
  "description": "Ensuring your prompt selects the accurate tool call (function) is crucial for building reliable and efficient AI workflows. Maxim's playground allows you to attach your tools (API, code or schema) and measure tool call accuracy for agentic systems."
}

<video url="https://www.youtube.com/embed/LGItsF0y5qk" />

Tool call usage is a core part of any agentic AI workflow. Maxim's playground allows you to effectively test if the right tools and are being chosen by the LLM and if they are getting successfully executed.

In Maxim, you can create [prompt tools](/docs/library/how-to/prompt-tools/create-a-code-tool) within the `library` section of your workspace. These could be executable or just the schema and then attached to your prompt for testing.

## Attach and run your tools in playground

<Steps>
  <Step>
    Create a new tool in the library. Use an API or code for executable tools and schema if you only want to test tool choice.

    ![Create Prompt tool](/images/docs/evaluate/how-to/evaluate-prompts/run-tool-call/create-tool.png)
  </Step>

  <Step>
    Select and attach tools to the prompt within the configuration section.

    ![Attach Prompt Tool](/images/docs/evaluate/how-to/evaluate-prompts/run-tool-call/attach-tool.png)
  </Step>

  <Step>
    Send your prompt referencing the tool usage instructions.

    ![Prompt with tool instructions](/images/docs/evaluate/how-to/evaluate-prompts/run-tool-call/prompt-tool-instruction.png)
  </Step>

  <Step>
    Check the assistant response with tool choice and arguments.

    ![Assistant message of tool choice](/images/docs/evaluate/how-to/evaluate-prompts/run-tool-call/tool-details.png)
  </Step>

  <Step>
    For executable tools, check the tool response message that is shown post execution.

    ![Tool message](/images/docs/evaluate/how-to/evaluate-prompts/run-tool-call/tool-response.png)
  </Step>

  <Step>
    Edit tool type messages manually to test for different responses.

    ![Tool message edit](/images/docs/evaluate/how-to/evaluate-prompts/run-tool-call/manual-tool-message.png)
  </Step>
</Steps>

By experimenting in the playground, you can now make sure your prompt is calling the right tools in specific scenarios and that the execution of the tool leads to the right responses.

To test tool call accuracy at scale across all your use cases, run experiments using a dataset and evaluators as shown below.

## Measure tool call accuracy across your test cases

<Steps>
  <Step>
    Set up your dataset with `input` and `expected tool calls` columns.

    ![Dataset creation](/images/docs/evaluate/how-to/evaluate-prompts/run-tool-call/create-dataset-tool.png)
  </Step>

  <Step>
    For each input, add the JSON of one or more expected tool calls and arguments you expect from the assistant.

    ![Dataset example](/images/docs/evaluate/how-to/evaluate-prompts/run-tool-call/dataset-tool.png)
  </Step>

  <Step>
    Trigger a test on the prompt which has the tools attached.

    ![Trigger test](/images/docs/evaluate/how-to/evaluate-prompts/run-tool-call/prompt-with-tools-test.png)
  </Step>

  <Step>
    Select your dataset from the dropdown.

    ![Prompt with tool instructions](/images/docs/evaluate/how-to/evaluate-prompts/run-tool-call/select-dataset-with-tools.png)
  </Step>

  <Step>
    Select the tool call accuracy evaluator under statistical evaluators and trigger the run. Add from evaluator store if not available in your workspace.

    ![Tool call accuracy evaluator](/images/docs/evaluate/how-to/evaluate-prompts/run-tool-call/tool-call-accuracy-eval.png)
  </Step>

  <Step>
    Once the test run is completed, the tool call accuracy scores will be 0 or 1 based on assistant output.
  </Step>

  <Step>
    To check details of the messages click on any entry and click on the `messages` tab.

    ![Messages including tool](/images/docs/evaluate/how-to/evaluate-prompts/run-tool-call/message-details-with-tool.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/save-prompt-session
meta: {
  "title": "Save and track Prompt experiments with sessions",
  "description": "Sessions act as a history by saving your prompt's complete state as you work. This allows you to experiment freely without fear of losing your progress."
}

## Create prompt sessions

* A session saves all parts of the playground state including variable values, conversation messages etc. When leaving without saving, you'll receive a prompt to save or discard changes.

* Save an ongoing session using the `save session` button in the prompt header. Unsaved sessions are marked with a red asterisk beside their name. Each saved session includes a timestamp and creator details.

![Prompt header](/images/docs/evaluate/how-to/evaluate-prompts/save-sessions/save-session.png)

* View the list of recent sessions by clicking on the arrow next to the `save session` button and see complete list using the button at the bottom of this list.

![Sessions list](/images/docs/evaluate/how-to/evaluate-prompts/save-sessions/sessions-list.png)

* To make organization and recall easier, tag your sessions by clicking the `tag` icon that shows on hover of a session list item. Add a name that provides information to other team members about the use of that session.

![Tag a session](/images/docs/evaluate/how-to/evaluate-prompts/save-sessions/tag-session.png)

<Callout>
  Save sessions quickly using Cmd+S ( Mac) or Ctrl+S (Windows/Linux).
</Callout>

## Next steps

* [Create and manage prompt versions](/docs/evaluate/how-to/evaluate-prompts/create-prompt-versions)
* [Run experiments on prompts](/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases)


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-prompts/use-prompt-partials
meta: {
  "title": "Use Prompt partials in your Prompts",
  "description": "Learn how to use Prompt partials within your Prompts"
}

![Using Prompt partials in the Prompt playground](/images/docs/evaluate/how-to/evaluate-prompts/prompt-partials/prompt-playground-with-partials.png)

<Steps>
  <Step>Create a new Prompt</Step>

  <Step>
    Add Prompt partials to your messages

    1. Type `{{` in either System or User message
    2. Select a Prompt partial from the dropdown list
    3. To use a specific version, continue typing after selecting the partial
    4. Choose the version from the list showing creation date and author

    <Callout type="info">The latest version is used by default if no specific version is selected</Callout>
  </Step>

  <Step>
    Configure variables and preview partials

    * Variables used in partials automatically appear in the Prompt variables section
    * Add variable values to test in the playground
    * Click on the partial content in variables view to:
      * Preview the partial content
      * Navigate directly to the partial editor

    ![Variables view with Prompt partials](/images/docs/evaluate/how-to/evaluate-prompts/prompt-partials/prompt-partial-preview.png)

    <Callout type="info">
      Partials are replaced with actual content during test runs using values from your test configuration or context sources
    </Callout>
  </Step>
</Steps>


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd
meta: {
  "title": "Automate workflow evaluation via CI/CD",
  "description": "Trigger test runs in CI/CD pipelines to evaluate workflows automatically."
}

<Callout>
  The following builds upon [Evaluate Prompts -> Automate prompt evaluation via CI/CD](/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd). Please refer to it if you haven't already.
</Callout>

## Pre-requisites

Apart from the pre-requisites mentioned in [Evaluate Prompts -> Automate prompt evaluation via CI/CD](/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd), you also need [**A workflow to test upon**](/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-your-ai-outputs-using-application-endpoint)

Pre-requisites mentioned earlier that you need:

1. [An API key from Maxim](/docs/observe/quickstart#2-generate-api-key)
2. [A dataset to test against](/docs/library/how-to/datasets/use-dataset-templates)
3. [Evaluators to evaluate the workflow against the dataset](/docs/library/how-to/evaluators/use-pre-built-evaluators)
4. and [**A workflow to test upon**](/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-your-ai-outputs-using-application-endpoint)

## Test runs via CLI

Apart from what was introduced earlier, you can use the `-w` flag in place of `-p` to specify the workflow to test upon.

### Installation

Use the following command template to install the CLI tool (if you are using Windows, please refer to the Windows example as well):

<Tabs items={["Command template", "Linux amd64 example", "Windows example"]}>
  ```bash tab="Command template"
  wget https://downloads.getmaxim.ai/cli/<VERSION>/<OS>/<ARCH>/maxim
  ```

  ```bash tab="Linux amd64 example"
  wget https://downloads.getmaxim.ai/cli/v1/linux/amd64/maxim
  ```

  ```powershell tab="Windows example"
  # Use `.exe` extension for windows downloads.
  wget https://downloads.getmaxim.ai/cli/v1/windows/amd64/maxim.exe
  ```
</Tabs>

For more please refer to [Evaluate Prompts -> Automate prompt evaluation via CI/CD -> Test runs via CLI](/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd#test-runs-via-cli)

### Triggering a test run

Use this template to trigger a test run:

```bash title="Shell command to trigger a test run"
# If you haven't added the binary to your PATH,
# replace `maxim` with the path to the binary you just downloaded
maxim test -w <workflow_id> -d <dataset_id> -e <comma_separated_evaluator_names>
```

Here are the arguments/flags that you can pass to the CLI to configure your test run

| Argument / Flag | Description                                                                                           |
| --------------- | ----------------------------------------------------------------------------------------------------- |
| -w              | Workflow ID or IDs; in case you send multiple IDs (comma separated), it will create a comparison run. |
| -d              | Dataset ID                                                                                            |
| -e              | Comma separated evaluator names <br /> Ex. `bias,clarity`                                             |
| --json          | (optional) Output the result in JSON format                                                           |

## Test runs via GitHub Action

Apart from what was introduced earlier, you can use the `workflow_id` "**with parameter**" in place of `prompt_version_id` to specify the workflow to test upon.

### Quick Start

In order to add the GitHub Action to your workflow, you can start by adding a step that uses `maximhq/actions/test-runs@v1` as follows:

<Callout type="warn">
  Please ensure that you have the following setup:

  * in GitHub action **secrets**
    * MAXIM\_API\_KEY
  * in GitHub action **variables**
    * WORKSPACE\_ID
    * DATASET\_ID
    * **WORKFLOW\_ID**
</Callout>

```yaml title=".github/workflows/test-runs.yml"
name: Run Test Runs with Maxim

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  TEST_RUN_NAME: "Test Run via GitHub Action"
  CONTEXT_TO_EVALUATE: "context"
  EVALUATORS: "bias, clarity, faithfulness"

jobs:
  test_run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v2
      - name: Running Test Run
        id: test_run
        uses: maximhq/actions/test-runs@v1
        with:
          api_key: ${{ secrets.MAXIM_API_KEY }}
          workflow_id: ${{ vars.WORKFLOW_ID }}
          test_run_name: ${{ env.TEST_RUN_NAME }}
          dataset_id: ${{ vars.DATASET_ID }}
          workflow_id: ${{ vars.WORKFLOW_ID }}
          context_to_evaluate: ${{ env.CONTEXT_TO_EVALUATE }}
          evaluators: ${{ env.EVALUATORS }}
      - name: Display Test Run Results
        if: success()
        run: |
          printf '%s\n' '${{ steps.test_run.outputs.test_run_result }}'
          printf '%s\n' '${{ steps.test_run.outputs.test_run_failed_indices }}'
          echo 'Test Run Report URL: ${{ steps.test_run.outputs.test_run_report_url }}'
```

This will trigger a test run on the platform and wait for it to complete before proceeding. The progress of the test run will be displayed in the **Running Test Run** section of the GitHub Action's logs as displayed below:

![GitHub Action Running Test Run Logs](/images/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/automate-via-ci-cd/github-action-running-test-run-logs.png)

### Inputs

The following are the inputs that can be used to configure the GitHub Action:

| Name                            | Description                                                                                                                                                                                                         | Required                                                         |
| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| `api_key`                       | Maxim API key                                                                                                                                                                                                       | Yes                                                              |
| `workspace_id`                  | Workspace ID to run the test run in                                                                                                                                                                                 | Yes                                                              |
| `test_run_name`                 | Name of the test run                                                                                                                                                                                                | Yes                                                              |
| `dataset_id`                    | Dataset ID for the test run                                                                                                                                                                                         | Yes                                                              |
| `workflow_id`                   | Workflow ID to run for the test run (do not use with `prompt_version_id`)                                                                                                                                           | Yes (No if `prompt_version_id` is provided)                      |
| `prompt_version_id`             | Prompt version ID to run for the test run <br />(discussed in [Evaluate Prompts -> Automate prompt evaluation via CI/CD](/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd), do not use with `workflow_id`) | Yes (No if `workflow_id` is provided)                            |
| `context_to_evaluate`           | Variable name to evaluate; could be any variable used in the workflow / prompt or a column name                                                                                                                     | No                                                               |
| `evaluators`                    | Comma separated list of evaluator names                                                                                                                                                                             | No                                                               |
| `human_evaluation_emails`       | Comma separated list of emails to send human evaluations to                                                                                                                                                         | No (required in case there is a human evaluator in `evaluators`) |
| `human_evaluation_instructions` | Overall instructions for human evaluators                                                                                                                                                                           | No                                                               |
| `concurrency`                   | Maximum number of concurrent test run entries running                                                                                                                                                               | No (defaults to 10)                                              |
| `timeout_in_minutes`            | Fail if test run overall takes longer than this many minutes                                                                                                                                                        | No (defaults to 15 minutes)                                      |

### Outputs

The outputs that are provided by the GitHub Action in case it doesn't fail are:

| Name                      | Description                        |
| ------------------------- | ---------------------------------- |
| `test_run_result`         | Result of the test run             |
| `test_run_report_url`     | URL of the test run report         |
| `test_run_failed_indices` | Indices of failed test run entries |

## Evaluating Prompts

Please refer to [Evaluate Prompts -> Automate prompt evaluation via CI/CD](/docs/evaluate/how-to/evaluate-prompts/automate-via-ci-cd)


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/create-workflow-for-local-api
meta: {
  "title": "Test your local AI endpoint",
  "description": "Learn how to test your local AI endpoint using Maxim's Workflows."
}

## Getting Started with Local Testing

Want to evaluate your AI application but it's running locally? No problem! We'll help you connect your local AI app to Maxim for testing.

<Steps>
  <Step>
    ### Create Your local AI Endpoint

    Create a simple Flask API that implements RAG (Retrieval-Augmented Generation) - a technique that enhances AI responses with relevant context from your data:

    ```python title="Example Flask API for RAG implementation"
    from flask import Flask, request

    app = Flask(__name__)

    @app.route('/rag', methods=['POST'])
    def rag_output():
        # Get the query from the request
        body = request.get_json()
        print(body)
        output = runPrompt(body['query'])
        response = {'response':output}
        return response

    # main driver function
    if __name__ == '__main__':
        # run() method of Flask class runs the application
        # on the local development server.
        app.run()
    ```

    This example shows a RAG application using Google's PaLM model with "Harry Potter" content. The endpoint runs at `http://localhost:5000/rag`.
  </Step>

  <Step>
    ### Set up Ngrok

    To make your local API accessible to Maxim, we'll use Ngrok - a secure tunneling tool that creates a public URL for your local server.

    Follow these steps on MacOS:

    ```bash title="Install and configure Ngrok"
    # Install Ngrok
    brew install ngrok/ngrok/ngrok

    # Set up your auth token (you'll need to sign up at ngrok.com)
    ngrok config add-authtoken your_token

    # Create a secure tunnel to your local server
    ngrok http 5000
    ```

    You can read about ngrok setup for other operating systems [here](https://ngrok.com/docs/getting-started).
  </Step>

  <Step>
    ### Connect to Maxim

    Now that your API is accessible, let's connect it to Maxim:

    1. Copy your Ngrok URL (looks like `https://xxxx-xx-xx-xxx-xx.ngrok.io`) and navigate to [Workflows](/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-your-ai-outputs-using-application-endpoint)
    2. Add your Ngrok URL as the API endpoint
    3. Start testing your AI responses

    <Callout type="info">
      Remember to keep both your Flask server and Ngrok running during testing. Once you stop Ngrok, you'll need a new URL for your next testing session.
    </Callout>
  </Step>
</Steps>


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/evaluate-simulated-sessions-for-agents
meta: {
  "title": "Evaluate simulated sessions for agents",
  "description": "Learn how to evaluate your AI agent's performance using automated simulated conversations. Get insights into how well your agent handles different scenarios and user interactions."
}

Follow these steps to test your AI agent with simulated sessions:

<Steps>
  <Step>
    ### Create a Dataset for testing

    * Configure the agent dataset template with:
    * **Agent scenarios**: Define specific situations for testing (e.g., "Update address", "Order an iPhone")
    * **Expected steps**: List expected actions and responses

    ![Agent Dataset template](/images/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/evaluate-simulated-sessions-for-agents/agent-dataset-template.png)

    ![Agent Dataset sample data](/images/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/evaluate-simulated-sessions-for-agents/agent-dataset-sample.png)
  </Step>

  <Step>
    ### Set up the Test Run

    * Navigate to your workflow, click "Test", and select "Simulated session" mode
    * Pick your agent dataset from the dropdown
    * Configure additional parameters like persona, tools, and context sources
    * Enable relevant evaluators

    ![Configure simulation Test Run](/images/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/evaluate-simulated-sessions-for-agents/agent-testrun-setup.png)
  </Step>

  <Step>
    ### Execute Test Run

    * Click "Trigger test run" to begin
    * The system simulates conversations for each scenario
  </Step>

  <Step>
    ### Review results

    * Each session runs end-to-end for thorough evaluation
    * You'll see detailed results for every scenario

    ![Simulation Test Run result](/images/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/evaluate-simulated-sessions-for-agents/simulation-testrun-result.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/scripting-to-configure-response-structures
meta: {
  "title": "Transform API data with Workflow scripts",
  "description": "Customize your API requests and responses using Workflow scripts"
}

Maxim Workflows provide powerful scripting capabilities to modify requests and responses. These scripts help you transform data, add authentication, and handle complex response structures.

<Callout type="info">Scripts are optional - you'll only need them when you want to modify your API's behavior.</Callout>

### Pre-request Script

The `prescript` function runs before sending requests to your API. Use it to modify request parameters, add headers, or transform data.

```javascript
function prescript(request) {
	// Add an authorization header
	request.headers["Authorization"] = "Bearer your-token";

	// Modify the request body
	request.data = {
		...JSON.parse(request.data || "{}"),
		timestamp: Date.now(),
	};

	return request;
}
```

### Post-response Script

The `postscriptV2` function processes API responses before displaying them. Use it to transform response data or extract specific fields:

```javascript
function postscriptV2(response, request) {
	// Extract only the 'message' field from response
	return {
		content: response.data.message,
		confidence: response.data.metadata?.confidence || 1.0,
	};
}
```

### Simulation Scripts

<Callout type="info">Simulation scripts only run for multi-turn conversations. They won't execute for single-turn tests.</Callout>

Use these scripts to set up and clean up multi-turn conversation tests:

```javascript
// Initialize conversation context
function preSimulation() {
	return {
		sessionId: generateUUID(),
		startTime: Date.now(),
	};
}

// Clean up after simulation
function postSimulation() {
	return {
		status: "completed",
		endTime: Date.now(),
	};
}
```

## Supported models

<Callout type="info">
  You don't need to use `require` or `import` to use these modules in scripts. These are directly available in the script environment.
</Callout>

* `axios`: Axios is a popular HTTP client library for JavaScript. It provides a simple API for making HTTP requests and supports various features like request and response interceptors, request cancellation, and automatic JSON parsing.


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/simulate-multi-turn-conversations
meta: {
  "title": "Simulate multi-turn conversations",
  "description": "Test your AI's conversational abilities with realistic, scenario-based simulations"
}

## Why simulate conversations?

Testing AI conversations manually is time-consuming and often misses edge cases. It helps you:

* Test how your AI maintains context across multiple exchanges
* Evaluate responses to different user emotions and behaviors
* Verify proper use of business context and policies
* Identify potential conversation dead-ends

## 1. Create a realistic scenario and be specific about the situation you want to test

* Customer requesting refund for a defective laptop
* New user needs help configuring account security settings
* Customer confused about unexpected charges on their bill

## 2. Define the user persona

* Frustrated customer seeking refund
* New user needing security help
* Confused customer with billing issues

<Callout type="info">
  Mix different emotional states and expertise levels to test how your agent adapts its communication style.
</Callout>

After defining the user persona, select the field where your agent's replies come from:

![Response configuration](/images/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/simulate-multi-turn-conversations/simulation-config-form.png)

## 3. Advanced settings (optional)

* **Maximum number of turns:** Set a limit for conversation turns. If no value's set, the simulation ends when complete
* **Reference tools**: Attach any tools you want to test with the simulation. You can learn more about setting up tools [here](/docs/library/how-to/prompt-tools/create-a-code-tool)
* **Reference context**: Add context sources to enhance conversations. Learn more [here](/docs/library/how-to/context-sources/bring-your-rag-via-an-api-endpoint)

![Workflow simulation - advanced settings](/images/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/simulate-multi-turn-conversations/workflow-simulation-advanced-settings.png)

## Example simulation

Here's a real-world example of a simulated conversation:

![Live simulation](/images/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/simulate-multi-turn-conversations/workflow-simulated-agentic-conversation.png)

<Callout type="info">
  This tests a refund scenario where:

  * Customer needs refund for defective product
  * Agent verifies purchase
  * Policy guides the process
  * Must resolve in 5 turns
</Callout>


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-multi-turn-conversations-manually
meta: {
  "title": "Test multi-turn conversations manually",
  "description": "Learn how to test and simulate multi-turn conversations with your AI endpoint using Maxim's interactive Workflows"
}

## Why do we need to test multi-turn conversations?

Real conversations create fascinating puzzles because:

* Testing single responses doesn't reveal the complete interaction pattern
* Just like human conversations, AI chats can take unexpected turns
* When something goes wrong, you need to replay the conversation - but what if you could change history?

These intriguing challenges make it crucial to test your AI's conversational abilities thoroughly before it faces real users.
Maxim solves this with an interactive Messages panel that lets you simulate, manipulate, and debug multi-turn conversations in real-time. Bring your application endpoint to create and test multi-turn conversations without any code integration.

## Configure your endpoint

Before testing conversations, you need to configure your endpoint:

1. Enter your AI endpoint URL (e.g., `https://astronomy-ai.example.com/chat`)
2. Configure the request body
   ```json
   {
     "query": "{{input}}"
   }
   ```
   Your application receives and processes messages correctly with this configuration.

## Start a conversation

1. Type your initial message in the input field
2. Click Send to start the conversation

## Edit and modify conversations

You can manipulate the conversation to test different scenarios:

* **Delete Messages**: Remove any message from the conversation history to test how your AI handles modified contexts
* **Edit History**: Change previous messages to simulate different conversation paths

![Manual simulation](/images/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-multi-turn-conversations-manually/manual-multi-turn-simulation.png)

## Example usage

Here's a typical workflow for testing multi-turn conversations:

1. Start with a simple query:
   ```
   User: "How old is the universe?"
   AI: "The universe is estimated to be around 13.8 billion years old..."
   ```

2. Follow up with related questions:
   ```
   User: "What's the Big Bang theory?"
   AI: "The Big Bang theory explains the origin of the universe..."
   ```

By using the Messages panel effectively, you can ensure your AI endpoint handles multi-turn conversations reliably and maintains appropriate context throughout the interaction.


url: https://getmaxim.ai/docs/evaluate/how-to/evaluate-workflows-via-api-endpoint/test-your-ai-outputs-using-application-endpoint
meta: {
  "title": "Test your AI application using an API endpoint",
  "description": "Expose your AI application to Maxim using your existing API endpoint"
}

## Create a Workflow for a public endpoint

Connect your application's API endpoint to Maxim. Enter your API URL and add any necessary headers and parameters.

Configure the payload for your API request. Include all data your backend needs to process requests. When running tests, attach a dataset to your workflow.

Reference column values from your dataset using `{{column_name}}` variables. These resolve during test runtime. Use variables in headers and parameters for flexible workflow configuration.

See it in action:

<video url="https://drive.google.com/file/d/12c3B1TWifq4cAa-43lF2enCLoN7PO0JX/preview" />

## Test your Workflow

Send messages to your API from the **Messages** panel to test your endpoint with a conversational experience. See this demonstrated at the end of the video above.

## Map the output for evaluation

Before running tests, tell us what part of your response to evaluate by mapping an `output` from the response payload.

Click the `Test` button in the top right corner to open the **Test run** configuration panel. Select your **Output** from the dropdown of mappable response fields. View the full response payload by clicking `Show response`. Optionally, map the **Context to Evaluate** field using the **Context field** selector.

See how to map outputs for evaluation:

<video url="https://drive.google.com/file/d/1gQz4zF_Mmy-l10pyOStM-8wuxTbW6ms4/preview" />

<Callout type="warn">
  **Note**

  * The `Test` button remains disabled until you send messages to your endpoint. The system requires a response payload structure for `output` mapping.
  * When mapping without triggering a test run, save your workflow explicitly. Map in the configuration sheet, click outside to close it, then click **Save workflow**
</Callout>


url: https://getmaxim.ai/docs/evaluate/how-to/optimize-evaluation-processes/customize-share-reports
meta: {
  "title": "Customize and share reports",
  "description": "The run report is a single source of truth for you to understand exactly how your AI system is performing during your experiments or pre-release testing. You can customize reports to gain insights and make decisions."
}

## Toggle columns

For prompt/workflow runs, by default we only show the input from the dataset and the retrieved context (if applicable) and output from the run. However, there might be cases where you want to see other dataset columns to analyze the output. Similarly, you may want to hide some already visible columns in order to see limited data while analyzing evaluations. To show/hide columns, follow the below steps:

<Steps>
  <Step>
    On the run report table header, click the `Toggle columns` button
  </Step>

  <Step>
    Clicking on this will open a dropdown with options of all columns from the dataset and the run result
  </Step>

  <Step>
    Select columns that you want to be visible. Use search if you have a lot of columns.

    ![Toggle columns](/images/docs/evaluate/how-to/optimize-evaluation-processes/customize-report/toggle-columns.png)
  </Step>

  <Step>
    You can also do this via the column header cell by hovering and clicking the `three dot button > Hide column`

    ![Hide column](/images/docs/evaluate/how-to/optimize-evaluation-processes/customize-report/hide-column.png)
  </Step>
</Steps>

## Pinning columns

While analyzing the report, you can horizontally scroll to see all columns. In case you have certain columns that you would want to always have as reference while looking at rest of the data, you can pin them. Eg. Pin inputs columns to the left and a certain evaluation column to the right while analyzing the retrieved context, output, etc.

To pin a column, click the 3 dots buttons on the row header and choose `Pin to left` or `Pin to right`. You can also unpin the column in the same way.

![Pin column](/images/docs/evaluate/how-to/optimize-evaluation-processes/customize-report/pin-column.png)

## Re-ordering columns

You can easily re-order all columns of the table by holding down the button shown in the below screenshot and dragging the column

![Re-order column](/images/docs/evaluate/how-to/optimize-evaluation-processes/customize-report/re-order-column.png)

## Search and filter

In case of large reports with a lot of entries, you can use the search or filters to easily reach the relevant entries you care about. Filtering allows you to put a combination of criteria. These could be performance metrics or evaluation scores.

You can also directly filter out the results that are failing on a particular metric by clicking the filter icon next to its score in the summary card.

![Filter table](/images/docs/evaluate/how-to/optimize-evaluation-processes/customize-report/filter-table.png)

![Search table](/images/docs/evaluate/how-to/optimize-evaluation-processes/customize-report/search-table.png)

## Share links

Share results of runs with external stakeholders via a read-only link that can be accessed without being on the Maxim dashboard. Click the `share report` button on the header of any run report, and a link to the current view will be copied to your clipboard.

![Shared report](/images/docs/evaluate/how-to/optimize-evaluation-processes/customize-report/share-report.png)


url: https://getmaxim.ai/docs/evaluate/how-to/optimize-evaluation-processes/re-use-configuration-via-presets
meta: {
  "title": "Re-use your test configurations using presets",
  "description": "As your team starts running tests regularly on your entities, make it simple and quick to configure tests and see results. Test presets are a way to help you reuse your configurations with a single click, reducing the time it takes to start a run. You can create labeled presets combining a dataset and evaluators and use them with any entity you want to test."
}

To create and use presets, follow the steps below:

<Steps>
  <Step>
    On the test configuration panel of any entity, after selecting the dataset and evaluators, click the ‘Save as preset’ button at the bottom of the panel. You can also add the context to be evaluated to the preset.

    ![Test presets](/images/docs/evaluate/how-to/optimize-evaluation-processes/presets/save-as-preset.png)
  </Step>

  <Step>
    Enter a preset name, add an optional description, and review your selections before saving.

    ![Preset configuration](/images/docs/evaluate/how-to/optimize-evaluation-processes/presets/preset-config.png)
  </Step>

  <Step>
    The next time you are configuring a test run, switch to the presets tab instead of custom and choose from available presets to pre-fill.

    ![Preset tab](/images/docs/evaluate/how-to/optimize-evaluation-processes/presets/presets-tab.png)
  </Step>

  <Step>
    Filter presets by prompt type or workspace entities

    ![Preset Lists](/images/docs/evaluate/how-to/optimize-evaluation-processes/presets/preset-scopes.png)
  </Step>

  <Step>
    Edit or delete a preset easily from the buttons within the preset card header.

    ![Modify a preset](/images/docs/evaluate/how-to/optimize-evaluation-processes/presets/modify-preset.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/evaluate/how-to/optimize-evaluation-processes/receive-notifications-test-runs
meta: {
  "title": "Receive notifications for Test Run status",
  "description": "Test runs are a core part of continuous testing workflows and could be triggered via UI or in the CI/CD pipeline. Teams need visibility into triggered runs, status updates, and result summaries without having to come to the dashboard to constantly check. Integrations with Slack and PagerDuty allow notifications to be configured for some of these events."
}

Set up notifications that will be triggered based on the status of a test run. These could be linked to Slack or PagerDuty and will send a message to the defined channel.

<Steps>
  <Step>
    Connect [Slack](/docs/observe/how-to/set-up-alerts/create-a-slack-integration) and [PagerDuty](/docs/observe/how-to/set-up-alerts/create-a-pagerduty-integration) via Settings > Integrations. Use these integrations for both log alerts and notifications.

    ![Integrations](/images/docs/evaluate/how-to/optimize-evaluation-processes/test-run-notifications/integrations.png)
  </Step>

  <Step>
    To set up notifications for test run status, go to `Settings > Notifications`.

    ![Set up notifications](/images/docs/evaluate/how-to/optimize-evaluation-processes/test-run-notifications/notifications.png)
  </Step>

  <Step>
    Click the `configure` button under the `test run status` section.

    ![Configure notifications](/images/docs/evaluate/how-to/optimize-evaluation-processes/test-run-notifications/configure-notifications.png)
  </Step>

  <Step>
    Select a channel from your configured integrations list that are already set up.

    ![Select channels](/images/docs/evaluate/how-to/optimize-evaluation-processes/test-run-notifications/select-channel.png)
  </Step>

  <Step>
    Select the statuses on which you would want to trigger a notification. By default, notifications trigger automatically for `Completed` or `Failed` status.

    ![Select statuses](/images/docs/evaluate/how-to/optimize-evaluation-processes/test-run-notifications/select-statuses.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/library/how-to/context-sources/bring-your-rag-via-an-api-endpoint
meta: {
  "title": "Bring your RAG via an API endpoint",
  "description": "To integrate RAG context into Maxim, you need to create a context source and add your RAG context API endpoint. This context source can then be used in prompts and workflows for inferencing, enabling the model to access and utilize the relevant context during processing."
}

<video url="https://www.youtube.com/embed/VXdRMXyKfYw" />

<Steps>
  <Step>
    You can create a new context source by clicking on the left navigation and then the plus icon.
  </Step>

  <Step>
    Provide a name for the context source.
  </Step>

  <Step>
    Enter the API endpoint. When we pass a query to this endpoint, it should return the retrieved context for that query as would be the real world scenario.
  </Step>

  <Step>
    Add necessary headers and parameters to your request and save the context source
  </Step>

  <Step>
    You can now use this in prompts and workflows.
  </Step>
</Steps>

<video url="https://drive.google.com/file/d/10K4-9SmTLUxv4ORD4qtbsxu_sOc2YqSM/preview" />


url: https://getmaxim.ai/docs/library/how-to/context-sources/evaluate-your-context
meta: {
  "title": "Evaluate your context",
  "description": "Learn how to evaluate the quality and effectiveness of your RAG context sources in Maxim for improved AI performance and accuracy."
}

Context sources in Maxim allow you to expose your RAG pipeline via a simple endpoint irrespective of the complex steps within it. This context source can then be linked as a variable in your prompt or workflow and selected for evaluation [here](/docs/evaluate/how-to/evaluate-prompts/rag-quality).


url: https://getmaxim.ai/docs/library/how-to/context-sources/ingest-files-as-a-context-source
meta: {
  "title": "Ingest files as a context source",
  "description": "Ingest files as a context source in Maxim to enable RAG context for your GenAI application."
}

Maxim allows you to upload various document formats (.pdf, .docx, .csv, .txt, .md) and automatically creates embeddings for use as context in your applications.

<Steps>
  <Step>
    Navigate to the Context Sources section in your workspace and click on the Plus icon.
  </Step>

  <Step>
    Select "Files" as your context source type.
  </Step>

  <Step>
    Drag and drop your files or click "Browse files" and select files from your system.
    Supported formats:

    * PDF documents (.pdf)
    * Word documents (.docx)
    * CSV files (.csv)
    * Text files (.txt)
    * Markdown files (.md)
  </Step>

  <Step>
    Wait for the files to upload and for embeddings to be generated. The status indicator will show progress.
  </Step>

  <Step>
    Once processing is complete, your files are ready to be used as context. You can now reference this context source in agent simulation.
  </Step>
</Steps>

For information on using context sources in agentic simulations and conversations, see our guide on [using context in prompts and workflows](/docs/evaluate/how-to/evaluate-prompts/rag-quality).


url: https://getmaxim.ai/docs/library/how-to/datasets/add-new-entries-using-sdk
meta: {
  "title": "Add Dataset entries using SDK",
  "description": "Learn how to add new entries to a Dataset using the Maxim SDK"
}

The Maxim SDK provides a convenient way to programmatically add new entries to your datasets. This guide will walk you through the process of using the SDK to add entries, helping you efficiently manage and update your datasets for AI training and evaluation.

## Getting your Dataset ID

Get the Dataset ID from the Maxim dashboard:

1. Click the three dots on your target Dataset
2. Select "Copy ID"
3. Use the copied Dataset ID in your code

## Adding entries to a Dataset

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  // [!code word:apiKey]
  const maxim = new Maxim({ apiKey: "" });

  await maxim.addDatasetEntries("dataset-id", [
      {
          input: {
              type: "text",
              payload: "your content here",
          },
          //optional
          expectedOutput: {
              type: "text",
              payload: "your content here",
          },
          //optional
          context: {
              type: "text",
              payload: "your content here",
          },
      },
  ]);
  ```

  ```python tab="Python"
  from maxim import Maxim, Config
  from maxim.logger import Logger, LoggerConfig

  # [!code word:api_key]
  maxim = Maxim(Config(api_key=""))

  maxim.maxim_api.add_dataset_entries(
      "dataset-id",
      [
          {
              "input": {
                  "type": "text",
                  "payload": "your content here",
              },
              # optional
              "expectedOutput": {
                  "type": "text",
                  "payload": "your content here",
              },
              # optional
              "context": {
                  "type": "text",
                  "payload": "your content here",
              },
          },
      ],
  )
  ```
</Tabs>

<Callout>You can insert a maximum of 100 entries at a time to your Dataset.</Callout>


url: https://getmaxim.ai/docs/library/how-to/datasets/create-dataset-with-files-and-images
meta: {
  "title": "Create a Dataset with images",
  "description": "Learn how to create a Dataset with images"
}

<Steps>
  <Step>
    You can add images to your Dataset by creating a column of type Images. We support both URL and local file paths.

    ![Create dataset with images](/images/docs/library/how-to/datasets/create-dataset-with-images/add-image-url-or-upload.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/library/how-to/datasets/curate-data-from-production
meta: {
  "title": "Curate data from production",
  "description": "Learn how to extract and transform production logs into structured Datasets for model training and evaluation"
}

<Steps>
  <Step>
    Select the logs from your log repository (preferably where you push your production data) and click on the `Add to Dataset` button in the top right corner.

    ![Curate data from production](/images/docs/library/how-to/datasets/curate-data-from-production/select-logs-from-production-for-curation.png)
  </Step>

  <Step>
    Next, you'll see a dialog where you can either choose an existing Dataset or create a new one. Let's create a fresh Dataset for this example.
    You can use one of our templates (we'll use "Dataset testing") or create a custom structure. Click the `Create Dataset` button when ready.

    ![Add curated logs to dataset dialog](/images/docs/library/how-to/datasets/curate-data-from-production/add-curated-logs-to-dataset-dialog.png)
  </Step>

  <Step>
    Now it's time to map your log columns to Dataset columns. In this example, we're mapping the Input field to the Dataset's Input column and Output to the Output column. Once you've set up your mappings, click "Add to Dataset".

    ![Add curated entries to dataset](/images/docs/library/how-to/datasets/curate-data-from-production/add-curated-entries-to-dataset.png)
  </Step>

  <Step>
    That's it! You'll receive a notification when your Dataset is ready. Simply click the `Open Dataset` button to start working with your newly created Dataset.

    ![Open curated Dataset](/images/docs/library/how-to/datasets/curate-data-from-production/open-curated-dataset.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/library/how-to/datasets/curate-golden-dataset-for-human-annotation
meta: {
  "title": "Curate a golden Dataset from Human Annotation",
  "description": "Learn how to curate a golden Dataset for human annotation"
}

Creating golden datasets is essential for scaling your application effectively. Maxim allows you to curate high-quality datasets directly from human annotations as your application evolves.

Follow these steps to curate dataset entries from human annotations:

<Steps>
  <Step>
    Set up a test run on a prompt or workflow and send the results to human raters for annotation. Learn more about human-in-the-loop evaluation in our [evaluation guide](/docs/evaluate/how-to/evaluate-prompts/human-annotation-pipeline).
  </Step>

  <Step>
    Navigate to the test run report after collecting human ratings.
  </Step>

  <Step>
    Locate the human evaluation card in the summary section, which shows rater emails and completion status.
  </Step>

  <Step>
    Click the **"View Details"** button next to completed raters' emails to access their detailed ratings.
  </Step>

  <Step>
    Navigate to the test run report after collecting human ratings.
  </Step>

  <Step>
    Locate the human evaluation card in the summary section showing rater emails and completion status.
  </Step>

  <Step>
    Click the `View details` button next to completed raters' emails to access their detailed ratings.
  </Step>

  <Step>
    Review the ratings, comments, and human-corrected outputs where available.
  </Step>

  <Step>
    Select the entries you want to preserve using the row checkboxes, then click the **"Add to Dataset"** button at the top.
  </Step>

  <Step>
    Select your target Dataset and map the relevant data to appropriate columns. For example, map human-corrected outputs to ground truth columns in your golden ataset. - Uncheck any columns you don't want to include in the dataset.
  </Step>
</Steps>

<video url="https://drive.google.com/file/d/1UnifOHr0dsM43_amNeQh9fK016CdJgI-/preview" />


url: https://getmaxim.ai/docs/library/how-to/datasets/use-dataset-templates
meta: {
  "title": "Create a Dataset using templates",
  "description": "Datasets are collections of data used for training, testing, and evaluating AI models within workflows and evaluations. Test your prompts, workflows or chains across test cases in this dataset and view results at scale. Begin with a template and customize column structure. Evolve your datasets over time from production logs or human annotation."
}

Create Datasets quickly with predefined structures using our templates:

![Dataset templates showing three template options for different testing scenarios](/images/docs/library/how-to/datasets/use-dataset-templates/dataset-templates.png)

## Prompt or Workflow testing

Choose this template for single-turn interactions based on individual inputs to test prompts or workflows.

**Example:** Input column with prompts like "Summarize this article about climate change" paired with an Expected Output column containing ideal responses.

## Agent simulation

Select this template for multi-turn simulations to test agent behaviors across conversation sequences.

**Example:** Scenario column with "Customer inquiring about return policy" and Expected Steps column outlining the agent's expected actions.

## Dataset testing

Use this template when evaluating against existing output data to compare expected and actual results.

**Example:** Input column with "What's the weather in New York?" and Expected Output column with "65°F and sunny" for direct evaluation.


url: https://getmaxim.ai/docs/library/how-to/datasets/use-splits-within-a-dataset
meta: {
  "title": "Use splits within a Dataset",
  "description": "Learn how to use splits within a Dataset"
}

Splits let you isolate specific rows from a dataset for targeted testing. Here's how to use them:

<Steps>
  <Step>
    In your Dataset, select the rows you want to include in a split. Click the `Add x entries to split` button that appears.

    ![Select rows and create a split](/images/docs/library/how-to/datasets/use-splits-within-a-dataset/select-rows-and-create-split.png)
  </Step>

  <Step>
    Attach your split to a test run for evaluation:

    ![Configure test with split](/images/docs/library/how-to/datasets/use-splits-within-a-dataset/split-test-config-form-selection.png)

    <Callout type="info">
      Splits function just like Datasets across the platform and can be used with Prompts, Workflows, and other testing features.
    </Callout>
  </Step>
</Steps>


url: https://getmaxim.ai/docs/library/how-to/datasets/use-variable-columns-in-datasets
meta: {
  "title": "Use variable columns in Datasets",
  "description": "Learn how to use variable columns in datasets"
}

Maxim provides a way to insert dynamic values into your entities at runtime via your Datasets in the form of variables. A variable is a collection of a key and a value.

You can refer to variables using the Jinja template syntax (double curly braces) `{{variable_name}}`.

You can populate variable values in various ways

* Dataset column
* Use a context source to retrieve it at a run time

<Callout type="info">
  Maxim has specific types of reserved columns that take priority over the variables you've defined. These columns include:

  * input
  * expectedOutput
  * output
  * expectedToolCalls
  * scenario
  * expectedSteps
</Callout>

<Steps>
  <Step>
    ### Variable usage in Prompt

    You can use variables in your Prompt to refer to dynamic values at runtime.
    For example, if you're creating a Prompt and want to provide context to the model, you can refer to the context via a variable in your system prompt

    ```plaintext
    You are a helpful assistant, answer a given answer with respect, be nice and respectful

    You may use {{context}} to answer the questions
    ```

    If you're using the Prompt playground, you can add variables via static values on the right side of the interface.

    Alternatively, if you're using it in a test run, you can create a context-named variable in your dataset. When the test run executes, this variable will be replaced with the values from your dataset column.

    <Callout type="info">
      You can use variables for Prompt Comparison and Prompt Chain in the same way as you do in your Prompt playground
    </Callout>

    ![Variable usage in Prompt](/images/docs/library/how-to/datasets/variables-usage/variable-usage-in-prompt-playground.png)
  </Step>

  <Step>
    ### Variable usage in API Workflow

    If you're using an API Workflow, you can add variables to your workflow body, headers, or query parameters in the same way.

    ![Variable usage in Prompt](/images/docs/library/how-to/datasets/variables-usage/variable-usage-in-workflow.png)
  </Step>

  <Step>
    ### Variable usage in Evaluators

    You can use variables in your custom evaluators in the same way as you do in your Prompts. This allows you to provide additional context to your evaluators for better results.

    ![Variable usage in Prompt](/images/docs/library/how-to/datasets/variables-usage/variable-usage-in-evaluators.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/library/how-to/evaluators/create-api-evaluators
meta: {
  "title": "Bring your existing Evaluators via API",
  "description": "Connect your evaluation system to Maxim using simple API endpoints."
}

import { Plus } from "lucide-react";

Connect your existing evaluation system to Maxim by exposing it via an API endpoint. This lets you reuse your Evaluators without rebuilding them.

<Steps>
  <Step>
    ### Create Evaluator

    Select `API-based` from the create menu to start building.

    ![Create a new API evaluator](/images/docs/library/how-to/evaluators/common/create-evaluator.png)
  </Step>

  <Step>
    ### Configure endpoint

    Add your API endpoint details including:

    * Headers
    * Query parameters
    * Request body

    For advanced transformations, use pre and post scripts under the `Scripts` tab.

    <Callout type="info">Use variables in the body, query parameters and headers</Callout>

    ![Configure API endpoint details](/images/docs/library/how-to/evaluators/api-evaluator/api-evaluator-editor.png)
  </Step>

  <Step>
    ### Map response fields

    Test your endpoint using the playground. On successful response, map your API response fields to:

    * Score (required)
    * Reasoning (optional)

    This mapping allows you to keep your API structure unchanged.

    ![Map API response to evaluator fields](/images/docs/library/how-to/evaluators/api-evaluator/api-evaluator-fields-mapping.png)
  </Step>

  <Step>
    ### Set pass criteria

    Configure two types of pass criteria:

    **Pass query**
    Define criteria for individual evaluation metrics

    Example: Pass if clarity score > 0.8

    **Pass evaluator (%)**
    Set threshold for overall evaluation across multiple entries

    Example: Pass if 80% of entries meet the clarity criteria

    ![Pass criteria configuration](/images/docs/library/how-to/evaluators/common/pass-criteria.png)
  </Step>

  <Step>
    ### Test your Evaluator

    Test your Evaluator in the playground before using it in your workflows. The right panel shows input fields for all variables used in your Evaluator.

    1. Fill in sample values for each variable
    2. Click **Run** to see how your Evaluator performs
    3. Iterate and improve your evaluator based on the results

    ![Testing an evaluator in the playground with input fields for variables](/images/docs/library/how-to/evaluators/common/evaluator-playground.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/library/how-to/evaluators/create-custom-ai-evaluator
meta: {
  "title": "Create custom AI Evaluators",
  "description": "Learn how to create custom AI Evaluators when built-in Evaluators don't meet your specific evaluation needs."
}

import { Plus } from "lucide-react";

While Maxim offers a comprehensive set of evaluators in the [Store](/docs/library/how-to/evaluators/use-pre-built-evaluators), you might need custom evaluators for specific use cases. Create your own AI evaluator by selecting an LLM as the judge and configuring custom evaluation instructions.

<Steps>
  <Step>
    ### Create new Evaluator

    Click the create button and select AI to start building your custom evaluator.

    ![Create AI Evaluator](/images/docs/library/how-to/evaluators/common/create-evaluator.png)
  </Step>

  <Step>
    ### Configure model and parameters

    Select the LLM you want to use as the judge and configure model-specific parameters based on your requirements.

    ![Model configuration for custom AI evaluator](/images/docs/library/how-to/evaluators/custom-ai-evaluators/model-config-for-ai-evaluator.png)
  </Step>

  <Step>
    ### Define evaluation logic

    Configure how your evaluator should judge the outputs:

    * **Requirements**: Define evaluation criteria in plain English

      ```plaintext
      "Check if the text uses punctuation marks correctly to clarify meaning"
      ```

    * **Evaluation scale**: Choose your scoring type

      * **Scale**: Score from 1 to 5
      * **Binary**: Yes/No response

    * **Grading logic**: Define what each score means

      ```plaintext
      1: Punctuation is consistently incorrect or missing; hampers readability
      2: Frequent punctuation errors; readability is often disrupted
      3: Some punctuation errors; readability is generally maintained
      4: Few punctuation errors; punctuation mostly aids in clarity
      5: Punctuation is correct and enhances clarity; no errors
      ```

      <Callout type="info">You can use variables in **Requirements** and **Grading logic**</Callout>

    ![Evaluation logic configuration](/images/docs/library/how-to/evaluators/custom-ai-evaluators/ai-evaluator-model-instruction.png)
  </Step>

  <Step>
    ### Normalize score (Optional)

    Convert your custom evaluator scores from a 1-5 scale to match Maxim's standard 0-1 scale. This helps align your custom evaluator with [pre-built evaluators](/docs/library/how-to/evaluators/use-pre-built-evaluators) in the Store.

    For example, a score of 4 becomes 0.8 after normalization.

    ![Score normalization toggle for AI evaluators](/images/docs/library/how-to/evaluators/custom-ai-evaluators/ai-evaluator-score-normalization.png)
  </Step>

  <Step>
    ### Set pass criteria

    Configure two types of pass criteria:

    **Pass query**
    Define criteria for individual evaluation metrics

    Example: Pass if evaluation score > 0.8

    **Pass evaluator (%)**
    Set threshold for overall evaluation across multiple entries

    Example: Pass if 80% of entries meet the evaluation criteria

    ![Pass criteria configuration](/images/docs/library/how-to/evaluators/common/pass-criteria.png)
  </Step>

  <Step>
    ### Test your Evaluator

    Test your Evaluator in the playground before using it in your workflows. The right panel shows input fields for all variables used in your Evaluator.

    1. Fill in sample values for each variable
    2. Click **Run** to see how your evaluator performs
    3. Iterate and improve your evaluator based on the results

    ![Testing an evaluator in the playground with input fields for variables](/images/docs/library/how-to/evaluators/common/evaluator-playground.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/library/how-to/evaluators/create-human-evaluators
meta: {
  "title": "Set up human evaluation",
  "description": "Set up human raters to review and assess AI outputs for quality control"
}

import { Plus } from "lucide-react";

Human evaluation is essential for maintaining quality control and oversight of your AI system's outputs. Create structured workflows for human reviewers to rate and provide feedback on AI responses.

<Steps>
  <Step>
    ### Create Evaluator

    Select `Human` from the create menu.

    ![Create human evaluator](/images/docs/library/how-to/evaluators/common/create-evaluator.png)
  </Step>

  <Step>
    ### Add evaluation instructions

    Write clear guidelines for human reviewers. These instructions appear during the review process and should include:

    * What aspects to evaluate
    * How to assign ratings
    * Examples of good and bad responses

    ![Add reviewer instructions](/images/docs/library/how-to/evaluators/human-evaluator/human-evaluator-instructions.png)
  </Step>

  <Step>
    ### Select evaluation type

    Choose between two rating formats:

    **Binary (Yes/No)**
    Simple binary evaluation

    ![Binary config](/images/docs/library/how-to/evaluators/human-evaluator/human-evaluator-binary-type.png)

    ![Binary config](/images/docs/library/how-to/evaluators/human-evaluator/human-evaluator-binary-type-interface.png)

    **Scale**
    Nuanced rating system for detailed quality assessment

    ![Scale config](/images/docs/library/how-to/evaluators/human-evaluator/human-evaluator-scale-type.png)

    ![Scale config](/images/docs/library/how-to/evaluators/human-evaluator/human-evaluator-scale-type-interface.png)
  </Step>

  <Step>
    ### Set pass criteria

    Configure two types of pass criteria:

    **Pass query**
    Define criteria for individual evaluation metrics

    Example: Pass if evaluation score > 0.8

    **Pass evaluator (%)**
    Set threshold for overall evaluation across multiple entries

    Example: Pass if 80% of entries meet the human evaluation criteria

    ![Pass criteria configuration](/images/docs/library/how-to/evaluators/common/pass-criteria.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/library/how-to/evaluators/create-programmatic-evaluator
meta: {
  "title": "Create Programmatic Evaluators",
  "description": "Build custom code-based evaluators using Javascript or Python"
}

import { Plus } from "lucide-react";

While Maxim offers several pre-built evaluators in the [Store](/docs/library/how-to/evaluators/use-pre-built-evaluators), you might need custom evaluators for specific use cases. Create programmatic evaluators using Javascript or Python with access to standard libraries.

<Steps>
  <Step>
    ### Create evaluator

    Select Programmatic from the create menu <Plus className="inline-block size-5" /> to start building

    ![Create programmatic evaluator](/images/docs/library/how-to/evaluators/common/create-evaluator.png)
  </Step>

  <Step>
    ### Configure evaluation settings

    Choose your programming language and set the Response type (Number or Boolean) from the top bar

    ![Evaluation configuration options](/images/docs/library/how-to/evaluators/programmatic-evaluator/config-top-bar.png)
  </Step>

  <Step>
    ### Write evaluation logic

    Define a function named `validate` in your chosen language. This function is required as Maxim uses it during execution.

    <Callout type="info">
      Code restrictions

      **Javascript**

      * No infinite loops
      * No debugger statements
      * No global objects (window, document, global, process)
      * No require statements
      * No with statements
      * No Function constructor
      * No eval
      * No setTimeout or setInterval

      **Python**

      * No infinite loops
      * No recursive functions
      * No global/nonlocal statements
      * No raise, try, or assert statements
      * No disallowed variable assignments
    </Callout>

    ![Code editor for evaluation logic](/images/docs/library/how-to/evaluators/programmatic-evaluator/code-editor.png)
  </Step>

  <Step>
    ### Evaluator console

    Monitor your evaluator execution with the built-in console. Add console logs for debugging to track what's happening during evaluation. All logs will appear in this view.

    ![Console showing debug logs during evaluator execution](/images/docs/library/how-to/evaluators/programmatic-evaluator/programmatic-evaluator-console.png)
  </Step>

  <Step>
    ### Set pass criteria

    Configure two types of pass criteria:

    **Pass query**
    Define criteria for individual evaluation metrics

    Example: Pass if evaluation score > 0.8

    **Pass evaluator (%)**
    Set threshold for overall evaluation across multiple entries

    Example: Pass if 80% of entries meet the evaluation criteria

    ![Pass criteria configuration](/images/docs/library/how-to/evaluators/common/pass-criteria.png)
  </Step>

  <Step>
    ### Test your evaluator

    Test your evaluator in the playground before using it in your workflows. The right panel shows input fields for all variables used in your evaluator.

    1. Fill in sample values for each variable
    2. Click **Run** to see how your evaluator performs
    3. Iterate and improve your evaluator based on the results

    ![Testing an evaluator in the playground with input fields for variables](/images/docs/library/how-to/evaluators/common/evaluator-playground.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/library/how-to/evaluators/use-pre-built-evaluators
meta: {
  "title": "Use pre-built Evaluators",
  "description": "Get started quickly with ready-made evaluators for common AI evaluation scenarios"
}

Maxim provides a collection of pre-built evaluators that you can use immediately for your AI evaluation needs. These include high-quality evaluators from Maxim and popular open-source libraries like RAGAS. Install them directly from the Evaluator Store.

The Evaluator Store contains three types of evaluators:

* **AI-based** - Uses LLMs as judges with well-curated prompts (Example: Clarity evaluator)
* **Programmatic** - Uses JavaScript or Python code to evaluate quality (Example: isValidJSON)
* **Statistical** - Traditional ML metrics for text comparison (Example: BLEU, ROUGE)

![Browse evaluators in the Evaluator Store](/images/docs/library/how-to/evaluators/pre-build-evaluators/evaluator-store-interface.png)

<Steps>
  <Step>
    ### Install evaluators

    Search or filter evaluators based on your requirements. Add them to your workspace by clicking the `Add to workspace` button.

    <Callout type="info">
      Read more about specific evaluators in their documentation after installing them
    </Callout>

    ![Install an evaluator from the store](/images/docs/library/how-to/evaluators/pre-build-evaluators/add-evaluator-to-workspace.png)
  </Step>

  <Step>
    ### Use installed evaluators

    Use your installed evaluators while [triggering test runs](/docs/evaluate/how-to/evaluate-prompts/bulk-comparisons-across-test-cases) on any entity.

    ![Select evaluators while triggering a test run](/images/docs/library/how-to/evaluators/pre-build-evaluators/using-installed-evaluator-in-test-run-trigger-sheet.png)
  </Step>
</Steps>


url: https://getmaxim.ai/docs/library/how-to/prompt-partials/create-prompt-partial
meta: {
  "title": "Create Prompt Partials",
  "description": "Store common prompt elements as reusable snippets that you can include across different prompts, helping you maintain consistency and reduce repetition."
}

import { Plus } from "lucide-react";

## Create a Prompt Partial

<Steps>
  <Step>Go to Library > Prompt Partials</Step>

  <Step>
    ### Add a new Partial

    Click <Plus className="inline-block size-5" /> to create
  </Step>

  <Step>
    ### Name your Partial

    Give your Partial a clear, descriptive name to find it easily when adding to Prompts

    Create folders to organize multiple Partials if needed

    ![Create Partial](/images/docs/library/how-to/prompt-partials/create-prompt-partial.png)
  </Step>

  <Step>
    ### Write your Partial content

    Add your content in the text area. Include variables that will be available in any Prompt using this Partial

    ![Add Prompt partial content](/images/docs/library/how-to/prompt-partials/add-prompt-partial-content.png)
  </Step>

  <Step>
    ### Publish your Partial

    Publish when ready. Add a description to help other users understand the Partial's purpose

    ![Publish Partial](/images/docs/library/how-to/prompt-partials/publish-prompt-partial.png)
  </Step>
</Steps>

### Next steps

[Add Partials to your Prompts](/docs/evaluate/how-to/evaluate-prompts/use-prompt-partials)


url: https://getmaxim.ai/docs/library/how-to/prompt-tools/create-a-code-tool
meta: {
  "title": "Create a code-based Prompt Tool",
  "description": "Code-based Prompt Tools allow you to create custom functions directly within the editor. This guide will show you how to create and test these tools."
}

## Creating a code-based tool

<Steps>
  <Step>
    Go to the left navigation bar and click on the "Prompts Tools" tab.
  </Step>

  <Step>
    On the Prompt Tools page, click the + button.
  </Step>

  <Step>
    Select "Code" as the tool type and click "Create".
  </Step>

  <Step>
    Write your custom function in JavaScript in the code editor.
  </Step>
</Steps>

<video url="https://drive.google.com/file/d/12E7FaogMtQ3gs0UE64pbKlOmEbwur-jM/preview" />

## Code editor interface

The interface provides:

* A code editor for writing your function
* An input panel on the right for testing
* A console at the bottom to view outputs

## Example: Travel price calculator

Here's an example of a prompt tool that calculates travel fares between cities:

```js
const pricesMap = {
	London_Barcelona: 3423,
	Barcelona_London: 3500,
	London_Chicago: 3021,
	Chicago_London: 3670,
	London_Madrid: 6375,
	Madrid_London: 6590,
	London_Paris: 5621,
	Paris_London: 5560,
	Barcelona_Chicago: 3000,
	Chicago_Barcelona: 3890,
	Barcelona_Madrid: 4000,
	Madrid_Barcelona: 4321,
	Barcelona_Paris: 2034,
	Paris_Barcelona: 2041,
	Chicago_Madrid: 6987,
	Madrid_Chicago: 6456,
	Chicago_Paris: 3970,
	Paris_Chicago: 3256,
	Madrid_Paris: 4903,
	Paris_Madrid: 4678,
};

function Travel_Prices(st1, st2) {
	const key1 = `${st1}_${st2}`;
	const key2 = `${st2}_${st1}`;

	if (pricesMap[key1] !== undefined) {
		return pricesMap[key1];
	} else if (pricesMap[key2] !== undefined) {
		return pricesMap[key2];
	} else {
		return "Price not found for the given stations";
	}
}

function Total_Travel_Price(cities) {
	if (cities.length < 2) {
		return "At least two cities are required to calculate travel price";
	}

	let total_price = 0;

	for (let i = 0; i < cities.length - 1; i++) {
		const price = Travel_Prices(cities[i], cities[i + 1]);
		if (typeof price === "string") {
			return price; // Return the error message if price not found
		}
		total_price += price;
	}

	return total_price;
}
```

<video url="https://drive.google.com/file/d/1iXSAKAiQ4DyMOX2tiO7DDXxhKrd2uV-1/preview" />


url: https://getmaxim.ai/docs/library/how-to/prompt-tools/create-a-prompt-tool
meta: {
  "title": "Create a Prompt Tool",
  "description": "Create and integrate custom prompt tools for specific tasks."
}

## Create a function via code in the editor

Creating a prompt tool involves developing a function tailored to a specific task, then making it accessible to LLM models by exposing it as a prompt tool. This allows you to mimic and test an agentic flow.

For creating a prompt tool:

<Steps>
  <Step>
    Go to the left navigation bar.
  </Step>

  <Step>
    Click on the "Prompts Tools" tab.
  </Step>

  <Step>
    This will direct you to the Prompt Tools page.
  </Step>

  <Step>
    Click on the + button.
  </Step>

  <Step>
    You have the option to select "Code" as the tool type.
  </Step>

  <Step>
    Click the "Create" button.
  </Step>

  <Step>
    Proceed to write your own custom function in javascript.
  </Step>
</Steps>

<video url="https://drive.google.com/file/d/12E7FaogMtQ3gs0UE64pbKlOmEbwur-jM/preview" />

Within the interface there is a designated area to input code. Adjacent to this, on the right-hand side, you can provide relevant inputs tailored
to the function's requirements. Upon execution, the output is displayed in the console located at the bottom of the screen.

In this example, we are creating a Prompt tool to calculate the total fare for traveling through a list of cities using a predefined fare map.
This prompt tool can then be attached to single prompts.

Here is the code snippet for the prompt tool:

```js title="tool.js"
const pricesMap = {
	London_Barcelona: 3423,
	Barcelona_London: 3500,
	London_Chicago: 3021,
	Chicago_London: 3670,
	London_Madrid: 6375,
	Madrid_London: 6590,
	London_Paris: 5621,
	Paris_London: 5560,
	Barcelona_Chicago: 3000,
	Chicago_Barcelona: 3890,
	Barcelona_Madrid: 4000,
	Madrid_Barcelona: 4321,
	Barcelona_Paris: 2034,
	Paris_Barcelona: 2041,
	Chicago_Madrid: 6987,
	Madrid_Chicago: 6456,
	Chicago_Paris: 3970,
	Paris_Chicago: 3256,
	Madrid_Paris: 4903,
	Paris_Madrid: 4678,
};

function Travel_Prices(st1, st2) {
	const key1 = `${st1}_${st2}`;
	const key2 = `${st2}_${st1}`;

	if (pricesMap[key1] !== undefined) {
		return pricesMap[key1];
	} else if (pricesMap[key2] !== undefined) {
		return pricesMap[key2];
	} else {
		return "Price not found for the given stations";
	}
}

function Total_Travel_Price(cities) {
	if (cities.length < 2) {
		return "At least two cities are required to calculate travel price";
	}

	let total_price = 0;

	for (let i = 0; i < cities.length - 1; i++) {
		const price = Travel_Prices(cities[i], cities[i + 1]);
		if (typeof price === "string") {
			return price; // Return the error message if price not found
		}
		total_price += price;
	}

	return total_price;
}
```

## Bring your custom functions via APIs

In Maxim you can expose function call via APIs. We generate function schema based on the Query Parameters and Payload. We collect variables from both of these and add them to function call / tools call object while sending it to the model.

For example

* If your payload looks like

  ```json title="Zipcode API payload"
  {
  	"check": "123333"
  }
  ```

* We convert this into JSON schema and while making requests to the model, we send the payload as

  ```json title="Payload sent to the model while making requests"
  {
  	"type": "function",
  	"function": {
  		"parameters": {
  			"type": "object",
  			"properties": {
  				"check": {
  					"type": "string"
  				}
  			}
  		},
  		"name": "clt473gri0006yzrl26rz79iu", // This is the ID of the function.
  		"description": "This function accepts a zipcode and returns the corresponding location information" // This is the description of the function.
  	}
  }
  ```

<video url="https://drive.google.com/file/d/1RXwoCSs23J054tfNTf5g8oyB0VwsAUs7/preview" />

## Create a Schema-based Prompt tool

The Schema prompt tool type provides a structured way to define tools for accurate and schema-compliant outputs. This type is especially useful for ensuring tool call accuracy.

### Steps to create a Schema-based Prompt Tool

<Steps>
  <Step>
    Go to the **Prompt Tools** section and create a new tool.
  </Step>

  <Step>
    Select **Schema** as the tool type.
  </Step>

  <Step>
    Define the schema for your prompt tool. For example, a schema for fetching stock price:

    ```json title="Function call schema"
    {
        "type": "function",
        "function": {
            "name": "get-stock-price",
            "parameters": {
                "type": "object",
                "properties": {
                    "stock": {
                        "type": "string"
                    }
                }
            },
            "description": "this function returns stock value"
        }
    }
    ```
  </Step>

  <Step>
    Save your schema-based prompt tool using the **Save** button in the editor.
  </Step>

  <Step>
    Add your tool to a prompt configuration to test whether the model accurately selects and uses it.
  </Step>
</Steps>

<video url="https://drive.google.com/file/d/1iXSAKAiQ4DyMOX2tiO7DDXxhKrd2uV-1/preview" />


url: https://getmaxim.ai/docs/library/how-to/prompt-tools/create-a-tool-schema
meta: {
  "title": "Create a Schema-based Prompt Tool",
  "description": "Schema-based prompt tools provide a structured way to define tools that ensure accurate and schema-compliant outputs. This approach is particularly useful when you need to guarantee that the LLM's responses follow a specific format."
}

## Creating a Schema Tool

<Steps>
  <Step>
    Navigate to the **Prompt Tools** section and click the + button.
  </Step>

  <Step>
    Select **Schema** as the tool type.
  </Step>

  <Step>
    Define your schema in the editor. Here's an example schema for a stock price tool:

    ```json title="Function call schema"
    {
        "type": "function",
        "function": {
            "name": "get-stock-price",
            "parameters": {
                "type": "object",
                "properties": {
                    "stock": {
                        "type": "string"
                    }
                }
            },
            "description": "this function returns stock value"
        }
    }
    ```
  </Step>

  <Step>
    Click the **Save** button to create your schema-based tool.
  </Step>
</Steps>

## Testing Your Schema Tool

After creating your schema-based tool:

1. Add it to a prompt configuration
2. Test if the model correctly identifies when to use it
3. Verify that the outputs match your schema's structure


url: https://getmaxim.ai/docs/library/how-to/prompt-tools/create-an-api-tool
meta: {
  "title": "Create an API-based Prompt Tool",
  "description": "Maxim allows you to expose external API endpoints as prompt tools. The platform automatically generates function schemas based on the API's query parameters and payload structure."
}

## Example

Here's how an API payload gets converted into a function schema:

1. Original API Payload:

```json title="Zipcode API payload"
{
	"check": "123333"
}
```

2. Generated Schema for LLM:

```json title="Payload sent to the model while making requests"
{
	"type": "function",
	"function": {
		"parameters": {
			"type": "object",
			"properties": {
				"check": {
					"type": "string"
				}
			}
		},
		"name": "clt473gri0006yzrl26rz79iu", // This is the ID of the function.
		"description": "This function accepts a zipcode and returns the corresponding location information" // This is the description of the function.
	}
}
```

<video url="https://drive.google.com/file/d/1RXwoCSs23J054tfNTf5g8oyB0VwsAUs7/preview" />


url: https://getmaxim.ai/docs/library/how-to/prompt-tools/evaluate-tool-call-accuracy
meta: {
  "title": "Evaluate Tool Call Accuracy",
  "description": "Learn how to evaluate the accuracy of tool calls"
}

You can learn more about it [here](/docs/evaluate/how-to/evaluate-prompts/run-prompt-tool-calls).


url: https://getmaxim.ai/docs/observe/how-to/evaluate-logs/auto-evaluation
meta: {
  "title": "Set up auto evaluation on logs",
  "description": "Evaluate captured logs automatically from the UI based on filters and sampling"
}

import { Save, Settings2 } from "lucide-react";
import { Button } from "ui";

## Why evaluate logs?

We know that evaluation is a necessary step while building an LLM, but since an LLM can be non-deterministic, all possible scenarios can never be covered; thus evaluating the LLM on live system also becomes crucial.

Evaluation on logs helps cover cases or scenarios that might not be covered by **Test runs**, ensuring that the LLM is performing optimally under various conditions. Additionally, it allows for potential issues to be identified early on which allows for making necessary adjustments to improve the overall performance of the LLM in time.

![Diagram of the evaluation iteration loop](/images/docs/observe/how-to/evaluate-logs/auto-evaluation/diagram-of-the-evaluation-iteration-loop.png)

<Callout title="Before you start">
  You need to have your **logging set up** to capture interactions between your LLM and users before you can evaluate them. To do so, you would need to integrate [Maxim SDK](/docs/observe/how-to/log-your-application/setting-up-trace) into your application.
</Callout>

## Setting up auto evaluation

<Steps>
  <Step>
    Navigate to the repository where you want to evaluate your logs.
  </Step>

  <Step>
    Click on `Configure evaluation` in the top right corner of the page and choose the <Code>Setup evaluation configuration</Code> option. This will open up the evaluation configuration sheet.

    <Img
      src={
      <div className="p-4 flex flex-col gap-1 items-end">
          <Button className="flex flex-row items-center border bg-background-primary" variant="ghost">
              <Settings2 className="h-4 w-4" strokeWidth={1.5} />
              Configure evaluation
          </Button>
          <div className="min-w-[8rem] overflow-hidden rounded-sm border bg-background-primary p-1 text-content-primary shadow-md">
              <div className="flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-md outline-hidden bg-background-highlight-primary text-content-primary">
                  Setup evaluation configuration
              </div>
              <div className="flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-md outline-hidden">
                  Create annotation queue
              </div>
          </div>
      </div>
  }
      alt="Screenshot of the repository page with the configure evaluation button highlighted"
    />
  </Step>

  <Step>
    The sheet's `Auto Evaluation` section has 3 parts:

    * `Select evaluators`: Choose the evaluators you want to use for your evaluation.
    * `Filters`: Setup filters to only evaluate logs that meet a certain criteria.
    * `Sampling`: Choose a sampling rate, this will help you control the amount of logs that are evaluated and prevent evaluating every log; which could potentially lead to very high costs.

    ![Screenshot of the evaluation configuration sheet](/images/docs/observe/how-to/evaluate-logs/auto-evaluation/screenshot-of-the-evaluation-configuration-sheet.png)

    <Callout>
      The `Human Evaluation` section below is explained in the [Set up human evaluation on logs](/docs/observe/how-to/evaluate-logs/human-evaluation) section
    </Callout>
  </Step>

  <Step>
    Finally click on the <Code className="inline-flex items-center gap-1 max-h-[23px]"><Save className="h-4 w-4" /> Save configuration</Code> button.
  </Step>
</Steps>

The configuration is now done and your logs should start evaluating automatically based on the filters and sampling rate you have set up! 🎉

## Making sense of evaluations on logs

In the logs' table view, you can find the evaluations on a trace in its row towards the left end, displaying the evaluation scores. You can sort the logs by evaluation scores as well by clicking on either of the evaluators' column header.

![Screenshot of the logs table with traces having evaluation](/images/docs/observe/how-to/evaluate-logs/auto-evaluation/screenshot-of-the-logs-table-with-traces-having-evaluation.png)

Click the trace to view detailed evaluation results. In the sheet, you will find the `Evaluation` tab, wherein you can see the evaluation in detail.

![Screenshot of the details sheet with the evaluation tab highlighted](/images/docs/observe/how-to/evaluate-logs/auto-evaluation/screenshot-of-the-details-sheet-with-the-evaluation-tab-highlighted.png)

The evaluation tab displays many details regarding the evaluation of the trace, let us see how you can navigate through them and get more insights into how your LLM is performing.

### Evaluation summary

![Screenshot of the evaluation summary](/images/docs/observe/how-to/evaluate-logs/auto-evaluation/screenshot-of-the-evaluation-summary.png)

Evaluation summary displays the following information (top to bottom, left to right):

* How many evaluators passed out of the total evaluators across the trace
* How much did all the evaluators' evaluation cost
* How many tokens were used across the all evaluators' evaluations
* What was the total time taken for the evaluation to process

### Trace evaluation card

In each card, you will find a tab switcher on the top right corner, this is used to navigate through the evaluation's details. Here is what you can find in in different tabs:

#### Overview tab

![Screenshot of the overview tab in trace evaluation card](/images/docs/observe/how-to/evaluate-logs/auto-evaluation/screenshot-of-the-overview-tab-in-trace-evaluation-card.png)

All the evaluators run on the trace level and their scores are displayed in a table here along with whether the evaluator passed or failed.

#### Individual evaluator's tab

![Screenshot of the individual evaluator's tab in trace evaluation card](/images/docs/observe/how-to/evaluate-logs/auto-evaluation/screenshot-of-the-individual-evaluator_s-tab-in-trace-evaluation-card.png)

This tab contains the following sections:

* **Result**: Shows whether the evaluator passed or failed.
* **Score**: Shows the score of the evaluator.
* **Reason** (shown where applicable): Displays the reasoning behind the score of the evaluator, if given.
  <div className="h-5" />
* **Cost** (shown where applicable): Shows the cost of the individual evaluator's evaluation.
* **Tokens used** (shown where applicable): Shows the number of tokens used by the individual evaluator's evaluation.
* **Model latency** (shown where applicable): Shows the time taken by the model to respond back with a result for an evaluator.
* **Time taken**: Shows the time taken by the evaluator to evaluate.
  <div className="h-5" />
* **Variables used to evaluate**: Shows the values that were used to replace the variables with while processing the evaluator.
  <div className="h-5" />
* **Logs**: These are logs that were generated during the evaluation process. They might be useful for debugging errors or issues that occurred during the evaluation.

### Tree view on the left panel

![Screenshot of the tree view on the left panel](/images/docs/observe/how-to/evaluate-logs/auto-evaluation/screenshot-of-the-tree-view-on-the-left-panel.png)

This view is essential for when you are evaluating the each log on the node level, essentially on each component of the trace (like a generation or retrieval, etc). This view helps with your perception as to what component's evaluation you are looking at on the right panel (and the component's place in the trace as well). We discuss more about the [Node Level Evaluation](/docs/observe/how-to/evaluate-logs/node-level-evaluation) further down.


url: https://getmaxim.ai/docs/observe/how-to/evaluate-logs/human-evaluation
meta: {
  "title": "Set up human evaluation on logs",
  "description": "Use human evaluation or rating to assess the quality of your logs and evaluate them."
}

import { ChevronRight, NotebookPen, Save, Settings2, SquareCheck } from "lucide-react";
import { Button } from "ui";
import { v4 as uuid } from "uuid";

## The need for human evaluation

While machine learning models can provide a baseline evaluation, they may not always capture the nuances of human perception, simply because they lack the ability to understand context and emotions behind some scenarios. Humans, in these scenarios, can also provide better comments and insights. This makes it essential to also have humans be a part of the evaluation process.

Human evaluation on logs are very similar to how human annotation is done on test runs, in fact, the **Human Evaluators** used in test runs are also used here. Let's see how we can set up a human evaluation pipeline for our logs.

<Callout title="Before you start">
  You need to have your **logging set up** to capture interactions between your LLM and users before you can evaluate them. To do so, you would need to integrate [Maxim SDK](/docs/observe/how-to/log-your-application/setting-up-trace) into your application.

  Also if you do not have a **Human Evaluator** created in your workspace, please create one by navigating to the **Evaluators** (<SquareCheck className="w-5 h-5 inline" />) tab from the sidebar, as we will need it to setup the human evaluation pipeline.
</Callout>

## Setting up human evaluation

<Steps>
  <Step>
    Navigate to the repository where you want to setup human evaluation on logs.
  </Step>

  <Step>
    Click on `Configure evaluation` in the top right corner of the page and choose the <Code>Setup evaluation configuration</Code> option. This will open up the evaluation configuration sheet.

    <Img
      src={
      <div className="p-4 flex flex-col gap-1 items-end">
          <Button className="flex flex-row items-center border bg-background-primary" variant="ghost">
              <Settings2 className="h-4 w-4" strokeWidth={1.5} />
              Configure evaluation
          </Button>
          <div className="min-w-[8rem] overflow-hidden rounded-sm border bg-background-primary p-1 text-content-primary shadow-md">
              <div className="flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-md outline-hidden bg-background-highlight-primary text-content-primary">
                  Setup evaluation configuration
              </div>
              <div className="flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-md outline-hidden">
                  Create annotation queue
              </div>
          </div>
      </div>
  }
      alt="Screenshot of the repository page with the configure evaluation button highlighted"
    />
  </Step>

  <Step>
    We need to focus on the `Human Evaluation` section below. Here we will see a dropdown under `Select evaluators`, we need to choose **Human Evaluators** to use for our evaluation from here.

    This will setup what evaluation we want to do upon our logs. Now we need to setup filtering criteria to determine which logs should be evaluated as evaluating all logs by hand can get out of hand very fast.

    ![Screenshot of the evaluation configuration sheet](/images/docs/observe/how-to/evaluate-logs/human-evaluation/screenshot-of-the-evaluation-configuration-sheet.png)

    <Callout>
      We talked about the [Auto evaluation](/docs/observe/how-to/evaluate-logs/auto-evaluation) section above. You can learn more about using other types of evaluators to evaluate your logs there.
    </Callout>
  </Step>

  <Step>
    Before we setup the filtering criteria though, we need to save this configuration, do this by clicking on the <Code className="inline-flex items-center gap-1 max-h-[23px]"><Save className="h-4 w-4" /> Save configuration</Code> button.
  </Step>

  <Step>
    Now to get to filtering criteria, we will click on `Configure evaluation` in the top right corner of the page again but choose the <Code>View annotation queue</Code> option this time. You will be taken to the annotation queue page.
  </Step>

  <Step>
    ![Screenshot of the annotation queue page](/images/docs/observe/how-to/evaluate-logs/human-evaluation/screenshot-of-the-annotation-queue-page.png)

    Here we will see a `Set up queue logic` button, click on it to setup the logic for the queue and click on the <Code>Save queue logic</Code> button finally to save.

    ![Screenshot of queue logic setup form](/images/docs/observe/how-to/evaluate-logs/human-evaluation/screenshot-of-queue-logic-setup-form.png)
  </Step>
</Steps>

Human evaluation is now setup to automatically keep adding logs that match the certain criteria you have given to the queue; over which annotation can now happen and thus be evaluated! ✍🏻

<Callout>
  Manually add logs to the queue by:

  * Selecting the logs you want to add to the queue by clicking on the checkboxes at the left of each log
  * Clicking on the <Code className="inline-flex items-center gap-1 max-h-[23px]"><NotebookPen className="h-4 w-4" /> Add to annotation queue</Code> button and you're done!

  ![Screenshot of the add to annotation queue button](/images/docs/observe/how-to/evaluate-logs/human-evaluation/screenshot-of-the-add-to-annotation-queue-button.png)
</Callout>

## Viewing annotations

There are 3 places where annotations can be viewed:

### The annotation queue page

Here each added log will have their human evaluators' scores displayed. The scores would be the average score of all the annotations done for an evaluator by different users. On editing the score, the individual score along with comment and rewritten output (if any) of the user editing the score will be shown with the ability to edit all of them.

![Screenshot of the annotation queue page with scores](/images/docs/observe/how-to/evaluate-logs/human-evaluation/screenshot-of-the-annotation-queue-page-with-scores.png)

#### Annotating the logs

On opening the annotation queue page, you will see a list of logs that have been added to the queue beside which there will be a **Select rating** dropdown.

Clicking on the **Select rating** dropdown will open a modal where you can select a rating for the log and optionally add a comment or provide a rewritten output if necessary.

![Screenshot of the annotation modal](/images/docs/observe/how-to/evaluate-logs/human-evaluation/screenshot-of-the-annotation-modal.png)

You can also click on one of the entries to open annotation sheet, wherein you can see the complete input and output and rate for all the evaluators in each entry at once.

After scoring an entry, click on the <Code className="inline-flex items-center gap-1 max-h-[23px]">Save and next <ChevronRight className="h-4 w-4" /></Code> button to move to the next log/entry and score it.

![Screenshot of the annotation sheet view](/images/docs/observe/how-to/evaluate-logs/human-evaluation/screenshot-of-the-annotation-sheet-view.png)

### The logs table

Similar to how the evaluator scores are shown for auto evaluation, human evaluators are also shown (again, the average score is shown here)

![Screenshot of the logs table with human evaluator scores](/images/docs/observe/how-to/evaluate-logs/human-evaluation/screenshot-of-the-logs-table-with-human-evaluator-scores.png)

### The trace details sheet (under the Evaluation tab)

On opening any trace, you will find a `Details` and `Evaluation` tab. The `Evaluation` tab here would display all the evaluations on that happened on the trace. We will focus on the **Human Evaluators** here but in order to make sense of other evaluators in this sheet you can refer to [Auto Evaluation -> Making sense of evaluations on logs](/docs/observe/how-to/evaluate-logs/auto-evaluation#making-sense-of-evaluations-on-logs)

The trace evaluation overview tab shows the average score of each **Human Evaluator** and **Rewritten Outputs**, if present, by each individual user.

![Screenshot of the trace evaluation overview tab with rewritten outputs](/images/docs/observe/how-to/evaluate-logs/human-evaluation/screenshot-of-the-trace-evaluation-overview-tab-with-rewritten-outputs.png)

Going further into each individual **Human Evaluator**, we see its `Score` (avg.) and `Result` (whether the particular evaluator's evaluation passed or failed). We also see a breakdown of the scores and their corresponding comments, if any, given by each user in this tab, thus giving you a granular view of the evaluation as well.

![Screenshot of an individual human evaluator's evaluation with per user score breakdown](/images/docs/observe/how-to/evaluate-logs/human-evaluation/screenshot-of-an-individual-human-evaluator_s-evaluation-with-per-user-score-breakdown.png)


url: https://getmaxim.ai/docs/observe/how-to/evaluate-logs/node-level-evaluation
meta: {
  "title": "Node level evaluation",
  "description": "Evaluate any component of your trace or log to gain insights into your agent's behavior."
}

## What is Node level evaluation (or Agentic evaluation)?

As your AI application grows in complexity, it becomes increasingly difficult to understand how it is performing on different flows and components. This granular insight becomes necessary to identify bottlenecks or low quality areas in your application's or agent's flow. By targeting the underperforming areas, you can optimize overall performance more effectively than using brute force approaches.

This is where Node level evaluation can help out. It enables you to evaluate a trace or its component (a span, generation or retrieval) in isolation. This can be done via the Maxim SDK's `logger` using a very simple API. Let us see how we can start evaluating our nodes.

<Callout title="Before you start">
  You need to have your **logging set up** to capture interactions between your LLM and users before you can evaluate them. To do so, you would need to integrate [Maxim SDK](/docs/observe/how-to/log-your-application/setting-up-trace) into your application.
</Callout>

## Understanding how the Maxim SDK logger evaluates

Two actions are mainly required to evaluate a node:

1. **Attach Evaluators**: This action defines what evaluators to run on the particular node, this needs to be called to start an evaluation on any component.
2. **Attach Variables**: Once evaluators are attached on a component, each evaluator waits for all the variables it needs to evaluate to be attached to it. Only after all the variables an evaluator needs are attached, does it start processing.

Once you have attached evaluators and variables to them, we will process the evaluator and display the results in the `Evaluation` tab under the respective node.

* The evaluator will not run until all of the variables it needs are attached to it.
* If we don't receive all the variables needed for an evaluator for over 5 minutes, we will start displaying a `Missing variables` message (although we will still process the evaluator even if variables are received after 5 minutes).
* The variables that an evaluator needs can be found in the evaluator's page. The evaluator test panel on the right has all the variables that the evalutor needs listed (all of them are required).

![Screenshot of evaluator test panel](/images/docs/observe/how-to/evaluate-logs/node-level-evaluation/screenshot-of-evaluator-test-panel.png)

> As per the image above, we can see that the evaluator needs `input`, `context` and `expectedOutput` variables.

### Attaching evaluators via Maxim SDK

We use the `withEvaluators` method to attach evaluators to any component within a trace or the trace itself. It is as easy as just listing the names of the evaluators you want to attach, which are available on the platform.

<Tabs groupId="language" items={["JS/TS", "Python"]} persist>
  ```typescript tab="JS/TS" title="Attaching evaluators to components"
  // [!code word:withEvaluators]
  component.evaluate.withEvaluators("evaluator");

  // example
  generation.evaluate.withEvaluators("clarity", "toxicity");
  ```

  ```python tab="Python" title="Attaching evaluators to components"
  # [!code word:with_evaluators]
  component.evaluate().with_evaluators("evaluator")

  # example
  generation.evaluate().with_evaluators("clarity", "toxicity")
  ```
</Tabs>

<Callout>
  If you list an evaluator that doesn't exist in your workspace but is available in the store, we will **auto install** it for you in the workspace.

  If the evaluator is not available in the store as well, we will **ignore** it.
</Callout>

### Providing variables to evaluators

Once evaluators are attached to a component, variables can be passed to them via the `withVariables` method. This method accepts a key-value pair of variable names to their values.

You also need to specify which evaluators you want these variables to be attached to, which can be done by passing the list of evaluator names as the second argument.

<Tabs groupId="language" items={["JS/TS", "Python"]} persist>
  ```typescript tab="JS/TS" title="Providing variables to evaluators"
  // [!code word:withVariables]
  component.evaluate.withVariables(
  	{ variableName: "value" }, // Key-value pair of variables
  	["evaluator"], // List of evaluators
  );

  // example
  retrieval.evaluate.withVariables(
      { output: assistantResponse.choices[0].message.content },
      ["clarity", "toxicity"],
  );
  ```

  ```python tab="Python" title="Providing variables to evaluators"
  # [!code word:with_variables]
  component.evaluate().with_variables(
      { "variableName": "value" }, # Key-value pair of variables
      ["evaluator"], # List of evaluators
  )

  # example
  retrieval.evaluate().with_variables(
      { "output": assistant_response.choices[0].message.content },
      ["clarity", "toxicity"],
  )
  ```
</Tabs>

<Callout>
  You can directly chain the `withVariables` method after attaching evaluators to any component. Allowing you to skip mentioning the evaluator names again.

  <Tabs groupId="language" items={["JS/TS", "Python"]} persist>
    ```typescript tab="JS/TS" title="Chaining withVariables method directly"
    // [!code word:withVariables]
    trace.evaluate
        .withEvaluators("clarity", "toxicity")
        .withVariables({
            input: userInput,
        });
    ```

    ```python tab="Python" title="Chaining withVariables method directly"
    # [!code word:with_variables]
    trace.evaluate()
        .with_evaluators("clarity", "toxicity")
        .with_variables({
        	"input": userInput,
        });
    ```
  </Tabs>
</Callout>

## Viewing evaluation results on evaluations tab

This is very similar to [Making sense of evaluations on logs](/docs/observe/how-to/evaluate-logs/auto-evaluation#making-sense-of-evaluations-on-logs), except that the evaluations for each component appear on their own card as it did for the trace.

![Screenshot of node level evaluation result](/images/docs/observe/how-to/evaluate-logs/node-level-evaluation/screenshot-of-node-level-evaluation-result.png)

## Code example for agentic evaluation

This example displays how Node level evaluation might fit in a workflow.

<Tabs groupId="language" items={["JS/TS", "Python"]} persist>
  ```typescript tab="JS/TS" title="agenticWorkflow.ts"
  // ...previous flow

  const generation = trace.generation({
  	id: uuid(),
  	messages: [
  		{
  			role: "system",
  			content: `You are a helpful assistant that helps people with their questions`,
  		},
  	],
  	model: "gpt-4o",
  	provider: "openai",
  	modelParameters: {
  		temperature: 0.7,
  		topP: 1,
  		maxTokens: 100,
  	},
  	name: "user-facing-chatbot",
  });

  // ...user message received

  generation.addMessages([
  	{
  		role: "user",
  		content: userMessage,
  	},
  ]);

  generation.evaluate
      .withEvaluators("clarity", "toxicity")
      .withVariables({
          input: userMessage,
      });

  // ...retrieve and process context

  generation.evaluate.withVariables(
  	{ context: context },
  	["toxicity"], // only toxicity requires context
  );

  // ...generate response

  generation.result(llmResponse);

  generation.evaluate.withVariables(
      { output: llmResponse.choices[0].message.content },
      ["clarity", "toxicity"],
  );

  // ...flow continues
  ```

  ```python tab="Python" title="agentic_workflow.py"
  # ...previous flow

  generationConfig = GenerationConfig(
  	id=str(uuid()),
  	messages=[
      	{
      		"role": "system",
      		"content": "You are a helpful assistant that helps people with their questions",
      	},
          {
              "role": "user",
              "content": user_input,
          },
  	]
  	model="gpt-4o",
  	provider="openai",
  	model_parameters={
  		"temperature": 0.7,
  		"topP": 1,
  		"maxTokens": 100,
  	},
  	name="user-facing-chatbot",
  )
  generation = trace.generation(generationConfig)

  generation.evaluate()
      .with_evaluators("clarity", "toxicity")
      .with_variables({
      	"input": user_input,
      });

  # ...retrieve and process context

  generation.evaluate().with_variables(
      { "context": context },
      ["toxicity"] # only toxicity requires context
  );

  # ...generate response

  generation.result(llm_response);

  generation.evaluate.withVariables(
      { output: llm_response.choices[0].message.content },
      ["clarity", "toxicity"]
  );

  # ...flow continues
  ```
</Tabs>

## Best practices

* Use evaluators selectively to monitor key performance metrics. Do not overdo with attaching too many evaluators.
* Setup sampling and filtering according to your needs to ensure accurate evaluation processing without eating up too much cost.
* Attach variables reliably to ensure no evaluation is left pending due to lack of variables.


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/add-spans-to-traces
meta: {
  "title": "Use spans to group units of work",
  "description": "Spans help you organize and track requests across microservices within traces. A trace represents the entire journey of a request through your system, while spans are smaller units of work within that trace."
}

<div className="w-full flex justify-end -mb-11">
  <LanguageSwitcher />
</div>

<Steps>
  <Step>
    ## Create spans with trace object

    <Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
      ```typescript tab="JS/TS"
      // Create a new trace instance with a unique identifier for request tracking
      const trace = logger.trace({id: "trace-id"});

      // Define a new span to track the question classification process
      const span = trace.span({
          id: "span-id",
          name: "customer-support--classify-question",
      });

      // Note: Replace 'trace.span' with 'span.span' when creating spans within an existing span
      ```

      ```python tab="Python"
      from maxim.logger.components.span import SpanConfig

      # Create a new trace instance with a unique identifier for request tracking
      trace = logger.trace({"id": "trace-id"})

      # Define a new span to track the question classification process
      span = trace.span(SpanConfig(
          id="span-id",
          name="customer-support--classify-question"
      ))

      # Note: Replace 'trace.span' with 'span.span' when creating spans within an existing span
      ```

      ```go tab="Go"
      // Create a new trace instance with a unique identifier for request tracking
      trace := logger.Trace(&logging.TraceConfig{
          Id: "trace-id",
      })

      // Define a new span to track the question classification process
      span := trace.Span(&logging.SpanConfig{
          Id: "span-id",
          Name: "customer-support--classify-question",
      })
      // Note: Replace 'trace.Span' with 'span.Span' when creating spans within an existing span
      ```

      ```java tab="Java"
      // Create a new trace instance with a unique identifier for request tracking
      Trace trace = logger.trace(new TraceConfig(
          "trace-id",
      ));

      // Define a new span to track the question classification process
      Span span = trace.span(new SpanConfig(
          "span-id",
          "customer-support--classify-question"
      ));
      // Note: Replace 'trace.span' with 'span.span' when creating spans within an existing span
      ```
    </Tabs>
  </Step>

  <Step>
    ## Create spans with logger object

    <Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
      ```typescript tab="JS/TS"
      const span = logger.traceSpan("trace-id", {
          id: "span-id",
          name: "customer-support--classify-question",
      });
      ```

      ```python tab="Python"
      from maxim.logger.components.span import SpanConfig

      span = logger.trace_add_span("trace-id", SpanConfig(
          id="span-id",
          name="customer-support--classify-question"
      ))
      ```

      ```go tab="Go"
      span := logger.AddSpanToTrace("trace-id", &logging.SpanConfig{
          Id: "span-id",
          Name: "customer-support--classify-question",
      })
      ```

      ```java tab="Java"
      Span span = logger.traceAddSpan("trace-id", new SpanConfig(
          "span-id",
          "customer-support--classify-question"
      ));
      ```
    </Tabs>
  </Step>
</Steps>


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/adding-llm-call
meta: {
  "title": "Log LLM generations in your AI application traces",
  "description": "Use generations to log individual calls to Large Language Models (LLMs)"
}

Each trace/span can contain multiple generations.

<div className="w-full flex justify-end -mb-11">
  <LanguageSwitcher />
</div>

<Steps>
  <Step>
    ## Send and record LLM request

    <Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
      ```typescript tab="JS/TS"
      // Initialize a trace with a unique ID
      const trace = logger.trace({id: "trace-id"});

      // Adding a generation
      const generation = trace.generation({
          id: "generation-id",
          name: "customer-support--gather-information",
          provider: "openai",
          model: "gpt-4o",
          modelParameters: { temperature: 0.7 },
          messages: [
              { "role": "system", "content": "you are a helpful assistant who helps gather customer information" },
              { "role": "user", "content": "My internet is not working" },
          ],
      });
      // Note: Replace 'trace.generation' with 'span.generation' when creating generations within an existing span

      // Execute the LLM call
      // const aiCompletion = await openai.chat.completions.create({ ... })
      ```

      ```python tab="Python"
      from maxim.logger.components.generation import GenerationConfig

      generation = trace.generation(GenerationConfig(
          id="generation-id",
          name="customer-support--gather-information",
          provider="openai",
          model="gpt-4o",
          messages=[
              { "role": "system", "content": "you are a helpful assistant who helps gather customer information" },
              { "role": "user", "content": "My internet is not working" },
          ],
          model_parameters={"temperature": 0.7},
      ))
      # Note: Replace 'trace.generation' with 'span.generation' when creating generations within an existing span

      # Execute the LLM call
      # aiCompletion = client.chat.completions.create(...)
      ```

      ```go tab="Go"
      generation := trace.AddGeneration(&logging.GenerationConfig{
          Id: "generation-id",
          Name: "customer-support--gather-information",
          Provider: "openai",
          Model: "gpt-4o",
          Messages: []logging.CompletionRequest{
              {
                  Role:    "system",
                  Content: "you are a helpful assistant who helps gather customer information",
              },
              {
                  Role:    "user",
                  Content: "My internet is not working",
              },
          },
          ModelParameters: map[string]interface{}{"temperature": 0.7},
      })
      // Note: Replace 'trace.AddGeneration' with 'span.AddGeneration' when creating generations within an existing span

      // Execute the LLM call
      // aiCompletion, err := client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{...})
      ```

      ```java tab="Java"
      Generation generation = trace.addGeneration(new GenerationConfig(
          "generation-id",
          "customer-support--gather-information",
          "openai",
          "gpt-4o",
          Arrays.asList(
                  new Message("system", "you are a helpful assistant who helps gather customer information"),
                  new Message("user", "My internet is not working"),
          ),
          Map.of("temperature", 0.7),
      ));
      // Note: Replace 'trace.addGeneration' with 'span.addGeneration' when creating generations within an existing span

      // Execute the LLM call
      // ChatCompletionResult aiCompletion = openAiService.createChatCompletion(new ChatCompletionRequest(...));
      ```
    </Tabs>
  </Step>

  <Step>
    ## Record LLM response

    <Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
      ```typescript tab="JS/TS"
      generation.result({
          id: "chatcmpl-123",
          object: "chat.completion",
          created: Date.now(),
          model: "gpt-4o",
          choices: [{
              index: 0,
              message: {
                  role: "assistant",
                  content: "Apologies for the inconvenience. Can you please share your customer id?"
              },
              finish_reason: "stop"
          }],
          usage: {
              prompt_tokens: 100,
              completion_tokens: 50,
              total_tokens: 150
          }
      });
      ```

      ```python tab="Python"
      generation.result({
          "id": "chatcmpl-123",
          "object": "chat.completion",
          "created": int(time.time()),
          "model": "gpt-4o",
          "choices": [{
              "index": 0,
              "message": {
                  "role": "assistant",
                  "content": "Apologies for the inconvenience. Can you please share your customer id?"
              },
              "finish_reason": "stop"
          }],
          "usage": {
              "prompt_tokens": 100,
              "completion_tokens": 50,
              "total_tokens": 150
          }
      })
      ```

      ```go tab="Go"
      generation.SetResult(map[string]interface{}{
          Id: "chatcmpl-123",
          Object: "chat.completion",
          Created: time.Now().Unix(),
          Model: "gpt-4o",
          Choices: []logging.ChatCompletionChoice{
              {
                  Index: 0,
                  Message: &logging.ChatCompletionMessage{
                      Role: "assistant",
                      Content: "Apologies for the inconvenience. Can you please share your customer id?",
                  },
                  FinishReason: "stop",
              },
          },
          Usage: &logging.Usage{
              PromptTokens: 100,
              CompletionTokens: 50,
              TotalTokens: 150,
          },
      })
      ```

      ```java tab="Java"
      generation.setResult(new ChatCompletionResult(
          "chatcmpl-123",
          "chat.completion",
          System.currentTimeMillis() / 1000L,
          "gpt-4o",
          Arrays.asList(new ChatCompletionChoice(
              0,
              new Message("assistant", "Apologies for the inconvenience. Can you please share your customer id?"),
              "stop"
          )),
          new Usage(100, 50, 150)
      ));
      ```
    </Tabs>
  </Step>
</Steps>


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/export-logs
meta: {
  "title": "Export logs and evaluation results as CSV",
  "description": "Learn how to export your logs and evaluation results as a CSV file, enabling easy analysis and reporting of your AI application's performance data."
}

Download your logs and their associated evaluation data in a single CSV file. Filter the export based on your specific requirements and time ranges.

## Export logs and evaluation data

<Steps>
  <Step>
    Open the log repository and select Export CSV option from the top-right menu.
  </Step>

  <Step>
    Select date range and apply filters for specific logs.
  </Step>

  <Step>
    Click Export CSV in the confirmation dialog.
  </Step>
</Steps>

<video url="https://drive.google.com/file/d/1f4ya56ZbmVbUnwjCAYJ6moL5RdaQa5HA/preview" />

## Your CSV export contains:

* Log entries with timestamps.
* Evaluation metrics for each log.
* Individual scores and evaluator feedback.


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/filters-and-saved-views
meta: {
  "title": "Configure filters and saved views",
  "description": "Learn how to efficiently filter and organize your logs with custom criteria and saved views for streamlined debugging and quick access to frequently used search patterns."
}

import { Bookmark } from "lucide-react";
import { Button } from "ui";

Filter and sort your logs using custom criteria to streamline debugging. Create saved views to quickly access your most-used search patterns.

## Create filters and saved views

<Steps>
  <Step>
    Open your log repository and type filters directly in the search bar or select from the filter menu
  </Step>

  <Step>
    Apply filters to display logs matching your specified criteria
  </Step>

  <Step>
    Click the bookmark icon <Button size="icon" variant="outline" className="inline-flex cursor-auto"><Bookmark className="h-4 w-4" /></Button> to save your current filter configuration
  </Step>

  <Step>
    Access your saved filters directly from the search bar for quick navigation
  </Step>
</Steps>

<video url="https://drive.google.com/file/d/1HMc0wNdZdgz2SvZbYgfeD3CAeFLyDH1R/preview" />


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/log-multiturn-interactions-as-session
meta: {
  "title": "Log multi-turn interactions as a session",
  "description": "Learn how to group related traces into sessions to track complete user interactions with your GenAI system."
}

Sessions are particularly useful for tracking multi-turn conversations or complex workflows that span multiple API calls or user interactions. Maintain context across multiple traces, analyze user behavior, and debug multi-interaction issues with sessions. Track the full lifecycle of user engagement by organizing traces into sessions.

<div className="w-full flex justify-end -mb-11">
  <LanguageSwitcher />
</div>

<Steps>
  <Step>
    ## Create a new session

    <Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
      ```typescript tab="JS/TS"
      const session = logger.session({
          id: "session-id",
          name: "session-name",
      });
      ```

      ```python tab="Python"
      from maxim.logger import SessionConfig

      session = logger.session(
          SessionConfig(id="session-id", name="session-name")
      )
      ```

      ```go tab="Go"
      session := logger.Session(&logging.SessionConfig{
          Id: "session-id",
          Name: "session-name",
      })
      ```

      ```java tab="Java"
      Session session = logger.session(
          new SessionConfig("session-id", "session-name")
      );
      ```
    </Tabs>
  </Step>

  <Step>
    ## Add a trace to the session

    After creating a `session` object, you can add multiple traces across the lifecycle of the conversation.

    <Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
      ```typescript tab="JS/TS"
      const trace = session.trace({
          id: "trace-id",
          name: "trace-name",
      });
      ```

      ```python tab="Python"
      from maxim.logger import TraceConfig

      trace = session.trace(
          TraceConfig(id="trace-id", name="trace-name")
      )
      ```

      ```go tab="Go"
      trace := session.AddTrace(&logging.TraceConfig{
          Id: "trace-id",
          Name: "trace-name",
      })
      ```

      ```java tab="Java"
      Trace trace = session.addTrace(
          new TraceConfig("trace-id", "trace-name")
      );
      ```
    </Tabs>
  </Step>

  <Step>
    ## Linking traces using session IDs

    <Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
      ```typescript tab="JS/TS"
      const trace = logger.trace({
          id: "trace-id",
          name: "trace-name",
          sessionId: "session-id",
      });
      ```

      ```python tab="Python"
      from maxim.logger import TraceConfig

      trace = logger.trace(
          TraceConfig(id="trace-id", name="trace-name", session_id="session-id")
      )
      ```

      ```go tab="Go"
      trace := logger.Trace(&logging.TraceConfig{
          Id: "trace-id",
          Name: "trace-name",
          SessionId: "session-id",
      })
      ```

      ```java tab="Java"
      Trace trace = logger.trace(
          new TraceConfig("trace-id", "trace-name", "session-id")
      );
      ```
    </Tabs>
  </Step>
</Steps>


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/logging-rag-pipeline
meta: {
  "title": "Capture your RAG pipeline",
  "description": "Retrieval-Augmented Generation (RAG) is a technique that enhances large language models by retrieving relevant information from external sources before generating responses."
}

This approach combines the power of pre-trained models with up-to-date, domain-specific knowledge, leading to more accurate and contextually appropriate outputs. To capture the RAG pipeline, you need to log it as a "Retrieval" entity. A Retrieval represents a query that fetches relevant context from a knowledge base or vector database.

<Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
  ```typescript tab="JS/TS"
  const retrieval = trace.retrieval({
  	id: "retrieval-id",
  	name: "National Geographic survey report 2025.pdf",
  });

  retrieval.input("best places 2025")
  retrieval.output([
  	"Tokyo, Japan",
  	"Barcelona, Spain",
  	"Singapore",
  	"Copenhagen, Denmark",
  	"Pune, India",
  	"Seoul, South Korea",
  ])

  // Note: Replace 'trace.retrieval' with 'span.retrieval' when creating retrievals within an existing span
  ```

  ```python tab="Python"
  from maxim.logger import RetrievalConfig

  retrieval = trace.retrieval(RetrievalConfig(
  	id="retrieval-id",
  	name="National Geographic survey report 2025.pdf",
  ))
  retrieval.input("best places 2025")
  retrieval.output([
  	"Tokyo, Japan",
  	"Barcelona, Spain",
  	"Singapore",
  	"Copenhagen, Denmark",
  	"Pune, India",
  	"Seoul, South Korea",
  ])

  # Note: Replace 'trace.retrieval' with 'span.retrieval' when creating retrievals within an existing span
  ```

  ```go tab="Go"
  retrieval := span.AddRetrieval(&logging.RetrievalConfig{
  	Id:       "retrieval-id",
  	Name:     "National Geographic survey report 2025.pdf",
  })
  retrieval.SetInput("best places 2025")
  retrieval.SetOutput([]string{
  	"Tokyo, Japan",
  	"Barcelona, Spain",
  	"Singapore",
  	"Copenhagen, Denmark",
  	"Pune, India",
  	"Seoul, South Korea",
  })
  // Note: Replace 'trace.AddRetrieval' with 'span.AddRetrieval when creating retrievals within an existing span
  ```

  ```java tab="Java"
  Retrieval retrieval = trace.addRetrieval(
  	new RetrievalConfig(
  		"retrieval-id",
  		"National Geographic survey report 2025.pdf"
  	)
  );

  retrieval.input("best places 2025");

  retrieval.output(Arrays.asList(
  	"Tokyo, Japan",
  	"Barcelona, Spain",
  	"Singapore",
  	"Copenhagen, Denmark",
  	"Pune, India",
  	"Seoul, South Korea",
  ));
  // Note: Replace 'trace.addRetrieval' with 'span.addRetrieval' when creating retrievals within an existing span
  ```
</Tabs>


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/send-user-feedback
meta: {
  "title": "Send feedback for AI application traces",
  "description": "Track and collect user feedback in application traces using Maxim's Feedback entity. Enhance your AI applications with structured user ratings and comments"
}

<div className="w-full flex justify-end -mb-11">
  <LanguageSwitcher />
</div>

## Add feedback to traces

<Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
  ```typescript tab="JS/TS"
  trace.feedback({
      score: 5,
      feedback: "Great job!",
  });
  ```

  ```python tab="Python"
  from maxim.logger.components import Feedback

  trace.feedback(feedback=Feedback(score=5, comment="Great job!"))
  ```

  ```go tab="Go"
  trace.SetFeedback(&logging.Feedback{
      Score:   5,
      Comment: StrPtr("Great job!"),
  })
  ```

  ```java tab="Java"
  Feedback feedback = trace.setFeedback(new Feedback(
      5,
      "Great job!"
  ));
  ```
</Tabs>


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/setting-up-trace
meta: {
  "title": "Setting up your first trace",
  "description": "Learn how to set up tracing using the Maxim platform"
}

We will cover the necessary steps to instrument your AI application and start monitoring and evaluating its performance.

<div className="w-full flex justify-end -mb-11">
  <LanguageSwitcher />
</div>

<Steps>
  <Step>
    ## Create a new repository on Maxim dashboard

    Select **Logs** from the sidebar and click the "Create repository" button.
  </Step>

  <Step>
    ## Install Maxim SDK

    <Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
      <Tab value="JS/TS">
        ```package-install title="Installing Maxim JS SDK"
        npm install @maximai/maxim-js
        ```
      </Tab>

      <Tab value="Python">
        ```bash
        pip install maxim-py
        ```
      </Tab>

      <Tab value="Go">
        ```bash
        go get github.com/maximhq/maxim-go
        ```
      </Tab>

      <Tab value="Java">
        [https://central.sonatype.com/artifact/ai.getmaxim/sdk](https://central.sonatype.com/artifact/ai.getmaxim/sdk)

        ```groovy
        implementation("ai.getmaxim:sdk:0.1.3")
        ```
      </Tab>
    </Tabs>
  </Step>

  <Step>
    ## Initialize Maxim SDK

    <Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
      ```typescript tab="JS/TS"
      import Maxim from "@maximai/maxim-js"

      // [!code word:apiKey]
      const maxim = new Maxim({ apiKey: "" });

      // [!code word:id]
      const logger = await maxim.logger({ id: "" });
      ```

      ```python tab="Python"
      from maxim import Maxim, Config
      from maxim.logger import Logger, LoggerConfig

      # [!code word:api_key]
      maxim = Maxim(Config(api_key=""))

      # [!code word:id]
      logger = Logger(LoggerConfig(id=""))
      ```

      ```go tab="Go"
      import "github.com/maximhq/maxim-go"

      // [!code word:ApiKey]
      mx := maxim.Init(&maxim.MaximSDKConfig{ApiKey: ""})

      // [!code word:Id]
      logger, err := mx.GetLogger(&logging.LoggerConfig{Id: ""})
      ```

      ```java tab="Java"
      import ai.getmaxim.sdk.Maxim;
      import ai.getmaxim.sdk.Config;

      // [!code word:"api-key"]
      Maxim maxim = new Maxim(new Config(null, "api-key", null, false));

      // [!code word:"id"]
      Logger logger = maxim.logger(new LoggerConfig("id"));
      ```
    </Tabs>
  </Step>

  <Step>
    ## Start tracing your application

    <Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
      ```typescript tab="JS/TS"
      const trace = logger.trace({
      	id: "trace-id", // Unique ID of the trace
      	name: "user-query",
      });

      trace.input("Hello, how are you?");
      trace.output("I'm fine, thank you!");
      trace.end();
      ```

      ```python tab="Python"
      from maxim.logger.components.trace import TraceConfig

      trace = logger.trace(TraceConfig(
      	id="trace-id", # Unique ID of the trace
      	name="user-query",
      ))

      trace.set_input("Hello, how are you?")
      trace.set_output("I'm fine, thank you!")
      trace.end()
      ```

      ```go tab="Go"
      trace := logger.Trace(&logging.TraceConfig{
      	Id: "trace-id", // Unique ID of the trace
      	Name: maxim.StrPtr("user-query"),
      })

      trace.SetInput("Hello, how are you?")
      trace.SetOutput("I'm fine, thank you!")
      trace.End()
      ```

      ```java tab="Java"
      import ai.getmaxim.sdk.logger.components.Trace;
      import ai.getmaxim.sdk.logger.components.TraceConfig;

      Trace trace = logger.trace(new TraceConfig(
      	"trace-id", // Unique ID of the trace
      	"user-query"
      ));

      trace.setInput("Hello, how are you?");
      trace.setOutput("I'm fine, thank you!");
      trace.end();
      ```
    </Tabs>
  </Step>
</Steps>


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/summary-emails
meta: {
  "title": "Set up automated email summaries to monitor your logs",
  "description": "Learn how to set up and manage weekly summary emails for your log repository"
}

Monitor your log repository performance with weekly statistical email updates. Receive key metrics about your repository every Monday.

## Set up summary emails

<Steps>
  <Step>
    Open your log repository and click the top-right menu.
  </Step>

  <Step>
    Select "Enable summary emails" for first-time setup or "Configure summary emails" to modify existing settings.
  </Step>

  <Step>
    Add recipient email addresses.
  </Step>

  <Step>
    Click "Enable summary emails" to confirm.
  </Step>
</Steps>

<video url="https://drive.google.com/file/d/1tol3_95R5lB8RHbBFUPrGhDB4pKNyYJ5/preview" />

## Manage your email preferences

To modify recipient list or disable summary emails:

1. Navigate to repository settings
2. Select "Configure summary emails"
3. Update email addresses as needed
4. Click "Save changes" to confirm

## Email content overview

Each weekly summary includes:

* View total traces logged this week
* Average user feedback scores
* Latency trends and performance metrics
* Weekly activity highlights

## Benefits of summary emails

* Track repository performance trends
* Monitor user feedback patterns
* Identify potential performance issues
* Make data-driven decisions

![Sample email showing weekly repository statistics and performance metrics](/images/docs/observe/how-to/log-your-application/maxim-log-repository-weekly-summary-email.png)


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/track-llm-errors
meta: {
  "title": "Track errors in traces",
  "description": "Learn how to effectively track and log errors from LLM results and Tool calls in your AI application traces to improve performance and reliability."
}

## Track LLM errors in your workflow

<Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
  ```typescript tab="JS/TS"
  // Create generation object
  const generation = trace.generation({
      id: "generation-id",
      name: "customer-support--gather-information",
      // Additional fields
  });

  // Track error
  generation.error({
      message: "Rate limit exceeded. Please try again later.",
      type: "RateLimitError",
      code: "429",
  });
  ```

  ```python tab="Python"
  from maxim.logger.components.generation import GenerationConfig, GenerationError

  # Create generation object
  generation = trace.generation(GenerationConfig(
      id="generation-id",
      name="customer-support--gather-information",
      # Additional fields
  ))

  # Track error
  generation.error(GenerationError(
      message="Rate limit exceeded. Please try again later.",
      type="RateLimitError",
      code="429",
  ))
  ```

  ```go tab="Go"
  // Create generation object
  generation := trace.AddGeneration(&logging.GenerationConfig{
      Id: "generation-id",
      Name: "customer-support--gather-information",
      // Additional fields
  })

  // Track error
  generation.SetError(&logging.GenerationError{
      Message: "Rate limit exceeded. Please try again later.",
      Type:    "RateLimitError",
      Code:    "429",
  })
  ```

  ```java tab="Java"
  // Create generation object
  Generation generation = trace.addGeneration(new GenerationConfig(
      "generation-id",
      "customer-support--gather-information",
      // Additional fields
  ));

  // Track error
  generation.error(new GenerationError(
      "Rate limit exceeded. Please try again later.",
      "429",
      "RateLimitError",
  ));
  ```
</Tabs>

<Callout>
  Learn how to track complete LLM flows in the [LLM logging guide](/docs/observe/how-to/log-your-application/adding-llm-call).
</Callout>

## Track errors from tool calls

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  const traceToolCall = trace.toolCall({
      id: "tool-call-id",
      name: "tool-call-name",
  });

  traceToolCall.error({
      message: "Service is currently unavailable. Please try again later.",
      type: "ServiceUnavailableError",
      code: "503",
  });
  ```

  ```python tab="Python"
  from maxim.logger import ToolCallConfig

  trace_tool_call = trace.tool_call(ToolCallConfig(
      id="tool-call-id",
      name="tool-call-name",
  ))

  trace_tool_call.error(ToolCallError(
      message="Service is currently unavailable. Please try again later.",
      type="ServiceUnavailableError",
      code="503",
  ))
  ```
</Tabs>

<Callout>
  Explore more on tool call tracking in the [Tool calls logging guide](/docs/observe/how-to/log-your-application/track-tool-calls).
</Callout>


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/track-token-usage-and-cost
meta: {
  "title": "Track token usage and costs",
  "description": "Learn how to efficiently track token usage and associated costs in your LLM application using Maxim's logging capabilities."
}

## Code examples by language

Log token usage by including the usage object in your generation result:

<Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
  ```typescript tab="JS/TS"
  generation.result({
      id: "chatcmpl-123",
      object: "chat.completion",
      created: Date.now(),
      model: "gpt-4o",
      choices: [{
          index: 0,
          message: {
              role: "assistant",
              content: "Apologies for the inconvenience. Can you please share your customer id?"
          },
          finish_reason: "stop"
      }],
      usage: {
          prompt_tokens: 100,
          completion_tokens: 50,
          total_tokens: 150
      }
  });
  ```

  ```python tab="Python"
  generation.result({
      "id": "chatcmpl-123",
      "object": "chat.completion",
      "created": int(time.time()),
      "model": "gpt-4o",
      "choices": [{
          "index": 0,
          "message": {
              "role": "assistant",
              "content": "Apologies for the inconvenience. Can you please share your customer id?"
          },
          "finish_reason": "stop"
      }],
      "usage": {
          "prompt_tokens": 100,
          "completion_tokens": 50,
          "total_tokens": 150
      }
  })
  ```

  ```go tab="Go"
  generation.SetResult(map[string]interface{}{
      Id: "chatcmpl-123",
      Object: "chat.completion",
      Created: time.Now().Unix(),
      Model: "gpt-4o",
      Choices: []logging.ChatCompletionChoice{
          {
              Index: 0,
              Message: &logging.ChatCompletionMessage{
                  Role: "assistant",
                  Content: "Apologies for the inconvenience. Can you please share your customer id?",
              },
              FinishReason: "stop",
          },
      },
      Usage: &logging.Usage{
          PromptTokens: 100,
          CompletionTokens: 50,
          TotalTokens: 150,
      },
  })
  ```

  ```java tab="Java"
  generation.setResult(new ChatCompletionResult(
      "chatcmpl-123",
      "chat.completion",
      System.currentTimeMillis() / 1000L,
      "gpt-4o",
      Arrays.asList(new ChatCompletionChoice(
          0,
          new Message("assistant", "Apologies for the inconvenience. Can you please share your customer id?"),
          "stop"
      )),
      new Usage(100, 50, 150)
  ));
  ```
</Tabs>

<Callout>
  Learn more about tracking [generation results](/docs/observe/how-to/log-your-application/adding-llm-call).
</Callout>

## Custom pricing

Need different pricing for your models? Read more on [custom pricing](/docs/observe/how-to/log-your-application/use-custom-pricing).


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/track-tool-calls
meta: {
  "title": "Track tool calls",
  "description": "Track external system calls triggered by LLM responses in your agentic workflows. Tool calls represent interactions with external services, allowing you to monitor execution time and responses."
}

## How to log tool calls

<Tabs groupId="language" items={["JS/TS", "Python"]}>
  ```typescript tab="JS/TS"
  const toolCall = completion.choices[0].message.tool_calls[0];
  const traceToolCall = trace.toolCall({
      id: toolCall.id,
      name: toolCall.function.name,
      description: "Get current temperature for a given location.",
      args: toolCall.function.arguments,
      tags: { location: toolCall.function.arguments["location"] }
  });
  // Note: Replace 'trace.toolCall' with 'span.toolCall' when you are creating tool calls within an existing span

  try {
      const result = callExternalService(toolCall.function.name, toolCall.function.arguments);
      traceToolCall.result(result);
  } catch (error) {
      traceToolCall.error(error);
  }

  ```

  ```python tab="Python"
  from maxim.logger import ToolCallConfig

  tool_call = completion.choices[0].message.tool_calls[0]
  tool_call_config = ToolCallConfig(
      id=tool_call.id,
      name=tool_call.function.name,
      description="Get current temperature for a given location.",
      args=tool_call.function.arguments,
      tags={ "location": toolCall.function.arguments["location"] }
  )
  trace_tool_call = trace.tool_call(tool_call_config)
  # Note: Replace 'trace.tool_call' with 'span.tool_call' when you are creating tool calls within an existing span

  try:
      result = call_external_service(tool_call.function.name, tool_call.function.arguments)
      trace_tool_call.result(result)
  except Exception as e:
      error = ToolCallError(message=str(e), type=type(e).__name__)
      trace_tool_call.error(error)
  ```
</Tabs>


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/use-custom-pricing
meta: {
  "title": "Set up custom token pricing",
  "description": "Learn how to set up custom token pricing in Maxim for accurate cost reporting in AI evaluations and logs, ensuring displayed costs match your actual expenses."
}

Configure your negotiated token costs for accurate cost reporting in AI evaluations and real-time logs. Custom token pricing ensures that displayed costs match your actual expenses based on special pricing agreements.

## How it works

<Steps>
  <Step>
    Enter your custom input and output token costs
  </Step>

  <Step>
    Apply costs to model configs and log repositories
  </Step>

  <Step>
    System calculates actual costs for each evaluation
  </Step>

  <Step>
    Standard pricing applies when no custom rates exist
  </Step>
</Steps>

## Create custom pricing structures

1. Navigate to **Settings > Models > Pricing**
2. Enter a model name pattern (string or regex) that matches your model names
3. Input your token usage cost (per 1000 tokens)

![Interface for creating custom token pricing structures](/images/docs/observe/custom-pricing-structures/add_pricing.png)

## Configure model pricing

1. Go to **Settings > Models > Model Configs**
2. Select a model config to edit
3. Locate the **Pricing structure** section
4. Choose your pricing structure from the dropdown

Custom pricing supports OpenAI, Microsoft Azure, Groq, HuggingFace, Together AI, Google Cloud, and Amazon Bedrock models.

![Attach the pricing structure to model config](/images/docs/observe/custom-pricing-structures/attach_pricing_model_config.png)

## Set up pricing for log repository

1. Open **Logs** from the sidebar
2. Select the log repository you want to configure custom pricing for
3. Find the **Pricing structure** section
4. Choose your pricing structure from the dropdown

![Attach pricing structure to log repository](/images/docs/observe/custom-pricing-structures/attach_pricing_log_repo.png)


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/use-data-connectors
meta: {
  "title": "Connect your logs to external platforms",
  "description": "Learn how to integrate Maxim with external observability platforms using data connectors for enhanced log analysis and monitoring."
}

Export your logs to external observability platforms to analyze and monitor application behavior in your preferred tools. Connect your logs to these platforms:

* New Relic
* Snowflake
* Any OpenTelemetry (OTLP) collector

<video url="https://drive.google.com/file/d/1xpqcz9CCvHuczSbWWYd-w7xyf7CbpY7-/preview" />

<Callout type="info">
  Navigate to your log repository, click the top-right menu, and select "Set up data connectors" to manage your connections.
</Callout>

## Connect to New Relic

<Steps>
  <Step>
    Select "New Relic" as your platform in the data connectors setup dialog.
  </Step>

  <Step>
    Name your connector - for example, Production connector.
  </Step>

  <Step>
    Enter your New Relic API key from your [account settings](https://one.newrelic.com/api-keys).
  </Step>

  <Step>
    ![Configure your New Relic connection settings](/images/docs/observe/data-connectors/new-relic-connector.png)
  </Step>
</Steps>

## Connect to Snowflake

<Steps>
  <Step>
    Select "Snowflake" as your platform in the data connectors setup dialog.
  </Step>

  <Step>
    Name your connector - for example, Production connector.
  </Step>

  <Step>
    Login to your Snowflake account.
  </Step>

  <Step>
    Open profile menu and select "Connect a tool to Snowflake".
  </Step>

  <Step>
    Select "Connectors/Drivers" tab and then "Go Driver Connection String".
  </Step>

  <Step>
    Select Warehouse, Database, Schema, and Connection method as "Password".
  </Step>

  <Step>
    ![Snowflake connection string dialog](/images/docs/observe/data-connectors/snowflake-connection-string.png)
  </Step>

  <Step>
    Copy the connection string and paste it in the data connectors setup dialog.
  </Step>

  <Step>
    ![Configure your Snowflake connection settings](/images/docs/observe/data-connectors/snowflake-connector.png)
  </Step>
</Steps>

## Connect to an OTLP collector

<Steps>
  <Step>
    Select "Other OTel collector" in the data connectors setup dialog
  </Step>

  <Step>
    Name your connector
  </Step>

  <Step>
    Configure the OTLP endpoint:

    * Choose HTTP or gRPC protocol
    * Enter your OTLP collector URL (Example: [https://otel-collector.acme.com](https://otel-collector.acme.com))
  </Step>

  <Step>
    Configure headers:

    * Add authentication headers (Example: x-api-key)
    * Include additional headers as needed
  </Step>

  <Step>
    ![Configure your OTLP collector settings](/images/docs/observe/data-connectors/otlp-connector.png)
  </Step>
</Steps>

## How we protect your data

* We encrypt all credentials in storage
* We transmit data securely using HTTPS encryption
* We recommend using dedicated API keys with minimal permissions


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/use-events
meta: {
  "title": "Use Events to send point-in-time information",
  "description": "Track application milestones and state changes using event logging"
}

Create events to mark specific points in time during your application execution. Capture additional metadata such as intermediate states and system milestones through events.

<div className="w-full flex justify-end -mb-11">
  <LanguageSwitcher />
</div>

## Attach an event to your trace

<Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
  ```typescript tab="JS/TS"
  await trace.event({
      id: "event-id",
      name: "travel-plan-emailed",
      tags: { "email": "JetSetJoe@travelo.com" },
  });
  // Note: Replace 'trace.event' with 'span.event' when you are creating events within a span
  ```

  ```python tab="Python"
  await trace.event(
      id="event-id",
      name="travel-plan-emailed",
      tags={"email": "JetSetJoe@travelo.com"}
  )
  # Note: Replace 'trace.event' with 'span.event' when you are creating events within a span
  ```

  ```go tab="Go"
  trace.AddEvent(
      "event-id",
      "travel-plan-emailed",
      map[string]string{"email": "JetSetJoe@travelo.com"}
  )
  // Note: Replace 'trace.AddEvent' with 'span.AddEvent' when you are creating events within a span
  ```

  ```java tab="Java"
  Event event = trace.addEvent(
      "event-id",
      "travel-plan-emailed",
      Map.of("email", "JetSetJoe@travelo.com")
  );
  // Note: Replace 'trace.addEvent' with 'span.addEvent' when you are creating events within a span
  ```
</Tabs>


url: https://getmaxim.ai/docs/observe/how-to/log-your-application/use-tags-on-nodes
meta: {
  "title": "Use tags on nodes",
  "description": "Tag your traces to group and filter workflow data effectively. Add tags to any node type - spans, generations, retrievals, events, and more."
}

<div className="w-full flex justify-end -mb-11">
  <LanguageSwitcher />
</div>

## Add tags to a trace

<Tabs groupId="language" items={["JS/TS", "Python", "Go", "Java"]}>
  ```typescript tab="JS/TS"
  const trace = logger.trace({
      id: "trace-id",
      name: "user-query",
      tags: {
          productId: "instaTravel",
          experimentId: "fastlane",
      },
  });
  ```

  ```python tab="Python"
  from maxim.logger.components.trace import TraceConfig

  trace = logger.trace(TraceConfig(
      id="trace-id",
      name="user-query",
      tags={
          "productId": "instaTravel",
          "experimentId": "fastlane",
      },
  ))
  ```

  ```go tab="Go"
  trace := logger.Trace(&logging.TraceConfig{
      Id: "trace-id",
      Name: "user-query",
      Tags: {
          "productId": "instaTravel",
          "experimentId": "fastlane",
      },
  })
  ```

  ```java tab="Java"
  import ai.getmaxim.sdk.logger.components.Trace;
  import ai.getmaxim.sdk.logger.components.TraceConfig;

  Trace trace = logger.trace(new TraceConfig(
      "trace-id",
      "user-query",
      Map.of("productId", "instaTravel", "experimentId", "fastlane"),
  ));
  ```
</Tabs>


url: https://getmaxim.ai/docs/observe/how-to/set-up-alerts/create-a-pagerduty-integration
meta: {
  "title": "Create a PagerDuty integration",
  "description": "Send alert notifications to your PagerDuty service by creating a PagerDuty integration in Maxim."
}

## Prerequisites

Gather these requirements before starting:

1. A PagerDuty account with permission to create a service and integration.
2. A PagerDuty service where you want to receive notifications.
3. The **Integration Key** (also known as the **Routing Key**) for the PagerDuty service.

Find this key by following these steps:

<Steps>
  <Step>
    In PagerDuty, navigate to **Services** → **Service Directory** and select or search the service you want to integrate with Maxim.
  </Step>

  <Step>
    Go to the **Integrations** tab.
  </Step>

  <Step>
    Click **Add or manage integrations**.
  </Step>

  <Step>
    Click the **+ New Integration** button.
  </Step>

  <Step>
    Enter a name for the integration (e.g., "Maxim Alerts").
  </Step>

  <Step>
    Select the integration type (usually "Events API v2").
  </Step>

  <Step>
    Click **Add Integration**.
  </Step>

  <Step>
    Copy the **Integration Key** that is generated.
  </Step>
</Steps>

## Creating the integration in Maxim

<Steps>
  <Step>
    Navigate to the **Settings** page in Maxim.
  </Step>

  <Step>
    Click on **Integrations** in the left sidebar.
  </Step>

  <Step>
    Click the **Add integration** button.
  </Step>

  <Step>
    Choose **PagerDuty** as the integration type.
  </Step>

  <Step>
    In the **Integration Key** field, paste the Integration Key you copied from PagerDuty.
  </Step>

  <Step>
    Give your integration a descriptive name (e.g., "Production Alerts").
  </Step>

  <Step>
    Click **Save** to create the integration.
  </Step>
</Steps>

<video url="https://drive.google.com/file/d/1o6MmU4jT1MY6u5YXer4GKcW0dJHK2lgY/preview" />

## Using the integration

Once the PagerDuty integration is created, you can select it as a notification channel when creating or editing alerts. Triggered alerts create incidents in the specified PagerDuty service.

## Editing or deleting the integration

Edit or delete an integration by hovering over the respective integration on the Integrations page and clicking on the edit/delete icons respectively.


url: https://getmaxim.ai/docs/observe/how-to/set-up-alerts/create-a-slack-integration
meta: {
  "title": "Create a Slack integration",
  "description": "Send alert notifications directly to your Slack channels by creating a Slack integration in Maxim."
}

## Prerequisites

Before you begin, ensure you have the following

1. A Slack workspace where you have permission to create a webhook.
2. The webhook URL for the Slack channel where you want to receive notifications.

<Callout type="info">
  You can learn more about setting up Slack's webhooks [here.](https://api.slack.com/messaging/webhooks)
</Callout>

## Creating the integration in Maxim

<Steps>
  <Step>
    Open the Settings page in Maxim.
  </Step>

  <Step>
    Click on **Integrations** in the left sidebar.
  </Step>

  <Step>
    Click the **Add integration** button.
  </Step>

  <Step>
    Choose **Slack** as the integration type.
  </Step>

  <Step>
    In the **Webhook URL** field, paste the webhook URL you copied from Slack.
  </Step>

  <Step>
    Give your integration a descriptive name (e.g., "Production Alerts").
  </Step>

  <Step>
    Click **Save** to create the integration.
  </Step>
</Steps>

<video url="https://drive.google.com/file/d/13M37GXnqLgsTLFzPKm2EB3oIlYgLhz9I/preview" />

## Using the integration

Select the Slack integration as a notification channel when creating or editing alerts. Maxim sends notifications for triggered alerts to the specified Slack channel via the webhook URL.

## Editing or deleting the integration

Edit or delete an integration by hovering over the respective integration on the Integrations page and clicking on the edit/delete icons respectively.


url: https://getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-performance-metrics
meta: {
  "title": "Setting up alerts for performance metrics",
  "description": "Learn how to set up alerts to monitor your application's performance metrics in Maxim."
}

Monitor your application's performance by setting up alerts for latency, token usage, and cost metrics. Create custom thresholds and receive notifications when metrics exceed your specified limits.

## Available metrics

Set up alerts for:

* **Latency**: Response times for API calls
* **Token Usage**: Token consumption per request
* **Cost**: API usage expenses

## Create an alert

<Steps>
  <Step>
    Open the Logs page and select your repository
  </Step>

  <Step>
    Select the **Alerts** tab and click **Create alert**
  </Step>

  <Step>
    Configure your alert settings:

    * Select **Log metrics** as the type of alert
    * Select a metric (Latency, Token Usage, or Cost)
    * Choose an operator (greater than, less than)
    * Enter the threshold value
    * Set minimum occurrence count
    * Define evaluation time range
  </Step>

  <Step>
    Select your preferred notification channels (Slack, or PagerDuty)
  </Step>

  <Step>
    Click **Create alert**
  </Step>
</Steps>

<video url="https://drive.google.com/file/d/1jJc3wkB8LPi1q0RY8kEOWBnfuTI7CwE4/preview" />

## Common alert configurations" or "Alert configuration examples

### Monitor response time

```
Metric: Latency
Operator: greater than
Threshold: 1000 ms
Minimum occurrences: 5
Time range: 5 minutes
```

Triggers when response time exceeds 1 second in 5 requests within 5 minutes.

### Setting up token consumption alerts

```
Metric: Token Usage
Operator: greater than
Threshold: 1000000
Minimum occurrences: 1
Time range: 1 hour
```

Triggers when hourly token usage exceeds 1 million.

### Monitor daily costs

```
Metric: Cost
Operator: greater than
Threshold: 100 USD
Minimum occurrences: 1
Time range: 24 hours
```

Triggers when daily costs exceed $100.

## Manage alerts

* **Edit**: Click the options menu (three dots) > Edit alert
* **Delete**: Click the options menu > Delete alert
* **Pause/Resume**: Click the options menu > Pause alert or Resume alert

## Best practices

1. Begin with conservative thresholds to avoid alert fatigue
2. Match time ranges to your application's usage patterns
3. Use multiple occurrence thresholds for latency alerts to prevent false alarms
4. Create layered alerts with different thresholds for comprehensive monitoring


url: https://getmaxim.ai/docs/observe/how-to/set-up-alerts/set-up-alerts-for-quality-metrics
meta: {
  "title": "Set up alerts for quality metrics",
  "description": "Learn how to set up alerts to monitor evaluation scores and quality checks in Maxim."
}

Monitor your AI application's quality with alerts for evaluation scores and quality checks. Receive notifications when AI responses don't meet expected quality standards.

## Choose evaluation metrics for alerts

Set up alerts for various evaluation scores, such as:

* Bias-check: Monitor potential biases in AI responses
* Toxicity: Check for inappropriate or harmful content
* Clarity: Validate clear and understandable output
* Factual accuracy: Verify generated information accuracy
* Custom evaluators: Monitor your defined evaluation metrics

## Create a quality alert

<Steps>
  <Step>
    Open the **Logs** page and select the repository you want to monitor.
  </Step>

  <Step>
    Select the **Alerts** tab and click **Create alert**
  </Step>

  <Step>
    Select **Evaluation scores** as the type of alert.
  </Step>

  <Step>
    Configure alert settings:

    * Choose an evaluation metric (e.g., "Bias-check")
    * The violation criteria is based on your evaluator's type and configuration.
    * Specify how many times this should occur
    * Set the evaluation time range
  </Step>

  <Step>
    Select the notification channels where you want to receive alerts.
  </Step>

  <Step>
    Click **Create alert** to save your configuration.
  </Step>
</Steps>

<video url="https://drive.google.com/file/d/1F7MeY-fHmI8zF1rPwSd23ronC2gumFGa/preview" />

## Example configurations

### Bias check alert

```
Metric: Bias-check
Minimum occurrences: 1
Time range: 15 minutes
```

Triggers when a response fails the bias check within 15 minutes.

### Toxicity alert

```
Metric: Toxicity
Condition: violates pass criteria
Score: 0.8
Minimum occurrences: 1
Time range: 5 minutes
```

Triggers when a response reaches 0.8 or higher toxicity score.

## Manage quality alerts

Manage your quality alerts in the following ways:

* **Edit an alert**: Click the options icon (three dots) on the alert card and select "Edit alert"
* **Delete an alert**: Click the options icon and select "Delete alert"
* **Pause/Resume an alert**: Click the options icon and select "Pause alert" or "Resume alert"

## Follow these best practices

1. Monitor critical evaluators with immediate alerts for bias and toxicity
2. Adjust score thresholds to match application requirements
3. Combine evaluation metrics for comprehensive monitoring
4. Review and adjust alert criteria based on performance
5. Document quality issues and solutions from alerts
