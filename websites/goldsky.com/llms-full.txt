# Benefits
Source: https://docs.goldsky.com/benefits

A quick overview of the Goldsky Advantage.

## Subgraphs

<AccordionGroup>
  <Accordion title="Upgraded developer experience" icon="computer-mouse">
    In addition to the standard subgraph development experience, Goldsky offers
    numerous developer experience improvements. Specifically:

    1. **Webhooks**: Enable efficient, instant, push-based communication and eliminate the need
       for API polling or manual data retrieval. This enables realtime
       notifications, data synchronization, and numerous other use cases.

    2. **Instant subgraphs**: Index contract data without a single line of code,
       allowing developers to explore contracts with ease, and for non-technical
       users to work with blockchains more easily.

    3. **Tags**: A ground-up rethink of subgraph endpoint management that allows you to seamlessly update your front-end interfaces with zero downtime or stale data.
  </Accordion>

  <Accordion title="Improved reliability and performance" icon="bolt-lightning">
    Goldsky proxies all data ingestion through an advanced load balancer with
    over 20+ RPC endpoints and automatically prioritizes between them based on
    latency, time of day, historical responsiveness, and more. This means that
    Goldsky indexes data more quickly, and with greater uptime reliability than
    the alternatives.
  </Accordion>

  <Accordion title="Custom chain support" icon="person-sign">
    On a dedicated indexing instance, Goldsky offers the ability to add custom
    RPC endpoints for any EVM-compatible chain with no downtime. This allows you
    to work with custom or private blockchains seamlessly.
  </Accordion>

  <Accordion title="Integrated with broader data stack" icon="link">
    Goldsky helps integrate Subgraph data into your broader infrastructure via
    Mirror, providing a level of flexibility and control that is not possible
    via API-based solutions. This unlocks more granular data integration,
    enabling advanced use cases such as multi-chain subgraphs.
  </Accordion>
</AccordionGroup>

## Mirror

<AccordionGroup>
  <Accordion title="Own your data" icon="database">
    By replicating data into your own database, you can co-locate it alongside
    your other app data (product, customer, and any off-chain data). This
    eliminates the need for brittle scraping and polling scripts, and simplifies
    your front-end queries.
  </Accordion>

  <Accordion title="Parallelizable" icon="line-columns">
    Mirror workers are parallelizable, enabling unrivaled performance and
    throughput. This means that working with large-scale datasets (eg. full
    chain replication) is the work of minutes and hours instead of days and
    weeks, allowing for faster iteration.
  </Accordion>
</AccordionGroup>

## Platform

<AccordionGroup>
  <Accordion title="Easy to work with" icon="wrench">
    With no token-based payments, your team no longer needs to worry about
    fluctuating service costs and operate a token trading desk to pay your
    service providers. In additions, Goldsky doesn't charge any per-query fees,
    making our costs and their rate of change highly predictable.
  </Accordion>

  <Accordion title="Enterprise-level support" icon="phone">
    Goldsky offers 24/7 on-call support and has a team of engineering staff
    available to assist with debugging, issue resolution, and proactive
    management.
  </Accordion>
</AccordionGroup>


# Indexing Abstract with Goldsky
Source: https://docs.goldsky.com/chains/abstract



## Overview

Goldsky is a high-performance data indexing provider for Abstract that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Abstract to make our product available to the ecosystem and provide dedicated support for Abstract data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Abstract subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Abstract Mainnet and Testnet are currently supported at the chain slugs `abstract` and `abstract-testnet` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing ApeChain with Goldsky
Source: https://docs.goldsky.com/chains/apechain



## Overview

Goldsky is a high-performance data indexing provider for ApeChain that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with the APE Foundation to make our product available to the ecosystem and provide dedicated support for ApeChain data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

ApeChain Subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

ApeChain's Mainnet and Curtis Testnet are available at the chain slugs `apechain-mainnet` and `apechain-curtis` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Artela with Goldsky
Source: https://docs.goldsky.com/chains/artela



## Overview

Goldsky is a high-performance data indexing provider for Artela that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Artela to make our product available to the ecosystem and provide dedicated support for Artela builders. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                            | Mirror                                              |
| ----------------------------- | ---------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                  | Yes                                                 |
| <strong>Benefit</strong>      | 10% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | All developers                                       | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Artela subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Artela's mainnet has the slug `artela` and the public testnet is available with the chain slug `artela-betanet`.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Arweave with Goldsky
Source: https://docs.goldsky.com/chains/arweave



<Info>Coming soon. If you're running into issues building on Arweave, please contact [support@goldsky.com](mailto:support@goldsky.com) and we'd be happy to help.</Info>


# Indexing Berachain with Goldsky
Source: https://docs.goldsky.com/chains/berachain



## Overview

Goldsky is a high-performance data indexing provider for Berachain that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Berachain to make our product available to the ecosystem and provide dedicated support for Berachain data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Berachain subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Berachain's mainnet and testnet are available under the chain slugs `berachain-mainnet` and `berachain-bepolia` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Blast with Goldsky
Source: https://docs.goldsky.com/chains/blast



## Overview

Goldsky is a high-performance data indexing provider for Blast that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Blast to make our product available to the ecosystem and provide dedicated support for Blast data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                            | Mirror                                              |
| ----------------------------- | ---------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                  | Yes                                                 |
| <strong>Benefit</strong>      | 10% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | All developers                                       | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Blast subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both Blast mainnet and testnet are available at the chain slugs `blast` and `blast-sepolia` respectively.

### Mirror

<Info>Support for Goldsky Mirror for Blast is currently in progress. If you'd like to be notified when support is launched publicly, contact us at [sales@goldsky.com](mailto:sales@goldsky.com).</Info>

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing BOB with Goldsky
Source: https://docs.goldsky.com/chains/build-on-bitcoin



## Overview

Goldsky is a high-performance data indexing provider for Build on Bitcoin (BOB) that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with BOB to make our product available to the ecosystem and provide dedicated support for BOB data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

BOB subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both BOB mainnet and testnet are available at the chain slugs `bob` and `bob-testnet` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Citrea with Goldsky
Source: https://docs.goldsky.com/chains/citrea



## Overview

Goldsky is a high-performance data indexing provider for the Citrea Devnet that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Citrea to make our product available to the ecosystem and provide dedicated support for Citrea data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Citrea Devnet subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Citrea's Devnet and Testnet are available at the chain slugs `citrea-devnet` and `citrea-testnet` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Cronos zkEVM with Goldsky
Source: https://docs.goldsky.com/chains/cronos-zkevm



## Overview

Goldsky is a high-performance data indexing provider for Cronos zkEVM that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Cronos to make our product available to the ecosystem and provide dedicated support for Cronoz zkEVM data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Cronos zkEVM subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Cronos zkEVM's Mainnet and Testnet are available at the chain slugs `cronos-zkevm` and `cronos-zkevm-sepolia` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Filecoin with Goldsky
Source: https://docs.goldsky.com/chains/filecoin



## Overview

Goldsky is a high-performance data indexing provider for Filecoin that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Filecoin to make our product available to the Filecoin ecosystem. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                        |
| ----------------------------- | ----------------------------------------------------- | ----------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Only Subgraphs as Data Source |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | N/A                           |
| <strong>Availability</strong> | All developers                                        | N/A                           |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Filecoin subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Filecoin Mainnet and Testnet are currently available at the chain slugs `filecoin` and `filecoin-testnet`.

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Flare with Goldsky
Source: https://docs.goldsky.com/chains/flare



## Overview

Goldsky is a high-performance data indexing provider for Flare that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Flare to make our product available to the ecosystem and provide dedicated support for Flare data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Flare subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Flare's Mainnet and Coston2 Testnet are available at the chain slugs `flare` and `flare-coston2` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing HyperEVM with Goldsky
Source: https://docs.goldsky.com/chains/hyperevm



## Overview

Goldsky is a high-performance data indexing provider for HyperEVM that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

HyperEVM subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

HyperEVM Mainnet is available at the chain slug `hyperevm`.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Immutable zkEVM with Goldsky
Source: https://docs.goldsky.com/chains/immutable-zkevm



## Overview

Goldsky is a high-performance data indexing provider for Immutable zkEVM that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Immutable zkEVM to make our product available to the ecosystem and provide dedicated support for Immutable zkEVM data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Immutable zkEVM subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both Immutable zkEVM mainnet and testnet are available at the chain slugs `imtbl-zkevm` and `imtbl-zkevm-testnet` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Overview
Source: https://docs.goldsky.com/chains/index

How to use Goldsky across EVM and non-EVM chains.

## All networks

Goldsky currently supports more than 90 networks across Subgraphs and Mirror. For the full list of supported networks, click through to the dedicated reference page [here](/chains/supported-networks).

## Partner networks

Goldsky works directly with leading and emerging networks to make its indexing solutions available to the developer ecosystem. For networks, this introduces several advantages:

* enhanced developer experience for your network
* seamless porting of applications from other major chains
* industry-leading infrastructure reliability and performance
* offloaded engineering and developer success

Chain-specific documentation for each of Goldsky's partner networks is linked below. This is currently a work-in-progress, so if you are building on one of our partner networks or would like to learn more about our network partnership approach, please don't hesitate to contact us at [support@goldsky.com](mailto:support@goldsky.com).

<CardGroup cols="3">
  <Card title="Abstract" icon={<svg width="31" height="30" viewBox="0 0 31 30" fill="none" xmlns="http://www.w3.org/2000/svg"> <circle cx="15.8" cy="15" r="15" fill="#080A0E"/> <circle cx="15.8" cy="15" r="15" fill="url(#paint0_linear_425_1229)"/> <path d="M7.98034 17.869L12.602 16.6388L12.6047 16.6362L15.2769 12.0354L14.0397 7.43994L11.8687 8.017L13.1059 12.6125C13.2114 13.0059 13.1586 13.4177 12.9529 13.7692C12.7471 14.1233 12.4174 14.3752 12.0217 14.4801L7.39999 15.7103L7.98034 17.869Z" fill="#00DE73"/> <path d="M7.98034 17.869L12.602 16.6388L12.6047 16.6362L15.2769 12.0354L14.0397 7.43994L11.8687 8.017L13.1059 12.6125C13.2114 13.0059 13.1586 13.4177 12.9529 13.7692C12.7471 14.1233 12.4174 14.3752 12.0217 14.4801L7.39999 15.7103L7.98034 17.869Z" fill="url(#paint1_linear_425_1229)"/> <path d="M19.0729 16.6388L23.6947 17.869L24.275 15.7103L19.6533 14.4801C19.2576 14.3752 18.9278 14.1233 18.7221 13.7692C18.5163 13.4177 18.4636 13.0059 18.5691 12.6125L19.8063 8.017L17.6352 7.43994L16.398 12.0354L19.0703 16.6362L19.0729 16.6388Z" fill="#00DE73"/> <path d="M19.0729 16.6388L23.6947 17.869L24.275 15.7103L19.6533 14.4801C19.2576 14.3752 18.9278 14.1233 18.7221 13.7692C18.5163 13.4177 18.4636 13.0059 18.5691 12.6125L19.8063 8.017L17.6352 7.43994L16.398 12.0354L19.0703 16.6362L19.0729 16.6388Z" fill="url(#paint2_linear_425_1229)"/> <path d="M18.5084 17.6094L21.893 20.9747L20.3049 22.5537L16.9204 19.1885C16.6302 18.8998 16.2477 18.7426 15.8362 18.7426C15.4247 18.7426 15.0422 18.8998 14.752 19.1885L11.3675 22.5537L9.77945 20.9747L13.1639 17.6094H18.5084Z" fill="#00DE73"/> <path d="M18.5084 17.6094L21.893 20.9747L20.3049 22.5537L16.9204 19.1885C16.6302 18.8998 16.2477 18.7426 15.8362 18.7426C15.4247 18.7426 15.0422 18.8998 14.752 19.1885L11.3675 22.5537L9.77945 20.9747L13.1639 17.6094H18.5084Z" fill="url(#paint3_linear_425_1229)"/> <defs> <linearGradient id="paint0_linear_425_1229" x1="15.8" y1="15" x2="15.8" y2="30" gradientUnits="userSpaceOnUse"> <stop stop-color="#F5F4F0" stop-opacity="0.12"/> <stop offset="1" stop-opacity="0"/> </linearGradient> <linearGradient id="paint1_linear_425_1229" x1="15.8375" y1="7.43994" x2="15.8375" y2="22.5536" gradientUnits="userSpaceOnUse"> <stop offset="0.04" stop-color="white" stop-opacity="0.06"/> <stop offset="0.555481" stop-opacity="0"/> <stop offset="1" stop-opacity="0.3"/> </linearGradient> <linearGradient id="paint2_linear_425_1229" x1="15.8375" y1="7.43994" x2="15.8375" y2="22.5536" gradientUnits="userSpaceOnUse"> <stop offset="0.04" stop-color="white" stop-opacity="0.06"/> <stop offset="0.555481" stop-opacity="0"/> <stop offset="1" stop-opacity="0.3"/> </linearGradient> <linearGradient id="paint3_linear_425_1229" x1="15.8375" y1="7.44001" x2="15.8375" y2="22.5537" gradientUnits="userSpaceOnUse"> <stop offset="0.04" stop-color="white" stop-opacity="0.06"/> <stop offset="0.555481" stop-opacity="0"/> <stop offset="1" stop-opacity="0.3"/> </linearGradient> </defs> </svg>} href="/chains/abstract" />

  <Card title="ApeChain" icon={<svg width="30" height="30" viewBox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"><clipPath id="clipPath1"><path d="M 14.966588 0.039606 L 14.966588 0.039606 C 23.232407 0.039606 29.933175 6.740377 29.933175 15.006194 L 29.933175 15.006194 C 29.933175 23.272013 23.232407 29.972782 14.966588 29.972782 L 14.966588 29.972782 C 6.70077 29.972782 0 23.272013 0 15.006194 L 0 15.006194 C 0 6.740377 6.70077 0.039606 14.966588 0.039606 Z"/></clipPath><g id="Group" clip-path="url(#clipPath1)"><path id="Path" fill="#0054fa" stroke="none" d="M 14.966588 0.039606 L 14.966588 0.039606 C 23.232407 0.039606 29.933176 6.740377 29.933176 15.006194 L 29.933176 15.006194 C 29.933176 23.272013 23.232407 29.972782 14.966588 29.972782 L 14.966588 29.972782 C 6.700769 29.972782 0 23.272013 0 15.006194 L 0 15.006194 C 0 6.740377 6.700769 0.039606 14.966588 0.039606 Z"/><path id="path1" fill="#ffffff" fill-rule="evenodd" stroke="none" d="M -5.826704 0.039606 C -5.953422 0.039606 -6.056147 0.142334 -6.056147 0.269054 L -6.056147 30.880379 C -6.056147 31.007061 -5.953422 31.109787 -5.826704 31.109787 L 34.298248 31.109787 C 34.424999 31.109787 34.527725 31.007061 34.527725 30.880379 L 34.527725 0.269054 C 34.527725 0.142334 34.424999 0.039606 34.298248 0.039606 L -5.826704 0.039606 Z M 10.052844 9.655094 L 7.983128 9.655094 L 6.648313 20.753532 L 8.313057 20.753532 L 8.508057 18.338816 L 9.437914 18.338816 L 9.632915 20.753532 L 11.38763 20.753532 L 10.052844 9.655094 Z M 8.628057 16.869028 L 8.807986 14.589384 L 8.942986 12.669668 L 8.987986 12.669668 L 9.122986 14.589384 L 9.317915 16.869028 L 8.628057 16.869028 Z M 13.280616 20.753532 L 15.065403 20.753532 L 15.065403 16.569099 L 15.620332 16.569099 C 16.895119 16.569099 17.675047 15.81917 17.675047 14.484384 L 17.675047 11.754812 C 17.675047 10.404953 16.895119 9.655094 15.620332 9.655094 L 13.280616 9.655094 L 13.280616 20.753532 Z M 15.470331 15.234313 L 15.080402 15.234313 L 15.080402 10.989882 L 15.470331 10.989882 C 15.770261 10.989882 15.890261 11.259882 15.890261 11.754812 L 15.890261 14.484384 C 15.890261 14.964313 15.770261 15.234313 15.470331 15.234313 Z M 19.730902 9.655094 L 19.730902 20.753532 L 23.210403 20.753532 L 23.210403 19.283745 L 21.515617 19.283745 L 21.515617 15.879171 L 23.030403 15.879171 L 23.030403 14.409384 L 21.515617 14.409384 L 21.515617 11.124882 L 23.210403 11.124882 L 23.210403 9.655094 L 19.730902 9.655094 Z"/></g></svg>} href="/chains/apechain" />

  <Card title="Artela" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_216_2)"><circle cx="125" cy="125" r="125" fill="white"/></g><path d="M125 12L25 185.958H56.5126L125 64.3564L193.487 185.958H225L125 12Z" fill="#0082FF"/><path d="M125 116.713L85.084 185.958H165.336L125 116.713Z" fill="#0082FF"/><defs><filter id="filter0_i_216_2" x="0" y="0" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_216_2"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_216_2"/></filter></defs></svg>} href="/chains/artela" />

  <Card title="Arweave" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_133_12)"><circle cx="125" cy="125" r="125" fill="black"/></g><circle cx="125" cy="125" r="100" fill="white"/><path d="M142.296 170.991C141.509 170.204 141.509 168.632 140.723 167.06C140.723 165.487 139.937 163.915 139.937 162.343C138.365 163.915 136.792 164.701 135.22 166.274C133.648 167.846 131.289 168.632 129.717 169.418C127.359 170.204 125.786 170.991 122.642 171.777C120.283 172.563 117.138 172.563 114.78 172.563C110.063 172.563 106.132 171.777 102.201 170.204C98.2705 168.632 95.1258 167.06 91.9812 164.701C88.8365 162.343 87.2642 159.198 85.6919 156.053C84.1195 152.909 83.3334 148.978 83.3334 145.047C83.3334 135.613 87.2642 127.752 94.3397 123.035C101.415 117.531 112.421 115.173 126.572 115.173H139.937V109.67C139.937 104.953 138.365 101.808 136.006 99.4497C132.862 97.0912 128.931 95.5189 123.428 95.5189C118.711 95.5189 115.566 96.305 113.208 98.6635C110.849 101.022 110.063 103.38 110.063 106.525H86.478C86.478 102.594 87.2642 98.6635 88.8365 95.5189C90.4088 92.3742 92.7673 89.2296 96.6982 86.0849C99.8428 83.7264 103.774 81.3679 108.491 79.7956C113.208 78.2233 118.711 77.4371 125 77.4371C130.503 77.4371 135.22 78.2233 139.937 79.7956C144.654 81.3679 148.585 82.9402 152.516 86.0849C155.66 88.4434 158.805 92.3742 160.377 96.305C161.95 100.236 163.522 104.953 163.522 110.456V149.764C163.522 154.481 163.522 158.412 164.308 161.557C165.094 164.701 165.881 167.846 166.667 169.418V170.991H142.296ZM119.497 154.481C121.855 154.481 124.214 154.481 125.786 153.695C128.145 152.909 129.717 152.123 131.289 151.336C132.862 150.55 134.434 149.764 135.22 148.192C136.006 147.406 137.579 145.833 138.365 145.047V129.324H126.572C122.642 129.324 119.497 129.324 117.138 130.11C114.78 130.896 112.421 131.682 110.849 133.255C109.277 134.827 107.704 135.613 106.918 137.972C106.132 139.544 106.132 141.903 106.132 143.475C106.132 146.619 106.918 148.978 109.277 151.336C111.635 153.695 115.566 154.481 119.497 154.481Z" fill="#222326"/><defs><filter id="filter0_i_133_12" x="0" y="3.05176e-05" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_133_12"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_133_12"/></filter></defs></svg>} href="/chains/arweave" />

  <Card title="Berachain" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_133_24)"><circle cx="125" cy="125" r="125" fill="#451D07"/></g><path d="M186.403 111.79C186.163 111.004 185.995 110.197 185.864 109.383C185.805 109.006 185.736 108.631 185.656 108.258C185.484 107.449 185.294 106.643 185.088 105.843C188.5 100.463 204.811 72.4018 186.25 54.3922C165.65 34.4041 141.581 60.6032 141.581 60.6032L141.656 60.7196C130.838 57.2955 119.108 56.8935 107.696 60.6021C107.555 60.4485 83.5644 34.4629 63.0253 54.3922C42.4797 74.3283 64.6656 106.581 64.7817 106.75C64.5453 107.508 64.3545 108.283 64.2288 109.069C62.0061 122.871 46.875 127.131 46.875 151.172C46.875 175.213 62.7044 194.988 95.0136 194.988H108.271C108.331 195.075 113.791 203.134 125 203.125C135.405 203.117 142.276 195.056 142.336 194.988H154.986C187.295 194.988 203.125 175.677 203.125 151.172C203.125 128.784 190.002 123.55 186.403 111.79Z" fill="#DEB69A"/><defs><filter id="filter0_i_133_24" x="0" y="9.15527e-05" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_133_24"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_133_24"/></filter></defs></svg>} href="/chains/berachain" />

  <Card title="Blast" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_133_27)"><circle cx="125" cy="125" r="125" fill="black"/></g><path d="M176.83 123.934L205.215 109.79L215 79.7579L195.431 65.5167H65.1292L35 87.8956H188.165L180.027 113.084H118.606L112.696 131.491H174.117L156.873 184.483L185.646 170.242L195.915 138.466L176.636 124.322L176.83 123.934Z" fill="#FCFC03"/><path d="M78.3046 161.717L96.0333 106.496L76.367 91.7708L46.8191 184.483H156.873L164.236 161.717H78.3046Z" fill="#FCFC03"/><defs><filter id="filter0_i_133_27" x="0" y="9.15527e-05" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_133_27"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_133_27"/></filter></defs></svg>} href="/chains/blast" />

  <Card title="BOB" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_288_131)"><circle cx="125" cy="125" r="125" fill="black"/></g><path d="M119.55 50H78.5244C77.1083 50 75.9603 51.148 75.9603 52.5641V93.5901C75.9603 95.0062 77.1083 96.1542 78.5244 96.1542H119.55C120.967 96.1542 122.115 95.0062 122.115 93.5901V52.5641C122.115 51.148 120.967 50 119.55 50Z" fill="#FF6400"/><path d="M119.55 153.846H78.5244C77.1083 153.846 75.9603 154.994 75.9603 156.41V197.436C75.9603 198.852 77.1083 200 78.5244 200H119.55C120.967 200 122.115 198.852 122.115 197.436V156.41C122.115 154.994 120.967 153.846 119.55 153.846Z" fill="#FF6400"/><path d="M119.55 101.924H78.5244C77.1083 101.924 75.9603 103.072 75.9603 104.488V145.514C75.9603 146.93 77.1083 148.078 78.5244 148.078H119.55C120.967 148.078 122.115 146.93 122.115 145.514V104.488C122.115 103.072 120.967 101.924 119.55 101.924Z" fill="#FF6400"/><path d="M171.476 153.846H130.45C129.034 153.846 127.886 154.994 127.886 156.41V197.436C127.886 198.852 129.034 200 130.45 200H171.476C172.892 200 174.04 198.852 174.04 197.436V156.41C174.04 154.994 172.892 153.846 171.476 153.846Z" fill="#FF6400"/><path d="M171.476 101.924H130.45C129.034 101.924 127.886 103.072 127.886 104.488V145.514C127.886 146.93 129.034 148.078 130.45 148.078H171.476C172.892 148.078 174.04 146.93 174.04 145.514V104.488C174.04 103.072 172.892 101.924 171.476 101.924Z" fill="#FF6400"/><defs><filter id="filter0_i_288_131" x="0" y="0" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_288_131"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_288_131"/></filter></defs></svg>} href="/chains/build-on-bitcoin" />

  <Card title="Citrea" icon={<svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"> <g filter="url(#filter0_i_425_1008)"> <circle cx="15" cy="15" r="15" fill="black"/> </g> <g clip-path="url(#clip0_425_1008)"> <path d="M15 12.8344C14.5147 12.8344 14.0802 12.5835 13.8376 12.1633L10.5562 6.47998H19.4438L16.1625 12.1633C15.9198 12.5835 15.4853 12.8344 15 12.8344Z" fill="white"/> <path d="M18.0374 14.5882C17.5521 14.5882 17.1176 14.3373 16.8749 13.9171C16.6323 13.4969 16.6323 12.9951 16.8749 12.5748L20.1562 6.8916L24.6 14.5882H18.0373H18.0374Z" fill="white"/> <path d="M10.5562 23.5195L13.8376 17.8363C14.0802 17.4161 14.5147 17.1652 15 17.1652C15.4853 17.1652 15.9198 17.4161 16.1625 17.8363L19.4438 23.5195H10.5562Z" fill="white"/> <path d="M16.875 17.4248C16.6323 17.0046 16.6323 16.5028 16.875 16.0825C17.1176 15.6623 17.5522 15.4114 18.0375 15.4114H24.6002L20.1564 23.1079L16.8751 17.4248L16.875 17.4248Z" fill="white"/> <path d="M5.40002 14.5882L9.84392 6.8916L13.1251 12.5748C13.3679 12.995 13.3679 13.4968 13.1251 13.9171C12.8825 14.3373 12.4479 14.5882 11.9626 14.5882H5.40002Z" fill="white"/> <path d="M5.40002 15.4114H11.9627C12.448 15.4114 12.8826 15.6623 13.1252 16.0825C13.3679 16.5028 13.3679 17.0046 13.1252 17.4248L9.84401 23.1079L5.40002 15.4114Z" fill="white"/> </g> <defs> <filter id="filter0_i_425_1008" x="0" y="0" width="30" height="30" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"> <feFlood flood-opacity="0" result="BackgroundImageFix"/> <feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/> <feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/> <feMorphology radius="0.6" operator="erode" in="SourceAlpha" result="effect1_innerShadow_425_1008"/> <feOffset/> <feGaussianBlur stdDeviation="0.15"/> <feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/> <feColorMatrix type="matrix" values="0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0.1 0"/> <feBlend mode="normal" in2="shape" result="effect1_innerShadow_425_1008"/> </filter> <clipPath id="clip0_425_1008"> <rect width="19.2" height="17.0395" fill="white" transform="translate(5.40002 6.47998)"/> </clipPath> </defs> </svg>} href="/chains/citrea" />

  <Card title="Cronos zkEVM" icon={<svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"> <g filter="url(#filter0_i_425_1009)"> <circle cx="15" cy="15" r="15" fill="#051221"/> </g> <path d="M14.9948 4.23425L5.81946 9.6278V20.4106L14.9948 25.8L24.162 20.4106V9.6278L14.9948 4.23425ZM21.4475 18.8122L14.9948 22.6032L8.54009 18.8122V11.222L14.9948 7.43105L21.4475 11.222V18.8122Z" fill="white"/> <path d="M14.9948 25.8L24.1619 20.4106V9.6278L14.9948 4.23425V7.43317L21.4475 11.2241V18.8144L14.9948 22.6032V25.7979V25.8Z" fill="white"/> <path d="M14.9865 4.23425L5.81946 9.62359V20.4064L14.9865 25.8V22.601L8.53386 18.8101V11.2199L14.9865 7.43105V4.23425Z" fill="white"/> <path d="M19.2727 17.5372L14.9904 20.0531L10.7038 17.5372V12.5009L14.9904 9.98071L19.2727 12.5009L17.4909 13.5496L14.9904 12.0781L12.4899 13.5496V16.4842L14.9904 17.9558L17.4909 16.4842L19.2727 17.5372Z" fill="white"/> <defs> <filter id="filter0_i_425_1009" x="0" y="0" width="30" height="30" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"> <feFlood flood-opacity="0" result="BackgroundImageFix"/> <feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/> <feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/> <feMorphology radius="0.6" operator="erode" in="SourceAlpha" result="effect1_innerShadow_425_1009"/> <feOffset/> <feGaussianBlur stdDeviation="0.15"/> <feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/> <feColorMatrix type="matrix" values="0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0.1 0"/> <feBlend mode="normal" in2="shape" result="effect1_innerShadow_425_1009"/> </filter> </defs> </svg>} href="/chains/cronos-zkevm" />

  <Card title="Filecoin" icon={<svg width="30" height="30" enable-background="new 0 0 40 40" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><mask id="a" height="40" maskUnits="userSpaceOnUse" width="40" x="0" y="0"><path clip-rule="evenodd" d="m0 0h40v40h-40z" fill="#fff" fill-rule="evenodd"/></mask><g clip-rule="evenodd" fill-rule="evenodd"><path d="m20 40c-11 0-20-9-20-20.1.1-11 9-20 20.1-19.9 11 .1 19.9 9 19.9 20.2-.1 10.9-9 19.8-20 19.8" fill="#0090ff" mask="url(#a)"/><path d="m21.9 17.6-.6 3.2 5.7.8-.4 1.5-5.6-.8c-.4 1.3-.6 2.7-1.1 3.9-.5 1.4-1 2.8-1.6 4.1-.8 1.7-2.2 2.9-4.1 3.2-1.1.2-2.3.1-3.2-.6-.3-.2-.6-.6-.6-.9 0-.4.2-.9.5-1.1.2-.1.7 0 1 .1.3.3.6.7.8 1.1.6.8 1.4.9 2.2.3.9-.8 1.4-1.9 1.7-3 .6-2.4 1.2-4.7 1.7-7.1v-.4l-5.3-.8.2-1.5 5.5.8.7-3.1-5.7-.9.2-1.6 5.9.8c.2-.6.3-1.1.5-1.6.5-1.8 1-3.6 2.2-5.2s2.6-2.6 4.7-2.5c.9 0 1.8.3 2.4 1 .1.1.3.3.3.5 0 .4 0 .9-.3 1.2-.4.3-.9.2-1.3-.2-.3-.3-.5-.6-.8-.9-.6-.8-1.5-.9-2.2-.2-.5.5-1 1.2-1.3 1.9-.7 2.1-1.2 4.3-1.9 6.5l5.5.8-.4 1.5z" fill="#fff"/></g></svg>} href="/chains/filecoin" />

  <Card title="Flare" icon={<svg width="24" height="25" viewBox="0 0 24 25" fill="none" xmlns="http://www.w3.org/2000/svg"> <g clip-path="url(#clip0_73_33)"> <path d="M17.397 9.49176L5.84891 9.48242C2.70013 9.48242 0.0825915 12.0491 6.81883e-05 15.3481C-0.00251066 15.4358 0.0684078 15.5089 0.15351 15.5089L11.7017 15.5169C14.8504 15.5183 17.4679 12.9516 17.5505 9.65392C17.553 9.56617 17.4821 9.49309 17.397 9.49309V9.49176Z" fill="#E62058"/> <path d="M23.389 0.00930425L5.88362 0C2.71615 0 0.0830814 2.56665 6.85928e-05 5.86567C-0.00252556 5.9534 0.0688138 6.02651 0.154421 6.02651L17.6598 6.03448C20.8273 6.03581 23.4603 3.46916 23.5433 0.171464C23.5459 0.0837383 23.4746 0.0106334 23.389 0.0106334V0.00930425Z" fill="#E62058"/> <path d="M2.99643 24.9993C4.65132 24.9993 5.99287 23.5519 5.99287 21.7664C5.99287 19.9811 4.65132 18.5337 2.99643 18.5337C1.34155 18.5337 0 19.9811 0 21.7664C0 23.5519 1.34155 24.9993 2.99643 24.9993Z" fill="#E62058"/> </g> <defs> <clipPath id="clip0_73_33"> <rect width="24" height="25" fill="white"/> </clipPath> </defs> </svg>} href="/chains/flare" />

  <Card title="Immutable" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_133_77)"><circle cx="125" cy="125" r="125" fill="white"/></g><path d="M193.25 54.3751H211.5L137.5 138.875C135.625 141 135.5 144.25 137.375 146.5L180 196.125H161L121.625 150.125L119.625 147.625C117.75 145.5 117.875 142.125 119.75 140L193.25 54.3751Z" fill="#17B5CB"/><path d="M152.125 146.5C150.875 145.125 150.5 143.125 150.875 141.375C151.25 140.375 151.875 139.375 152.625 138.625L157.5 133.25L211.875 196.125H194.625C194.625 196.125 162.75 158.875 152.125 146.5Z" fill="#17B5CB"/><path d="M57.125 196.125H38.125L96.375 128.75C98.25 126.625 98.25 123.375 96.375 121.25L38.625 54.3751H56.875L115 121.75C116.5 123.75 116.75 126.625 115.125 128.5C99.875 146.125 57.125 196.125 57.125 196.125Z" fill="#17B5CB"/><path d="M88.875 54.3751H71.375L125.25 116.75C125.25 116.75 125.375 116.625 131.375 109.5C133 107.625 133.375 105.25 131.5 103.375C120.25 91.8751 88.875 54.3751 88.875 54.3751Z" fill="#17B5CB"/><path d="M141.375 97.8751L178.75 54.3751H161L141.5 77.1251L135.875 83.7501C134 85.8751 133.875 89.1251 135.75 91.3751C137.125 93.0001 141.375 97.8751 141.375 97.8751Z" fill="#17B5CB"/><path d="M108.875 152.625L71.5 196.125H89.125L108.625 173.375L114.25 166.75C116.125 164.625 116.25 161.375 114.375 159.125C113 157.5 108.875 152.625 108.875 152.625Z" fill="#17B5CB"/><defs><filter id="filter0_i_133_77" x="0" y="9.15527e-05" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_133_77"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_133_77"/></filter></defs></svg>} href="/chains/immutable-zkevm" />

  <Card title="Ink" icon={<svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"> <g filter="url(#filter0_i_425_1014)"> <circle cx="15" cy="15" r="15" fill="#7132F5"/> </g> <g clip-path="url(#clip0_425_1014)"> <path fill-rule="evenodd" clip-rule="evenodd" d="M24.48 15C24.48 9.76437 20.2357 5.52002 15 5.52002C9.76437 5.52002 5.52002 9.76437 5.52002 15C5.52002 20.2357 9.76437 24.48 15 24.48C20.2357 24.48 24.48 20.2357 24.48 15ZM16.3531 22.095C16.3531 22.7381 15.8252 23.2609 15.076 23.2759C15.0552 23.2761 15.0345 23.2762 15.0137 23.2762H14.9864C10.4218 23.2688 6.72383 19.5663 6.72383 15C6.72383 10.4292 10.4292 6.72384 15 6.72384C15.0247 6.72384 15.0493 6.72395 15.0739 6.72417C15.9194 6.73923 16.3531 7.26196 16.3531 7.90504C16.3531 8.55955 15.7745 9.04116 15.1615 9.04116C14.5486 9.04118 14.5185 9.04118 13.9318 9.08818C13.345 9.1352 12.7383 9.61682 12.7383 10.2694C12.7383 10.9258 13.2715 11.4545 13.9318 11.4545H19.1354C19.7936 11.4545 20.3269 11.9831 20.3269 12.6357C20.3269 13.2882 19.7936 13.8169 19.1354 13.8169H11.1204C10.4602 13.8169 9.92699 14.3475 9.92699 15.002C9.92699 15.6545 10.4602 16.1832 11.1204 16.1832H15.1616C15.8198 16.1832 16.3531 16.7118 16.3531 17.3663C16.3531 18.0189 15.8198 18.5475 15.1616 18.5475H13.9318C13.2715 18.5475 12.7383 19.0762 12.7383 19.7287C12.7383 20.3832 13.2848 20.861 13.9318 20.9099C13.9828 20.9138 14.0294 20.9173 14.0723 20.9206C14.3 20.938 14.4244 20.9474 14.549 20.9526C14.699 20.9589 14.8493 20.9589 15.1806 20.9589C15.8388 20.9589 16.3531 21.4424 16.3531 22.095Z" fill="white"/> </g> <defs> <filter id="filter0_i_425_1014" x="0" y="0" width="30" height="30" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"> <feFlood flood-opacity="0" result="BackgroundImageFix"/> <feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/> <feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/> <feMorphology radius="0.6" operator="erode" in="SourceAlpha" result="effect1_innerShadow_425_1014"/> <feOffset/> <feGaussianBlur stdDeviation="0.15"/> <feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/> <feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/> <feBlend mode="normal" in2="shape" result="effect1_innerShadow_425_1014"/> </filter> <clipPath id="clip0_425_1014"> <rect width="18.96" height="18.96" fill="white" transform="translate(5.52002 5.52002)"/> </clipPath> </defs> </svg>} href="/chains/ink" />

  <Card title="IOTA EVM" icon={<svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"> <g filter="url(#filter0_i_425_1015)"> <circle cx="15" cy="15" r="15" fill="#131F37"/> </g> <path d="M19.7693 7.20229C20.0106 7.20159 20.2462 7.12941 20.4465 6.99486C20.6467 6.86032 20.8026 6.66944 20.8944 6.44635C20.9863 6.22325 21.01 5.97795 20.9625 5.74141C20.915 5.50487 20.7984 5.28771 20.6276 5.11736C20.4568 4.94701 20.2393 4.83112 20.0026 4.7843C19.7659 4.73749 19.5207 4.76187 19.2978 4.85436C19.075 4.94685 18.8846 5.10329 18.7506 5.30394C18.6167 5.50459 18.5452 5.74044 18.5452 5.9817C18.5457 6.30586 18.675 6.61653 18.9045 6.84541C19.1341 7.0743 19.4451 7.20266 19.7693 7.20229Z" fill="white"/> <path d="M21.1456 24.4428C21.387 24.4422 21.6227 24.3701 21.8231 24.2356C22.0235 24.1011 22.1795 23.9102 22.2714 23.687C22.3633 23.4638 22.387 23.2184 22.3395 22.9818C22.2919 22.7452 22.1753 22.528 22.0044 22.3576C21.8334 22.1872 21.6158 22.0714 21.379 22.0247C21.1422 21.978 20.8969 22.0025 20.674 22.0952C20.4512 22.1879 20.2608 22.3445 20.127 22.5454C19.9932 22.7462 19.9219 22.9822 19.9222 23.2236C19.9231 23.5474 20.0525 23.8576 20.2818 24.0861C20.5112 24.3147 20.8218 24.443 21.1456 24.4428Z" fill="white"/> <path d="M20.119 10.2538C20.3215 10.2532 20.5193 10.1927 20.6874 10.0798C20.8555 9.96684 20.9864 9.80664 21.0635 9.6194C21.1406 9.43215 21.1605 9.22625 21.1207 9.02771C21.0808 8.82917 20.983 8.64689 20.8396 8.5039C20.6962 8.3609 20.5137 8.26361 20.315 8.22431C20.1164 8.18501 19.9105 8.20546 19.7235 8.28309C19.5365 8.36071 19.3766 8.49202 19.2642 8.66043C19.1518 8.82885 19.0917 9.0268 19.0917 9.22931C19.0918 9.36405 19.1185 9.49746 19.1702 9.6219C19.2219 9.74634 19.2976 9.85936 19.393 9.95451C19.4884 10.0497 19.6017 10.1251 19.7262 10.1764C19.8508 10.2278 19.9843 10.2541 20.119 10.2538Z" fill="white"/> <path d="M22.8505 10.646C23.053 10.6456 23.2509 10.5851 23.4191 10.4723C23.5873 10.3595 23.7183 10.1993 23.7955 10.0121C23.8727 9.82487 23.8927 9.61894 23.8529 9.42036C23.8132 9.22177 23.7154 9.03942 23.5721 8.89636C23.4287 8.7533 23.2461 8.65593 23.0475 8.61658C22.8488 8.57722 22.6429 8.59763 22.4559 8.67523C22.2688 8.75283 22.1089 8.88415 21.9964 9.05257C21.8839 9.221 21.8239 9.41899 21.8239 9.62152C21.8243 9.89348 21.9326 10.1542 22.1251 10.3463C22.3176 10.5384 22.5786 10.6462 22.8505 10.646Z" fill="white"/> <path d="M19.7693 12.8479C19.9418 12.8476 20.1103 12.7962 20.2535 12.7002C20.3968 12.6042 20.5083 12.4679 20.5742 12.3085C20.64 12.1491 20.657 11.9737 20.6232 11.8046C20.5894 11.6355 20.5062 11.4802 20.3842 11.3584C20.2621 11.2365 20.1067 11.1536 19.9375 11.1201C19.7684 11.0865 19.5931 11.1039 19.4338 11.1699C19.2745 11.236 19.1383 11.3478 19.0425 11.4912C18.9468 11.6346 18.8956 11.8032 18.8956 11.9756C18.896 12.2071 18.9882 12.429 19.152 12.5925C19.3158 12.7561 19.5378 12.8479 19.7693 12.8479Z" fill="white"/> <path d="M22.5006 13.2181C22.6731 13.2177 22.8415 13.1662 22.9847 13.0701C23.1279 12.9739 23.2393 12.8375 23.305 12.6781C23.3707 12.5186 23.3876 12.3432 23.3537 12.1742C23.3197 12.0051 23.2364 11.8499 23.1143 11.7281C22.9921 11.6064 22.8366 11.5236 22.6674 11.4902C22.4982 11.4567 22.3229 11.4742 22.1637 11.5404C22.0045 11.6066 21.8684 11.7185 21.7727 11.862C21.6771 12.0055 21.6261 12.1741 21.6262 12.3466C21.6264 12.4612 21.6492 12.5747 21.6932 12.6806C21.7373 12.7864 21.8017 12.8825 21.8829 12.9635C21.9641 13.0444 22.0604 13.1085 22.1664 13.1522C22.2724 13.1959 22.386 13.2183 22.5006 13.2181Z" fill="white"/> <path d="M24.6856 14.0685C24.858 14.0682 25.0265 14.0168 25.1698 13.9208C25.313 13.8248 25.4246 13.6885 25.4904 13.529C25.5562 13.3696 25.5733 13.1943 25.5395 13.0252C25.5057 12.8561 25.4225 12.7008 25.3004 12.579C25.1784 12.4571 25.023 12.3742 24.8538 12.3406C24.6846 12.3071 24.5093 12.3244 24.35 12.3905C24.1907 12.4566 24.0546 12.5684 23.9588 12.7118C23.863 12.8552 23.8119 13.0238 23.8119 13.1962C23.8123 13.4277 23.9045 13.6496 24.0683 13.8131C24.2321 13.9766 24.4541 14.0685 24.6856 14.0685Z" fill="white"/> <path d="M21.6922 15.3103C21.8431 15.31 21.9906 15.265 22.1159 15.1809C22.2412 15.0968 22.3388 14.9775 22.3964 14.838C22.4539 14.6984 22.4688 14.545 22.4391 14.397C22.4094 14.249 22.3366 14.1132 22.2297 14.0066C22.1228 13.9 21.9868 13.8275 21.8387 13.7983C21.6907 13.769 21.5373 13.7843 21.3979 13.8422C21.2585 13.9002 21.1395 13.9981 21.0557 14.1237C20.972 14.2492 20.9274 14.3968 20.9276 14.5477C20.9279 14.7502 21.0087 14.9443 21.1521 15.0873C21.2954 15.2302 21.4897 15.3104 21.6922 15.3103Z" fill="white"/> <path d="M18.9608 14.94C19.1117 14.9399 19.2592 14.895 19.3846 14.8111C19.51 14.7271 19.6077 14.6079 19.6654 14.4684C19.723 14.3289 19.7381 14.1755 19.7085 14.0275C19.679 13.8795 19.6063 13.7435 19.4995 13.6369C19.3927 13.5302 19.2567 13.4576 19.1087 13.4282C18.9607 13.3988 18.8072 13.414 18.6678 13.4717C18.5284 13.5295 18.4092 13.6274 18.3254 13.7528C18.2416 13.8783 18.1968 14.0259 18.1968 14.1768C18.1972 14.3792 18.2778 14.5732 18.421 14.7163C18.5643 14.8594 18.7583 14.9398 18.9608 14.94Z" fill="white"/> <path d="M23.8991 16.1606C24.05 16.1603 24.1975 16.1153 24.3228 16.0312C24.4482 15.9472 24.5458 15.8278 24.6033 15.6883C24.6608 15.5488 24.6757 15.3953 24.646 15.2474C24.6164 15.0994 24.5435 14.9635 24.4366 14.8569C24.3297 14.7504 24.1937 14.6779 24.0456 14.6486C23.8976 14.6194 23.7442 14.6347 23.6048 14.6926C23.4654 14.7505 23.3464 14.8484 23.2626 14.974C23.1789 15.0996 23.1343 15.2472 23.1345 15.3981C23.1348 15.6006 23.2156 15.7946 23.359 15.9376C23.5023 16.0806 23.6966 16.1608 23.8991 16.1606Z" fill="white"/> <path d="M20.5998 16.9019C20.7291 16.9016 20.8554 16.863 20.9628 16.791C21.0702 16.7189 21.1538 16.6166 21.2031 16.4971C21.2524 16.3775 21.2652 16.246 21.2398 16.1192C21.2144 15.9924 21.152 15.876 21.0604 15.7846C20.9689 15.6933 20.8523 15.6311 20.7255 15.606C20.5986 15.5808 20.4671 15.5939 20.3477 15.6434C20.2282 15.693 20.1261 15.7768 20.0543 15.8844C19.9825 15.9919 19.9442 16.1184 19.9442 16.2477C19.9442 16.3337 19.9613 16.4188 19.9943 16.4983C20.0273 16.5777 20.0756 16.6498 20.1365 16.7106C20.1973 16.7713 20.2696 16.8195 20.3491 16.8523C20.4286 16.8851 20.5138 16.902 20.5998 16.9019Z" fill="white"/> <path d="M22.8066 17.7303C22.9359 17.73 23.0623 17.6914 23.1697 17.6193C23.2771 17.5473 23.3607 17.445 23.41 17.3254C23.4593 17.2059 23.472 17.0744 23.4466 16.9476C23.4212 16.8208 23.3588 16.7043 23.2673 16.613C23.1757 16.5216 23.0592 16.4595 22.9323 16.4343C22.8054 16.4092 22.674 16.4223 22.5545 16.4718C22.4351 16.5214 22.333 16.6052 22.2612 16.7128C22.1893 16.8203 22.151 16.9467 22.151 17.0761C22.1514 17.2497 22.2206 17.4161 22.3435 17.5387C22.4664 17.6614 22.633 17.7303 22.8066 17.7303Z" fill="white"/> <path d="M17.8896 16.5089C18.0189 16.5088 18.1453 16.4703 18.2528 16.3984C18.3603 16.3264 18.4441 16.2242 18.4935 16.1046C18.5429 15.9851 18.5557 15.8535 18.5304 15.7267C18.5051 15.5998 18.4427 15.4833 18.3512 15.3919C18.2597 15.3005 18.1431 15.2383 18.0162 15.2131C17.8894 15.1879 17.7579 15.2009 17.6384 15.2504C17.5189 15.3 17.4167 15.3838 17.3449 15.4914C17.273 15.5989 17.2347 15.7254 17.2347 15.8548C17.2349 16.0283 17.3039 16.1947 17.4267 16.3174C17.5495 16.44 17.716 16.5089 17.8896 16.5089Z" fill="white"/> <path d="M19.4004 17.9696C19.5082 17.9693 19.6135 17.9371 19.703 17.877C19.7924 17.8169 19.8621 17.7316 19.9031 17.6319C19.9441 17.5322 19.9547 17.4226 19.9334 17.3169C19.9122 17.2113 19.8601 17.1143 19.7837 17.0382C19.7073 16.9621 19.6101 16.9104 19.5044 16.8896C19.3986 16.8687 19.2891 16.8797 19.1895 16.9211C19.09 16.9625 19.005 17.0325 18.9453 17.1222C18.8855 17.2119 18.8537 17.3173 18.8538 17.4251C18.8539 17.4968 18.8681 17.5677 18.8957 17.6339C18.9232 17.7 18.9635 17.7601 19.0142 17.8107C19.065 17.8612 19.1252 17.9013 19.1915 17.9286C19.2578 17.9558 19.3287 17.9698 19.4004 17.9696Z" fill="white"/> <path d="M20.3371 19.4712C20.4319 19.4709 20.5244 19.4426 20.603 19.3897C20.6817 19.3369 20.7429 19.2619 20.7789 19.1743C20.815 19.0867 20.8243 18.9904 20.8056 18.8975C20.787 18.8046 20.7412 18.7194 20.6741 18.6525C20.607 18.5856 20.5216 18.5401 20.4287 18.5217C20.3358 18.5033 20.2394 18.5129 20.1519 18.5492C20.0644 18.5855 19.9897 18.647 19.9371 18.7257C19.8845 18.8045 19.8564 18.8971 19.8564 18.9919C19.8564 19.055 19.8688 19.1174 19.893 19.1756C19.9172 19.2339 19.9526 19.2868 19.9973 19.3313C20.0419 19.3758 20.0949 19.4111 20.1533 19.4351C20.2116 19.4591 20.2741 19.4714 20.3371 19.4712Z" fill="white"/> <path d="M19.1788 19.7785C19.2564 19.7783 19.3321 19.7552 19.3965 19.712C19.461 19.6689 19.5111 19.6075 19.5407 19.5359C19.5703 19.4642 19.5779 19.3853 19.5627 19.3093C19.5475 19.2333 19.5101 19.1634 19.4552 19.1087C19.4003 19.0539 19.3304 19.0166 19.2544 19.0015C19.1783 18.9864 19.0995 18.9942 19.0279 19.0239C18.9562 19.0537 18.895 19.1039 18.8519 19.1684C18.8089 19.2329 18.7859 19.3087 18.7859 19.3862C18.7861 19.4903 18.8276 19.5901 18.9012 19.6636C18.9749 19.7372 19.0747 19.7785 19.1788 19.7785Z" fill="white"/> <path d="M18.1521 18.645C18.2469 18.6447 18.3394 18.6164 18.418 18.5636C18.4967 18.5107 18.5579 18.4358 18.5939 18.3482C18.63 18.2606 18.6393 18.1642 18.6206 18.0713C18.602 17.9785 18.5562 17.8932 18.4891 17.8263C18.422 17.7594 18.3366 17.7139 18.2437 17.6955C18.1508 17.6771 18.0545 17.6867 17.967 17.723C17.8795 17.7593 17.8047 17.8208 17.7521 17.8996C17.6995 17.9784 17.6714 18.071 17.6714 18.1657C17.6715 18.2288 17.684 18.2912 17.7082 18.3494C17.7324 18.4076 17.7678 18.4604 17.8125 18.5049C17.8571 18.5494 17.9101 18.5847 17.9684 18.6088C18.0266 18.6328 18.0891 18.6451 18.1521 18.645Z" fill="white"/> <path d="M16.6668 17.5992C16.7746 17.5989 16.8798 17.5667 16.9693 17.5067C17.0588 17.4466 17.1284 17.3614 17.1695 17.2617C17.2105 17.1621 17.2211 17.0525 17.1999 16.9468C17.1787 16.8412 17.1267 16.7442 17.0504 16.6681C16.9741 16.592 16.8769 16.5402 16.7712 16.5193C16.6655 16.4984 16.556 16.5092 16.4564 16.5505C16.3569 16.5918 16.2719 16.6617 16.212 16.7513C16.1522 16.841 16.1202 16.9463 16.1202 17.0541C16.1203 17.1258 16.1345 17.1967 16.1621 17.2629C16.1896 17.3291 16.2299 17.3892 16.2806 17.4398C16.3314 17.4905 16.3916 17.5306 16.4579 17.5579C16.5241 17.5853 16.5951 17.5993 16.6668 17.5992Z" fill="white"/> <path d="M16.6448 13.5013C16.7526 13.501 16.8579 13.4688 16.9473 13.4088C17.0368 13.3487 17.1065 13.2635 17.1475 13.1638C17.1885 13.0642 17.1991 12.9546 17.1779 12.8489C17.1567 12.7433 17.1047 12.6463 17.0284 12.5702C16.9521 12.4941 16.855 12.4423 16.7493 12.4214C16.6436 12.4005 16.534 12.4113 16.4345 12.4526C16.3349 12.4939 16.2499 12.5638 16.19 12.6534C16.1302 12.7431 16.0983 12.8484 16.0983 12.9562C16.0984 13.0279 16.1126 13.0988 16.1401 13.165C16.1676 13.2312 16.2079 13.2913 16.2586 13.3419C16.3094 13.3926 16.3696 13.4327 16.4359 13.46C16.5022 13.4874 16.5732 13.5014 16.6448 13.5013Z" fill="white"/> <path d="M16.8848 11.5833C16.9798 11.5832 17.0726 11.5549 17.1514 11.502C17.2303 11.4492 17.2917 11.3741 17.3279 11.2864C17.3641 11.1986 17.3735 11.1021 17.3548 11.009C17.3361 10.9159 17.2903 10.8304 17.2231 10.7634C17.1558 10.6964 17.0702 10.6508 16.9771 10.6324C16.8839 10.614 16.7874 10.6236 16.6998 10.6601C16.6121 10.6966 16.5372 10.7582 16.4846 10.8372C16.432 10.9162 16.404 11.0091 16.4041 11.104C16.4042 11.1671 16.4167 11.2295 16.4409 11.2877C16.4651 11.3459 16.5006 11.3987 16.5452 11.4432C16.5899 11.4878 16.6428 11.523 16.7011 11.5471C16.7594 11.5711 16.8218 11.5834 16.8848 11.5833Z" fill="white"/> <path d="M17.4095 10.0137C17.487 10.0134 17.5627 9.99019 17.627 9.94692C17.6914 9.90365 17.7414 9.8423 17.7709 9.7706C17.8004 9.6989 17.8079 9.62008 17.7926 9.54409C17.7773 9.4681 17.7399 9.39834 17.6849 9.34363C17.63 9.28892 17.5601 9.2517 17.4841 9.23668C17.4081 9.22166 17.3293 9.22951 17.2577 9.25924C17.1861 9.28897 17.1249 9.33924 17.0819 9.40371C17.0388 9.46818 17.0159 9.54396 17.0159 9.62148C17.016 9.67308 17.0262 9.72416 17.0461 9.77179C17.0659 9.81943 17.0949 9.8627 17.1315 9.89912C17.168 9.93554 17.2114 9.96441 17.2591 9.98407C17.3068 10.0037 17.3579 10.0138 17.4095 10.0137Z" fill="white"/> <path d="M16.4912 9.27597C16.5862 9.27583 16.6789 9.24754 16.7578 9.19467C16.8367 9.14181 16.8981 9.06675 16.9343 8.97898C16.9705 8.89122 16.9798 8.79469 16.9612 8.7016C16.9425 8.60851 16.8967 8.52305 16.8294 8.45601C16.7622 8.38898 16.6766 8.34339 16.5835 8.325C16.4903 8.30662 16.3938 8.31627 16.3062 8.35273C16.2185 8.38919 16.1436 8.45083 16.091 8.52984C16.0384 8.60886 16.0104 8.70171 16.0105 8.79665C16.0105 8.85971 16.0229 8.92216 16.0471 8.9804C16.0713 9.03865 16.1067 9.09155 16.1514 9.13608C16.196 9.18061 16.2491 9.21588 16.3074 9.23989C16.3657 9.26389 16.4282 9.27615 16.4912 9.27597Z" fill="white"/> <path d="M21.5832 18.7972C21.691 18.7969 21.7963 18.7647 21.8858 18.7046C21.9752 18.6445 22.0449 18.5592 22.0859 18.4595C22.1269 18.3599 22.1375 18.2503 22.1162 18.1446C22.095 18.0389 22.0429 17.9419 21.9665 17.8658C21.8901 17.7898 21.7929 17.738 21.6872 17.7172C21.5814 17.6964 21.4719 17.7073 21.3723 17.7487C21.2728 17.7902 21.1878 17.8601 21.1281 17.9499C21.0683 18.0396 21.0365 18.145 21.0366 18.2528C21.0367 18.3244 21.0509 18.3954 21.0785 18.4615C21.106 18.5277 21.1463 18.5877 21.197 18.6383C21.2478 18.6889 21.308 18.7289 21.3743 18.7562C21.4406 18.7835 21.5115 18.7974 21.5832 18.7972Z" fill="white"/> <path d="M15.2466 8.63878C15.3543 8.63836 15.4595 8.60603 15.5489 8.54587C15.6382 8.48571 15.7078 8.40043 15.7487 8.30077C15.7896 8.20112 15.8001 8.09158 15.7789 7.98597C15.7576 7.88036 15.7055 7.78342 15.6292 7.70739C15.5528 7.63136 15.4557 7.57965 15.35 7.55878C15.2443 7.53792 15.1348 7.54884 15.0353 7.59016C14.9358 7.63149 14.8508 7.70136 14.791 7.79096C14.7312 7.88057 14.6993 7.98589 14.6993 8.09362C14.6994 8.16536 14.7136 8.23638 14.7412 8.30262C14.7687 8.36886 14.8091 8.42902 14.8599 8.47965C14.9107 8.53028 14.9711 8.5704 15.0374 8.5977C15.1037 8.62501 15.1748 8.63896 15.2466 8.63878Z" fill="white"/> <path d="M15.6176 10.9512C15.7254 10.9509 15.8306 10.9186 15.9201 10.8585C16.0096 10.7984 16.0793 10.7132 16.1203 10.6135C16.1613 10.5138 16.1719 10.4042 16.1506 10.2985C16.1294 10.1928 16.0772 10.0958 16.0009 10.0198C15.9245 9.94369 15.8273 9.89197 15.7215 9.87113C15.6158 9.85029 15.5062 9.86127 15.4067 9.90268C15.3072 9.94409 15.2222 10.0141 15.1624 10.1038C15.1027 10.1935 15.0708 10.2989 15.071 10.4067C15.0714 10.5514 15.1291 10.6899 15.2316 10.792C15.3341 10.8941 15.4729 10.9513 15.6176 10.9512Z" fill="white"/> <path d="M15.0059 13.153C15.1352 13.1527 15.2616 13.1141 15.369 13.042C15.4763 12.97 15.56 12.8677 15.6093 12.7482C15.6586 12.6286 15.6713 12.4971 15.6459 12.3703C15.6205 12.2435 15.5581 12.1271 15.4666 12.0357C15.375 11.9444 15.2584 11.8822 15.1316 11.8571C15.0047 11.8319 14.8733 11.845 14.7538 11.8945C14.6343 11.9441 14.5323 12.0279 14.4604 12.1355C14.3886 12.243 14.3503 12.3695 14.3503 12.4988C14.3504 12.5848 14.3674 12.6699 14.4004 12.7494C14.4334 12.8288 14.4817 12.9009 14.5426 12.9617C14.6035 13.0224 14.6757 13.0706 14.7552 13.1034C14.8347 13.1362 14.9199 13.1531 15.0059 13.153Z" fill="white"/> <path d="M13.017 13.1523C13.1678 13.1515 13.3151 13.1059 13.4401 13.0215C13.5651 12.937 13.6622 12.8174 13.7193 12.6777C13.7763 12.5381 13.7907 12.3846 13.7606 12.2368C13.7305 12.089 13.6573 11.9534 13.5502 11.8472C13.443 11.7409 13.3068 11.6688 13.1588 11.64C13.0107 11.6111 12.8574 11.6268 12.7182 11.685C12.579 11.7432 12.4602 11.8413 12.3768 11.967C12.2934 12.0927 12.2491 12.2403 12.2495 12.3912C12.2498 12.4916 12.2699 12.591 12.3086 12.6836C12.3474 12.7762 12.404 12.8603 12.4753 12.931C12.5466 13.0017 12.6311 13.0576 12.7241 13.0956C12.817 13.1336 12.9166 13.1529 13.017 13.1523Z" fill="white"/> <path d="M10.7013 13.6763C10.8737 13.6759 11.0421 13.6243 11.1853 13.5282C11.3285 13.4321 11.4399 13.2958 11.5056 13.1363C11.5713 12.9769 11.5883 12.8016 11.5544 12.6325C11.5205 12.4635 11.4373 12.3083 11.3152 12.1865C11.1931 12.0647 11.0377 11.9819 10.8685 11.9484C10.6994 11.9149 10.5241 11.9323 10.3649 11.9984C10.2056 12.0644 10.0695 12.1762 9.97376 12.3196C9.87801 12.463 9.8269 12.6316 9.8269 12.804C9.827 12.9187 9.84969 13.0323 9.89369 13.1382C9.93769 13.2441 10.0021 13.3403 10.0833 13.4213C10.1645 13.5023 10.2609 13.5666 10.3669 13.6103C10.473 13.654 10.5866 13.6765 10.7013 13.6763Z" fill="white"/> <path d="M8.16588 14.9181C8.36841 14.9177 8.56628 14.8572 8.73447 14.7444C8.90266 14.6316 9.03364 14.4714 9.11086 14.2842C9.18807 14.097 9.20806 13.891 9.16829 13.6925C9.12852 13.4939 9.03078 13.3115 8.88742 13.1685C8.74406 13.0254 8.56151 12.928 8.36284 12.8887C8.16417 12.8493 7.95829 12.8697 7.77122 12.9473C7.58414 13.0249 7.42427 13.1562 7.31179 13.3247C7.19931 13.4931 7.13928 13.6911 7.13928 13.8936C7.13966 14.1656 7.24801 14.4263 7.44051 14.6184C7.63301 14.8105 7.89392 14.9183 8.16588 14.9181Z" fill="white"/> <path d="M5.54343 17.0102C5.78472 17.0096 6.02043 16.9375 6.22078 16.8031C6.42114 16.6686 6.57714 16.4778 6.66909 16.2547C6.76104 16.0316 6.78481 15.7863 6.73739 15.5497C6.68998 15.3131 6.5735 15.0959 6.40269 14.9255C6.23187 14.7551 6.01438 14.6391 5.77769 14.5922C5.54099 14.5453 5.29572 14.5697 5.07285 14.6622C4.84998 14.7546 4.65951 14.9111 4.52552 15.1117C4.39152 15.3124 4.32001 15.5483 4.32001 15.7896C4.32019 15.95 4.35199 16.1089 4.41357 16.2571C4.47515 16.4053 4.56532 16.5399 4.67893 16.6532C4.79253 16.7666 4.92735 16.8564 5.07568 16.9177C5.22401 16.9789 5.38295 17.0103 5.54343 17.0102Z" fill="white"/> <path d="M7.13925 12.3678C7.34175 12.3672 7.53954 12.3067 7.70764 12.1938C7.87574 12.0809 8.00661 11.9207 8.08372 11.7334C8.16082 11.5462 8.18071 11.3403 8.14086 11.1417C8.10101 10.9432 8.00322 10.7609 7.85983 10.6179C7.71644 10.4749 7.53389 10.3776 7.33524 10.3383C7.13659 10.299 6.93075 10.3195 6.74371 10.3971C6.55668 10.4747 6.39685 10.606 6.2844 10.7744C6.17195 10.9429 6.11194 11.1408 6.11194 11.3433C6.11212 11.478 6.13884 11.6114 6.19057 11.7358C6.2423 11.8602 6.31802 11.9732 6.41342 12.0683C6.50881 12.1635 6.62201 12.2389 6.74655 12.2902C6.87108 12.3416 7.00453 12.368 7.13925 12.3678Z" fill="white"/> <path d="M9.67323 11.126C9.84569 11.1257 10.0142 11.0743 10.1574 10.9783C10.3007 10.8823 10.4123 10.746 10.4781 10.5865C10.5439 10.4271 10.561 10.2518 10.5271 10.0827C10.4933 9.91358 10.4101 9.75829 10.2881 9.63645C10.1661 9.5146 10.0106 9.43167 9.84146 9.39813C9.6723 9.36459 9.49699 9.38194 9.33769 9.448C9.17839 9.51406 9.04225 9.62586 8.94646 9.76927C8.85068 9.91268 8.79956 10.0813 8.79956 10.2537C8.79994 10.4852 8.89215 10.707 9.05595 10.8706C9.21976 11.0341 9.44177 11.126 9.67323 11.126Z" fill="white"/> <path d="M9.30231 8.81506C9.4747 8.81436 9.64302 8.76259 9.78601 8.66629C9.929 8.56999 10.0403 8.43348 10.1057 8.27399C10.1712 8.11451 10.1879 7.9392 10.1538 7.77022C10.1197 7.60123 10.0362 7.44614 9.91405 7.32454C9.79185 7.20293 9.63636 7.12026 9.46721 7.08697C9.29806 7.05368 9.12283 7.07126 8.96367 7.13749C8.8045 7.20372 8.66853 7.31563 8.57293 7.45909C8.47732 7.60255 8.42638 7.77112 8.42651 7.94351C8.42661 8.0583 8.44935 8.17194 8.49345 8.27791C8.53755 8.38389 8.60213 8.48012 8.68349 8.56109C8.76485 8.64206 8.8614 8.70617 8.96759 8.74975C9.07378 8.79333 9.18753 8.81553 9.30231 8.81506Z" fill="white"/> <path d="M11.6188 8.29183C11.7697 8.29141 11.9171 8.24627 12.0423 8.16212C12.1676 8.07797 12.2651 7.95858 12.3225 7.81904C12.3799 7.6795 12.3947 7.52608 12.3649 7.37815C12.3352 7.23022 12.2623 7.09442 12.1554 6.98793C12.0485 6.88143 11.9124 6.809 11.7644 6.77981C11.6163 6.75061 11.463 6.76595 11.3236 6.82389C11.1843 6.88182 11.0653 6.97976 10.9816 7.10532C10.8979 7.23087 10.8533 7.37842 10.8535 7.52931C10.8536 7.62963 10.8736 7.72893 10.9121 7.82155C10.9507 7.91416 11.0071 7.99827 11.0782 8.06908C11.1493 8.13988 11.2336 8.196 11.3264 8.23422C11.4191 8.27244 11.5185 8.29201 11.6188 8.29183Z" fill="white"/> <path d="M13.6069 8.29264C13.7362 8.29236 13.8626 8.25375 13.97 8.1817C14.0774 8.10964 14.161 8.00738 14.2103 7.88781C14.2596 7.76825 14.2723 7.63676 14.2469 7.50995C14.2215 7.38314 14.1591 7.26671 14.0676 7.17536C13.976 7.08401 13.8595 7.02185 13.7326 6.99672C13.6057 6.97159 13.4743 6.98463 13.3548 7.03419C13.2354 7.08374 13.1333 7.16759 13.0614 7.27514C12.9896 7.38269 12.9513 7.50912 12.9513 7.63844C12.9517 7.81208 13.0209 7.97847 13.1438 8.10111C13.2667 8.22376 13.4333 8.29264 13.6069 8.29264Z" fill="white"/> <path d="M13.9785 10.6021C14.1078 10.6018 14.2342 10.5632 14.3416 10.4911C14.449 10.419 14.5327 10.3167 14.5819 10.1971C14.6312 10.0775 14.6439 9.94597 14.6185 9.81914C14.593 9.69231 14.5305 9.57588 14.4389 9.48456C14.3473 9.39324 14.2306 9.33113 14.1037 9.30608C13.9768 9.28103 13.8453 9.29417 13.7259 9.34384C13.6065 9.3935 13.5044 9.47746 13.4327 9.5851C13.3609 9.69275 13.3227 9.81924 13.3229 9.9486C13.3233 10.1222 13.3925 10.2885 13.5154 10.411C13.6384 10.5336 13.8049 10.6023 13.9785 10.6021Z" fill="white"/> <path d="M11.9905 10.6247C12.1414 10.6243 12.2887 10.5792 12.414 10.4951C12.5392 10.4109 12.6367 10.2916 12.6941 10.1521C12.7516 10.0126 12.7664 9.8592 12.7367 9.71129C12.707 9.56338 12.6341 9.42759 12.5273 9.32106C12.4205 9.21453 12.2845 9.14205 12.1365 9.11278C11.9885 9.0835 11.8352 9.09874 11.6958 9.15657C11.5565 9.21439 11.4374 9.31222 11.3536 9.43769C11.2699 9.56315 11.2252 9.71063 11.2252 9.86149C11.2253 9.96187 11.2451 10.0612 11.2836 10.1539C11.3222 10.2466 11.3786 10.3308 11.4497 10.4017C11.5207 10.4726 11.6051 10.5288 11.6979 10.567C11.7907 10.6053 11.8901 10.6249 11.9905 10.6247Z" fill="white"/> <path d="M9.82681 16.4218C9.90433 16.4215 9.98002 16.3983 10.0443 16.355C10.1087 16.3117 10.1587 16.2504 10.1882 16.1787C10.2176 16.107 10.2252 16.0282 10.2099 15.9522C10.1946 15.8762 10.1572 15.8064 10.1022 15.7517C10.0473 15.697 9.97745 15.6598 9.9014 15.6448C9.82535 15.6297 9.74656 15.6376 9.67497 15.6673C9.60338 15.6971 9.54221 15.7473 9.49917 15.8118C9.45614 15.8763 9.43317 15.952 9.43317 16.0296C9.43354 16.1337 9.47518 16.2335 9.54896 16.307C9.62275 16.3805 9.72266 16.4218 9.82681 16.4218Z" fill="white"/> <path d="M11.3999 16.2257C11.4946 16.2253 11.5871 16.1968 11.6656 16.1439C11.7442 16.091 11.8053 16.016 11.8412 15.9284C11.8772 15.8407 11.8864 15.7444 11.8676 15.6516C11.8489 15.5588 11.8031 15.4736 11.736 15.4067C11.6688 15.3399 11.5834 15.2945 11.4905 15.2761C11.3976 15.2578 11.3013 15.2674 11.2139 15.3038C11.1264 15.3401 11.0517 15.4016 10.9991 15.4803C10.9465 15.5591 10.9185 15.6517 10.9185 15.7464C10.9185 15.8095 10.9311 15.872 10.9553 15.9302C10.9796 15.9885 11.0151 16.0414 11.0598 16.0859C11.1045 16.1304 11.1576 16.1657 11.2159 16.1897C11.2743 16.2136 11.3368 16.2259 11.3999 16.2257Z" fill="white"/> <path d="M13.1261 15.5723C13.2339 15.5721 13.3392 15.54 13.4287 15.48C13.5183 15.42 13.5881 15.3349 13.6292 15.2352C13.6704 15.1356 13.6811 15.026 13.66 14.9203C13.6388 14.8146 13.5869 14.7175 13.5106 14.6414C13.4343 14.5652 13.3372 14.5133 13.2315 14.4924C13.1257 14.4714 13.0161 14.4822 12.9166 14.5235C12.817 14.5648 12.7319 14.6347 12.672 14.7243C12.6122 14.8139 12.5802 14.9193 12.5802 15.0271C12.5804 15.1717 12.638 15.3104 12.7403 15.4126C12.8427 15.5148 12.9814 15.5723 13.1261 15.5723Z" fill="white"/> <path d="M13.5411 17.3161C13.6704 17.3159 13.7968 17.2774 13.9043 17.2055C14.0118 17.1335 14.0956 17.0313 14.145 16.9117C14.1944 16.7922 14.2072 16.6607 14.1819 16.5338C14.1566 16.407 14.0942 16.2905 14.0027 16.199C13.9112 16.1076 13.7946 16.0454 13.6677 16.0202C13.5408 15.995 13.4093 16.008 13.2898 16.0576C13.1704 16.1071 13.0682 16.1909 12.9964 16.2985C12.9245 16.4061 12.8862 16.5325 12.8862 16.6619C12.8865 16.8354 12.9557 17.0017 13.0784 17.1243C13.2012 17.2469 13.3676 17.3159 13.5411 17.3161Z" fill="white"/> <path d="M11.4439 17.7295C11.5517 17.7293 11.657 17.6972 11.7466 17.6373C11.8361 17.5773 11.9059 17.4921 11.947 17.3925C11.9882 17.2928 11.9989 17.1832 11.9778 17.0775C11.9567 16.9718 11.9047 16.8748 11.8284 16.7986C11.7521 16.7224 11.655 16.6706 11.5493 16.6496C11.4435 16.6286 11.334 16.6394 11.2344 16.6807C11.1348 16.722 11.0497 16.7919 10.9898 16.8815C10.93 16.9712 10.898 17.0765 10.898 17.1843C10.8982 17.329 10.9558 17.4676 11.0581 17.5698C11.1605 17.6721 11.2992 17.7295 11.4439 17.7295Z" fill="white"/> <path d="M9.58602 17.7075C9.68076 17.7073 9.77329 17.6789 9.85192 17.6261C9.93056 17.5732 9.99178 17.4983 10.0278 17.4107C10.0639 17.3231 10.0732 17.2267 10.0545 17.1338C10.0359 17.041 9.99012 16.9557 9.92303 16.8888C9.85594 16.8219 9.77053 16.7764 9.67759 16.758C9.58465 16.7396 9.48835 16.7492 9.40085 16.7855C9.31335 16.8218 9.23857 16.8833 9.18597 16.9621C9.13336 17.0409 9.10529 17.1335 9.10529 17.2282C9.10538 17.2913 9.11789 17.3537 9.1421 17.4119C9.16631 17.4701 9.20174 17.5229 9.24638 17.5674C9.29102 17.6119 9.34399 17.6472 9.40227 17.6713C9.46054 17.6953 9.52298 17.7076 9.58602 17.7075Z" fill="white"/> <path d="M9.60804 19.212C9.7158 19.2117 9.82106 19.1795 9.91053 19.1195C9.99999 19.0594 10.0696 18.9741 10.1107 18.8745C10.1517 18.7749 10.1623 18.6653 10.1411 18.5596C10.1199 18.454 10.0679 18.357 9.99161 18.2809C9.91531 18.2048 9.81817 18.153 9.71246 18.1321C9.60675 18.1112 9.49721 18.122 9.39767 18.1633C9.29814 18.2046 9.21308 18.2745 9.15324 18.3641C9.0934 18.4538 9.06146 18.5591 9.06146 18.6669C9.06156 18.7385 9.07577 18.8095 9.10328 18.8757C9.1308 18.9419 9.17109 19.002 9.22184 19.0526C9.2726 19.1033 9.33283 19.1434 9.39909 19.1707C9.46535 19.1981 9.53635 19.2121 9.60804 19.212Z" fill="white"/> <path d="M11.8368 19.4953C11.9661 19.495 12.0925 19.4564 12.1999 19.3843C12.3073 19.3123 12.3909 19.21 12.4402 19.0904C12.4895 18.9709 12.5023 18.8394 12.4769 18.7126C12.4515 18.5858 12.389 18.4693 12.2975 18.378C12.2059 18.2867 12.0894 18.2245 11.9625 18.1994C11.8357 18.1742 11.7042 18.1873 11.5847 18.2368C11.4653 18.2864 11.3632 18.3702 11.2914 18.4778C11.2195 18.5853 11.1812 18.7118 11.1812 18.8411C11.1813 18.9271 11.1983 19.0122 11.2313 19.0916C11.2643 19.1711 11.3127 19.2432 11.3735 19.304C11.4344 19.3647 11.5067 19.4129 11.5861 19.4457C11.6656 19.4785 11.7508 19.4954 11.8368 19.4953Z" fill="white"/> <path d="M14.4374 19.2121C14.5883 19.212 14.7358 19.1671 14.8612 19.0831C14.9866 18.9992 15.0844 18.8799 15.142 18.7405C15.1997 18.601 15.2147 18.4476 15.1852 18.2996C15.1556 18.1516 15.0829 18.0156 14.9761 17.909C14.8693 17.8023 14.7334 17.7297 14.5853 17.7003C14.4373 17.6709 14.2839 17.686 14.1444 17.7438C14.005 17.8016 13.8859 17.8994 13.802 18.0249C13.7182 18.1504 13.6735 18.298 13.6735 18.4489C13.6738 18.6513 13.7545 18.8453 13.8977 18.9884C14.0409 19.1315 14.235 19.2119 14.4374 19.2121Z" fill="white"/> <path d="M15.9673 21.108C16.1398 21.1076 16.3083 21.0561 16.4514 20.96C16.5946 20.8638 16.7061 20.7274 16.7717 20.5679C16.8374 20.4085 16.8543 20.2331 16.8204 20.0641C16.7864 19.895 16.7031 19.7398 16.581 19.618C16.4588 19.4963 16.3034 19.4134 16.1342 19.38C15.965 19.3466 15.7897 19.3641 15.6304 19.4303C15.4712 19.4965 15.3351 19.6084 15.2395 19.7519C15.1438 19.8954 15.0928 20.064 15.093 20.2365C15.0931 20.3511 15.1159 20.4646 15.1599 20.5704C15.204 20.6763 15.2684 20.7724 15.3496 20.8534C15.4308 20.9343 15.5272 20.9984 15.6332 21.0421C15.7391 21.0858 15.8527 21.1082 15.9673 21.108Z" fill="white"/> <path d="M12.7544 21.3701C12.9052 21.3699 13.0526 21.3251 13.1779 21.2413C13.3033 21.1575 13.401 21.0384 13.4588 20.8991C13.5165 20.7598 13.5317 20.6065 13.5024 20.4586C13.4731 20.3106 13.4006 20.1747 13.2941 20.0679C13.1876 19.9611 13.0519 19.8883 12.9041 19.8586C12.7562 19.8289 12.6029 19.8436 12.4634 19.901C12.324 19.9583 12.2046 20.0557 12.1204 20.1808C12.0363 20.306 11.991 20.4532 11.9905 20.604C11.9903 20.7045 12.0099 20.804 12.0482 20.8969C12.0865 20.9898 12.1427 21.0742 12.2137 21.1454C12.2846 21.2165 12.3689 21.273 12.4617 21.3115C12.5545 21.3501 12.6539 21.37 12.7544 21.3701Z" fill="white"/> <path d="M10.9412 22.8519C11.0921 22.8515 11.2396 22.8063 11.3649 22.7221C11.4901 22.6379 11.5876 22.5184 11.645 22.3788C11.7024 22.2392 11.7171 22.0857 11.6872 21.9377C11.6573 21.7897 11.5843 21.6539 11.4772 21.5475C11.3702 21.441 11.234 21.3687 11.0859 21.3397C10.9377 21.3107 10.7843 21.3262 10.645 21.3844C10.5057 21.4425 10.3868 21.5407 10.3033 21.6664C10.2198 21.7922 10.1754 21.9398 10.1758 22.0908C10.1766 22.2931 10.2576 22.4869 10.4011 22.6296C10.5446 22.7723 10.7388 22.8523 10.9412 22.8519Z" fill="white"/> <path d="M12.4485 24.7699C12.621 24.7696 12.7895 24.7182 12.9328 24.6222C13.0761 24.5261 13.1876 24.3897 13.2534 24.2303C13.3192 24.0708 13.3362 23.8955 13.3024 23.7264C13.2685 23.5572 13.1852 23.4019 13.0631 23.2801C12.941 23.1583 12.7855 23.0754 12.6163 23.042C12.4471 23.0085 12.2718 23.026 12.1125 23.0921C11.9532 23.1583 11.8171 23.2702 11.7214 23.4137C11.6257 23.5572 11.5747 23.7259 11.5748 23.8984C11.5752 24.1298 11.6674 24.3515 11.8313 24.515C11.9951 24.6784 12.2171 24.7701 12.4485 24.7699Z" fill="white"/> <path d="M16.47 25.1189C16.6725 25.1183 16.8703 25.0578 17.0384 24.9449C17.2065 24.832 17.3374 24.6718 17.4145 24.4845C17.4916 24.2973 17.5115 24.0914 17.4716 23.8928C17.4318 23.6943 17.334 23.512 17.1906 23.369C17.0472 23.226 16.8646 23.1287 16.666 23.0894C16.4673 23.0501 16.2615 23.0706 16.0745 23.1482C15.8874 23.2258 15.7276 23.3571 15.6151 23.5255C15.5027 23.694 15.4427 23.8919 15.4427 24.0944C15.4428 24.2292 15.4694 24.3626 15.5211 24.487C15.5728 24.6115 15.6485 24.7245 15.744 24.8196C15.8394 24.9148 15.9526 24.9902 16.0772 25.0415C16.2018 25.0929 16.3352 25.1192 16.47 25.1189Z" fill="white"/> <path d="M14.2631 23.2874C14.4356 23.2869 14.604 23.2354 14.7472 23.1393C14.8904 23.0431 15.0019 22.9067 15.0675 22.7473C15.1332 22.5878 15.1501 22.4125 15.1162 22.2434C15.0822 22.0743 14.9989 21.9191 14.8768 21.7973C14.7546 21.6756 14.5991 21.5928 14.4299 21.5594C14.2608 21.526 14.0855 21.5435 13.9262 21.6096C13.767 21.6758 13.6309 21.7877 13.5352 21.9312C13.4396 22.0747 13.3886 22.2434 13.3887 22.4158C13.3889 22.5304 13.4117 22.6439 13.4557 22.7498C13.4998 22.8556 13.5642 22.9517 13.6454 23.0327C13.7266 23.1136 13.823 23.1777 13.9289 23.2214C14.0349 23.2651 14.1485 23.2875 14.2631 23.2874Z" fill="white"/> <path d="M18.1521 22.9404C18.3546 22.94 18.5525 22.8796 18.7207 22.7667C18.8889 22.6539 19.0199 22.4938 19.0971 22.3065C19.1743 22.1193 19.1943 21.9134 19.1545 21.7148C19.1147 21.5162 19.017 21.3339 18.8736 21.1908C18.7303 21.0477 18.5477 20.9504 18.3491 20.911C18.1504 20.8717 17.9445 20.8921 17.7574 20.9697C17.5704 21.0473 17.4105 21.1786 17.298 21.347C17.1855 21.5154 17.1255 21.7134 17.1255 21.916C17.1256 22.0506 17.1522 22.184 17.2039 22.3084C17.2555 22.4328 17.3312 22.5458 17.4265 22.6409C17.5218 22.7361 17.635 22.8115 17.7595 22.8629C17.884 22.9143 18.0174 22.9406 18.1521 22.9404Z" fill="white"/> <path d="M10.0236 20.9778C10.1529 20.9774 10.2792 20.9387 10.3865 20.8665C10.4938 20.7944 10.5773 20.6921 10.6265 20.5725C10.6757 20.4529 10.6883 20.3215 10.6629 20.1947C10.6374 20.0679 10.5749 19.9516 10.4833 19.8603C10.3918 19.769 10.2752 19.7069 10.1483 19.6818C10.0215 19.6568 9.89008 19.6699 9.77067 19.7194C9.65125 19.769 9.54921 19.8529 9.47742 19.9604C9.40563 20.0679 9.36731 20.1943 9.36731 20.3236C9.3674 20.4097 9.38446 20.4949 9.4175 20.5744C9.45054 20.6538 9.49892 20.726 9.55988 20.7868C9.62083 20.8475 9.69316 20.8957 9.77274 20.9284C9.85231 20.9612 9.93756 20.978 10.0236 20.9778Z" fill="white"/> <defs> <filter id="filter0_i_425_1015" x="0" y="0" width="30" height="30" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"> <feFlood flood-opacity="0" result="BackgroundImageFix"/> <feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/> <feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/> <feMorphology radius="0.6" operator="erode" in="SourceAlpha" result="effect1_innerShadow_425_1015"/> <feOffset/> <feGaussianBlur stdDeviation="0.15"/> <feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/> <feColorMatrix type="matrix" values="0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0.1 0"/> <feBlend mode="normal" in2="shape" result="effect1_innerShadow_425_1015"/> </filter> </defs> </svg>} href="/chains/iota" />

  <Card title="Lumia" icon={<svg width="30" height="30" viewBox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"><path id="Path" fill="#03000a" stroke="none" d="M 15 0 L 15 0 C 23.284271 0 30 6.715729 30 15 L 30 15 C 30 23.284271 23.284271 30 15 30 L 15 30 C 6.715728 30 -0 23.284271 -0 15 L -0 15 C -0 6.715729 6.715728 0 15 0 Z"/><path id="path1" fill="#ffffff" stroke="none" d="M 10.568167 14.732917 L 2.823742 14.732917 L 10.971375 10.949665 L 14.742792 2.825724 L 14.751666 10.543457 C 14.754667 12.856125 12.880792 14.735875 10.565208 14.735875 L 10.568167 14.732917 Z"/><path id="path2" fill="#ffffff" stroke="none" d="M 19.765209 14.732917 L 27.509626 14.732917 L 19.361958 10.949665 L 15.590584 2.825724 L 15.581667 10.543457 C 15.578709 12.856125 17.452541 14.735875 19.768166 14.735875 L 19.765209 14.732917 Z"/><path id="path3" fill="#ffffff" stroke="none" d="M 19.765209 15.59275 L 27.509626 15.59275 L 19.361958 19.376041 L 15.590584 27.499958 L 15.581667 19.782207 C 15.578709 17.469582 17.452541 15.589792 19.768166 15.589792 L 19.765209 15.59275 Z"/><path id="path4" fill="#ffffff" stroke="none" d="M 10.568167 15.59275 L 2.823742 15.59275 L 10.971375 19.376041 L 14.742792 27.499958 L 14.751666 19.782207 C 14.754667 17.469582 12.880792 15.589792 10.565208 15.589792 L 10.568167 15.59275 Z"/></svg>} href="/chains/lumia" />

  <Card title="Manta" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_133_91)"><circle cx="125" cy="125" r="125" fill="white"/></g><path fill-rule="evenodd" clip-rule="evenodd" d="M58.1182 214.64C30.8424 194.259 13.1454 161.685 13.1454 125C13.1454 63.2473 63.2473 13.1454 125 13.1454C175.985 13.1454 231.658 43.6141 245.109 90.2852C230.061 38.1454 181.998 0 125 0C55.9783 0 0 55.9783 0 125C0 194.022 55.9783 250 125 250C194.022 250 250 194.022 250 125V118.41H243.138C242.935 118.41 242.629 118.41 242.221 118.41C241.406 118.41 240.252 118.377 238.757 118.309C235.802 118.206 231.726 118.002 227.14 117.595C217.765 116.78 207.065 115.184 199.796 112.263C188.077 107.575 181.454 101.393 173.913 94.1234L173.471 93.7164C165.897 86.4469 157.405 78.2609 142.629 71.3315C128.091 64.5041 112.024 65.591 100.238 68.0367C94.2594 69.2595 89.0961 70.8899 85.4617 72.2147C83.6273 72.8601 82.1672 73.4715 81.1484 73.8791C75.2038 76.3927 69.4294 79.3477 63.6209 82.0992C63.6209 82.0992 75.068 85.2242 80.7063 86.9906C80.9102 87.0586 81.2164 87.1602 81.6578 87.2961C82.507 87.568 83.7297 88.0094 85.1898 88.6211C88.1797 89.8438 92.0859 91.6438 95.9578 94.1578C102.072 98.0641 107.133 103.023 109.205 109.07C96.2633 110.768 84.1031 116.474 75 121.807C69.4973 125.034 64.9117 128.295 61.6848 130.706C60.0883 131.93 58.7976 132.948 57.9144 133.662C57.1671 134.273 45.4823 144.565 45.4823 144.565C45.4823 144.565 60.5638 146.162 68.0027 147.656C72.962 148.641 79.6195 150.272 86.6172 152.684C93.6141 155.129 100.68 158.322 106.555 162.432C112.432 166.542 116.746 171.263 118.852 176.698C125.034 192.527 118.954 207.914 106.759 216.576C94.7008 225.136 76.3927 227.174 58.0842 214.606L58.1182 214.64ZM100.781 234.205C105.706 232.609 110.292 230.265 114.436 227.344C131.012 215.557 139.742 193.954 131.148 171.943C127.752 163.281 121.298 156.691 114.13 151.664C106.93 146.637 98.675 142.969 90.9984 140.285C85.9711 138.519 81.0805 137.16 76.8342 136.141C78.3625 135.156 79.9594 134.138 81.6914 133.118C91.8141 127.174 104.586 121.705 116.814 121.705C135.258 121.705 148.981 127.377 162.092 133.424C163.281 133.967 164.47 134.545 165.693 135.088C177.412 140.591 189.708 146.366 204.008 146.366C218.309 146.366 228.669 142.459 236.107 138.383C229.484 193.818 182.303 236.82 125.068 236.82C116.746 236.82 108.627 235.904 100.849 234.205H100.781ZM221.841 130.333C216.916 131.998 210.938 133.22 203.974 133.22C192.765 133.22 183.118 128.737 170.89 123.03C169.803 122.52 168.716 122.011 167.595 121.502C155.333 115.829 141.169 109.816 122.724 108.73C120.245 96.5352 111.005 88.1797 103.057 83.0844C102.174 82.507 101.291 81.9633 100.408 81.4539C101.189 81.2836 102.004 81.0805 102.853 80.9102C113.519 78.7023 126.223 78.1586 136.991 83.2539C149.762 89.2664 156.998 96.1953 164.504 103.397L164.776 103.634C172.487 111.039 180.673 118.818 194.905 124.524C202.82 127.684 213.077 129.382 221.841 130.333Z" fill="url(#paint0_linear_133_91)"/><defs><filter id="filter0_i_133_91" x="0" y="3.05176e-05" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_133_91"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_133_91"/></filter><linearGradient id="paint0_linear_133_91" x1="-2.24185" y1="127.31" x2="252.242" y2="122.656" gradientUnits="userSpaceOnUse"><stop stop-color="#29CCB9"/><stop offset="0.49" stop-color="#0091FF"/><stop offset="1" stop-color="#FF66B7"/></linearGradient></defs></svg>} href="/chains/manta" />

  <Card
    title="Mode"
    icon={<svg width="30" height="30" viewBox="0 0 40 40" fill="none" xmlns="http://www.w3.org/2000/svg">
<g filter="url(#filter0_i_475_527)">
<circle cx="20" cy="20" r="20" fill="#D7FF00"/>
</g>
<path fill-rule="evenodd" clip-rule="evenodd" d="M20.6998 21.5926L25.0157 9.43994H31.98V30.6631H27.3079V20.0903L29.1767 14.0553L27.8448 13.5803L21.7737 30.6631H18.2115L12.1404 13.5803L10.8084 14.0553L12.6721 20.0903V30.6631H8V9.43994H14.9694L19.2853 21.5926V25.1599H20.6998V21.5926Z" fill="black"/>
<defs>
<filter id="filter0_i_475_527" x="0" y="0" width="40" height="40" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB">
<feFlood flood-opacity="0" result="BackgroundImageFix"/>
<feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/>
<feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/>
<feMorphology radius="0.8" operator="erode" in="SourceAlpha" result="effect1_innerShadow_475_527"/>
<feOffset/>
<feGaussianBlur stdDeviation="0.2"/>
<feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/>
<feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/>
<feBlend mode="normal" in2="shape" result="effect1_innerShadow_475_527"/>
</filter>
</defs>
</svg>
}
    href="/chains/mode"
  />

  <Card title="Morph" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_302_100)"><circle cx="125" cy="125" r="125" fill="white"/></g><path fill-rule="evenodd" clip-rule="evenodd" d="M45 205H100.097V122.372C100.097 119.701 103.938 119.288 104.506 121.897L117.506 181.563H150.563V161.029C150.563 159.797 151.562 158.798 152.794 158.798H205V45H149.903V65.5345C149.903 66.7664 148.905 67.7649 147.673 67.7649H92.7122L97.229 88.4967C97.532 89.8873 96.473 91.202 95.05 91.202H45V205Z" fill="#14A800"/><defs><filter id="filter0_i_302_100" x="0" y="0" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_302_100"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_302_100"/></filter></defs></svg>} href="/chains/morph" />

  <Card title="Plume" icon={<svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"> <g filter="url(#filter0_i_425_1016)"> <circle cx="15" cy="15" r="15" fill="#FF3D00"/> </g> <path fill-rule="evenodd" clip-rule="evenodd" d="M7.03125 7.03242V13.323L9.27684 15.571V9.2805L15.5363 9.3015L13.2918 7.05225L7.03242 7.03125L7.03125 7.03242ZM18.4192 12.0638V17.7434L16.1737 15.4952V9.81567L18.4192 12.0638ZM12.1585 18.3345H17.8289L15.5845 16.0853H9.91406L12.1585 18.3345ZM21.6105 20.9114V15.2604L19.365 13.0123V19.3028L13.1055 19.2819L15.35 21.5311L21.0105 21.5501L22.2043 22.7457L22.9687 22.8552L22.8231 22.126L21.6105 20.9114Z" fill="white"/> <defs> <filter id="filter0_i_425_1016" x="0" y="0" width="30" height="30" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"> <feFlood flood-opacity="0" result="BackgroundImageFix"/> <feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/> <feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/> <feMorphology radius="0.6" operator="erode" in="SourceAlpha" result="effect1_innerShadow_425_1016"/> <feOffset/> <feGaussianBlur stdDeviation="0.15"/> <feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/> <feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/> <feBlend mode="normal" in2="shape" result="effect1_innerShadow_425_1016"/> </filter> </defs> </svg>} href="/chains/plume" />

  <Card title="Rise" icon={<svg width="21" height="25" viewBox="0 0 21 25" fill="none" xmlns="http://www.w3.org/2000/svg"> <g clip-path="url(#clip0_71_16)"> <path d="M14.6764 0.0322266H0.0488281V4.18441H14.6764C15.8304 4.18441 16.7661 5.11408 16.7661 6.26055V8.33668H6.44606C2.91295 8.33668 0.0488281 11.1724 0.0488281 14.6706V24.9455H4.22817V15.1817L14.805 24.945H20.9464L7.45141 12.4889H16.7661V8.3569H20.9464V6.26055C20.9464 2.82073 18.1384 0.0322266 14.6764 0.0322266Z" fill="white"/> </g> <defs> <clipPath id="clip0_71_16"> <rect width="21" height="25" fill="white"/> </clipPath> </defs> </svg>} href="/chains/rise" />

  <Card title="Sei" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_133_116)"><circle cx="125" cy="125" r="125" fill="white"/></g><path d="M124.684 245C159.924 245 191.626 229.89 213.63 205.811L207.775 199.366C200.529 191.391 188.177 190.709 180.088 197.839L167.598 208.848C150.607 223.827 124.78 222.882 108.94 206.703L101.13 198.727C93.6306 191.067 81.3421 190.801 73.5133 198.127L50.6058 219.565C71.0279 235.501 96.7436 245 124.684 245ZM224.702 184.154L227.545 187.284C238.621 169.127 245 147.807 245 125C245 97.3453 235.621 71.8755 219.861 51.5823C216.932 52.3313 214.132 53.7646 211.714 55.8961L199.224 66.9056C182.233 81.8838 156.406 80.9397 140.566 64.7611L132.756 56.7846C125.256 49.125 112.968 48.858 105.139 56.1846L73.0042 86.2585L57.3708 69.7342L89.5058 39.6602C106.399 23.8502 132.917 24.4264 149.1 40.9551L156.909 48.9317C164.251 56.429 176.219 56.8665 184.093 49.9254L196.582 38.9159C198.679 37.0682 200.907 35.4637 203.234 34.1003C182.156 15.9658 154.704 5 124.684 5C64.3258 5 14.3485 49.328 5.69346 107.123C22.1636 97.6676 43.549 100.14 57.3829 114.269L66.4147 123.493C73.4349 130.664 84.7595 131.42 92.6806 125.248L119.454 104.386C135.524 91.8643 158.274 92.4701 173.647 105.829L210.103 137.508L195.094 154.594L158.637 122.916C151.513 116.724 140.97 116.444 133.524 122.247L106.75 143.109C89.6568 156.428 65.2196 154.796 50.0707 139.323L41.039 130.099C33.5394 122.439 21.2509 122.172 13.4222 129.499L5 137.38C7.60464 162.727 18.1188 185.736 34.0401 203.913L57.88 181.602C74.7737 165.793 101.291 166.368 117.474 182.898L125.284 190.874C132.625 198.371 144.593 198.809 152.467 191.868L164.957 180.859C182.41 165.473 209.066 166.944 224.702 184.154Z" fill="#C1121F"/><defs><filter id="filter0_i_133_116" x="0" y="3.05176e-05" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_133_116"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_133_116"/></filter></defs></svg>} href="/chains/sei" />

  <Card title="Sophon" icon={<svg width="31" height="30" viewBox="0 0 31 30" fill="none" xmlns="http://www.w3.org/2000/svg"> <g filter="url(#filter0_i_425_1239)"> <circle cx="15.2" cy="15" r="15" fill="white"/> </g> <g clip-path="url(#clip0_425_1239)"> <path d="M16.2389 11.7839C16.2937 11.7736 16.3417 11.7702 16.4034 11.7736H16.4651L16.5097 11.7805L16.7943 11.7873C18.5771 11.8456 20.4731 12.1165 21.9097 12.5211C23.9291 13.0902 25.016 13.8856 25.1326 14.8936L25.1429 14.9622V14.9862L25.136 15.0753C25.016 16.0936 23.912 16.8993 21.8549 17.4753C20.3874 17.8902 18.4606 18.1645 16.6606 18.2228L16.4137 18.2262H16.352C16.112 18.2262 15.9817 18.1371 15.9303 17.9931L15.92 17.9588L15.9166 17.9176C15.9103 17.8708 15.9137 17.8232 15.9267 17.7777C15.9397 17.7323 15.9619 17.6901 15.992 17.6536C16.0434 17.5885 16.1154 17.5508 16.1977 17.5268C16.2423 17.5165 16.28 17.5131 16.3349 17.5096L16.3863 17.5062L16.5989 17.4993L17.0034 17.4822L17.7303 17.4376L17.9669 17.4171L18.3303 17.3862L18.6869 17.3519C21.1349 17.0845 22.7977 16.5291 23.8126 15.7851C24.0471 15.6064 24.2226 15.3616 24.3166 15.0822C24.3509 14.9451 24.3371 14.8525 24.2926 14.7393C24.1383 14.3211 23.5211 13.9165 22.6331 13.5668L22.4171 13.4845L22.2046 13.4091L22.0949 13.3748L21.872 13.3028L21.7589 13.2685L21.5223 13.1999L21.2823 13.1313L21.1623 13.1005L20.8503 13.0251L20.528 12.9496L20.2674 12.8982L20 12.8399L19.8663 12.8159L19.5954 12.7713L19.3211 12.7268L19.184 12.7062L18.8411 12.6582L18.4914 12.6171L18.3509 12.6033L18.2137 12.5896L17.9291 12.5622L17.6514 12.5416L17.3703 12.5245L16.8149 12.5039H16.4C16.2046 12.5005 16.0983 12.4696 16.0057 12.3668C15.9371 12.2879 15.9063 12.1851 15.92 12.0856C15.944 11.9039 16.0606 11.8113 16.2354 11.7771L16.2389 11.7839Z" fill="black"/> <path d="M16.4926 12.0377H16.448C16.3934 12.0311 16.3381 12.0311 16.2835 12.0377C16.208 12.0548 16.1909 12.072 16.1806 12.1234C16.1806 12.1508 16.1875 12.1748 16.2046 12.192C16.2286 12.2262 16.256 12.2365 16.328 12.2434H16.5406C16.7257 12.2434 16.9109 12.2502 17.0995 12.2571L17.3875 12.2708L17.6686 12.2845L17.9532 12.3085L18.2343 12.3325L18.5223 12.3634L18.8 12.3977L18.944 12.4114L19.2217 12.4525L19.3657 12.4731L19.64 12.5177L19.9143 12.5622L20.0515 12.5897L20.3189 12.6411L20.5863 12.696L20.8435 12.7542L20.9737 12.7885L21.2275 12.8468L21.3509 12.8811L21.5977 12.9497L21.8926 13.0354L21.9509 13.0525L22.1772 13.128L22.2903 13.1622L22.5063 13.2411C23.6206 13.6491 24.3406 14.1188 24.5395 14.6468C24.6012 14.8182 24.6149 14.9622 24.5737 15.144C24.5052 15.4182 24.3097 15.6925 23.9669 15.9977C22.8937 16.776 21.2137 17.3142 18.9235 17.5851L18.7143 17.6091L18.3543 17.6434L17.9875 17.6777C17.7509 17.6982 17.5109 17.712 17.2675 17.7291L17.0172 17.7428L16.6092 17.76L16.3966 17.7668H16.3349L16.2972 17.7737L16.2629 17.7805C16.2286 17.7908 16.208 17.8011 16.1943 17.8182C16.1842 17.8301 16.1772 17.8442 16.1737 17.8594V17.9005C16.184 17.9417 16.2115 17.9622 16.3109 17.9691H16.4103C18.2617 17.928 20.2743 17.6537 21.7863 17.2285C23.7063 16.6868 24.7246 15.96 24.8652 15.1131L24.8755 15.0514L24.8823 14.9828C24.8137 14.1051 23.8572 13.3611 21.9715 12.8091L21.8412 12.7748C20.2677 12.3595 18.6535 12.1181 17.0275 12.0548L16.7875 12.048L16.4926 12.0377Z" fill="black"/> <path d="M13.9863 11.7737H14.0823C14.3326 11.7805 14.4594 11.9005 14.4834 12.0823C14.4902 12.1296 14.487 12.1779 14.474 12.224C14.461 12.27 14.4385 12.3128 14.408 12.3497C14.3566 12.4114 14.2846 12.4525 14.2023 12.4731L14.1337 12.4868L14.0994 12.4903L14.0137 12.4971L13.8011 12.504L13.5954 12.5108L13.1977 12.5314L12.6834 12.5657L12.4331 12.5828L12.0697 12.6171C10.7669 12.7371 9.68 12.9394 8.77828 13.2068L8.61371 13.2548L8.37371 13.3337C7.73763 13.5366 7.13552 13.8336 6.58743 14.2148C6.35238 14.3944 6.17678 14.6405 6.08343 14.9211C6.04914 15.0583 6.06286 15.1474 6.10743 15.264C6.23086 15.6034 6.66628 15.9325 7.30057 16.2343L7.40343 16.2788L7.58514 16.3611C7.75314 16.4297 7.928 16.4983 8.11657 16.5634L8.30514 16.6285L8.528 16.6971L8.64114 16.7348L8.75771 16.7691L8.99771 16.8377L9.11771 16.872L9.36457 16.9337L9.488 16.9611L9.74171 17.0228L10.0023 17.0777L10.3314 17.1463L10.6674 17.2045L10.9417 17.2491L11.216 17.2937L11.4903 17.328L11.7714 17.3623L12.0491 17.3965L12.3303 17.424L12.6114 17.4445L12.8891 17.4651L13.1703 17.4788L13.6331 17.4925L13.8594 17.496H13.9966C14.192 17.496 14.2983 17.5303 14.3909 17.6331C14.4594 17.712 14.4903 17.8114 14.4766 17.9143C14.4491 18.1268 14.2949 18.216 14.0754 18.2263H13.9657L13.8663 18.2194L13.6023 18.2091C11.8749 18.1613 10.1586 17.9163 8.48686 17.4788C6.47086 16.9097 5.384 16.1143 5.26743 15.1028L5.25714 15.0343V15.0103L5.264 14.9211C5.384 13.9028 6.488 13.0971 8.54514 12.5211C10.0126 12.1063 11.9394 11.832 13.7394 11.7737L13.9863 11.7703V11.7737Z" fill="black"/> <path d="M13.9897 12.0343L13.7497 12.0377C11.9669 12.096 10.0606 12.3669 8.61373 12.7714C6.69373 13.3132 5.67544 14.04 5.53487 14.8869L5.52459 14.9486L5.51773 15.0172C5.5863 15.8949 6.54287 16.6354 8.42516 17.1874L8.5623 17.2286C9.90973 17.6057 11.6789 17.8697 13.3692 17.9417L13.6092 17.952L13.8903 17.9589L13.9726 17.9657H14.1029C14.1783 17.9589 14.2057 17.9383 14.2126 17.8972L14.2194 17.88C14.221 17.867 14.2196 17.8538 14.2155 17.8413C14.2113 17.8289 14.2045 17.8175 14.1954 17.808C14.1714 17.7772 14.144 17.7634 14.072 17.76L13.9932 17.7566H13.8594L13.3932 17.7497L13.1532 17.7394L12.872 17.7257L12.5874 17.7052L12.3063 17.6812L12.0217 17.6537L11.7372 17.6229L11.456 17.5886L11.1749 17.5509L10.8972 17.5063L10.6229 17.4617L10.28 17.4L10.2114 17.3863L9.94744 17.3314L9.68344 17.2766L9.4263 17.2149L9.29944 17.184L9.04916 17.1223L8.92573 17.088L8.68573 17.0194L8.44573 16.9509L8.22287 16.8754C8.02402 16.8069 7.83544 16.7417 7.65716 16.6697L7.4823 16.6012L7.29373 16.5189C6.51887 16.1657 6.02173 15.7817 5.86059 15.3566C5.79887 15.1852 5.78516 15.0412 5.8263 14.8594C5.89487 14.5852 6.0903 14.3074 6.43316 14.0023C6.88573 13.6732 7.45144 13.3852 8.12344 13.1452L8.29487 13.0869L8.53487 13.008C9.60196 12.6922 10.6975 12.482 11.8057 12.3806L12.0457 12.3566L12.4126 12.3223L12.9234 12.288L13.184 12.2709L13.688 12.2469L13.9966 12.2366L14.0754 12.2297L14.1166 12.2263L14.1372 12.2194C14.1714 12.2126 14.192 12.1989 14.2057 12.1852C14.2139 12.1759 14.22 12.165 14.2235 12.1531C14.2271 12.1413 14.228 12.1288 14.2263 12.1166C14.2194 12.0652 14.1989 12.0446 14.1132 12.0343H13.9897Z" fill="black"/> <path d="M15.2 10.2308C15.2823 10.2308 15.3509 10.248 15.4057 10.2857C15.4571 10.3234 15.4949 10.3783 15.5223 10.4468C15.5394 10.4914 15.5531 10.5394 15.56 10.6045L15.5669 10.6525L15.5737 10.7554L15.584 10.8823L15.5943 11.1394L15.608 11.5234L15.6217 12.192L15.6354 13.104L15.6423 14.1291V16.0834L15.632 17.088L15.6183 17.976L15.608 18.4765L15.5943 18.8605L15.5806 19.1691L15.5703 19.3268L15.56 19.4057C15.5529 19.4561 15.5403 19.5055 15.5223 19.5531C15.501 19.6176 15.4603 19.6739 15.4057 19.7143C15.3509 19.752 15.2823 19.7691 15.2 19.7691C15.1177 19.7691 15.0491 19.752 14.9943 19.7143C14.9385 19.6744 14.8965 19.618 14.8743 19.5531C14.8569 19.502 14.8454 19.4491 14.84 19.3954L14.8297 19.3474L14.8229 19.2445L14.816 19.1177L14.8023 18.8605L14.7852 18.2537L14.7749 17.808L14.7646 16.896L14.7577 15.8708V13.9165L14.7646 12.912L14.7749 12.192L14.7852 11.7463L14.7954 11.3211L14.8092 10.9783L14.8229 10.7554L14.8297 10.6525C14.84 10.5634 14.8537 10.5051 14.8743 10.4468C14.9017 10.3783 14.9394 10.3234 14.9909 10.2857C15.0491 10.248 15.1177 10.2308 15.1966 10.2308H15.2Z" fill="black"/> <path d="M15.2 10.4915C15.1657 10.4915 15.152 10.4915 15.1417 10.5017C15.1349 10.5017 15.128 10.5155 15.1212 10.536L15.0972 10.632L15.0903 10.6766L15.0835 10.7692L15.0697 10.9886L15.056 11.328L15.0457 11.7497L15.0355 12.1955L15.0252 12.9155L15.0183 13.9166V15.8709L15.0252 16.8892L15.0355 17.8012L15.0457 18.2469L15.0629 18.8503L15.0766 19.1006L15.0835 19.224L15.0903 19.32C15.0972 19.3886 15.1075 19.4229 15.1212 19.4606C15.128 19.4846 15.1349 19.4949 15.1417 19.4983C15.152 19.5052 15.1657 19.5086 15.2 19.5086C15.2343 19.5086 15.248 19.5086 15.2549 19.4983C15.2617 19.4983 15.2686 19.4846 15.2789 19.464L15.296 19.4057L15.2995 19.3715L15.3097 19.3063L15.32 19.1555L15.3337 18.8537L15.3475 18.4697L15.3577 17.9726L15.3715 17.088L15.3817 16.0835V14.1292L15.3749 13.1075L15.3612 12.1989L15.3475 11.5303L15.3337 11.1532L15.3235 10.8995L15.3166 10.776L15.3063 10.68C15.2995 10.6115 15.2926 10.5772 15.2789 10.5395C15.2686 10.5155 15.2617 10.5052 15.2549 10.5017C15.248 10.4949 15.2309 10.4915 15.2 10.4915Z" fill="black"/> <path d="M15.6389 14.5543L16.28 14.5577L17.2812 14.568L18.0012 14.5783L18.56 14.5886L18.9646 14.6023L19.2835 14.616L19.4686 14.6263L19.5577 14.6332L19.6263 14.6469C19.6709 14.6538 19.7052 14.664 19.7429 14.6778C19.8115 14.7018 19.8629 14.7395 19.9006 14.7943C19.9383 14.8492 19.9589 14.9177 19.9589 15C19.9589 15.0823 19.9383 15.1509 19.9006 15.2058C19.8616 15.261 19.8065 15.3029 19.7429 15.3257L19.6092 15.36L19.5543 15.3669L19.4686 15.3738L19.2903 15.3875L18.8686 15.4046L18.3269 15.4183L17.6515 15.432L16.6915 15.4423H15.8549L15.2 15.4458H14.5417L13.5029 15.4389L12.5669 15.4286L11.8366 15.4115L11.4355 15.4012L11.1166 15.3875L10.9623 15.3772L10.8595 15.3703C10.7892 15.3647 10.72 15.3497 10.6537 15.3257C10.5905 15.3038 10.5355 15.2632 10.496 15.2092C10.4556 15.1484 10.4352 15.0764 10.4377 15.0035C10.4377 14.9178 10.4583 14.8492 10.496 14.7943C10.5264 14.7502 10.5678 14.7148 10.616 14.6915L10.6572 14.6778L10.712 14.6606L10.7395 14.6503L10.7703 14.6469L10.8389 14.6332H10.8835L10.9863 14.6229L11.1097 14.616L11.4355 14.6023L11.7303 14.592L12.3955 14.5783L13.1155 14.568L13.7052 14.5612L14.7577 14.5543H15.6389Z" fill="black"/> <path d="M15.6389 14.8148H14.7612L13.7086 14.8217L13.1189 14.8285L12.4023 14.8388L11.7372 14.8525L11.4457 14.8628L11.1234 14.8765L11.0034 14.8834L10.88 14.8937L10.8183 14.9005L10.7909 14.9074L10.7497 14.9211L10.7223 14.9314C10.7167 14.9348 10.712 14.9395 10.7086 14.9451C10.7052 14.952 10.6983 14.9691 10.6983 15C10.6983 15.0342 10.7052 15.0514 10.7086 15.0582L10.7463 15.0822C10.7737 15.0925 10.8012 15.0994 10.8423 15.1028L10.8834 15.1097L10.9794 15.1165L11.1269 15.1268L11.4423 15.1405L11.8434 15.1508L12.5703 15.168L13.5063 15.1782L14.5417 15.1851H15.8514L16.688 15.1817L17.648 15.1714L18.32 15.1577L18.8583 15.144L19.2732 15.1268L19.4446 15.1165L19.5269 15.1097C19.5817 15.1028 19.616 15.0925 19.6503 15.0822C19.6743 15.072 19.6846 15.0651 19.6846 15.0582C19.6949 15.0514 19.6983 15.0342 19.6983 15C19.6983 14.9657 19.6949 14.952 19.688 14.9417C19.6846 14.9382 19.6743 14.928 19.6503 14.9211L19.616 14.9108L19.5886 14.904L19.5303 14.8937L19.4514 14.8868L19.2697 14.8765L18.9543 14.8628L18.5532 14.8491L17.9977 14.8388L17.2777 14.8285L16.28 14.8182H15.6389V14.8148Z" fill="black"/> </g> <defs> <filter id="filter0_i_425_1239" x="0.200012" y="0" width="30" height="30" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"> <feFlood flood-opacity="0" result="BackgroundImageFix"/> <feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/> <feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/> <feMorphology radius="0.6" operator="erode" in="SourceAlpha" result="effect1_innerShadow_425_1239"/> <feOffset/> <feGaussianBlur stdDeviation="0.15"/> <feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/> <feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/> <feBlend mode="normal" in2="shape" result="effect1_innerShadow_425_1239"/> </filter> <clipPath id="clip0_425_1239"> <rect width="19.8857" height="9.6" fill="white" transform="translate(5.25714 10.2)"/> </clipPath> </defs> </svg>} href="/chains/sophon" />

  <Card title="Stellar" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_101_7)"><circle cx="125" cy="125" r="125" fill="white"/></g><path d="M189.503 68.9459L167.812 79.9818L63.2368 133.259C62.8562 130.595 62.704 127.855 62.704 125.115C62.7801 90.7133 90.7125 62.8571 125.114 62.8571C135.693 62.8571 146.12 65.597 155.406 70.7725L167.812 64.4554L169.638 63.5421C135.617 38.8063 88.0486 46.4173 63.389 80.3624C53.9514 93.3772 48.852 109.056 48.852 125.115C48.852 127.018 48.9281 128.997 49.0803 130.899C49.537 136.455 46.5687 141.707 41.5455 144.295L35 147.644V163.17L54.2558 153.352L60.4968 150.155L66.6617 147.035L176.869 90.8655L189.275 84.5484L214.848 71.4575V55.9311L189.503 68.9459Z" fill="black"/><path d="M214.924 87.0601L72.9027 159.365L60.4968 165.682L35 178.696V194.223L60.3446 181.36L82.0359 170.324L186.763 116.971C187.144 119.711 187.296 122.451 187.296 125.191C187.296 159.593 159.364 187.449 124.962 187.449C114.307 187.449 103.803 184.709 94.518 179.458L93.7569 179.838L80.2854 186.688C114.307 211.348 161.875 203.813 186.611 169.792C196.049 156.777 201.148 141.174 201.148 125.115C201.148 123.136 201.072 121.233 200.92 119.255C200.463 113.699 203.431 108.447 208.455 105.859L215 102.51V87.0601H214.924Z" fill="black"/><defs><filter id="filter0_i_101_7" x="0" y="3.05176e-05" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_101_7"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_101_7"/></filter></defs></svg>} href="/chains/stellar" />

  <Card title="Sui" icon={<svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"> <g filter="url(#filter0_i_425_1119)"> <circle cx="15" cy="15" r="15" fill="#4DA2FF"/> </g> <g clip-path="url(#clip0_425_1119)"> <path fill-rule="evenodd" clip-rule="evenodd" d="M20.2139 13.1451C21.1182 14.2804 21.659 15.7163 21.659 17.2782C21.659 18.8402 21.1018 20.3199 20.1737 21.4616L20.0933 21.5603L20.0723 21.4351C20.0541 21.3292 20.0331 21.2214 20.0084 21.1136C19.5435 19.0712 18.0291 17.3203 15.5364 15.9017C13.8529 14.9463 12.8893 13.7972 12.6363 12.4902C12.4728 11.6453 12.5943 10.7967 12.829 10.0696C13.0638 9.34347 13.4127 8.73423 13.7096 8.36795L14.6796 7.18235C14.8495 6.97409 15.1683 6.97409 15.3381 7.18235L20.2149 13.1451H20.2139ZM21.7475 11.9604L15.2477 4.01374C15.1235 3.86212 14.8915 3.86212 14.7673 4.01374L8.2684 11.9604L8.24739 11.9869C7.05174 13.4712 6.33563 15.3574 6.33563 17.4107C6.33563 22.1924 10.2185 26.0689 15.0075 26.0689C19.7965 26.0689 23.6794 22.1924 23.6794 17.4107C23.6794 15.3574 22.9633 13.4712 21.7676 11.9878L21.7466 11.9613L21.7475 11.9604ZM9.82393 13.1195L10.4049 12.4079L10.4222 12.5395C10.4359 12.6436 10.4533 12.7477 10.4734 12.8528C10.8497 14.8266 12.1933 16.4717 14.4394 17.7459C16.3922 18.8575 17.5294 20.1354 17.8564 21.5365C17.9934 22.1211 18.0172 22.6965 17.9578 23.1999L17.9542 23.2309L17.9258 23.2446C17.0444 23.6748 16.0533 23.9168 15.0066 23.9168C11.3338 23.9168 8.35609 20.9447 8.35609 17.2773C8.35609 15.7026 8.90505 14.2567 9.8221 13.1176L9.82393 13.1195Z" fill="white"/> </g> <defs> <filter id="filter0_i_425_1119" x="0" y="0" width="30" height="30" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"> <feFlood flood-opacity="0" result="BackgroundImageFix"/> <feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/> <feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/> <feMorphology radius="0.6" operator="erode" in="SourceAlpha" result="effect1_innerShadow_425_1119"/> <feOffset/> <feGaussianBlur stdDeviation="0.15"/> <feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/> <feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/> <feBlend mode="normal" in2="shape" result="effect1_innerShadow_425_1119"/> </filter> <clipPath id="clip0_425_1119"> <rect width="17.3438" height="22.2" fill="white" transform="translate(6.33563 3.90002)"/> </clipPath> </defs> </svg>} href="/chains/sui" />

  <Card title="Swellchain" icon={<svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"> <g filter="url(#filter0_i_425_1012)"> <circle cx="15" cy="15" r="15" fill="white"/> </g> <g clip-path="url(#clip0_425_1012)"> <path d="M24.4575 17.2334L24.4316 17.2607C24.4195 17.2732 24.4072 17.2856 24.3948 17.298L17.2883 24.3931C16.0503 25.6291 14.0432 25.6291 12.8052 24.3931L10.8428 22.4338C10.7036 22.2955 10.7034 22.0705 10.8418 21.9315C10.8826 21.8906 10.9327 21.8603 10.9878 21.8431C13.0912 21.1938 14.8004 20.4442 16.1152 19.5945C19.2463 17.5709 22.0271 16.7838 24.4575 17.2334ZM22.9676 11.3969C25.5914 12.2237 25.2852 14.1539 25.32 14.9148C22.4465 13.4081 18.9897 13.9604 14.9495 16.5715C11.9381 18.5177 9.41562 19.3202 7.382 18.9788C5.34839 18.6374 4.89093 16.1538 4.79999 15.4938C7.03456 16.6236 10.0292 15.9752 13.7838 13.5486C17.2826 11.2874 20.3438 10.5702 22.9676 11.3969ZM17.2883 5.72703L19.1323 7.56778C19.2712 7.70636 19.2714 7.93138 19.1328 8.07028C19.0834 8.11985 19.0204 8.15368 18.9518 8.16751C16.2823 8.70529 14.1475 9.49132 12.5471 10.5256C9.80482 12.2979 7.4679 13.1217 5.53641 12.9969L5.53662 12.9964L12.8052 5.72703C14.0432 4.49106 16.0503 4.49106 17.2883 5.72703Z" fill="url(#paint0_linear_425_1012)"/> </g> <defs> <filter id="filter0_i_425_1012" x="0" y="0" width="30" height="30" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"> <feFlood flood-opacity="0" result="BackgroundImageFix"/> <feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/> <feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/> <feMorphology radius="0.6" operator="erode" in="SourceAlpha" result="effect1_innerShadow_425_1012"/> <feOffset/> <feGaussianBlur stdDeviation="0.15"/> <feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/> <feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/> <feBlend mode="normal" in2="shape" result="effect1_innerShadow_425_1012"/> </filter> <linearGradient id="paint0_linear_425_1012" x1="15.06" y1="4.80005" x2="14.8588" y2="39.4024" gradientUnits="userSpaceOnUse"> <stop stop-color="#3068EF"/> <stop offset="1" stop-color="#1322AC"/> </linearGradient> <clipPath id="clip0_425_1012"> <rect width="20.52" height="20.52" fill="white" transform="translate(4.79999 4.80005)"/> </clipPath> </defs> </svg>} href="/chains/swellchain" />

  <Card
    title="TAC"
    icon={<svg width="30" height="30" viewBox="0 0 40 41" fill="none" xmlns="http://www.w3.org/2000/svg">
<g filter="url(#filter0_i_519_19)">
<circle cx="20" cy="20" r="19" fill="#1E162B"/>
</g>
<path fill-rule="evenodd" clip-rule="evenodd" d="M40 20C40 31.0456 31.0458 40 20 40C8.95433 40 0 31.0456 0 20C0 8.95429 8.95433 0 20 0C31.0458 0 40 8.95429 40 20ZM20.8037 33.8765V14.8493C20.8037 14.3878 21.1777 14.0138 21.6391 14.0138H32.0689C33.134 14.0138 33.7924 15.1748 33.2461 16.0889L22.3563 34.3055C21.9211 35.0334 20.8037 34.7248 20.8037 33.8765ZM19.1963 25.1293V6.10194C19.1963 5.25385 18.0789 4.94532 17.6437 5.67324L6.75402 23.8897C6.20755 24.8038 6.86614 25.9648 7.93115 25.9648H18.3609C18.8223 25.9648 19.1963 25.5907 19.1963 25.1293Z" fill="#F2EBFF"/>
<defs>
<filter id="filter0_i_519_19" x="1" y="1" width="38" height="38" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB">
<feFlood flood-opacity="0" result="BackgroundImageFix"/>
<feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/>
<feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/>
<feMorphology radius="0.8" operator="erode" in="SourceAlpha" result="effect1_innerShadow_519_19"/>
<feOffset/>
<feGaussianBlur stdDeviation="0.2"/>
<feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/>
<feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/>
<feBlend mode="normal" in2="shape" result="effect1_innerShadow_519_19"/>
</filter>
</defs>
</svg>
}
    href="/chains/tac"
  />

  <Card title="Taiko" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_107_2)"><circle cx="125" cy="125" r="125" fill="white"/></g><path d="M214.81 167.332L182.681 123.679C179.19 118.936 174.022 116.094 168.518 115.468C167.265 115.322 166.147 114.595 165.52 113.499C164.882 112.402 164.815 111.071 165.318 109.908C167.523 104.829 167.656 98.9334 165.285 93.5412L143.548 43.8927C140.327 36.5204 133.044 31.7658 125 31.7658C116.957 31.7658 109.674 36.5316 106.452 43.8927L84.7152 93.5412C82.3547 98.9334 82.4778 104.829 84.6816 109.908C85.185 111.071 85.1067 112.402 84.4802 113.499C83.8426 114.595 82.735 115.322 81.482 115.468C75.978 116.094 70.8096 118.936 67.3192 123.679L35.1898 167.332C30.424 173.809 29.943 182.49 33.9592 189.46C37.9865 196.418 45.7504 200.345 53.738 199.461L107.604 193.465C113.455 192.816 118.5 189.762 121.789 185.309C122.539 184.291 123.736 183.687 125 183.687C126.264 183.687 127.45 184.291 128.211 185.309C131.5 189.762 136.545 192.816 142.396 193.465L196.262 199.461C204.25 200.356 212.014 196.429 216.041 189.46C220.057 182.49 219.576 173.809 214.81 167.332ZM98.7662 99.7724L120.536 50.068C121.32 48.2893 123.076 47.137 125.011 47.137C126.947 47.137 128.703 48.2893 129.486 50.068L151.257 99.7724C151.939 101.339 151.793 103.151 150.854 104.583C149.914 106.015 148.326 106.876 146.603 106.876H103.409C101.697 106.876 100.098 106.015 99.1578 104.583C98.218 103.151 98.0726 101.339 98.755 99.7724H98.7662ZM109.942 175.442C109.17 176.975 107.671 178.004 105.971 178.194L52.0488 184.191C50.1245 184.403 48.2451 183.464 47.2719 181.785C46.2985 180.107 46.4216 178.015 47.5739 176.449L79.7369 132.752C80.7549 131.376 82.3883 130.593 84.0999 130.694C85.8115 130.783 87.3553 131.734 88.2168 133.222L88.2504 133.278L109.786 170.576L109.819 170.631C110.681 172.12 110.725 173.932 109.953 175.453L109.942 175.442ZM129.43 158.572C128.513 160.149 126.835 161.134 125 161.134C123.176 161.134 121.488 160.161 120.57 158.583L104.024 129.933C103.107 128.356 103.107 126.398 104.024 124.821C104.942 123.243 106.62 122.259 108.454 122.259H141.535C143.358 122.259 145.047 123.221 145.965 124.809C146.882 126.398 146.882 128.345 145.965 129.922L129.43 158.572ZM202.739 181.785C201.766 183.464 199.898 184.415 197.963 184.202L144.041 178.205C142.34 178.015 140.841 176.986 140.069 175.453C139.298 173.921 139.342 172.108 140.203 170.631L140.237 170.576L161.772 133.278L161.806 133.222C162.667 131.734 164.211 130.783 165.923 130.694C167.634 130.604 169.268 131.376 170.286 132.752L202.449 176.449C203.601 178.015 203.712 180.107 202.75 181.785H202.739Z" fill="#E81899"/><defs><filter id="filter0_i_107_2" x="0" y="3.05176e-05" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_107_2"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_107_2"/></filter></defs></svg>} href="/chains/telos" />

  <Card title="Telos" icon={<svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"> <g filter="url(#filter0_i_425_1111)"> <circle cx="15" cy="15" r="15" fill="black"/> </g> <g clip-path="url(#clip0_425_1111)"> <path d="M15 23.4C10.3685 23.4 6.59998 19.6315 6.59998 15C6.59998 10.3685 10.3685 6.59998 15 6.59998C19.6315 6.59998 23.4 10.3685 23.4 15C23.4 19.6315 19.6315 23.4 15 23.4ZM15 9.65452C12.0523 9.65452 9.65452 12.0523 9.65452 15C9.65452 17.9476 12.0523 20.3455 15 20.3455C17.9476 20.3455 20.3455 17.9476 20.3455 15C20.3455 12.0523 17.9476 9.65452 15 9.65452Z" fill="url(#paint0_linear_425_1111)"/> </g> <defs> <filter id="filter0_i_425_1111" x="0" y="0" width="30" height="30" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"> <feFlood flood-opacity="0" result="BackgroundImageFix"/> <feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/> <feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/> <feMorphology radius="0.6" operator="erode" in="SourceAlpha" result="effect1_innerShadow_425_1111"/> <feOffset/> <feGaussianBlur stdDeviation="0.15"/> <feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/> <feColorMatrix type="matrix" values="0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0.1 0"/> <feBlend mode="normal" in2="shape" result="effect1_innerShadow_425_1111"/> </filter> <linearGradient id="paint0_linear_425_1111" x1="8.84711" y1="9.07601" x2="20.0769" y2="23.5662" gradientUnits="userSpaceOnUse"> <stop stop-color="#00F2FE"/> <stop offset="0.535" stop-color="#4FACFE"/> <stop offset="0.975" stop-color="#C471F5"/> </linearGradient> <clipPath id="clip0_425_1111"> <rect width="16.8" height="16.8" fill="white" transform="translate(6.59998 6.59998)"/> </clipPath> </defs> </svg>} href="/chains/telos" />

  <Card title="Treasure" icon={<svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"> <g filter="url(#filter0_i_425_1011)"> <circle cx="15" cy="15" r="15" fill="white"/> </g> <g clip-path="url(#clip0_425_1011)"> <path d="M7.08002 11.2495V18.7544C7.08002 19.9969 7.74056 21.1429 8.80818 21.7603L13.7316 24.6156C14.5226 25.0748 15.4943 25.0748 16.2854 24.6156L21.2088 21.7603C22.2764 21.1391 22.9369 19.993 22.9369 18.7544V11.2495C22.9369 10.007 22.2764 8.86101 21.2088 8.24364L16.2854 5.38442C15.4943 4.92525 14.5226 4.92525 13.7316 5.38442L8.80818 8.23978C7.74056 8.86101 7.08002 10.007 7.08002 11.2495Z" fill="#DC2626"/> <path d="M21.0206 14.8032L19.2117 14.1858C18.5473 13.9581 18.0519 13.3948 17.9137 12.7041L17.1494 8.91494C17.1303 8.81848 17.0458 8.74902 16.9459 8.74902C16.8461 8.74902 16.7616 8.81848 16.7424 8.91494L15.9781 12.7041C15.8399 13.3948 15.3445 13.9581 14.6801 14.1858L12.8713 14.8032C12.7868 14.8302 12.733 14.9112 12.733 15C12.733 15.0887 12.7907 15.1659 12.8713 15.1967L14.6801 15.8141C15.3445 16.0418 15.8399 16.6051 15.9781 17.2958L16.7424 21.0849C16.7616 21.1814 16.8461 21.2508 16.9459 21.2508C17.0458 21.2508 17.1303 21.1814 17.1494 21.0849L17.9137 17.2958C18.0519 16.6051 18.5473 16.0418 19.2117 15.8141L21.0206 15.1967C21.1051 15.1697 21.1588 15.0887 21.1588 15C21.1588 14.9112 21.1012 14.834 21.0206 14.8032Z" fill="#FFFDF7"/> <path d="M11.2892 11.3268L11.9267 11.5428C12.1609 11.6238 12.3376 11.8206 12.3837 12.0676L12.6525 13.4065C12.6602 13.4413 12.6909 13.4644 12.7255 13.4644C12.76 13.4644 12.7907 13.4413 12.7985 13.4065L13.0673 12.0676C13.1172 11.8245 13.29 11.6238 13.5281 11.5428L14.1656 11.3268C14.1964 11.3152 14.2156 11.2882 14.2156 11.2573C14.2156 11.2264 14.1964 11.1994 14.1656 11.1878L13.5281 10.9718C13.2939 10.8907 13.1172 10.6939 13.0673 10.447L12.7985 9.10805C12.7907 9.07333 12.76 9.05017 12.7255 9.05017C12.6909 9.05017 12.6602 9.07333 12.6525 9.10805L12.3837 10.447C12.3338 10.6901 12.1609 10.8907 11.9267 10.9718L11.2892 11.1878C11.2585 11.1994 11.2393 11.2264 11.2393 11.2573C11.2393 11.2882 11.2585 11.3152 11.2892 11.3268Z" fill="#FFFDF7"/> <path d="M12.7791 17.9017L11.9227 17.6122C11.6078 17.5042 11.3735 17.238 11.3083 16.91L10.9473 15.1157C10.9396 15.0695 10.8973 15.0386 10.8513 15.0386C10.8052 15.0386 10.7629 15.0695 10.7552 15.1157L10.3942 16.91C10.329 17.238 10.0947 17.5042 9.77978 17.6122L8.92338 17.9017C8.88498 17.9132 8.85809 17.9518 8.85809 17.9942C8.85809 18.0367 8.88498 18.0753 8.92338 18.0869L9.77978 18.3763C10.0947 18.4843 10.329 18.7506 10.3942 19.0785L10.7552 20.8728C10.7629 20.9191 10.8052 20.9499 10.8513 20.9499C10.8973 20.9499 10.9357 20.9152 10.9473 20.8728L11.3083 19.0785C11.3735 18.7506 11.6078 18.4843 11.9227 18.3763L12.7791 18.0869C12.8175 18.0753 12.8444 18.0367 12.8444 17.9942C12.8444 17.9518 12.8175 17.9132 12.7791 17.9017Z" fill="#FFFDF7"/> </g> <defs> <filter id="filter0_i_425_1011" x="0" y="0" width="30" height="30" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"> <feFlood flood-opacity="0" result="BackgroundImageFix"/> <feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/> <feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/> <feMorphology radius="0.6" operator="erode" in="SourceAlpha" result="effect1_innerShadow_425_1011"/> <feOffset/> <feGaussianBlur stdDeviation="0.15"/> <feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/> <feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/> <feBlend mode="normal" in2="shape" result="effect1_innerShadow_425_1011"/> </filter> <clipPath id="clip0_425_1011"> <rect width="15.8547" height="19.92" fill="white" transform="translate(7.08002 5.04004)"/> </clipPath> </defs> </svg>} href="/chains/treasure" />

  <Card title="Unichain" icon={<svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"> <g clip-path="url(#clip0_425_1013)"> <g filter="url(#filter0_i_425_1013)"> <circle cx="15" cy="15" r="15" fill="url(#paint0_radial_425_1013)"/> </g> <path d="M26.7855 14.7959C20.3828 14.7959 15.1979 9.58554 15.1979 3.16309H14.7485V14.7959H3.16089V15.2471C9.56357 15.2471 14.7485 20.4575 14.7485 26.8799H15.1979V15.2471H26.7855V14.7959Z" fill="white"/> </g> <defs> <filter id="filter0_i_425_1013" x="0" y="0" width="30" height="30" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"> <feFlood flood-opacity="0" result="BackgroundImageFix"/> <feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/> <feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/> <feMorphology radius="0.6" operator="erode" in="SourceAlpha" result="effect1_innerShadow_425_1013"/> <feOffset/> <feGaussianBlur stdDeviation="0.15"/> <feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/> <feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/> <feBlend mode="normal" in2="shape" result="effect1_innerShadow_425_1013"/> </filter> <radialGradient id="paint0_radial_425_1013" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(15 15) rotate(90) scale(15)"> <stop stop-color="#FC74FE"/> <stop offset="1" stop-color="#F50DB4"/> </radialGradient> <clipPath id="clip0_425_1013"> <rect width="30" height="30" fill="white"/> </clipPath> </defs> </svg>} href="/chains/unichain" />

  <Card title="Viction" icon={<svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"> <path d="M25.6568 4.40869V13.6222C22.2509 14.49 18.6799 14.9506 15.0001 14.9506C11.3203 14.9506 7.74923 14.4889 4.34326 13.6222V4.40869C7.74923 5.27539 11.3203 5.7371 15.0001 5.7371C18.6799 5.7371 22.2509 5.27645 25.6568 4.40869Z" fill="#F8F6D7"/> <path d="M4.34326 25.7036V16.49C7.74923 15.6223 11.3203 15.1616 15.0001 15.1616C18.6799 15.1616 22.2509 15.6233 25.6568 16.49V25.7036C22.2509 24.8369 18.6799 24.3751 15.0001 24.3751C11.3203 24.3751 7.74923 24.8358 4.34326 25.7036Z" fill="#F8F6D7"/> </svg>} href="/chains/viction" />

  <Card title="World Chain" icon={<svg width="30" height="30" viewBox="0 0 30 30" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_11768_16033)"><path d="M28.7694 9.17031C28.011 7.38864 26.9387 5.79039 25.5525 4.40175C24.1663 3.0131 22.5971 1.93886 20.8187 1.17904C18.9617 0.393013 17.0002 0 14.9863 0C12.9463 0 10.9847 0.393013 9.15392 1.17904C7.37544 1.93886 5.78005 3.0131 4.39388 4.40175C3.00772 5.79039 1.9354 7.38864 1.17693 9.17031C0.392311 11.0044 0 12.9695 0 15.0131C0 17.0305 0.392311 18.9956 1.17693 20.8559C1.9354 22.6375 3.00772 24.2358 4.39388 25.6245C5.78005 27.0131 7.37544 28.0873 9.15392 28.821C11.0109 29.607 12.9724 30 14.9863 30C17.0002 30 18.9617 29.607 20.8187 28.821C22.5971 28.0612 24.1925 26.9869 25.5787 25.5983C26.9649 24.2096 28.0372 22.6114 28.7957 20.8297C29.5803 18.9695 29.9726 17.0044 29.9726 14.9869C29.9726 12.9695 29.5541 11.0044 28.7694 9.17031ZM10.017 13.5982C10.6185 11.1877 12.8155 9.43228 15.4047 9.43228H25.8141C26.4941 10.7162 26.9125 12.131 27.0695 13.5982H10.017ZM27.0695 16.4279C26.9125 17.8952 26.4679 19.3101 25.8141 20.5939H15.4047C12.8155 20.5939 10.6447 18.8122 10.017 16.4279H27.0695ZM6.38159 6.39301C8.68315 4.08734 11.7432 2.82969 14.9863 2.82969C18.2294 2.82969 21.2895 4.08734 23.5909 6.39301C23.6694 6.47162 23.7218 6.52402 23.8002 6.60262H15.4047C13.1555 6.60262 11.0632 7.46725 9.46775 9.0655C8.21238 10.3232 7.4016 11.8952 7.14006 13.5982H2.9031C3.21695 10.8734 4.42004 8.35808 6.38159 6.39301ZM14.9863 27.1965C11.7432 27.1965 8.68315 25.9388 6.38159 23.6332C4.42004 21.6682 3.21695 19.1266 2.9031 16.4279H7.14006C7.42776 18.131 8.23853 19.7031 9.46775 20.9607C11.0632 22.559 13.1555 23.4236 15.4047 23.4236H23.8002C23.7218 23.5022 23.6694 23.5546 23.5909 23.6332C21.2895 25.9388 18.2294 27.1965 14.9863 27.1965Z" fill="white"/></g><defs><clipPath id="clip0_11768_16033"><rect width="30" height="30" fill="white"/></clipPath></defs></svg>} href="/chains/worldchain" />

  <Card title="X1" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_122_8)"><circle cx="125" cy="125" r="125" fill="black"/></g><path fill-rule="evenodd" clip-rule="evenodd" d="M99.028 51.0649C100.013 51.9568 100.023 52.1851 100.023 75.0527C100.023 97.5101 99.9965 98.1662 99.0771 99.0855C98.1577 100.005 97.5016 100.031 75.2579 100.031C53.4434 100.031 52.3321 99.9888 51.268 99.1276L50.152 98.2243L50.031 75.8487C49.9374 58.4465 50.0442 53.2127 50.5131 52.3009C51.7351 49.9243 51.8481 49.9139 75.7472 50.0434C96.906 50.1583 98.0835 50.2098 99.028 51.0649ZM198.924 51.2412L200 52.3179V75.1794C200 96.9977 199.959 98.0868 199.1 99.0359C198.208 100.021 197.982 100.031 175.11 100.031C152.65 100.031 151.994 100.005 151.075 99.0855C150.155 98.1662 150.129 97.5101 150.129 75.3212C150.129 50.6853 150.123 50.751 152.615 50.2542C153.198 50.1384 163.613 50.0708 175.761 50.1039L197.847 50.1644L198.924 51.2412ZM148.969 101.191L150.129 102.351V125.082V147.813L148.969 148.973L147.809 150.134H125.076H102.343L101.183 148.973L100.023 147.813V125.082V102.351L101.183 101.191L102.343 100.031H125.076H147.809L148.969 101.191ZM99.0771 151.079C99.9965 151.998 100.023 152.654 100.023 174.896C100.023 196.711 99.9809 197.82 99.1192 198.884L98.2154 200H75.2607H52.3056L51.2288 198.923L50.152 197.847V174.985C50.152 153.167 50.1931 152.078 51.0525 151.129C51.9445 150.143 52.1704 150.134 75.0424 150.134C97.5016 150.134 98.1577 150.16 99.0771 151.079ZM198.884 151.037L200 151.941V174.893V197.847L198.923 198.923L197.846 200H174.983C153.163 200 152.074 199.959 151.124 199.1C150.139 198.208 150.129 197.982 150.129 175.112C150.129 152.654 150.155 151.998 151.075 151.079C151.994 150.16 152.65 150.134 174.894 150.134C196.71 150.134 197.82 150.175 198.884 151.037Z" fill="white"/><defs><filter id="filter0_i_122_8" x="0" y="3.05176e-05" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_122_8"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_122_8"/></filter></defs></svg>} href="/chains/x1" />

  <Card title="Xai" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_114_3)"><circle cx="125" cy="125" r="125" fill="white"/></g><path fill-rule="evenodd" clip-rule="evenodd" d="M125 26.9471L215 179.053H35L125 26.9471ZM125 80.3056L83.8877 149.798H166.112L125 80.3056Z" fill="#FF001B"/><defs><filter id="filter0_i_114_3" x="0" y="3.05176e-05" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_114_3"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_114_3"/></filter></defs></svg>} href="/chains/xai" />

  <Card title="ZER" icon={<svg width="30" height="30" viewBox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"><clipPath id="clipPath1"><path d="M 0 0 L 30 0 L 30 30 L 0 30 Z"/></clipPath><g id="Group" clip-path="url(#clipPath1)"><path id="Path" fill="#ffffff" stroke="none" d="M 15 30 C 6.715964 30 0 23.284092 0 15 C 0 6.715979 6.715964 0 15 0 C 23.28409 0 30 6.715979 30 15 C 30 23.284092 23.285591 30 15 30 Z"/><path id="path1" fill="#06003c" stroke="none" d="M 15.068318 6.954546 C 9.840546 6.954546 7.772727 10.14934 7.772727 14.853546 C 7.772727 19.557682 9.840546 22.909092 15.068318 22.909092 C 20.296091 22.909092 22.363636 19.557411 22.363636 14.853546 C 22.363636 10.149586 20.296091 6.954546 15.068318 6.954546 Z M 15.068999 20.331137 C 12.684613 20.331137 11.449677 18.931908 11.076927 16.576908 C 11.04885 16.399364 11.184641 16.238045 11.363754 16.238045 L 18.737455 16.238045 C 18.915955 16.238045 19.051638 16.398409 19.024637 16.575682 C 18.662045 18.931364 17.454136 20.331137 15.069273 20.331137 L 15.068999 20.331137 Z M 11.332773 13.684362 C 11.156319 13.684362 11.021496 13.527531 11.045222 13.35169 C 11.375863 10.902559 12.612477 9.532568 15.068999 9.532568 C 17.525591 9.532568 18.73391 10.903051 19.054773 13.352905 C 19.07782 13.528254 18.943228 13.684362 18.767046 13.684362 L 11.332773 13.684362 Z"/></g></svg>} href="/chains/zero" />

  <Card title="ZetaChain" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_133_119)"><circle cx="125" cy="125" r="125" fill="#005741"/></g><path d="M157.456 152.631V169.513H87.8128C88.7734 158.402 92.3587 150.584 104.757 139.591L157.456 94.6331V134.085H176.427V61.5028H68.557V97.6823H87.5274V80.4733H144.836L92.3935 125.229L92.2681 125.348C70.2903 144.806 68.543 160.511 68.543 179.015V188.497H176.434V152.645H157.463L157.456 152.631Z" fill="white"/><defs><filter id="filter0_i_133_119" x="0" y="3.05176e-05" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_133_119"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_133_119"/></filter></defs></svg>} href="/chains/zetachain" />

  <Card title="zkSync Era" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><g filter="url(#filter0_i_159_737)"><circle cx="125" cy="125" r="125" fill="black"/></g><path fill-rule="evenodd" clip-rule="evenodd" d="M233.478 125.013L171.942 63.6588V108.581L110.848 153.555L171.942 153.607V186.341L233.478 125.013Z" fill="white"/><path fill-rule="evenodd" clip-rule="evenodd" d="M16.5215 124.987L78.058 186.315V141.758L139.151 96.4192L78.058 96.3671V63.6588L16.5215 124.987Z" fill="white"/><defs><filter id="filter0_i_159_737" x="0" y="0" width="250" height="250" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feColorMatrix in="SourceAlpha" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 127 0" result="hardAlpha"/><feMorphology radius="5" operator="erode" in="SourceAlpha" result="effect1_innerShadow_159_737"/><feOffset/><feGaussianBlur stdDeviation="1.25"/><feComposite in2="hardAlpha" operator="arithmetic" k2="-1" k3="1"/><feColorMatrix type="matrix" values="0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0.1 0"/><feBlend mode="normal" in2="shape" result="effect1_innerShadow_159_737"/></filter></defs></svg>} href="/chains/zksync" />

  <Card title="Zora" icon={<svg width="30" height="30" viewBox="0 0 250 250" fill="none"xmlns="http://www.w3.org/2000/svg"><mask id="mask0_133_122" style={{maskType: "alpha"}} maskUnits="userSpaceOnUse" x="0" y="0" width="250" height="250"><path d="M125 250C194.036 250 250 194.036 250 125C250 55.9644 194.036 0 125 0C55.9644 0 0 55.9644 0 125C0 194.036 55.9644 250 125 250Z" fill="#D9D9D9"/></mask><g mask="url(#mask0_133_122)"><path d="M292.363 -54.3445H-38.5742V276.592H292.363V-54.3445Z" fill="#A1723A"/><g filter="url(#filter0_f_133_122)"><path d="M134.547 244.743C208.215 244.743 267.935 185.041 267.935 111.395C267.935 37.7481 208.215 -21.9542 134.547 -21.9542C60.8793 -21.9542 1.15967 37.7481 1.15967 111.395C1.15967 185.041 60.8793 244.743 134.547 244.743Z" fill="#531002"/></g><g filter="url(#filter1_f_133_122)"><path d="M148.926 202.536C208.675 202.536 257.112 154.082 257.112 94.3105C257.112 34.5394 208.675 -13.9146 148.926 -13.9146C89.176 -13.9146 40.7393 34.5394 40.7393 94.3105C40.7393 154.082 89.176 202.536 148.926 202.536Z" fill="#2B5DF0"/></g><g filter="url(#filter2_f_133_122)"><path d="M146.954 208.72C209.244 208.72 259.741 158.207 259.741 95.8952C259.741 33.5839 209.244 -16.9295 146.954 -16.9295C84.6645 -16.9295 34.1685 33.5839 34.1685 95.8952C34.1685 158.207 84.6645 208.72 146.954 208.72Z" fill="url(#paint0_radial_133_122)"/></g><g filter="url(#filter3_f_133_122)"><path d="M165.662 124.304C197.042 124.304 222.48 98.8659 222.48 67.4861C222.48 36.1063 197.042 10.6679 165.662 10.6679C134.282 10.6679 108.844 36.1063 108.844 67.4861C108.844 98.8659 134.282 124.304 165.662 124.304Z" fill="#FCB8D4"/></g><g filter="url(#filter4_f_133_122)"><path d="M165.623 90.1361C178.154 90.1361 188.312 79.978 188.312 67.4474C188.312 54.9169 178.154 44.7588 165.623 44.7588C153.093 44.7588 142.935 54.9169 142.935 67.4474C142.935 79.978 153.093 90.1361 165.623 90.1361Z" fill="white"/></g><g filter="url(#filter5_f_133_122)"><path d="M150.549 293.986C263.153 293.986 354.437 202.702 354.437 90.0974C354.437 -22.507 263.153 -113.791 150.549 -113.791C37.9446 -113.791 -53.3394 -22.507 -53.3394 90.0974C-53.3394 202.702 37.9446 293.986 150.549 293.986Z" fill="url(#paint1_radial_133_122)" fill-opacity="0.9"/></g></g><defs><filter id="filter0_f_133_122" x="-29.7618" y="-52.8757" width="328.618" height="328.54" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feGaussianBlur stdDeviation="15.4607" result="effect1_foregroundBlur_133_122"/></filter><filter id="filter1_f_133_122" x="-21.1037" y="-75.7576" width="340.059" height="340.136" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feGaussianBlur stdDeviation="30.9215" result="effect1_foregroundBlur_133_122"/></filter><filter id="filter2_f_133_122" x="10.9774" y="-40.1206" width="271.954" height="272.032" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feGaussianBlur stdDeviation="11.5955" result="effect1_foregroundBlur_133_122"/></filter><filter id="filter3_f_133_122" x="62.4616" y="-35.7143" width="206.401" height="206.401" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feGaussianBlur stdDeviation="23.1911" result="effect1_foregroundBlur_133_122"/></filter><filter id="filter4_f_133_122" x="112.013" y="13.8374" width="107.22" height="107.22" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feGaussianBlur stdDeviation="15.4607" result="effect1_foregroundBlur_133_122"/></filter><filter id="filter5_f_133_122" x="-76.5304" y="-136.982" width="454.159" height="454.159" filterUnits="userSpaceOnUse" color-interpolation-filters="sRGB"><feFlood flood-opacity="0" result="BackgroundImageFix"/><feBlend mode="normal" in="SourceGraphic" in2="BackgroundImageFix" result="shape"/><feGaussianBlur stdDeviation="11.5955" result="effect1_foregroundBlur_133_122"/></filter><radialGradient id="paint0_radial_133_122" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(165.978 64.0653) rotate(128.228) scale(213.009 212.992)"><stop offset="0.286458" stop-color="#387AFA"/><stop offset="0.647782" stop-color="#387AFA" stop-opacity="0"/></radialGradient><radialGradient id="paint1_radial_133_122" cx="0" cy="0" r="1" gradientUnits="userSpaceOnUse" gradientTransform="translate(150.549 90.0974) rotate(90) scale(203.888)"><stop offset="0.598958" stop-opacity="0"/><stop offset="0.671875"/><stop offset="0.734375" stop-opacity="0"/></radialGradient></defs></svg>} href="/chains/zora" />
</CardGroup>


# Indexing Ink with Goldsky
Source: https://docs.goldsky.com/chains/ink



## Overview

Goldsky is a high-performance data indexing provider for Ink that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Ink to make our product available to the ecosystem and provide dedicated support for Ink data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Ink Mainnet and Sepolia are available at the chain slugs `ink` and `ink-sepolia` respectively. Subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Check out this [GitHub repo](https://github.com/JavierTrujilloG/ink-sepolia-subgraphs) to see example implementations of each deployment method using the [Inko ETH (IETH)](https://explorer-sepolia.inkonchain.com/token/0x87B4bD3A657c4D739730e115Dad11246c0D8D2b1?tab=contract) ERC-20 token on Ink Sepolia.

### Mirror

Mirror pipelines allow users to replicate data into their own infrastructure (any of the [supported sinks](/mirror/sinks/supported-sinks)) in real time.

For a complete overview of how to deploy Mirror pipelines, including a video walkthrough, check the [Create a Pipeline](/mirror/create-a-pipeline). Below, we will look at a few of the different ways by which you can deploy Mirror pipelines using [Direct Indexing datasets](/mirror/sources/direct-indexing) for Ink Sepolia (remember that you can also use [subgraphs as source](/mirror/sources/subgraphs) to your pipelines); in specific, we'll be streaming the `ink_sepolia.receipt_transactions` direct indexing dataset into a PostgreSQL database.

<Note>
  Remember to first create a [Secret](/mirror/manage-secrets) in order for Mirror to be able to write the data into the database of your choice.
</Note>

Pipelines can be deployed on Goldsky in 3 ways:

* Using [Goldsky Flow](mirror/create-a-pipeline#goldsky-flow) in the dashboard:

  * Drop a `Data Source` card. Select `Ink` as the chain. Then `Enriched Transactions` as the onchain dataset to use.

  * Add a `Sink` card and select your pre-configured sink.

  * Deploy pipeline.

* Using the [interactive CLI](/mirror/create-a-pipeline#creating-mirror-pipelines-with-the-cli):

  * Enter command `goldsky pipeline create <pipeline-name>`.

  * This will kick off a guided flow with the first step to choose the dataset type. Choose `Direct Indexing`.

  * Next, select `Ink Sepolia` as chain with the enter key, then `Enriched Transactions` as the dataset by pressing space on the selected dataset which you can select using the up and down arrow keys. We will process historical data so select `Process all historical data` by pressing the enter key. This is the same as the yaml config setting of `start_at: earliest`. If you prefer to only ingest data starting when your pipeline is deployed, select `Process data from the time this pipeline is created` instead, this is equivalent to the yaml config setting `start_at: latest`.

  * When asked to choose another source, choose `No` by pressing enter.

  * Add your pre-configured sink. In this example case, we chose PostgreSQL and as a next step we selected the database schema.

  * Pipeline will be deployed automatically.

* Using a [pipeline configuration file](/mirror/create-a-pipeline#non-interactive-pipeline-configuration):

  * This makes it easier to set up complex pipelines involving multiple sources, multiple sinks, and more complex, SQL-based transformations. For the full reference documentation on, click [here](/reference/config-file/pipeline).

  * As in the previous steps, we'll be deploying a pipeline to stream Ink Sepolia transactions. Unlike the other methods, we have added a transformation to only select specific data attributes from the whole dataset. This is the configuration file:

```yaml ink-sepolia-transactions.yaml
name: ink-sepolia-transactions
resource_size: s
apiVersion: 3
sources:
  ink_sepolia_receipt_transactions:
    dataset_name: ink_sepolia.receipt_transactions
    version: 1.0.0
    type: dataset
    start_at: earliest
transforms:
  subset_transform:
    primary_key: id
    sql: |
      SELECT
        id,
        from_address,
        to_address,
        input,
        `value`,
        block_hash,
        block_number
        FROM ink_sepolia_receipt_transactions
sinks:
  postgres_ink_sepolia_transactions:
    type: postgres
    table: ink_sepolia_transactions
    schema: public
    secret_name: <YOUR_SECRET_NAME>
    description: 'Postgres sink for: ink_sepolia_transactions'
    from: subset_transform
```

* Add your corresponding secret name and run `goldsky pipeline apply ink-sepolia-transactions.yaml --status ACTIVE` to deploy the pipeline.

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing IOTA EVM with Goldsky
Source: https://docs.goldsky.com/chains/iota



## Overview

Goldsky is a high-performance data indexing provider for IOTA EVM that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with IOTA EVM to make our product available to the ecosystem and provide dedicated support for IOTA EVM data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

IOTA EVM subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both IOTA EVM's latest testnet and mainnet are currently supported at the chain slug `iota-testnet` and `iota` respectively.

### Mirror

<Info>Support for Goldsky Mirror for IOTA EVM is currently in progress. If you'd like to be notified when support is launched publicly, contact us at [sales@goldsky.com](mailto:sales@goldsky.com).</Info>

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Lumia with Goldsky
Source: https://docs.goldsky.com/chains/lumia



## Overview

Goldsky is a high-performance data indexing provider for Lumia that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Lumia to make our product available to the Lumia ecosystem. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror |
| ----------------------------- | ----------------------------------------------------- | ------ |
| <strong>Enablement</strong>   | Yes                                                   | No     |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | N/A    |
| <strong>Availability</strong> | All developers                                        | N/A    |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Lumia subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both Lumia Mainnet and Testnet are currently available at the chain slugs `lumia` and `lumia-testnet` respectively.

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Manta Pacific with Goldsky
Source: https://docs.goldsky.com/chains/manta



## Overview

Goldsky is a high-performance data indexing provider for Manta that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Manta to make our product available to the ecosystem and provide dedicated support for Manta data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Manta subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both Manta Pacific mainnet and testnet are available at the chain slugs `manta-pacific-mainnet` and `manta-pacific-testnet` respectively.

### Mirror

<Info>Support for Goldsky Mirror for Manta is currently in progress. If you'd like to be notified when support is launched publicly, contact us at [sales@goldsky.com](mailto:sales@goldsky.com).</Info>

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Mode with Goldsky
Source: https://docs.goldsky.com/chains/mode



## Overview

Goldsky is a high-performance data indexing provider for Mode that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Mode to make our product available to the ecosystem and provide dedicated support for Mode data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                               |
| ----------------------------- | ----------------------------------------------------- | ---------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                  |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 100% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | Select developers                                    |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Mode subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both Mode mainnet and testnet are available at the chain slugs `mode-mainnet` and `mode-testnet` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Morph with Goldsky
Source: https://docs.goldsky.com/chains/morph



## Overview

Goldsky is a high-performance data indexing provider for Morph that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Morph to make our product available to the ecosystem and provide dedicated support for Morph data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Morph subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Morph Mainnet and Holesky Testnet are currently available at the chain slugs `morph` and `morph-testnet` respectively.

### Mirror

Subgraphs indexed on Morph by Goldsky can be "mirrored" into another database, as flat files, or as a Kafka topic. To learn more about mirroring subgraph data into your own infrastructure, visit the [dedicated page on subgraph-based Mirror pipelines](/mirror/sources/subgraphs).

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Plume with Goldsky
Source: https://docs.goldsky.com/chains/plume



## Overview

Goldsky is a high-performance data indexing provider for Plume that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Plume to make our product available to the Plume ecosystem. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror |
| ----------------------------- | ----------------------------------------------------- | ------ |
| <strong>Enablement</strong>   | Yes                                                   | No     |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | N/A    |
| <strong>Availability</strong> | All developers                                        | N/A    |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Plume subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both Plume Mainnet and Devnet are currently available using the chain slugs `plume-mainnet` and `plume-sepolia` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing RISE with Goldsky
Source: https://docs.goldsky.com/chains/rise



## Overview

Goldsky is a high-performance data indexing provider for RISE that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with RISE to make our product available to the ecosystem and provide dedicated support for Rise data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

RISE subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

RISE Testnet is currently supported at the chain slug `rise-sepolia`.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Sei Network with Goldsky
Source: https://docs.goldsky.com/chains/sei



## Overview

Goldsky is a high-performance data indexing provider for Sei EVM that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Sei EVM subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Sei EVM is currently available on mainnet using the chain slug `sei`. Unfortunately, due to the nature of Sei's testnet and devnet, they don't return a genesis block starting at block number 0, so they cannot be indexed on Goldsky at this time.

### Mirror

Subgraphs indexed on Sei EVM mainnet by Goldsky can be "mirrored" into another database, as flat files, a Kafka topic, or using SQS. To learn more about mirroring subgraph data into your own infrastructure, visit the [dedicated page on subgraph-based Mirror pipelines](/mirror/sources/subgraphs).

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Sophon with Goldsky
Source: https://docs.goldsky.com/chains/sophon



## Overview

Goldsky is a high-performance data indexing provider for Sophon that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Sophon to make our product available to the ecosystem and provide dedicated support for Sophon data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Sophon subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Sophon Mainnet and Testnet are currently supported at the chain slugs `sophon` and `sophon-testnet` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Streaming Stellar data with Goldsky
Source: https://docs.goldsky.com/chains/stellar



## Overview

Goldsky is a high-performance data indexing provider for Stellar that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases via [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Stellar to make our product available to the ecosystem and provide dedicated support for Stellar data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Mirror                                              |
| ----------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                 |
| <strong>Benefit</strong>      | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Mirror

Mirror pipelines allow users to replicate data into their own infrastructure (any of the [supported sinks](/mirror/sinks/supported-sinks)) in real time. The supported data sources are the following direct indexing datasets: checkpoints, packages, transactions, epochs and events.

For a complete overview of how to deploy Mirror pipelines, including a video walkthrough, check the [Create a Pipeline](/mirror/create-a-pipeline). Below, we will look at a few of the different ways by which you can deploy Mirror pipelines; Here, we'll be streaming the `stellar.transactions` direct indexing dataset into a PostgreSQL database.

<Note>
  Remember to first create a [Secret](/mirror/manage-secrets) in order for Mirror to be able to write the data into the database of your choice.
</Note>

Pipelines can be deployed on Goldsky in 3 ways:

* Using [Goldsky Flow](mirror/create-a-pipeline#goldsky-flow) in the dashboard:
  * Drop a `Data Source` card. Select `Stellar` as the chain. Then `Enriched Transactions` as the onchain dataset to use.
  * Add a `Sink` card and select your pre-configured sink.
  * Deploy pipeline.
* Using the [interactive CLI](/mirror/create-a-pipeline#creating-mirror-pipelines-with-the-cli):
  * Enter command `goldsky pipeline create <pipeline-name>`.
  * This will kick off a guided flow with the first step to choose the dataset type. Choose `Direct Indexing`.
  * Next, select `Stellar` as chain with the enter key, then `Transactions` as the dataset by pressing space on the selected dataset which you can select using the up and down arrow keys. We will process historical data so select `Process all historical data` by pressing the enter key. This is the same as the yaml config setting of `start_at: earliest`. If you prefer to only ingest data starting when your pipeline is deployed, select `Process data from the time this pipeline is created` instead, this is equivalent to the yaml config setting `start_at: latest`.
  * When asked to choose another source, choose `No` by pressing enter.
  * Add your pre-configured sink. In this example case, we chose PostgreSQL and as a next step we selected the database schema.
  * Pipeline will be deployed automatically.
* Using a [pipeline configuration file](/mirror/create-a-pipeline#non-interactive-pipeline-configuration):
  * This makes it easier to set up complex pipelines involving multiple sources, multiple sinks, and more complex, SQL-based transformations. For the full reference documentation on, click [here](/reference/config-file/pipeline).
  * As in the previous steps, we'll be deploying a pipeline to stream Stellar transactions. Unlike the other methods, we have added a transformation to only select specific data attributes from the whole dataset. This is the configuration file:

```yaml stellar-transactions.yaml
name: untitled
resource_size: s
apiVersion: 3
sources:
  stellar_transactions:
    type: dataset
    dataset_name: stellar.transactions
    version: 1.0.0
    start_at: earliest
transforms:
  subset_transform:
    type: sql
    sql: >-
      SELECT id, transaction_hash, ledger_sequence, account, account_muxed,
      tx_result from stellar_transactions
    primary_key: id
sinks:
  postgres_stellar_transactions:
    type: postgres
    table: stellar_transactions
    schema: public
    secret_name: <YOUR_SECRET_NAME>
    description: 'Postgres sink for: stellar_transactions'
    from: subset_transform
```

* Add your corresponding secret name and run `goldsky pipeline apply stellar-transactions.yaml --status ACTIVE` to deploy the pipeline.

## Getting support

<Snippet file="getting-help.mdx" />


# Streaming Sui data with Goldsky
Source: https://docs.goldsky.com/chains/sui



## Overview

Goldsky is a high-performance data indexing provider for Sui that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases via [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Sui to make our product available to the ecosystem and provide dedicated support for Sui data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Mirror                                              |
| ----------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                 |
| <strong>Benefit</strong>      | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Mirror

Mirror pipelines allow users to replicate data into their own infrastructure (any of the [supported sinks](/mirror/sinks/supported-sinks)) in real time. The supported data sources are the following direct indexing datasets: checkpoints, packages, transactions, epochs and events.

For a complete overview of how to deploy Mirror pipelines, including a video walkthrough, check the [Create a Pipeline](/mirror/create-a-pipeline). Below, we will look at a few of the different ways by which you can deploy Mirror pipelines; Here, we'll be streaming the `sui.transactions` direct indexing dataset into a PostgreSQL database.

<Note>
  Remember to first create a [Secret](/mirror/manage-secrets) in order for Mirror to be able to write the data into the database of your choice.
</Note>

Pipelines can be deployed on Goldsky in 3 ways:

* Using [Goldsky Flow](mirror/create-a-pipeline#goldsky-flow) in the dashboard:
  * Drop a `Data Source` card. Select `Sui` as the chain. Then `Enriched Transactions` as the onchain dataset to use.
  * Add a `Sink` card and select your pre-configured sink.
  * Deploy pipeline.
* Using the [interactive CLI](/mirror/create-a-pipeline#creating-mirror-pipelines-with-the-cli):
  * Enter command `goldsky pipeline create <pipeline-name>`.
  * This will kick off a guided flow with the first step to choose the dataset type. Choose `Direct Indexing`.
  * Next, select `Sui` as chain with the enter key, then `Raw Transactions` as the dataset by pressing space on the selected dataset which you can select using the up and down arrow keys. We will process historical data so select `Process all historical data` by pressing the enter key. This is the same as the yaml config setting of `start_at: earliest`. If you prefer to only ingest data starting when your pipeline is deployed, select `Process data from the time this pipeline is created` instead, this is equivalent to the yaml config setting `start_at: latest`.
  * When asked to choose another source, choose `No` by pressing enter.
  * Add your pre-configured sink. In this example case, we chose PostgreSQL and as a next step we selected the database schema.
  * Pipeline will be deployed automatically.
* Using a [pipeline configuration file](/mirror/create-a-pipeline#non-interactive-pipeline-configuration):
  * This makes it easier to set up complex pipelines involving multiple sources, multiple sinks, and more complex, SQL-based transformations. For the full reference documentation on, click [here](/reference/config-file/pipeline).
  * As in the previous steps, we'll be deploying a pipeline to stream Sui transactions. Unlike the other methods, we have added a transformation to only select specific data attributes from the whole dataset. This is the configuration file:

```yaml sui-transactions.yaml
apiVersion: 3
name: sui-raw-transactions
sources:
  sui_transactions:
    dataset_name: sui.transactions
    version: 1.0.0
    type: dataset
transforms:
  subset_transform:
    primary_key: id
    sql: |
      SELECT
        id,
        transaction_digest,
        transaction_kind,
        effects_json,
        transaction_json
        FROM sui_transactions
sinks:
  postgres_sui_transactions:
    type: postgres
    table: sui_transactions
    schema: public
    secret_name: <YOUR_SECRET_NAME>
    description: 'Postgres sink for: sui_transactions'
    from: subset_transform
```

* Add your corresponding secret name and run `goldsky pipeline apply sui-transactions.yaml --status ACTIVE` to deploy the pipeline.

## Getting support

<Snippet file="getting-help.mdx" />


# Supported networks
Source: https://docs.goldsky.com/chains/supported-networks



## Subgraphs

Goldsky currently supports the following chains on Subgraphs.

<Snippet file="supported-chains-subgraphs.mdx" />

## Mirror

Goldsky currently supports the following chains on Mirror.

<Snippet file="supported-chains-mirror.mdx" />


# Indexing Swellchain with Goldsky
Source: https://docs.goldsky.com/chains/swellchain



## Overview

Goldsky is a high-performance data indexing provider for Swellchain that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Swellchain to make our product available to the ecosystem and provide dedicated support for Swellchain data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Swellchain subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both Swellchain mainnet and testnet are available at the chain slugs `swell` and `swell-testnet` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing TAC with Goldsky
Source: https://docs.goldsky.com/chains/tac



## Overview

Goldsky is a high-performance data indexing provider for TAC that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with TAC to make our product available to the ecosystem and provide dedicated support for TAC data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

TAC subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

TAC Turin Testnet is currently supported at the chain slug `tac-turin`.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Taiko with Goldsky
Source: https://docs.goldsky.com/chains/taiko



## Overview

Goldsky is a high-performance data indexing provider for Taiko that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Taiko to make our product available to the ecosystem and provide dedicated support for Taiko data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Taiko subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both Taiko's latest testnet (Hekla) and mainnet are currently supported at the chain slug `taiko-hekla-testnet` and `taiko` respectively.

### Mirror

<Info>Support for Goldsky Mirror for Taiko is currently in progress. If you'd like to be notified when support is launched publicly, contact us at [sales@goldsky.com](mailto:sales@goldsky.com).</Info>

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Telos with Goldsky
Source: https://docs.goldsky.com/chains/telos



## Overview

Goldsky is a high-performance data indexing provider for Telos that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Telos to make our product available to the ecosystem and provide dedicated support for Telos data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                            | Mirror                                              |
| ----------------------------- | ---------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                  | Yes                                                 |
| <strong>Benefit</strong>      | 10% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | All developers                                       | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Telos subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both Telos mainnet and testnet are available at the chain slugs `telos` and `telos-testnet` respectively.

### Mirror

<Info>Support for Goldsky Mirror for Telos is currently in progress. If you'd like to be notified when support is launched publicly, contact us at [sales@goldsky.com](mailto:sales@goldsky.com).</Info>

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Treasure with Goldsky
Source: https://docs.goldsky.com/chains/treasure



## Overview

Goldsky is a high-performance data indexing provider for Treasure that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Treasure to make our product available to the ecosystem and provide dedicated support for Treasure data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                            | Mirror                                              |
| ----------------------------- | ---------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                  | Yes                                                 |
| <strong>Benefit</strong>      | 10% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                    | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Treasure Subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Treasure Mainnet and Topaz Testnet are currently supported at the chain slugs `treasure` and `treasure-topaz` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Unichain with Goldsky
Source: https://docs.goldsky.com/chains/unichain



## Overview

Goldsky is a high-performance data indexing provider for Unichain that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Unichain to make our product available to the ecosystem and provide dedicated support for Unichain data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                            | Mirror                                              |
| ----------------------------- | ---------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                  | Yes                                                 |
| <strong>Benefit</strong>      | 10% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                    | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Unichain subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Unichain's Mainnet and Sepolia Testnet are currently supported at the chain slugs `unichain` and `unichain-sepolia` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Viction with Goldsky
Source: https://docs.goldsky.com/chains/viction



## Overview

Goldsky is a high-performance data indexing provider for Viction that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Viction to make our product available to the ecosystem and provide dedicated support for Viction data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Viction subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Viction Mainnet and Testnet are currently supported at the chain slugs `viction` and `viction-testnet` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing World Chain with Goldsky
Source: https://docs.goldsky.com/chains/worldchain



## Overview

Goldsky is a high-performance data indexing provider for World Chain that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with World Chain to make our product available to the ecosystem and provide dedicated support for World Chain data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                            | Mirror                                              |
| ----------------------------- | ---------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                  | Yes                                                 |
| <strong>Benefit</strong>      | 10% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                    | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

World Chain subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both World Chain Mainnet and Sepolia Testnet are currently supported at the chain slugs `worldchain` and `worldchain-sepolia` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing OKX's X1 with Goldsky
Source: https://docs.goldsky.com/chains/x1



## Overview

Goldsky is a high-performance data indexing provider for X1 that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with OKX to make our product available to the X1 ecosystem. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                            | Mirror |
| ----------------------------- | ---------------------------------------------------- | ------ |
| <strong>Enablement</strong>   | Yes                                                  | No     |
| <strong>Benefit</strong>      | 10% discount on Subgraph workers and entities stored | N/A    |
| <strong>Availability</strong> | All developers                                       | N/A    |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

X1 subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

X1 testnet is currently available at the chain slug `x1-testnet`, and support for mainnet will be added at public launch.

### Mirror

<Info>Support for Goldsky Mirror for X1 is currently in progress. If you'd like to be notified when support is launched publicly, contact us at [sales@goldsky.com](mailto:sales@goldsky.com).</Info>

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Xai with Goldsky
Source: https://docs.goldsky.com/chains/xai



## Overview

Goldsky is a high-performance data indexing provider for Xai that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with Xai to make our product available to the ecosystem and provide dedicated support for Xai data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

Xai subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both Xai mainnet and testnet are available at the chain slugs `xai` and `xai-testnet` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing ZER with Goldsky
Source: https://docs.goldsky.com/chains/zero



## Overview

Goldsky is a high-performance data indexing provider for ZER that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with ZER to make our product available to the ecosystem and provide dedicated support for ZER data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

ZER subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

ZER Mainnet and Testnet are currently supported at the chain slugs `zero` and `zero-sepolia` respectively.

### Mirror

<Snippet file="chain-specific/mirror-quickstart.mdx" />

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing ZetaChain with Goldsky
Source: https://docs.goldsky.com/chains/zetachain



## Overview

Goldsky is a high-performance data indexing provider for ZetaChain that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with ZetaChain to make our product available to the ecosystem and provide dedicated support for ZetaChain data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

ZetaChain subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both ZetaChain mainnet and testnet are available at the chain slugs `zetachain-mainnet` and `zetachain-testnet` respectively.

### Mirror

<Info>Support for Goldsky Mirror for ZetaChain is currently in progress. If you'd like to be notified when support is launched publicly, contact us at [sales@goldsky.com](mailto:sales@goldsky.com).</Info>

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing zkSync Era with Goldsky
Source: https://docs.goldsky.com/chains/zksync



## Overview

Goldsky is a high-performance data indexing provider for zkSync Era that makes it easy to extract, transform, and load on-chain data to power both application and analytics use cases. Goldsky offers two primary approaches to indexing and accessing blockchain data: [Subgraphs](/subgraphs) (high-performance subgraphs) and [Mirror](/mirror) (real-time data replication pipelines).

### Scope

Goldsky has partnered with zkSync Era to make our product available to the ecosystem and provide dedicated support for zkSync Era data indexing. The full scope of our partnership (which products are enabled, what partner-exclusive benefit is available, and who this benefit is available to) is outlined below.

|                               | Subgraphs                                             | Mirror                                              |
| ----------------------------- | ----------------------------------------------------- | --------------------------------------------------- |
| <strong>Enablement</strong>   | Yes                                                   | Yes                                                 |
| <strong>Benefit</strong>      | 100% discount on Subgraph workers and entities stored | 10% discount on Pipeline workers and events written |
| <strong>Availability</strong> | Select developers                                     | All developers                                      |

<Snippet file="apply-perks.mdx" />

## Getting started

To use Goldsky, you'll need to create an account, install the CLI, and log in.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

### Subgraphs

zkSync Era subgraphs can be deployed on Goldsky in 2 ways:

<Snippet file="chain-specific/subgraphs-quickstart.mdx" />

Both zkSync Era mainnet and testnet are available at the chain slugs `zksync-era` and `zksync-era-testnet` respectively.

### Mirror

Subgraphs indexed on zkSync Era by Goldsky can be "mirrored" into another database, as flat files, or as a Kafka topic. To learn more about mirroring subgraph data into your own infrastructure, visit the [dedicated page on subgraph-based Mirror pipelines](/mirror/sources/subgraphs).

## Getting support

<Snippet file="getting-help.mdx" />


# Indexing Zora with Goldsky
Source: https://docs.goldsky.com/chains/zora



<Info>Coming soon. If you're running into issues building on Zora, please contact [support@goldsky.com](mailto:support@goldsky.com) and we'd be happy to help</Info>


# Frequently asked questions
Source: https://docs.goldsky.com/faq

Collection of frequently (and not-so-frequently) asked questions.

## Subgraphs

<AccordionGroup>
  <Accordion title="Do my subgraph requests need to be authenticated?">
    Endpoints are by default publicly accessible but you can make your endpoints private so that it's only accessible by
    authenticated users, see [private endpoints](./subgraphs/graphql-endpoints).
    Regardless of the access type, endpoints are typically rate-limited preventing abuse, and are not publicly indexed
    or searchable.
    As a best practice, you may want to proxy your requests to prevent leaking your endpoint URL from your
    front-end.
  </Accordion>

  <Accordion title="A subgraph I deployed instantly synced to 100%. Is that an error?">
    No! If Goldsky has already indexed that subgraph (unique subgraphs identified by their IPFS hash), it will sync instantly, though you will be provided your own endpoint with your own rate limits applied. Query away.
  </Accordion>

  <Accordion title="How many requests can Goldsky handle on an subgraph endpoint?">
    By default, the Scale plan is restricted to 50 requests every 10 seconds. However, our Enterprise plans scale horizontally and our highest-use endpoints are seamlessly handling thousands of requests a second at peak. If you need a higher rate limit than what you have enabled on your account, please contact us!
  </Accordion>

  <Accordion title="Do Goldsky subgraphs support subscriptions?">
    Not at the moment, though similar functionality for live queries can be accomplished by polling our querying endpoints. We also do support webhooks, which can be similarly useful for certain push-based use cases.
  </Accordion>

  <Accordion title="I keep getting 524 errors with instant subgraphs, what is wrong?">
    Deployments with a lot of metadata can sometimes time out the IPFS server. You can try again (right away, and if that isn't working, a bit later) and eventually one attempt should work. This is a limitation of the IPFS server, but we're exploring options to workaround this. If you continue to face issues, contact our support team at [support@goldsky.com](mailto:support@goldsky.com) and we'll help manually port it over.
  </Accordion>

  <Accordion title="I am getting a 'store error' issue, how do I fix it?">
    You may get `store error: column "x" specified more than once` when using Goldsky's [Instant Subgraphs functionality](/subgraphs/guides/create-a-no-code-subgraph). Multiple ABIs might be causing name conflicts due to conflicting fields or event names in the ABI. You can try splitting multiple ABIs into multiple subgraphs. There will be a mitigation for this in a future version. If you run into issues deploying or with the subgraph separately, contact our support team at [support@goldsky.com](mailto:support@goldsky.com).
  </Accordion>
</AccordionGroup>

## Mirror

<AccordionGroup>
  <Accordion title="What region(s) is data sent from?">
    Mirror pipelines write data from `us-west-2` on AWS from a dynamic range of IP addresses. If you need VPC peering / static IPs for your allow list, contact us at [support@goldsky.com](mailto:support@goldsky.com) to discuss your use case.
  </Accordion>

  <Accordion title="Can I set a resource size while creating a pipeline?">
    Yes! Add `--resource size <size>` to your `goldsky pipeline create <name>` command, and the resource size will be set prior to deployment of the pipeline, preventing the need for a pipeline update (which restarts the resource).
  </Accordion>

  <Accordion title="Can a pipeline write to a sink with past data from a previous pipeline?">
    Yes - if the primary key is the same (which is the default), the pipeline will upsert and not rewrite data. If its already there (based on the primary key) it will skip and move to the next record until it identifies data that isnt already in the destination sink. Its important to note that this only applies for databases that support upserts, such as Postgres, MySQL, and Elasticsearch. This does not work on S3, and duplicate data is written.
  </Accordion>

  <Accordion title="How big is each dataset?">
    Your destination sink and indexes kept will vastly influence how much storage you need for your data. We are working on publishing a record count for raw data tables to serve as a starting point for approximation, but in the meantime feel free to contact support for a better estimate for your specific use case!
  </Accordion>
</AccordionGroup>

## Platform

<AccordionGroup>
  <Accordion title="How does Goldsky secure my project API keys?">
    API keys are only kept hashed (meaning after its displayed for the first time, you need to copy and save it locally in order to access it, we wont be able to restore it for you!). If your API key is lost, you can reset / generate a new one from the settings page in the web app.
  </Accordion>

  <Accordion title="Can I use Goldsky on my custom chain or rollup?">
    Goldsky can support any EVM-compatible chain. If we don't support it in our shared indexing infrastructure, contact us to get set up with a dedicated indexer. Once set up, we can add new chains to your instance in a \~1 hour turnarount time or less.
  </Accordion>

  <Accordion title="Are different versions of the same subgraph charged separately?">
    Yes, every version of a subgraph incurs a separate worker fee and storage (in terms of entities) is also counted separately. Be sure to delete old versions of a subgraph you no longer need to query to minimize wasteful spend.
  </Accordion>
</AccordionGroup>

## Other

<Accordion title="I have a question not addressed here, can you help?">
  For help with anything else not answered on this documentation page, feel free
  to try the doc-wide search with the top-bar, and if that doesn't help you find
  what you're looking for, don't hesitate to contact our support team at
  [support@goldsky.com](mailto:support@goldsky.com).
</Accordion>


# Support
Source: https://docs.goldsky.com/getting-support

Our team is on standby to help you get the most out of our products.

## Starter + Scale

You can reach out to us any time with any questions, issues, concerns, or product ideas & feedback. Here are a few ways to do so:

* Tweet at us or send us a DM at [@goldskyio](https://x.com/goldskyio)
* Email us at [support@goldsky.com](mailto:support@goldsky.com)

For Starter plan users, we do not provide any response time estimates. For Scale plan users, we target a response time of 24-48 hours on a best-effort basis.

## Enterprise

If you an Enterprise user, you have additional options for getting help, including:

* Directly to your named Customer Success Manager via email
* Via your dedicated Slack support channel
* Via our Telegram support bot

Response times are defined on a company-by-company basis in your Support SLA.


# GitHub
Source: https://docs.goldsky.com/github-repo





# Introduction to Goldsky
Source: https://docs.goldsky.com/introduction



Goldsky is the go-to data indexer for web3 builders, offering high-performance subgraph hosting and realtime data replication pipelines.

<CodeGroup>
  ```bash Install
  curl https://goldsky.com | sh
  ```

  ```bash Migrate subgraph
  goldsky subgraph deploy <name>/<version> --from-url <thegraph-query-url>
  ```

  ```bash Deploy pipeline
  goldsky pipeline create <name>
  ```
</CodeGroup>

Goldsky offers two core self-serve products that can be used independently or in conjunction to power your data stack.

## Subgraphs

**Flexible indexing with typescript, with support for webhooks and more.**

<CardGroup cols={2}>
  <Card title="Quickstart" icon="gauge-max" href="/subgraphs/deploying-subgraphs" iconType="duotone" color="#FF9FFB">
    Get started guide to index and query on-chain data
  </Card>

  <Card title="Full docs" icon="book" href="/subgraphs/introduction" iconType="duotone" color="#FF9FFB">
    Dive into detailed documentation on Goldsky Subgraphs
  </Card>
</CardGroup>

## Mirror

**Get live blockchain data in your database or message queues with a single yaml config.**

<CardGroup cols={2}>
  <Card title="Quickstart" icon="gauge-max" href="/mirror/create-a-pipeline" iconType="duotone" color="#94FFAB">
    Get started guide to stream data into your own infrastructure
  </Card>

  <Card title="Full docs" icon="book" href="/mirror/introduction" iconType="duotone" color="#94FFAB">
    Dive into detailed documentation on Goldsky Mirror
  </Card>
</CardGroup>

Mirror can use Goldsky-hosted subgraphs as a data source, allowing you to get your data into any of our sinks without any data lag.

\--

<Snippet file="getting-help.mdx" />


# About Mirror Pipelines
Source: https://docs.goldsky.com/mirror/about-pipeline



<Note>
  We recently released v3 of pipeline configurations which uses a more intuitive
  and user friendly format to define and configure pipelines using a yaml file.
  For backward compatibility purposes, we will still support the previous v2
  format. This is why you will find references to each format in each yaml file
  presented across the documentation. Feel free to use whichever is more
  comfortable for you but we encourage you to start migrating to v3 format.
</Note>

## Overview

A Mirror Pipeline defines flow of data from `sources -> transforms  -> sinks`. It is configured in a `yaml` file which adheres to Goldsky's pipeline schema.

The core logic of the pipeline is defined in `sources`, `transforms` and `sinks` attributes.

* `sources` represent origin of the data into the pipeline.
* `transforms` represent data transformation/filter logic to be applied to either a source and/or transform in the pipeline.
* `sinks` represent destination for the source and/or transform data out of the pipeline.

Each `source` and `transform` has a unique name which is referenceable in other `transform` and/or `sink`, determining dataflow within the pipeline.

While the pipeline is configured in yaml, [goldsky pipeline CLI commands](/reference/cli#pipeline) are used to take actions on the pipeline such as: `start`, `stop`, `get`, `delete`, `monitor` etc.

Below is an example pipeline configuration which sources from `base.logs` Goldsky dataset, filters the data using `sql` and sinks to a `postgresql` table:

<Tabs>
  <Tab title="v3">
    ```yaml base-logs.yaml
    apiVersion: 3
    name: base-logs-pipeline
    resource_size: s
    sources:
      base.logs:
        dataset_name: base.logs
        version: 1.0.0
        type: dataset
        description: Enriched logs for events emitted from contracts. Contains the
          contract address, data, topics, decoded event and metadata for blocks and
          transactions.
        display_name: Logs
    transforms:
      filter_logs_by_block_number:
            sql: SELECT * FROM base.logs WHERE block_number > 5000
            primary_key: id
    sinks:
      postgres_base_logs:
        type: postgres
        table: base_logs
        schema: public
        secret_name: GOLDSKY_SECRET
        description: "Postgres sink for: base.logs"
        from: filter_logs_by_block_number
    ```

    <Info>
      Keys in v3 format for sources, transforms and sinks are user provided values. In the above example, the source reference name `base.logs` matches the actual dataset name. This is the convention that you'll typically see across examples and autogenerated configurations.
      However, you can use a custom name as the key.
    </Info>
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml base-logs.yaml
    name: base-logs-pipeline
    resource_size: s
    apiVersion: 3
    definition:
      sources:
        - referenceName: base.logs
          type: dataset
          version: 1.0.0
      transforms: []
      sinks:
        - type: postgres
          table: base_logs
          schema: public
          secretName: GOLDSKY_SECRET
          description: 'Postgres sink for: base.logs'
          sourceStreamName: base.logs
          referenceName: postgres_base_logs
    ```
  </Tab>
</Tabs>

You can find the complete Pipeline configuration schema in the [reference](/reference/config-file/pipeline) page.

## Development workflow

Similar to the software development workflow of `edit -> compile -> run`, there's an implict iterative workflow of `configure -> apply -> monitor` for developing pipelines.

1. `configure`: Create/edit the configuration yaml file.
2. `apply`: Apply the configuration aka run the pipeline.
3. `monitor`: Monitor how the pipeline behaves. This will help create insights that'll generate ideas for the first step.

Eventually, you'll end up with a configuration that works for your use case.

Creating a Pipeline configuration from scratch is challenging. However, there are tools/guides/examples that make it easier to [get started](/mirror/create-a-pipeline).

## Understanding Pipeline Runtime Lifecycle

The `status` attribute represents the desired status of the pipeline and is provided by the user. Applicable values are:

* `ACTIVE` means the user wants to start the pipeline.
* `INACTIVE` means the user wants to stop the pipeline.
* `PAUSED` means the user wants to save-progress made by the pipeline so far and stop it.

A pipeline with status `ACTIVE` has a runtime status as well. Runtime represents the execution of the pipeline. Applicable runtime status values are:

* `STARTING` means the pipeline is being setup.
* `RUNNING` means the pipeline has been setup and is processing records.
* `FAILING` means the pipeline has encountered errors that prevents it from running successfully.
* `TERMINATED` means the pipeline has failed and the execution has been terminated.

There are several [goldsky pipeline CLI commands](/reference/config-file/pipeline#pipeline-runtime-commands) that help with pipeline execution.

For now, let's see how these states play out on successful and unsuccessful scenarios.

### Successful pipeline lifecycle

In this scenario the pipeline is succesfully setup and processing data without encountering any issues.
We consider the pipeline to be in a healthy state which translates into the following statuses:

* Desired `status` in the pipeline configuration is `ACTIVE`
* Runtime Status goes from `STARTING` to `RUNNING`

<div style={{display: 'flex', justifyContent: 'center'}}>
  ```mermaid
  stateDiagram-v2
      state ACTIVE {
        [*] --> STARTING
        STARTING --> RUNNING
      }
  ```
</div>

Let's look at a simple example below where we configure a pipeline that consumes Logs from Base chain and streams them into a Postgres database:

<Tabs>
  <Tab title="v3">
    ```yaml base-logs.yaml
    name: base-logs-pipeline
    resource_size: s
    apiVersion: 3
    sources:
      base.logs:
        dataset_name: base.logs
        version: 1.0.0
        type: dataset
        description: Enriched logs for events emitted from contracts. Contains the
          contract address, data, topics, decoded event and metadata for blocks and
          transactions.
        display_name: Logs
    transforms: {}
    sinks:
      postgres_base_logs:
        type: postgres
        table: base_logs
        schema: public
        secret_name: GOLDSKY_SECRET
        description: "Postgres sink for: base.logs"
        from: base.logs
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml base-logs.yaml
    name: base-logs-pipeline
    definition:
      sources:
        - referenceName: base.logs
          type: dataset
          version: 1.0.0
      transforms: []
      sinks:
        - type: postgres
          table: base_logs
          schema: public
          secretName: GOLDSKY_SECRET
          description: 'Postgres sink for: base.logs'
          sourceStreamName: base.logs
          referenceName: postgres_base_logs
    ```
  </Tab>
</Tabs>

Let's attempt to run it using the command `goldsky pipeline apply base-logs.yaml --status ACTIVE` or `goldsky pipeline start base-logs.yaml`

```
 goldsky pipeline apply base-logs.yaml --status ACTIVE

  Successfully validated config file

  Successfully applied config to pipeline: base-logs-pipeline

To monitor the status of your pipeline:

Using the CLI: `goldsky pipeline monitor base-logs-pipeline`
Using the dashboard: https://app.goldsky.com/dashboard/pipelines/stream/base-logs-pipeline/1
```

At this point we have set the desired status to `ACTIVE`. We can confirm this using `goldsky pipeline list`:

```
 goldsky pipeline list
 Listing pipelines

 Name                           Version  Status  Resource 
                                                 Size     

 base-logs-pipeline             1        ACTIVE  s        


```

We can then check the runtime status of this pipeline using the `goldsky pipeline monitor base-logs-pipeline` command:

<img className="block mx-auto" width="450" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/working-with-pipelines/pipe-running.png" />

We can see how the pipeline starts in `STARTING` status and becomes `RUNNING` as it starts processing data successfully into our Postgres sink.
This pipeline will start processing the historical data of the source dataset, reach its edge and continue streaming data in real time until we either stop it or it encounters error(s) that interrupts it's execution.

### Unsuccessful pipeline lifecycle

Let's now consider the scenario where the pipeline encounters errors during its lifetime and ends up failing.

There can be multitude of reasons for a pipeline to encounter errors such as:

* secrets not being correctly configured
* sink availability issues
* policy rules on the sink preventing the pipeline from writing records
* resource size incompatiblity
* and many more

These failure scenarios prevents a pipeline from getting-into or staying-in a `RUNNING` runtime status.

<div style={{display: 'flex', justifyContent: 'center'}}>
  ```mermaid
  ---
  title: Healthy pipeline becomes unhealthy
  ---
  stateDiagram-v2
      state status:ACTIVE {
        [*] --> STARTING
        STARTING --> RUNNING
        RUNNING --> FAILING
        FAILING --> TERMINATED
      }
  ```

  ```mermaid
  ---
  title: Pipeline cannot start
  ---
  stateDiagram-v2
      state status:ACTIVE {
        [*] --> STARTING
        STARTING --> FAILING
        FAILING --> TERMINATED
      }
  ```
</div>

A Pipeline can be in an `ACTIVE` desired status but a `TERMINATED` runtime status in scenarios that lead to terminal failure.

Let's see an example where we'll use the same configuration as above but set a `secret_name` that does not exist.

<Tabs>
  <Tab title="v3">
    ```yaml bad-base-logs.yaml
    name: bad-base-logs-pipeline
    resource_size: s
    apiVersion: 3
    sources:
      base.logs:
        dataset_name: base.logs
        version: 1.0.0
        type: dataset
        description: Enriched logs for events emitted from contracts. Contains the
          contract address, data, topics, decoded event and metadata for blocks and
          transactions.
        display_name: Logs
    transforms: {}
    sinks:
      postgres_base_logs:
        type: postgres
        table: base_logs
        schema: public
        secret_name: YOUR_DATABASE_SECRET
        description: "Postgres sink for: base.logs"
        from: base.logs
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml bad-base-logs.yaml
    name: bad-base-logs-pipeline
    definition:
      sources:
        - referenceName: base.logs
          type: dataset
          version: 1.0.0
      transforms: []
      sinks:
        - type: postgres
          table: base_logs
          schema: public
          secretName: YOUR_DATABASE_SECRET
          description: 'Postgres sink for: base.logs'
          sourceStreamName: base.logs
          referenceName: postgres_base_logs
    ```
  </Tab>
</Tabs>

Let's start it using the command `goldsky pipeline apply bad-base-logs.yaml`.

```
 goldsky pipeline apply bad-base-logs.yaml

  Successfully validated config file

  Successfully applied config to pipeline: base-logs-pipeline

To monitor the status of your pipeline:

Using the CLI: `goldsky pipeline monitor bad-base-logs-pipeline`
Using the dashboard: https://app.goldsky.com/dashboard/pipelines/stream/bad-base-logs-pipeline/1
```

The pipeline configuration is valid, however, the pipeline runtime will encounter error since the secret that contains credentials to communicate with the sink does not exist.

Running `goldsky pipeline monitor bad-base-logs-pipeline` we see:

<img className="block mx-auto" width="450" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/working-with-pipelines/pipe-failing.png" />

As expected, the pipeline has encountered a terminal error. Please note that the desired status is still `ACTIVE` even though the pipeline runtime status is `TERMINATED`

```
 goldsky pipeline list
 Listing pipelines

 Name                           Version  Status    Resource 
                                                   Size     

 bad-base-logs-pipeline         1        ACTIVE    s        

```

## Runtime visibility

Pipeline runtime visibility is an important part of the pipeline development workflow. Mirror pipelines expose:

1. Runtime status and error messages
2. Logs emitted by the pipeline
3. Metrics on `Records received`, which counts all the records the pipeline has received from source(s) and, `Records written` which counts all records the pipeline has written to sink(s).
4. [Email notifications](/mirror/about-pipeline#email-notifications)

Runtime status, error messages and metrics can be seen via two methods:

1. Pipeline dashboard at `https://app.goldsky.com/dashboard/pipelines/stream/<pipeline_name>/<version>`
2. `goldsky pipeline monitor <name_or_path_to_config_file>` CLI command

Logs can only be seen in the pipeline dashboard.

Mirror attempts to surface appropriate and actionable error message and status for users, however, there is always room for imporovements. Please [reachout](/getting-support) if you think the experience can be improved.

### Email notifications

If a pipeline fails terminally the project members will get notified via an email.

<img className="block mx-auto" width="450" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/working-with-pipelines/pipeline-failed-email.png" />

You can configure this nofication in the [Notifications section](https://app.goldsky.com/dashboard/settings#notifications) of your project

## Error handling

There are two broad categories of errors.

**Pipeline configuration schema error**

This means the schema of the pipeline configuration is not valid. These errors are usually caught before pipeline execution. Some possible scenarios:

* a required attribute is missing
* transform SQL has syntax errors
* pipeline name is invalid

**Pipeline runtime error**

This means the pipeline encountered error during execution at runtime.

Some possible scenarios:

* credentails stored in the secret are incorrect or do not have needed access privilages
* sink availability issues
* poison-pill record that breaks the business logic in the transforms
* `resource_size` limitation

Transient errors are automatically retried as per retry-policy (for upto 6 hours) whearas non-transient ones immediately terminate the pipeline.

While many errors can be resolved by user intervention, there is a possibility of platform errors as well. Please [reachout to support](/getting-support) for investigation.

## Resource sizing

`resource_size` represents the compute (vCPUs and RAM) available to the pipeline. There are several options for pipeline sizes: `s, m, l, xl, xxl`. This attribute influences [pricing](/pricing/summary#mirror) as well.

Resource sizing depends on a few different factors such as:

* number of sources, transforms, sinks
* expected amount of data to be processed.
* transform sql involves joining multiple sources and/or transforms

Here's some general information that you can use as reference:

* A `small` resource size is usually enough in most use case: it can handle full backfill of small chain datasets and write to speeds of up to 300K records per second. For pipelines using
  subgraphs as source it can reliably handle up to 8 subgraphs.
* Larger resource sizes are usually needed when backfilling large chains or when doing large JOINS (example: JOIN between accounts and transactions datasets in Solana)
* It's recommended to always follow a defensive approach: start small and scale up if needed.

## Snapshots

A Pipeline snapshot captures a point-in-time state of a `RUNNING` pipeline allowing users to resume from it in the future.

It can be useful in various scenarios:

* evolving your `RUNNING` pipeline (eg: adding a new source, sink) without losing progress made so far.
* recover from new bug introductions where the user fix the bug and resume from an earlier snapshot to reprocess data.

Please note that snapshot only contains info about the progress made in reading the source(s) and the sql transform's state. It isn't representative of the state of the source/sink. For eg: if all data in the sink database table is deleted, resuming the pipeline from a snapshot does not recover it.

Currently, a pipeline can only be resumed from the latest available snapshot. If you need to resume from older snapshots, please [reachout to support](/getting-support)

Snapshots are closely tied to pipeline runtime in that all [commands](/reference/config-file/pipeline#pipeline-runtime-commands) that changes pipeline runtime has options to trigger a new snapshot and/or resume from the latest one.

```mermaid
%%{init: { 'gitGraph': {'mainBranchName': 'myPipeline-v1'}, 'theme': 'default' , 'themeVariables': { 'git0': '#ffbf60' }}}%%
gitGraph
    commit id: " " type: REVERSE tag:"start"
    commit id: "snapshot1"
    commit id: "snapshot2"
    commit id: "snapshot3"
    commit id: "snapshot4" tag:"stop" type: HIGHLIGHT
    branch myPipeline-v2
    commit id: "snapshot4 " type: REVERSE tag:"start"
```

### When are snapshots taken?

1. When updating a `RUNNING` pipeline, a snapshot is created before applying the update. This is to ensure that there's an up-to-date snapshot in case the update introduces issues.
2. When pausing a pipeline.
3. Automatically on regular intervals.  For `RUNNING` pipelines in healthy state, automatic snapshots are taken every 4 hours to ensure minimal data loss in case of errors.
4. Users can request snapshot creation via the following CLI command:

* `goldsky pipeline snapshot create <name_or_path_to_config>`
* `goldsky pipeline apply <name_or_path_to_config> --from-snapshot new`
* `goldsky pipeline apply <name_or_path_to_config> --save-progress true` (CLI version \< `11.0.0`)

5. Users can list all snapshots in a pipeline via the following CLI command:

* `goldsky pipeline snapshot list <name_or_path_to_config>`

### How long does it take to create a snapshot

The amount of time it takes for a snapshot to be created depends largly on two factors. First, the amount of state accumulated during pipeline execution. Second, how fast records are being processed end-end in the pipeline.

In case of a long running snapshot that was triggered as part of an update to the pipeline, any future updates are blocked until snapshot is completed. Users do have an option to cancel the update request.

There is a scenario where the the pipeline was healthy at the time of starting the snapshot however, became unhealthy later preventing snapshot creation. Here, the pipeline will attempt to recover however, may need user intervention that involves restarting from last successful snapshot.

### Scenarios and Snapshot Behavior

Happy Scenario:

* Suppose a pipeline is at 50% progress, and an automatic snapshot is taken.
* The pipeline then progresses to 60% and is in a healthy state. If you pause the pipeline at this point, a new snapshot is taken.
* You can later start the pipeline from the 60% snapshot, ensuring continuity from the last known healthy state.

Bad Scenario:

* If the pipeline reaches 50%, and an automatic snapshot is taken.
* It then progresses to 60% but enters a bad state. Attempting to pause the pipeline in this state will fail.
* If you restart the pipeline, it will resume from the last successful snapshot at 50%, there was no snapshot created at 60%

<Snippet file="getting-help.mdx" />


# Getting started
Source: https://docs.goldsky.com/mirror/create-a-pipeline

Step by step instructions on how to create a Goldsky Mirror pipeline.

<video controls className="w-full aspect-video" src="https://gssupport.s3.us-east-2.amazonaws.com/docs/Deploying+Mirror+Pipelines+-+docs.mp4" />

You have two options to create a Goldsky Mirror pipeline:

1. **[Goldsky Flow](/mirror/create-a-pipeline#goldsky-flow)**: With a guided web experience in the dashboard
2. **[CLI](/mirror/create-a-pipeline#creating-mirror-pipelines-with-the-cli)**: interactively or by providing a pipeline configuration

## Goldsky Flow

Flow allows you to deploy pipelines by simply dragging and dropping its component into a canvas. You can open up Flow by going to the [Pipelines page](https://app.goldsky.com/dashboard/pipelines) on the dashboard and clicking on the `New pipeline` button.

<img className="block mx-auto" width="300" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/0-start-flow.png" />

You'll be redirected to Goldsky Flow, which starts with an empty canvas representing the initial state.

<img src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/0-empty-state.png" />

The draggable components that will make up the pipeline are located on the left side menu.

<img className="block mx-auto" width="300" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/1-side-menu.png" />

Let's now look at how we can deploy a simple pipeline; in the following section we are going to see the steps needed to stream Ethereum raw logs into a ClickHouse database. Since the steps are the same for any pipeline, feel free to adapt the components to fit your specific use case.

1. **Select the Data Source**

Start by dragging and dropping a `Data Source` card onto the canvas. Once you do that, you'll to need select the chain you are interested in. We currently support [100+ chains](/chains/supported-networks). For this example we are going to choose `Ethereum`.

<img className="block mx-auto" width="300" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/2-chain-list.png" />

Next, we need to define the type of data source we want to use:

* Onchain datasets: these are [Direct Indexing datasets](/mirror/sources/direct-indexing) representing both raw data (e.g. Raw Blocks) as well as curated datasets (e.g. ERC-20 Transfers)
* [Subgraphs](/mirror/sources/subgraphs): this can be community subgraphs or existing subgraphs in your project for the choosen network

For this example, we are going to choose `Raw Logs`.

<img className="block mx-auto" width="300" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/3-raw-logs.png" />

After selecting the data source, you have some optional configuration fields to use, in the case of `Onchain Datasets` you can configure:

* `Start indexing at`: here you can define whether you want to do a full backfill (`Beginning`) or read from edge (`Current`)
* `Filter by contract address`: optional contract address (in lowercase) to filter from
* `Filter by topics`: optional list of topics (in lowercase) to filter from, separated by commas.
* `View Schema`: view the data schema to get a better idea of the shape of the data as well as see some sample records.

<img className="block mx-auto" width="300" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/4-raw-logs-selected.png" />

2. **(Optional) Select a Transform**

Optionally select a Transform for your data by clicking on the `+` button at the top right edge of the `Data Source` card and you'll have the option to add a Transform or a Sink.

<img className="block mx-auto" width="300" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/5-add-transform.png" />

Tranforms are optional intermediate compute processors that allow you to modify the original data (you can find more information on the support Transform types [here](/mirror/transforms/transforms-overview)). For this example, we are going to create a simple SQL transform to select a subset of the available
data in the source. To do that, select `Custom SQL`.

<img className="block mx-auto" width="300" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/6-add-transform-2.png" />

Click on the `Query` field of the card to bring up the SQL editor.

<img src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/7-add-sql-code.png" />

In this inline editor you can define the logic of your transformation and run the
SQL code at the top right corner to experiment with the data and see the result of your queries. For this example we are adding `SELECT id, block_number, transaction_hash, data FROM source_1`

If you click on the `Run` button on the top right corner you'll see a preview of the final shape of the data. Once satisfied with the results in your Transforms, press `Save` to add it to the pipeline.

3. **Select the Sink**

The last pipeline component to define is the [Sink](/mirror/sinks/supported-sinks), this is, the destination of our data. Click on the `+` button at the top right edge of the `Transform Card` and select a Sink.

<img className="block mx-auto" width="300" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/8-choose-sink.png" />

If you already have configured any sinks previously (for more information, see [Mirror Secrets](/mirror/manage-secrets)) you'll be able to
choose it from the list. Alternatively, you'll need to create a new sink by creating its corresponding secret. In our example, well use an existing sink to a ClickHouse database.

<img className="block mx-auto" width="300" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/9-clickhouse.png" />

Once you select the sink, you'll have some configuration options available to define how the data will be written into your database as well as anoter `Preview Output` button to see the what the final shape of the data will be; this is a very convenient utility
in cases where you might have multiple sources and transforms in your pipeline and you want to iterate on its logic without having to redeploy the actual pipeline every time.

4. **Confirm and deploy**

Last but not least, we need to define a name for the pipeline. You can do that at the top center input of your screen. For this example, we are going to call it `ethereum-raw-logs`
Up to this point, your canvas should look similar to this:

<img src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/11-full-canvas.png" />

Click on the `Deploy` button on the top right corner and specify the [resource size](/mirror/about-pipeline#resource-sizing); for this example you can choose the default `Small`.

<img className="block mx-auto" width="300" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/12-deploy.png" />

You should now be redirected to the pipelines details page

<img src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/13-pipe-details.png" />

Congratulations, you just deployed your first pipeline using Goldsky Flow! 

Assuming the sink is properly configured you should start seeing data flowing into your database after a few seconds.

If you would like to update the components of your pipeline and deploy a newer version (more on this topic [here](/mirror/about-pipeline))
you can click on the `Update Pipeline` button on the top right corner of the page and it will take you back onto the Flow canvas so you can do any updates on it.

<img src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/14-update-pipes.png" />

There's a couple of things about Flow worth highlighting:

* Pipelines are formally defined using [configuration files](/reference/config-file/pipeline) in YAML. Goldsky Flow abstract that complexity for us so that we can just
  create the pipeline by dragging and dropping its components. You can at any time see the current configuration definition of the pipeline by switching the view to `YAML` on the top left corner. This is quite useful
  in cases where you'd like to version control your pipeline logic and/or automate its deployment via CI/CD using the CLI (as explained in next section)

<img src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/flow/15-yaml-view.png" />

* Pipeline components are interconnected via reference names: in our example, the source has a default reference name of `source_1`; the transform (`sql_1`) reads from `source_1` in its SQL query; the sink (`sink_1`)
  reads the result from the transform (see its `Input source` value) to finally emit the data into the destination. You can modify the reference names of every component of the pipeline on the canvas, just bear in mind
  the connecting role these names play.

Read on the following sections if you would like to know how to deploy pipelines using the CLI.

## Goldsky CLI

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

There are two ways in which you can create pipelines with the CLI:

* Interactive
* Non-Interactive

### Guided CLI experience

This is a simple and guided way to create pipelines via the CLI.
Run `goldsky pipeline create <your-pipeline-name>` in your terminal and follow the prompts.

In short, the CLI guides you through the following process:

1. Select one or more source(s)
2. Depending on the selected source(s), define transforms
3. Configure one or more sink(s)

### Custom Pipeline Configuration File

This is an advanced way to create a new pipeline. Instead of using the guided CLI experience (see above), you create the pipeline configuration on your own.
A pipeline configuration is a YAML structure with the following top-level properties:

```yaml
name: <your-pipeline-name>
apiVersion: 3
sources: {}
transforms: {}
sinks: {}
```

Both `sources` and `sinks` are required with a minimum of one entry each. `transforms` is optional and an empty object (`{}`) can be used if no transforms are needed.

Full configuration details for Pipelines is available in the [reference](/reference/config-file/pipeline) page.

As an example, see below a pipeline configuration which uses the Ethereum Decoded Logs dataset as source, uses a transform to select specific data fields and sinks that data into a Postgres database whose connection details are stored within the `A_POSTGRESQL_SECRET` secret:

<Accordion title="Example pipeline configuration">
  <CodeGroup>
    ```yaml pipeline.yaml
    name: ethereum-decoded-logs
    apiVersion: 3
    sources:
      ethereum_decoded_logs:
        dataset_name: ethereum.decoded_logs
        version: 1.0.0
        type: dataset
        start_at: latest

    transforms:
      select_relevant_fields:
        sql: |
          SELECT
              id,
              address,
              event_signature,
              event_params,
              raw_log.block_number as block_number,
              raw_log.block_hash as block_hash,
              raw_log.transaction_hash as transaction_hash
          FROM
              ethereum_decoded_logs
        primary_key: id

    sinks:
      postgres:
        type: postgres
        table: eth_logs
        schema: goldsky
        secret_name: A_POSTGRESQL_SECRET
        from: select_relevant_fields
    ```
  </CodeGroup>
</Accordion>

Note that to create a pipeline from configuration that sinks to your datastore, you need to have a [secret](/mirror/manage-secrets) already configured on your Goldsky project and reference it in the sink configuration.

Run `goldsky pipeline apply <your-pipeline-config-file-path>` in your terminal to create a pipeline.

Once your pipeline is created, run `goldsky pipeline start <your_pipeline_name>` to start your pipeline.

## Monitor a pipeline

When you create a new pipeline, the CLI automatically starts to monitor the status and outputs it in a table format.

If you want to monitor an existing pipeline at a later time, use the `goldsky pipeline monitor <your-pipeline-name>` CLI command. It refreshes every ten seconds and gives you insights into how your pipeline performs.

Or you may monitor in the Pipeline Dashboard page at `https://app.goldsky.com/dashboard/pipelines/stream/<pipeline_name>/<version>` where you can see the pipeline's `status`, `logs`, `metrics`.


# CryptoHouse - Free Blockchain Analytics powered by ClickHouse and Goldsky
Source: https://docs.goldsky.com/mirror/cryptohouse



We have partnered with [Clickhouse](https://clickhouse.com/) to allow you to interact with some onchain datasets for free at [crypto.clickhouse.com](https://crypto.clickhouse.com/).

As of today, users of CryptoHouse can query Solana blocks, transactions, token\_transfers, block\_rewards, accounts, and tokens for free. Similar datasets are available for Ethereum and Base. We plan to expand the data available and expose more blockchains in the coming months!

Remember that you can see more information about each dataset in our [reference](/chains/supported-networks#mirror) and [schemas](/reference/schema/non-EVM-schemas#solana) pages.

If you want to learn more about this initiative, head to this [Clickhouse blog post](https://clickhouse.com/blog/announcing-cryptohouse-free-blockchain-analytics) for more information.


# Data Quality at Goldsky
Source: https://docs.goldsky.com/mirror/data-quality



# Mirror Indexing

Goldsky Mirror datasets are populated through various indexers, which write the data into a data stream.

The data stream is then accessible directly by users through Mirror pipelines. Internally, we copy the data to a data lake which is then used to power various features and also used for Data QA.

Data quality is managed during ingestion, and also through periodic checks.

Emitted data quality is managed through various database guarantees, depending on the destination of the data.

## Ingestion-level Consistency

### Chain Continuity

When first ingesting a block, we check for a continuous block hash chain. If the chain is not valid (ie the parent hash does not match the hash we have of the preceding block number), we issue deletes and updates into our dataset and walk backwards until we reach a consistent chain again.

All deletes and updates are propagated through to downstream sinks. This means if you have a Mirror pipeline writing chain data into a database, and that chain goes through a reorg or a rollback, **all the changes will automatically propagate to your database as well.**

### Write Guarantees

During ingestion, we ensure we have the full set of data for a block before emitting it into the various datasets. When emitting, we acquire full consistency acknowledgement from our various data sinks before marking the block as ingested.

### Schema Strictness

Our datasets follow strict typed schemas, causing writes that dont fit into said schemas to fail completely.

## Dataset Validation Checks

In rare cases, RPC nodes can give us invalid data that may be missed during ingestion checks. For every dataset, we run checks on a daily basis and repair the data if any issues are seen.

These checks validate:

1. Missing blocks (EVM) - we record the minimum and maximum block numbers for each date, and look for gaps in the data
2. Missing transactions (EVM) - We count unique transaction hashes per block and compare it with the `transaction_count` for the block.
3. Missing logs (EVM) - We compare the maximum log index per block with the number of logs per block.

This framework will allow us to proactively address data quality issues in a structured and efficient manner. Much like unit tests in a software codebase, these checks will help prevent future regressions. Once a check is implemented for one chain, it can be seamlessly applied across others, ensuring consistency and scalability.

## Destination Level Consistency

To prevent missing when writing, mirror pipelines are built with an **at least once guarantee**.

### Snapshots

We do automatic fault tolerance every min with snapshot recovery every 4 hrs. When a pipeline is updated or is forced to terminate, a snapshot is persisted, and used for the next incarnation of the pipeline. This allows for continuity of the data being sent.

### Database Consistency

For every row of data the pipeline needs to send, we make sure we have full acknowledgement from the database before moving into the next set of data to be sent. If its not acknowledged, the snapshots will not show that set of data is sent, and if any restarts or errors happen with the pipeline, the snapshot will be pessimistic and risk resending data over missing data.

### Sink Downtime Handling

If a write into a sink (database or channel) errors for whatever reason, the pipeline will automatically restart just that batch for that sink. If it continues to error, the pipeline will restart the writers. Finally, if all fails for a prolonged period of time, the pipeline will fail, and when the user restarts it, it will resume from the last saved snapshot.


# AWS S3
Source: https://docs.goldsky.com/mirror/extensions/channels/aws-s3



<Warning>
  The sub-second realtime and reorg-aware advantages of mirror are greatly
  diminished when using our S3 connector due to the constraints of file-based
  storage. If possible, it's highly recommended to use one of the other channels
  or sinks instead!
</Warning>

The files are created in [Parquet](https://parquet.apache.org/) format.

Files will be emitted on an interval, essentially mimicing a mini-batch system.

Data will also be append-only, so if there is a reorg, data with the same id will be emitted. It's up to the downstream consumers of this data to deduplicate the data.

Full configuration details for this sink is available in the [reference](/reference/config-file/pipeline#file) page.

## Secrets

Create an AWS S3 secret with the following CLI command:

```shell
goldsky secret create --name AN_AWS_S3_SECRET --value '{
  "accessKeyId": "Type.String()",
  "secretAccessKey": "Type.String()",
  "region": "Type.String()",
  "type": "s3"
}'
```


# AWS SQS
Source: https://docs.goldsky.com/mirror/extensions/channels/aws-sqs



When you need to react to every new event coming from the blockchain or subgraph, SQS can be a simple and resilient way get started. SQS works with any mirror source, including subgraph updates and on-chain events.

Mirror Pipelines will send events to an SQS queue of your choosing. You can then use AWS's SDK to process events, or even create a lambda to do serverless processing of the events.

SQS is append-only, so any events will be sent with the metadata needed to handle mutations as needed.

Full configuration details for SQS sink is available in the [reference](/reference/config-file/pipeline#sqs) page.

## Secrets

Create an AWS SQS secret with the following CLI command:

```shell
goldsky secret create --name AN_AWS_SQS_SECRET --value '{
  "accessKey": "Type.String()",
  "secretAccessKey": "Type.String()",
  "region": "Type.String()",
  "type": "sqs"
}'
```

<Note>
  Secret requires `sqs:SendMessage` permission. Refer to [AWS SQS permissions documentation](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-api-permissions-reference.html) for more information.
</Note>


# Kafka
Source: https://docs.goldsky.com/mirror/extensions/channels/kafka



[Kafka](https://kafka.apache.org/) is a distributed streaming platform that is used to build real-time data pipelines and streaming applications. It is designed to be fast, scalable, and durable.

You can use Kafka to deeply integrate into your existing data ecosystem. Goldsky supplies a message format that allows you to handle blockchain forks and reorganizations with your downstream data pipelines.

Kafka has a rich ecosystem of SDKs and connectors you can make use of to do advanced data processing.

<Warning>
  **Less Magic Here**

  The Kafka integration is less end to end - while Goldsky will handle a ton of the topic partitioning balancing and other details, using Kafka is a bit more involved compared to getting data directly mirrored into a database.
</Warning>

Full configuration details for Kafka sink is available in the [reference](/reference/config-file/pipeline#kafka) page.

## Secrets

```shell

goldsky secret create --name A_KAFKA_SECRET --value '{
  "type": "kafka",
  "bootstrapServers": "Type.String()",
  "securityProtocol": "Type.Enum(SecurityProtocol)",
  "saslMechanism": "Type.Optional(Type.Enum(SaslMechanism))",
  "saslJaasUsername": "Type.Optional(Type.String())",
  "saslJaasPassword": "Type.Optional(Type.String())",
  "schemaRegistryUrl": "Type.Optional(Type.String())",
  "schemaRegistryUsername": "Type.Optional(Type.String())",
  "schemaRegistryPassword": "Type.Optional(Type.String())"
}'
```


# Overview
Source: https://docs.goldsky.com/mirror/extensions/channels/overview

Use channels to integrate Mirror into your existing data stack.

## What are channels?

Channels are a special type of [Sink](/mirror/sinks/supported-sinks) that represent intermediate storage layers designed to absorb the Goldsky
firehose.  They aren't designed to be queryable on their own - instead, you
should plan to connect them to your existing data stack or sink that's not
currently supported by Goldsky.

<CardGroup cols={1}>
  <Card title="AWS S3" icon="aws" href="/mirror/extensions/channels/aws-s3" iconType="duotone">
    AWS S3 offers unparalleled scalability and durability for storing vast
    amounts of data.
  </Card>

  <Card title="AWS SQS" icon="aws" href="/mirror/extensions/channels/aws-sqs" iconType="duotone">
    AWS SQS provides reliable message queuing for decoupling distributed systems
    with ease.
  </Card>

  <Card title="Kafka" icon="bars-staggered" href="/mirror/extensions/channels/kafka" iconType="duotone">
    Kafka excels in handling high-throughput, real-time data streams with strong
    fault tolerance.
  </Card>
</CardGroup>

## What should I use?

### For durable storage

Goldsky supports export of raw blockchain or custom data to [AWS S3](/mirror/extensions/channels/aws-s3) or GCS, either in Iceberg format or in plain Parquet format. GCS support is currently available upon request, please reach out to us at [support@goldsky.com](mailto:support@goldsky.com).

Keep in mind that the blockchain is eventually consistent, so reorgs in append-only mode is portrayed differently.

Once data is in S3, you can use a solution like AWS Athena to query, or merge with existing spark pipelines.

### For processing events as they come

If your backend needs to process and receive data as it appears on the blockchain, you can consider using our SQS or Kafka channel sinks. These sinks are append-only, so chain reorgs are not handled for you, but you do receive all the metadata required to handle reorgs yourself.

An example architecture would have:

1. An SQS queue
2. An AWS Lambda function processing said queue

Mirror will send each event as a message into the SQS queue, and the lambda function can process it however you need through making additional enrichments, calling discord/telegram bots, or inserting into another datastore that we dont yet support.

### For more processing

In addition to AWS S3, we support direct emits to [Kafka](/mirror/extensions/channels/kafka). Kafka can store messages at scale, making it a great choice as an initial holding place for data before you do further processing.

Our team can work with many different strategies and can give guidance on how to integrate with our data format inside Kafka.  Reach out to our support team at [support@goldsky.com](mailto:support@goldsky.com) if you'd like to learn more.


# Extensions
Source: https://docs.goldsky.com/mirror/extensions/overview



Goldsky Mirror's primary function is to simplify the ingestion of blockchain data into your data warehouse for querying and analytics, providing the data as-is. **Extensions** enhance Mirror's capabilities beyond this core use case, enabling new destinations for your data and the ability to transform it.

Types of Extensions:

* Channels: Intermediate storage layers that facilitate further integration into your data stack. They handle high-throughput data streams and enable flexible data processing.
* Transforms: Tools for filtering and aggregating data in-stream, allowing you to shape the data to meet your specific needs.

By leveraging Extensions, you can customize and extend your Mirror pipelines to better fit your unique data workflows and integration requirements.

For more details on Extensions, visit our [Channels documentation](/mirror/extensions/channels/overview) and [Transforms documentation](/mirror/transforms/transforms-overview).


# Decode contract events
Source: https://docs.goldsky.com/mirror/guides/decoding-contract-events

Sync contract events to a database with the contract ABI using Mirror.

This guide explains how to decode raw contract events on-the-fly using [Mirror Decoding Functions](/reference/mirror-functions/decoding-functions) within transforms in Mirror pipelines.

## What youll need

1. A Goldky account and the CLI installed

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

1. A basic understanding of the [Mirror product](/mirror)
2. A destination sink to write your data to. In this example, we will use [PostgresSQL Sink](/mirror/sinks/postgres)

## Preface

To get decoded contract data on EVM chains in a Mirror pipeline, there are three options:

1. Decode data with a subgraph, then use a [subgraph entity source](/mirror/sources/subgraphs).
2. Use the `decoded_logs` and `decoded_traces` [direct indexing](/mirror/sources/direct-indexing) datasets. These are pre-decoded datasets, with coverage for common contracts, events, and functions.
3. Use the `raw_logs` dataset and decode inside a pipeline [transform](/reference/config-file/pipeline).

In this guide we are going to focus on the third method. We will use as example the [Friendtech contract](https://basescan.org/address/0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4) deployed in Base but the same logic applies to any other contract and chain for which there's an availabe Raw Log Direct Indexing dataset as per [this list](/mirror/sources/direct-indexing).

## Pipeline definition

<Tip>
  In the `_gs_fetch_abi` function call below, we pull from a gist. You can also pull from basescan directly with an api key. \
  \
  `_gs_fetch_abi('<basescan-link>', 'etherscan'), `
</Tip>

<Tabs>
  <Tab title="v3">
    ```yaml event-decoding-pipeline.yaml
    name: decoding-contract-events
    apiVersion: 3
    sources:
      my_base_raw_logs:
        type: dataset
        dataset_name: base.raw_logs
        version: 1.0.0
    transforms:
      friendtech_decoded:
        primary_key: id
        # Fetch the ABI from a gist (raw)
        sql: >
          SELECT 
            `id`,
            _gs_log_decode(
             	_gs_fetch_abi('https://gist.githubusercontent.com/jeffling/0320808b7f3cc0e8d9cc6c3b113e8156/raw/99bde70acecd4dc339b5a81aae39954973f5d178/gistfile1.txt', 'raw'), 
                `topics`, 
                `data`
            ) AS `decoded`, 
            block_number, 
            transaction_hash 
          FROM my_base_raw_logs
          WHERE address='0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4'
      friendtech_clean:
        primary_key: id
        # Clean up the previous transform, unnest the values from the `decoded` object. 
        sql: >
          SELECT 
            `id`, 
            decoded.event_params AS `event_params`, 
            decoded.event_signature AS `event_signature`,
            block_number,
            transaction_hash
            FROM friendtech_decoded 
            WHERE decoded IS NOT NULL
    sinks:
      friendtech_events:
        secret_name: EXAMPLE_SECRET
        type: postgres
        from: friendtech_clean
        schema: decoded_events
        table: friendtech
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml
    sources:
      - type: dataset
        referenceName: base.raw_logs
        version: 1.0.0
    transforms:
      - referenceName: friendtech_decoded
        type: sql
        primaryKey: id
        # Fetch the ABI from basescan, then use it to decode from the friendtech address.
        sql: >
          SELECT 
            `id`,
            _gs_log_decode(
                _gs_fetch_abi('https://api.basescan.org/api?module=contract&action=getabi&address=0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4', 'etherscan'), 
                `topics`, 
                `data`
            ) AS `decoded`, 
            block_number, 
            transaction_hash 
          FROM base.raw_logs
          WHERE address='0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4'
      - referenceName: friendtech_clean
        primaryKey: id
        type: sql
        # Clean up the previous transform, unnest the values from the `decoded` object.
        sql: >
          SELECT 
            `id`, 
            decoded.event_params AS `event_params`, 
            decoded.event_signature AS `event_signature`,
            block_number,
            transaction_hash
            FROM friendtech_decoded 
            WHERE decoded IS NOT NULL
    sinks:
      - referenceName: friendtech_events
        secretName: EXAMPLE_SECRET
        type: postgres
        sourceStreamName: friendtech_clean
        schema: decoded_events
        table: friendtech
    ```
  </Tab>
</Tabs>

There are two important transforms in this pipeline definition which are responsible for decoding the contract; we'll explain how they work in detail. If you copy and use this configuration file, make sure to update:

1. Your `secret_name` (v2: `secretName`). If you already [created a secret](/mirror/manage-secrets), you can find it via the [CLI command](/reference/cli#secret) `goldsky secret list`.
2. The schema and table you want the data written to, by default it writes to `decoded_events.friendtech`.

### Decoding transforms

Let's start analyzing the first transform:

```sql Transform: friendtech_decoded
SELECT 
    `id`,
    _gs_log_decode(
        _gs_fetch_abi('https://api.basescan.org/api?module=contract&action=getabi&address=0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4', 'etherscan'), 
        `topics`, 
        `data`
    ) AS `decoded`, 
    block_number, 
    transaction_hash 
    FROM base.raw_logs
    WHERE address='0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4'
```

Looking at the [Raw Logs schema](/reference/schema/EVM-schemas#raw-logs) we see there are standard log columns such as `id`, `block_number` and `transaction_hash`. Since its columns
`topics` and `data` are encoded we need to make use of the [\_gs\_log\_decode](/reference/mirror-functions/decoding-functions#gs-log-decode) to decode the data. This function takes the following parameters:

1. The contract ABI: rather than specifying ABI directly into the SQL query, which would made the code considerably less legible, we have decided to make use of the [\_gs\_fetch\_abi](/reference/mirror-functions/decoding-functions#gs_fetch_abi) function
   to fetch the ABI from the BaseScan API but you could also fetch it from an external public repository like Github Gist if you preferred.
2. `topics`: as a second argument to the decode function we pass in the name of the column in our dataset that contains the topics as comma-separated string.
3. `data`: as a third argument to the decode function we pass in the name of the column in our dataset that contains the encoded data.

   <Note>
     Some columns are surrounded by backticks, this is because they are reserved words in Flink SQL. Common columns that need backticks are: data, output, value, and a full list can be found [here](https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/sql/overview/#reserved-keywords).
   </Note>

We are storing the decoding result in a new column called `decoded` which is a [nested ROW](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/types/#row) with the properties `event_param::TEXT[]` and `event_signature::TEXT`. We create a second transform that reads from the resulting dataset of this first SELECT query to access the decoded data:

```sql Transform: friendtech_clean
SELECT 
    `id`, 
    decoded.event_params AS `event_params`, 
    decoded.event_signature AS `event_signature`,
    block_number,
    transaction_hash
    FROM friendtech_decoded 
    WHERE decoded IS NOT NULL
```

Notice how we add a filter for `decoded IS NOT NULL` as a safety measure to discard processing potential issues in the decoding phase.

## Deploying the pipeline

As a last step, to deploy this pipeline and start sinking decoded data into your database simply execute:

`goldsky pipeline apply <yaml_file>`

## Conclusion

In this guide we have explored an example implementation of how we can use [Mirror Decoding Functions](/reference/mirror-functions/decoding-functions) to decode raw contract events and stream them into our PostgreSQL database.
This same methodology can be applied to any contract of interest for any chain with `raw_log` and `raw_traces` Direct Indexing datasets available ([see list](/mirror/sources/direct-indexing)).

Goldsky also provides alternative decoding methods:

* Decoded datasets: `decoded_logs` and `decoded_traces`
* [Subgraphs entity sources](/mirror/sources/subgraphs) to your pipelines.

Decoding contracts on the flight is a very powerful way of understanding onchain data and making it usable for your users.

<Snippet file="getting-help.mdx" />


# Decode traces
Source: https://docs.goldsky.com/mirror/guides/decoding-traces

Sync traces to a database with the contract ABI using Mirror.

This guide explains how to decode traces on-the-fly using [Mirror Decoding Functions](/reference/mirror-functions/decoding-functions) within transforms in Mirror pipelines.

## What youll need

1. A Goldky account and the CLI installed

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

1. A basic understanding of the [Mirror product](/mirror)
2. A destination sink to write your data to. In this example, we will use [PostgresSQL Sink](/mirror/sinks/postgres)

## Preface

In this guide we are going to show how to decode traces of a contract with Goldsky Mirror. We will use as example the [Friendtech contract](https://basescan.org/address/0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4) deployed in Base but the same logic applies to any other contract and chain for which there's an availabe Raw Traces Direct Indexing dataset as per [this list](/mirror/sources/direct-indexing).

## Pipeline definition

<Tabs>
  <Tab title="v3">
    ```yaml traces-decoding-pipeline.yaml
    name: decoding-traces
    apiVersion: 3
    sources:
      my_base_raw_traces:
        type: dataset
        dataset_name: base.raw_traces
        version: 1.0.0
    transforms:
      friendtech_decoded:
        primary_key: id
        # Fetch the ABI from basescan, then use it to decode from the friendtech address.
        sql: >
          SELECT 
            `id`,
            _gs_tx_decode(
              	_gs_fetch_abi('https://gist.githubusercontent.com/jeffling/0320808b7f3cc0e8d9cc6c3b113e8156/raw/99bde70acecd4dc339b5a81aae39954973f5d178/gistfile1.txt', 'raw'), 
                `input`, 
                `output`
            ) AS `decoded`, 
            block_number, 
            transaction_hash 
          FROM my_base_raw_traces
          WHERE address='0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4'
      friendtech_clean:
        primary_key: id
        # Clean up the previous transform, unnest the values from the `decoded` object.
        sql: >
          SELECT 
            `id`, 
            decoded.`function` AS `function_name`,
            decoded.decoded_inputs AS `decoded_inputs`,
            decoded.decoded_outputs AS `decoded_outputs`,
            block_number,
            transaction_hash
            FROM friendtech_decoded 
            WHERE decoded IS NOT NULL
    sinks:
      friendtech_logs:
        secret_name: EXAMPLE_SECRET
        type: postgres
        from: friendtech_clean
        schema: decoded_logs
        table: friendtech
    ```
  </Tab>
</Tabs>

There are two important transforms in this pipeline definition which are responsible for decoding the contract; we'll explain how they work in detail. If you copy and use this configuration file, make sure to update:

1. Your `secret_name`. If you already [created a secret](/mirror/manage-secrets), you can find it via the [CLI command](/reference/cli#secret) `goldsky secret list`.
2. The schema and table you want the data written to, by default it writes to `decoded_logs.friendtech`.

### Decoding transforms

Let's start analyzing the first transform:

```sql Transform: friendtech_decoded
SELECT 
    `id`,
    _gs_tx_decode(
        _gs_fetch_abi('https://api.basescan.org/api?module=contract&action=getabi&address=0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4', 'etherscan'), 
        `input`, 
        `output`
    ) AS `decoded`, 
    block_number, 
    transaction_hash 
    FROM base.raw_traces
    WHERE address='0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4'
```

Looking at the [Raw Traces schema](/reference/schema/EVM-schemas#raw-traces) we see there are standard traces columns such as `id`, `block_number` and `transaction_hash`. Since the columns
`input` and `output` are encoded we need to make use of the [\_gs\_tx\_decode](/reference/mirror-functions/decoding-functions#gs-tx-decode) to decode the data. This function takes the following parameters:

1. The contract ABI: rather than specifying ABI directly into the SQL query, which would made the code considerably less legible, we have decided to make use of the [\_gs\_fetch\_abi](/reference/mirror-functions/decoding-functions#gs_fetch_abi) function
   to fetch the ABI from the BaseScan API but you could also fetch it from an external public repository like Github Gist if you preferred.
2. `input`: as a second argument which refer to the data sent along with the message call.
3. `output`: as a third argument which refer to the data returned by the message call.

   <Note />

We are storing the decoding result in a new column called `decoded` which is a [nested ROW](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/types/#row) with the properties `event_param::TEXT[]` and `event_signature::TEXT`. We create a second transform that reads from the resulting dataset of this first SELECT query to access the decoded data:

```sql Transform: friendtech_clean
SELECT 
    `id`, 
    decoded.`function` AS `function_name`,
    decoded.decoded_inputs AS `decoded_inputs`,
    decoded.decoded_outputs AS `decoded_outputs`,
    block_number,
    transaction_hash
    FROM friendtech_decoded 
    WHERE decoded IS NOT NULL
```

Notice how we add a filter for `decoded IS NOT NULL` as a safety measure to discard processing potential issues in the decoding phase.

## Deploying the pipeline

As a last step, to deploy this pipeline and start sinking decoded data into your database simply execute:

`goldsky pipeline apply <yaml file>`

## Conclusion

In this guide we have explored an example implementation of how we can use [Mirror Decoding Functions](/reference/mirror-functions/decoding-functions) to decode raw logs and stream them into our PostgreSQL database.
This same methodology can be applied to any contract of interest for any chain with `raw_log` and `raw_traces` Direct Indexing datasets available ([see list](/mirror/sources/direct-indexing)).

Goldsky also provides alternative decoding methods:

* Decoded datasets: `decoded_logs` and `decoded_traces`
* [Subgraphs entity sources](/mirror/sources/subgraphs) to your pipelines.

Decoding contracts on the flight is a very powerful way of understanding onchain data and making it usable for your users.

<Snippet file="getting-help.mdx" />


# Export contract events to Postgres
Source: https://docs.goldsky.com/mirror/guides/export-events-to-database



The `goldsky` CLI provides wizard to create pipelines. Based on the input you provide, the CLI generates a pipeline definition for you behind the scenes.

To create a new pipeline with the wizard, use the following command:

```shell
goldsky pipeline create <your-pipeline-name>
```

## What you'll need

1. An idea of the data you're interested in indexing (eg. contract address)
2. A destination sink to write your data to

## Walkthrough

<Steps>
  <Step title="Use the CLI wizard">
    In this example, we will create a pipeline that indexes Bored Ape Yacht Club contract events to a NeonDB (Postgres) database. This will include all transfers and other auxillary events associated to that address, with our ethereum decoded dataset as the source.

    Initiate the wizard:

    ```shell
    goldsky pipeline create bored-ape-transfers
    ```

    1. **Select a Data Source**: Choose *Direct Indexing*.
    2. **Choose Data Type**: Opt for *Ethereum - Decoded Logs*.
    3. **Data Processing Time**: Pick *Process data from the time this pipeline is created*.
    4. **Additional Sources**: Select *No* when asked to add more sources.
    5. **Data Filtering**: Choose *Yes, filter the indexed on-chain data*.
    6. **Contract Address**: Enter the [Bored Ape Yacht Club](https://boredapeyachtclub.com) contract address, `0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d`, when prompted.
    7. **Transformation**: Choose *No* when asked to add another transform.
    8. **Set Up Sink**: Choose *Postgres*. Remember to have a
       Postgres [Neon DB](https://neon.tech)instance to connect to.
    9. **Set Up Secret**: Connect your sink by following the prompts or selecting an existing one. This information is stored in your Goldsky account.
    10. **Choose Schema**: Choose *Yes* to select default 'public' schema or choose your preferred alternative schema.
    11. **Map Data to Sink Tables**: Select *Yes* when asked to automatically map data to sink tables. Choose *No* if you wish to customize the table name.
    12. **Additional Sinks**: Select *No* when asked to add another sink.
  </Step>

  <Step title="Monitor the pipeline">
    Upon successful completion of these steps, an active pipeline is created. Data should start appearing in your database shortly. Monitor the table that is displayed. "RUNNING" status should appear after a minute or two. To monitor progress at any time, use:

    ```shell
    goldsky pipeline monitor bored-ape-transfers
    ```
  </Step>

  <Step title="Get the pipeline's definition">
    You can get the generated pipeline definition using:

    ```shell
    goldsky pipeline get-definition bored-ape-transfers
    ```
  </Step>

  <Step title="Explore">
    For a full list of all available commands, use:

    ```shell
    goldsky pipeline --help
    ```
  </Step>
</Steps>

<Snippet file="getting-help.mdx" />


# Merging cross chain subgraphs
Source: https://docs.goldsky.com/mirror/guides/merging-crosschain-subgraphs



This pipeline is named `poap-extended-1`. It pulls data from two `subgraph_entity` sources, does not perform any transformations, and stores the result into two separate PostgreSQL sinks.

<Tabs>
  <Tab title="v3">
    ```yaml cross-chain-pipeline.yaml
    name: poap-extended-1
    apiVersion: 3
    sources:
      hashflow_cross_chain.pool_created:
        type: subgraph_entity
        name: pool_created
        subgraphs:
          - name: polymarket
            version: 1.0.0
          - name: hashflow
            version: 1.0.0
      hashflow_cross_chain.update_router_permissions:
        type: subgraph_entity
        name: update_router_permissions
        subgraphs:
          - name: polymarket
            version: 1.0.0
          - name: hashflow
            version: 1.0.0
    transforms: {}
    sinks:
      pool_created_sink:
        type: postgres
        from: hashflow_cross_chain.pool_created
        table: test_pool_created
        schema: public
        secret_name: API_POSTGRES_CREDENTIALS
      update_router_permissions_sink:
        type: postgres
        from: hashflow_cross_chain.update_router_permissions
        table: test_update_router_permissions
        schema: public
        secret_name: API_POSTGRES_CREDENTIALS
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml
      sources:
        - type: subgraphEntity
          deployments:
            - id: QmbsFSmqsWFFcbxnGedXifyeTbKBSypczRcwPrBxdQdyXE
            - id: QmNSwC6QjZSFcSm2Tmoy6Van7g6zSEqD3yz4tDWRFdZiKh
            - id: QmZUh5Rp3edMhYj3wCH58zSNvZvrPSQyeM6AN5HTmyw2Ch
          referenceName: hashflow_cross_chain.pool_created
          entity:
            name: pool_created
        - type: subgraphEntity
          deployments:
            - id: QmbsFSmqsWFFcbxnGedXifyeTbKBSypczRcwPrBxdQdyXE
            - id: QmNSwC6QjZSFcSm2Tmoy6Van7g6zSEqD3yz4tDWRFdZiKh
            - id: QmZUh5Rp3edMhYj3wCH58zSNvZvrPSQyeM6AN5HTmyw2Ch
          referenceName: hashflow_cross_chain.update_router_permissions
          entity:
            name: update_router_permissions
      transforms: []
      sinks:
        - type: postgres
          sourceStreamName: hashflow_cross_chain.pool_created
          table: test_pool_created
          schema: public
          secretName: API_POSTGRES_CREDENTIALS
          referenceName: pool_created_sink
        - type: postgres
          sourceStreamName: hashflow_cross_chain.update_router_permissions
          table: test_update_router_permissions
          schema: public
          secretName: API_POSTGRES_CREDENTIALS
          referenceName: update_router_permissions_sink
    ```
  </Tab>
</Tabs>

You can run the above example by copying the file into a local yaml file and running the following Goldsky CLI command:

```bash
goldsky pipeline apply <path_to_configuration> --status ACTIVE
```


# Operating pipelines
Source: https://docs.goldsky.com/mirror/guides/operating-pipelines

Guide to common pipeline operations

### Deploying a pipeline

There are two main ways by which you can deploy a pipeline: in the web app or by using the CLI.

<Note>
  If you prefer to deploy pipelines using a web interface instead check the [Pipeline Builder](/mirror/create-a-pipeline#creating-mirror-pipelines-with-the-pipeline-builder)
</Note>

#### `apply` command + pipeline configuration

The [goldsky pipeline apply](/reference/cli#pipeline-apply) command expects a pipeline configuration file. For example:

<Tabs>
  <Tab title="v3">
    ```yaml base-logs.yaml
    name: base-logs-pipeline
    resource_size: s
    apiVersion: 3
    sources:
      base.logs:
        dataset_name: base.logs
        version: 1.0.0
        type: dataset
        description: Enriched logs for events emitted from contracts. Contains the
          contract address, data, topics, decoded event and metadata for blocks and
          transactions.
        display_name: Logs
    transforms: {}
    sinks:
      postgres_base_logs:
        type: postgres
        table: base_logs
        schema: public
        secret_name: GOLDSKY_SECRET
        description: "Postgres sink for: base.logs"
        from: base.logs
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml base-logs.yaml
    name: base-logs-pipeline
    definition:
      sources:
        - referenceName: base.logs
          type: dataset
          version: 1.0.0
      transforms: []
      sinks:
        - type: postgres
          table: base_logs
          schema: public
          secretName: GOLDSKY_SECRET
          description: 'Postgres sink for: base.logs'
          sourceStreamName: base.logs
          referenceName: postgres_base_logs
    ```
  </Tab>
</Tabs>

Please save the configuration in a file and run `goldsky pipeline apply <path_to_config_file> --status ACTIVE` to deploy the pipeline.

### Pausing a pipeline

There are several ways by which you can pause a pipeline:

#### 1. `pause` command

`goldsky pipeline pause <name>` will attempt to take a snapshot before pausing the pipeline. The snapshot is successfully taken only if the
pipeline is in a healthy state. After snapshot completes, the pipeline desired status to `PAUSED` runtime status to `TERMINATED`.

Example:

```
> goldsky pipeline pause base-logs-pipeline
  Successfully paused pipeline: base-logs-pipeline
Pipeline paused and progress saved. You can restart it with "goldsky pipeline start base-logs-pipeline".
```

#### 2. `stop` command

You can stop a pipeline using the command `goldsky pipeline stop <name>`. Unlike the `pause` command, stopping a pipeline doesn't try to take a snapshot. Mirror will directly set pipeline to `INACTIVE` desired status and `TERMINATED` runtime status.

Example:

```
> goldsky pipeline stop base-logs-pipeline

  Pipeline stopped. You can restart it with "goldsky pipeline start base-logs-pipeline".
```

#### 3. `apply` command + `INACTIVE` or `PAUSED` status

We can replicate the behaviour of the `pause` and `stop` commands using `pipeline apply` and setting the `--status` flag to  `INACTIVE` or `PAUSED`.

Following up with our previous example, we could stop our deployed pipeline with `goldsky pipeline apply <name_or_path_to_config> --status INACTIVE`

```
goldsky pipeline apply base-logs.yaml --status INACTIVE

  Successfully validated config file

  Successfully applied config to pipeline: base-logs-pipeline
```

### Restarting a pipeline

There are two ways to restart an already deployed pipeline:

#### 1. `restart` command

As in: `goldsky pipeline restart <name> --from-snapshot last|none`

Example:

```
goldsky pipeline restart base-logs-pipeline --from-snapshot last

  Successfully restarted pipeline: base-logs-pipeline

Pipeline restarted. It's safe to exit now (press Ctrl-C). Or you can keep this terminal open to monitor the pipeline progress, it'll take a moment.

 Validating request
 Fetching pipeline
 Validating pipeline status
 Fetching runtime details

 Timestamp    Status      Total records received  Total records written  Errors 

 02:54:44 PM  STARTING                         0                      0  []                                       

```

This command will open up a monitor for your pipeline after deploying.

#### 2. `apply` command + `ACTIVE` status

Just as you can stop a pipeline changing its status to `INACTIVE` you can also restart it by setting it to `ACTIVE`

Following up with our previous example, we could restart our stopped pipeline with `goldsky pipeline apply base-logs-pipeline --status ACTIVE`

```
goldsky pipeline apply base-logs.yaml --status ACTIVE

  Successfully validated config file

  Successfully applied config to pipeline: base-logs-pipeline

To monitor the status of your pipeline:

Using the CLI: `goldsky pipeline monitor base-logs`
Using the dashboard: https://app.goldsky.com/dashboard/pipelines/stream/base-logs-pipeline/9
```

Unlike the `start` command, this method won't open up the monitor automatically.

### Applying updates to pipeline configuration

For example:

```yaml base-logs.yaml
name: base-logs-pipeline
description: a new description for my pipeline
resource_size: xxl

```

<Tabs>
  <Tab title="CLI versions > 11.0.0">
    ```
     goldsky pipeline apply base-logs.yaml --from-snapshot last
     
       Successfully validated config file
     
       Successfully applied config to pipeline: base-logs-pipeline
    ```
  </Tab>

  <Tab title="Older CLI versions">
    ```
       goldsky pipeline apply base-logs.yaml --use-latest-snapshot
       
         Successfully validated config file
       
         Successfully applied config to pipeline: base-logs-pipeline
    ```
  </Tab>
</Tabs>

In this example we are changing the pipeline `description` and `resource_size` of the pipeline using its latest succesful snapshot available and informing Mirror
to not take a snapshot before applying the update. This is a common configuration to apply in a situation where you found issues with your pipeline and would like to restart from the last
healthy checkpoint.

For a more complete reference on the configuration attributes you can apply check [this reference](/reference/config-file/pipeline).

### Deleting a pipeline

Although pipelines with desired status `INACTIVE` don't consume any resources (and thus, do not imply a billing cost on your side) it's always nice to keep your project
clean and remove pipelines which you aren't going to use any longer.
You can delete pipelines with the command `goldsky pipeline delete`:

```
> goldsky pipeline delete base-logs-pipeline

 Deleted pipeline with name: base-logs-pipeline
```

### In-flight requests

Sometimes you might experience that you are not able to perform a specific action on your pipeline because an in-flight request is currently being processed.
What this means is that there was a previous operation performed in your pipeline which hasn't finished yet and needs to be either processed or discarded before you can apply
your specific operation. A common scenario for this is your pipeline is busy taking a snapshot.

Consider the following example where we recently paused a pipeline (thus triggering a snapshot) and we immediately try to delete it:

```
> goldsky pipeline delete base-logs-pipeline
 Cannot process request, found existing request in-flight.

* To monitor run 'goldsky pipeline monitor base-logs-pipeline --update-request'
* To cancel run 'goldsky pipeline cancel-update base-logs-pipeline'
```

Let's look at what process is still to be processed:

```
> goldsky pipeline monitor base-logs-pipeline --update-request

  Monitoring update progress

  You may cancel the update request by running goldsky pipeline cancel-update base-logs-pipeline

Snapshot creation in progress:                             33%
```

We can see that the snapshot is still taking place. Since we want to delete the pipeline we can go ahead and stop this snapshot creation:

```
> goldsky pipeline cancel-update base-logs-pipeline

  Successfully cancelled the in-flight update request for pipeline base-logs-pipeline
```

We can now succesfully remove the pipeline:

```
> goldsky pipeline delete base-log-pipeline

 Deleted pipeline with name: base-logs-pipeline
```

As you saw in this example, Mirror provides you with commands to see the current in-flight requests in your pipeline and decide whether you want to discard them or wait for them to be processed.


# Stream DEX trades
Source: https://docs.goldsky.com/mirror/guides/stream-DEX-trades

Stream Decentralized Exchange trade events to your database

Welcome to our guide on using Mirror to create pipelines for decoding and streaming data from decentralized exchanges (DEX) into your datawarehouse.
Mirror is a powerful tool designed to facilitate the seamless integration of blockchain data into different [sinks](/mirror/sinks/supported-sinks), enabling you to leverage the vast amounts of information generated by DEXs for analytics, reporting, and more.

## What you'll need

1. A Goldky account and the CLI installed

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

1. A basic understanding of the [Mirror product](/mirror)
2. A destination sink to write your data to. In this example, we will use [the PostgreSQL Sink](/mirror/sinks/postgres)

## Introduction

Most of the Decentralized Exchanges these days are based entirely on the Uniswap protocol or have strong similarities with it.

<Note>
  If you need a high level overview of how Uniswap works you can check out [this reference page](https://docs.uniswap.org/contracts/v2/concepts/protocol-overview/how-uniswap-works)
</Note>

Having that in mind, we can narrow our focus on identifying events emitted by Uniswap contracts and use them to identify similar events emitted by all DEXes in the blockchain.
There are a number of different events we could track. In this guide we will track the `Swap` and `PoolCreated` events as they are arguably two of the most important events to track when wanting to make sense of trading activity in a DEX.

For this example implementation, we will choose the [Raw Log Direct Indexing](https://docs.goldsky.com/mirror/sources/direct-indexing) for the Base chain as the source of our pipeline but you could choose any other chain for which the Raw Log dataset if that is preferred.
Raw logs need to be decoded for us to be able to identify the events we want to track. For that purpose, we will use [Decoding Transform Functions](/reference/mirror-functions/decoding-functions) to dinamically fetch the ABIs of both [UniswapV3Factory](https://basescan.org/address/0x33128a8fc17869897dce68ed026d694621f6fdfd) and [UniswapV3Pool](https://basescan.org/address/0xcccc03b23cd798c06828c377466f267e59bb9739) contracts from the Basescan API since they contain
the actual definition for the PoolCreated and Swap events.

<Note>
  It's worth mentioning that Uniswap has different versions and it's possible that some event definitions might differ. In this example we'll focus on UniswapV3. Depending on the events you are interested in tracking you might want to refine this example accordingly but the principles explained will stay the same.
</Note>

Let's now see all these concepts applied in an example pipeline definition:

## Pipeline Definition

<Tabs>
  <Tab title="v3">
    ```yaml base-dex-trades.yaml
    name: base-dex-trades
    apiVersion: 3
    sources:
      base_logs:
        type: dataset
        dataset_name: base.logs
        version: 1.0.0
        filter: topics like '0x783cca1c0412dd0d695e784568c96da2e9c22ff989357a2e8b1d9b2b4e6b7118%' OR topics like '0xc42079f94a6350d7e6235f29174924f928cc2ac818eb64fed8004e115fbcca67%'
    transforms:
      factory_decoded:
        primary_key: id
        # Fetch the ABI of UniswapV3Factory in Base and use it to decode PoolCreated events
        sql: >
          SELECT 
            `id`,
            _gs_log_decode(
                _gs_fetch_abi('https://gist.githubusercontent.com/JavierTrujilloG/7df78272e689bf102cbe97ae86607d94/raw/9733aaa132a2c3e82cccbe5b0681d3270d696c83/UniswapV3Factory-ABI.json', 'raw'), 
                `topics`, 
                `data`
            ) AS `decoded`, 
            block_number, 
            transaction_hash 
          FROM base_logs  
      pool_decoded: 
        primary_key: id
        # Fetch the ABI of a UniswapV3Pool in Base and use it to decode Swap events
        sql: >
          SELECT 
            `id`,
            _gs_log_decode(
                _gs_fetch_abi('https://gist.githubusercontent.com/JavierTrujilloG/d3d2d80fbfd3415dd8e11aa498bd0909/raw/b8df8303e51ac7ad9ac921f25bfa84936bb4bc63/UniswapV3Pool-ABI.json', 'raw'), 
                `topics`, 
                `data`
            ) AS `decoded`, 
            block_number, 
            transaction_hash 
          FROM base_logs
      factory_clean:
        primary_key: id
        # Clean up the previous transform, unnest the values from the `decoded` object to get PoolCreated event data
        sql: >
          SELECT 
            `id`, 
            decoded.event_params AS `event_params`, 
            decoded.event_signature AS `event_signature`,
            block_number,
            transaction_hash
            FROM factory_decoded 
            WHERE decoded IS NOT NULL
            AND decoded.event_signature = 'PoolCreated'
      pool_clean:
        primary_key: id
        # Clean up the previous transform, unnest the values from the `decoded` object to get Swap event data
        sql: >
          SELECT 
            `id`, 
            decoded.event_params AS `event_params`, 
            decoded.event_signature AS `event_signature`,
            block_number,
            transaction_hash
            FROM pool_decoded 
            WHERE decoded IS NOT NULL
            AND decoded.event_signature = 'Swap'
    sinks:
      poolcreated_events_sink:
        secret_name: <YOUR_SECRET>
        type: postgres
        schema: decoded_events
        table: poolcreated
        from: factory_clean
      swaps_event_sink:
        secret_name: <YOUR_SECRET>
        type: postgres
        schema: decoded_events
        table: swaps
        from: pool_clean
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml base-dex-trades.yaml
    sources:
      - type: dataset
        referenceName: base.logs
        version: 1.0.0
        filter: topics like '0x783cca1c0412dd0d695e784568c96da2e9c22ff989357a2e8b1d9b2b4e6b7118%' OR topics like '0xc42079f94a6350d7e6235f29174924f928cc2ac818eb64fed8004e115fbcca67%'
    transforms:
      - referenceName: factory_decoded
        type: sql
        primaryKey: id
        # Fetch the ABI of UniswapV3Factory in Base and use it to decode PoolCreated events
        sql: >
          SELECT 
            `id`,
            _gs_log_decode(
                _gs_fetch_abi('https://gist.githubusercontent.com/JavierTrujilloG/7df78272e689bf102cbe97ae86607d94/raw/9733aaa132a2c3e82cccbe5b0681d3270d696c83/UniswapV3Factory-ABI.json', 'raw'), 
                `topics`, 
                `data`
            ) AS `decoded`, 
            block_number, 
            transaction_hash 
          FROM base.logs  
      - referenceName: pool_decoded
        type: sql
        primaryKey: id
        # Fetch the ABI of a UniswapV3Pool in Base and use it to decode Swap events
        sql: >
          SELECT 
            `id`,
            _gs_log_decode(
                _gs_fetch_abi('https://gist.githubusercontent.com/JavierTrujilloG/d3d2d80fbfd3415dd8e11aa498bd0909/raw/b8df8303e51ac7ad9ac921f25bfa84936bb4bc63/UniswapV3Pool-ABI.json', 'raw'), 
                `topics`, 
                `data`
            ) AS `decoded`, 
            block_number, 
            transaction_hash 
          FROM base.logs
      - referenceName: factory_clean
        primaryKey: id
        type: sql
        # Clean up the previous transform, unnest the values from the `decoded` object to get PoolCreated event data
        sql: >
          SELECT 
            `id`, 
            decoded.event_params AS `event_params`, 
            decoded.event_signature AS `event_signature`,
            block_number,
            transaction_hash
            FROM factory_decoded 
            WHERE decoded IS NOT NULL
            AND decoded.event_signature = 'PoolCreated'
      - referenceName: pool_clean
        primaryKey: id
        type: sql
        # Clean up the previous transform, unnest the values from the `decoded` object to get Swap event data
        sql: >
          SELECT 
            `id`, 
            decoded.event_params AS `event_params`, 
            decoded.event_signature AS `event_signature`,
            block_number,
            transaction_hash
            FROM pool_decoded 
            WHERE decoded IS NOT NULL
            AND decoded.event_signature = 'Swap'
    sinks:
      - referenceName: poolcreated_events_sink
        secretName: <YOUR_SECRET>
        type: postgres
        sourceStreamName: factory_clean
        schema: decoded_events
        table: poolcreated
      - referenceName: swaps_event_sink
        secretName: <YOUR_SECRET>
        type: postgres
        sourceStreamName: pool_clean
        schema: decoded_events
        table: swaps
    ```
  </Tab>
</Tabs>

<Note>
  If you copy and use this configuration file, make sure to update:

  1. Your `secretName`. If you already [created a secret](/mirror/manage-secrets), you can find it via the [CLI command](/reference/cli#secret) `goldsky secret list`.
  2. The schema and table you want the data written to, by default it writes to `decoded_events` schema.
</Note>

Let's deconstruct this pipeline starting at the top:

### Fast Scan Source

```sql source
sources:
  base_logs:
    type: dataset
    dataset_name: base.logs
    version: 1.0.0
    filter: topics like '0x783cca1c0412dd0d695e784568c96da2e9c22ff989357a2e8b1d9b2b4e6b7118%' OR topics like '0xc42079f94a6350d7e6235f29174924f928cc2ac818eb64fed8004e115fbcca67%'
```

We start the pipeline defining a filter on the data source. This is a new feature called Quick Ingestion by which we are able to filter on the original dataset at a much faster rate than if we were doing normal filtering with transforms. Not all datasets currently have this feature but `base_logs` does and so we want to make use of it for this example dramatically.
We add as source filters the function signatures of the PoolCreated and Swap events:

* `PoolCreated (index_topic_1 address token0, index_topic_2 address token1, index_topic_3 uint24 fee, int24 tickSpacing, address pool)` maps to `0x783cca1c0412dd0d695e784568c96da2e9c22ff989357a2e8b1d9b2b4e6b7118`
* `Swap (index_topic_1 address sender, index_topic_2 address recipient, int256 amount0, int256 amount1, uint160 sqrtPriceX96, uint128 liquidity, int24 tick)` maps to `0xc42079f94a6350d7e6235f29174924f928cc2ac818eb64fed8004e115fbcca67`

It's worth highlighting that we add a `%` to the end of each filter as `topics` contains more than just the function signature.

If the dataset we are using has the option for Quick Ingestion this filter will be pre-applied and it will speed up ingestion dramatically. Because not all datasets have this option enabled yet we'll add some redundancy on the transform by adding extra filters on these events to make sure that we are targeting these two events regardless of this feature being available or not.

Next, there are 4 transforms in this pipeline definition which we'll explain how they work. We'll start from the top:

### Decoding Transforms

```sql Transform: factory_decoded
SELECT 
        `id`,
        _gs_log_decode(
            _gs_fetch_abi('https://gist.githubusercontent.com/JavierTrujilloG/7df78272e689bf102cbe97ae86607d94/raw/9733aaa132a2c3e82cccbe5b0681d3270d696c83/UniswapV3Factory-ABI.json', 'raw'), 
            `topics`, 
            `data`
        ) AS `decoded`, 
        block_number, 
        transaction_hash 
      FROM base.logs 
```

```sql Transform: pool_decoded
SELECT 
    `id`,
    _gs_log_decode(
        _gs_fetch_abi('https://gist.githubusercontent.com/JavierTrujilloG/d3d2d80fbfd3415dd8e11aa498bd0909/raw/b8df8303e51ac7ad9ac921f25bfa84936bb4bc63/UniswapV3Pool-ABI.json', 'raw'), 
        `topics`, 
        `data`
    ) AS `decoded`, 
    block_number, 
    transaction_hash 
    FROM base.logs
```

The first two transforms fetch the ABI for UniswapV3Factory and a UniswapV3Pool to allows to decode Dex events and filter by `PoolCreated` and `Swap` events in the following transforms.
As explained in the [Decoding Contract Events guide](/mirror/guides/decoding-contract-events), we first make use of the `_gs_fetch_abi` function to get the ABIs from Basescan and pass it as first argument
to the function `_gs_log_decode` to decode its topics and data. We store the result in a `decoded` [ROW](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/types/#row) which we unnest on the next transform.

### Event Filtering Transforms

```sql Transform: factory_clean
SELECT 
    `id`, 
    decoded.event_params AS `event_params`, 
    decoded.event_signature AS `event_signature`,
    block_number,
    transaction_hash
    FROM factory_decoded 
    WHERE decoded IS NOT NULL
    AND decoded.event_signature = 'PoolCreated'
```

```sql Transform: pool_clean
SELECT 
    `id`, 
    decoded.event_params AS `event_params`, 
    decoded.event_signature AS `event_name`,
    block_number,
    transaction_hash
    FROM pool_decoded 
    WHERE decoded IS NOT NULL
    AND decoded.event_signature = 'Swap'
```

In the next two transforms we take the result of the previous encoding for each contract and filter by the PoolCreated and Swap events:

* `id`:  This is the Goldsky provided `id`, it is a string composed of the dataset name, block hash, and log index, which is unique per event, here's an example: `log_0x60eaf5a2ab37c73cf1f3bbd32fc17f2709953192b530d75aadc521111f476d6c_18`
* `decoded.event_params AS 'event_params'`: event\_params is an array containing the params associated to each event. For instance, in the case of Swap events, `event_params[1]` is the sender. You could use this for further analysis in downstream processing.
* `decoded.event_signature AS 'event_name'`: the decoder will output the event name as event\_signature, excluding its arguments.
* `WHERE decoded IS NOT NULL`: to leave out potential null results from the decoder
* `AND decoded.event_signature = 'PoolCreated'`: we use this value to filter only for 'PoolCreated' or 'Swap' events. As explained before, this filter will be redundant for datasets with Quick Ingestion enabled but we add it here in this example in case you would like to try out with a different dataset which doesn't have that option enabled.

If you would like to filter by other events like `Mint` you could easily add them to these queries; for example: `WHERE decoded.event_signature IN ('Swap', 'Mint')`

Both resulting datasets will be used as sources to two different tables at our sink: `decoded_events.poolcreated` & `decoded_events.swaps`

## Deploying the pipeline

Assuming we are using the same filename for the pipeline configuration as in this example we can deploy this pipeline with the [CLI pipeline create command](/reference/cli#pipeline-create):

`goldsky pipeline apply base-dex-trades.yaml --status ACTIVE`

Here's an example Swap record from our sink:

| id                                                                         | event\_params                                                                                                                                                                      | event\_name | block\_number | transaction\_hash                                                  |
| -------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | ------------- | ------------------------------------------------------------------ |
| log\_0x18db9278e431b3bb65c151857448227a649d9f8fe3fd0cdf2b9835eb8c71d8ae\_4 | 0x508fdf90951c1a31faa5dcd119f3b60e0e0e87fb,0x508fdf90951c1a31faa5dcd119f3b60e0e0e87fb,-256654458505550,500000000000000000,3523108129873998835611448265535,631691941157619701,75899 | Swap        | 1472162       | 0xd8a1b2c1296479f31f048aaf753e16f3d7d908fd17e6697b8850fdf209f080f6 |

We can see that it corresponds to the Swap event of this transaction:

<img className="block mx-auto" width="450" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/guides/stream-dex-trades/swap.png" />

This concludes our successful deployment of a Mirror pipeline streaming DEX Trade events from Base chain into our database using inline decoders. Congrats\\! 

## Conclusion

In this guide, we've walked through the process of using Mirror to decode and stream DEX events, specifically focusing on Swap and PoolCreated events, into a PostgreSQL database.
Along the process we have seen an example implementation of how to do inline decoding using the ABI of Factory contracts with the [Decoding Transform Functions](/reference/mirror-functions/decoding-functions)

By understanding and leveraging these events, you can harness the power of real-time blockchain data to enhance your trading strategies, optimize liquidity management, and perform detailed market analysis.
Experience the transformative power of Mirror today and redefine your approach to blockchain data integration.

<Snippet file="getting-help.mdx" />


# Sync dataset to Postgres
Source: https://docs.goldsky.com/mirror/guides/sync-dataset-to-postgres



This pipeline is named `decoded-logs-pipeline`. It pulls data from a curated goldsky dataset, without performing any transformations, and stores the result into a PostgreSQL sink, in a table called `eth_logs` in the `goldsky` schema.

<Tabs>
  <Tab title="v3">
    ```yaml decoded-logs-pipeline.yaml
    name: decoded-logs-pipeline
    apiVersion: 3
    sources:
      my_ethereum_decoded_logs:
        dataset_name: ethereum.decoded_logs
        version: 1.0.0
        type: dataset
        start_at: latest

    transforms:
      logs:
        sql: |
          SELECT
              id,
              address,
              event_signature,
              event_params,
              raw_log.block_number as block_number,
              raw_log.block_hash as block_hash,
              raw_log.transaction_hash as transaction_hash
          FROM
              my_ethereum_decoded_logs
        primary_key: id

    sinks:
      logs_sink:
        type: postgres
        table: eth_logs
        schema: goldsky
        secret_name: API_POSTGRES_CREDENTIALS
        from: logs
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml
      sources:
        - referenceName: ethereum.decoded_logs
          version: 1.0.0
          type: dataset
          startAt: latest

      transforms:
        - sql: |
            SELECT
                id,
                address,
                event_signature,
                event_params,
                raw_log.block_number as block_number,
                raw_log.block_hash as block_hash,
                raw_log.transaction_hash as transaction_hash
            FROM
                ethereum.decoded_logs
          referenceName: logs
          type: sql
          primaryKey: id

      sinks:
        - type: postgres
          table: eth_logs
          schema: goldsky
          secretName: API_POSTGRES_CREDENTIALS
          sourceStreamName: logs
          referenceName: logs_sink

    ```
  </Tab>
</Tabs>

You can start above pipeline by running:

```bash
goldsky pipeline start pipeline.yaml
```

Or

```bash
goldsky pipeline apply pipeline.yaml --status ACTIVE
```


# Sync subgraph to postgres
Source: https://docs.goldsky.com/mirror/guides/sync-subgraph-to-postgres



This pipeline pulls data from a single `subgraph_entity` source, processes the data with a single SQL transformation, and stores the result into a PostgreSQL sink.

You will need to have the existing subgraph with the name/version combo of `polymarket/1.0.0` as a prerequisite to running this pipeline.

<Tabs>
  <Tab title="v3">
    ```yaml sync-subgraphs-postgres-pipeline.yaml
    name: syncing-a-subgraph-into-postgres
    apiVersion: 3
    sources:
      polygon.fixed_product_market_maker:
        type: subgraph_entity
        name: fixed_product_market_maker
        subgraphs:
        - name: polymarket
          version: 1.0.0
    transforms:
      negative_fpmm_scaled_liquidity_parameter:
        sql: SELECT id FROM polygon.fixed_product_market_maker WHERE scaled_liquidity_parameter < 0
        primary_key: id
    sinks:
      postgres_polygon_sink:
        type: postgres
        from: negative_fpmm_scaled_liquidity_parameter
        table: test_negative_fpmm_scaled_liquidity_parameter
        schema: public
        secret_name: API_POSTGRES_CREDENTIALS
    ```
  </Tab>

  <Tab title="v2(deprecated)">
    ```yaml
    sources:
      - type: subgraphEntity
        deployments:
          - id: QmVcgRByfiFSzZfi7RZ21gkJoGKG2jeRA1DrpvCQ6ficNb
        entity:
          name: fixed_product_market_maker
        referenceName: polygon.fixed_product_market_makername
    transforms:
      - referenceName: negative_fpmm_scaled_liquidity_parameter
        type: sql
        sql: SELECT id FROM polygon.fixed_product_market_maker WHERE scaled_liquidity_parameter < 0
        primaryKey: id
    sinks:
      - type: postgres
        sourceStreamName: negative_fpmm_scaled_liquidity_parameter
        table: test_negative_fpmm_scaled_liquidity_parameter
        schema: public
        secretName: API_POSTGRES_CREDENTIALS
        referenceName: postgres_polygon_sink
    ```
  </Tab>
</Tabs>

You can start above pipeline by running:

```bash
goldsky pipeline start pipeline.yaml
```

Or

```bash
goldsky pipeline apply pipeline.yaml --status ACTIVE
```


# ERC-1155 Transfers
Source: https://docs.goldsky.com/mirror/guides/token-transfers/ERC-1155-transfers

Create a table containing ERC-1155 Transfers for several, or all token contracts.

ERC-1155 is a standard for EVM ecosystems that allows for the creation of both fungible and non-fungible assets within a single contract. The process of transferring ERC-1155 tokens into a database is fundamental, unlocking opportunities for data analysis, tracking, and the development of innovative solutions.

This guide is part of a series of tutorials on how you can stream transfer data into your datawarehouse using Mirror pipelines. Here we will be focusing on ERC-1155 Transfers, visit the following two other guides for other types of Transfers:

* [Native Transfers](/mirror/guides/token-transfers/Native-transfers)
* [ERC-20 Transfers](/mirror/guides/token-transfers/ERC-20-transfers)
* [ERC-721 Transfers](/mirror/guides/token-transfers/ERC-721-transfers)

## What you'll need

1. A Goldky account and the CLI installed

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

2. A basic understanding of the [Mirror product](/mirror)
3. A destination sink to write your data to. In this example, we will use [the PostgreSQL Sink](/mirror/sinks/postgres)

## Introduction

1. Use the readily available ERC-1155 dataset for the chain you are interested in: this is the easiest and quickest method to get you streaming token transfers into your sink of choice with minimum code.
2. Build the ERC-1155 Transfers pipeline from scratch using raw or decoded logs: this method takes more code and time to implement but it's a great way to learn about how you can use decoding functions in case you
   want to build more customized pipelines.

Let's explore both method below with more detail:

## Using the ERC-1155 Transfers Source Dataset

Every EVM chain has its own ERC-1155 dataset available for you to use as source in your pipelines. You can check this by running the `goldsky dataset list` command and finding the EVM of your choice.
For this example, let's use `apex` chain and create a simple pipeline definition using its ERC-20 dataset that writes the data into a PostgreSQL instance:

```yaml apex-erc1155-transfers.yaml
name: apex-erc1155-pipeline
resource_size: s
apiVersion: 3
sources:
  apex.erc1155_transfers:
    dataset_name: apex.erc1155_transfers
    version: 1.3.0
    type: dataset
    start_at: earliest
transforms: {}
sinks:
  postgres_apex.erc1155_transfers_public_apex_erc1155_transfers:
    type: postgres
    table: apex_erc1155_transfers
    schema: public
    secret_name: <YOUR_SECRET>
    description: "Postgres sink for Dataset: apex.erc1155_transfers"
    from: apex.erc1155_transfers
```

<Note>
  If you copy and use this configuration file, make sure to update:

  1. Your `secretName`. If you already [created a secret](/mirror/manage-secrets), you can find it via the [CLI command](/reference/cli#secret) `goldsky secret list`.
  2. The schema and table you want the data written to, by default it writes to `public.apex_erc1155_transfers`.
</Note>

<Warning>
  If you are use ClickHouse as a sink for this dataset you'll need to add the following schema\_override to avoid potential data precision errors for big numbers, see example:

  ```
  postgres_apex.erc1155_transfers_public_apex_erc1155_transfers:
      type: clickhouse
      table: apex_erc1155_transfers
      secret_name: <YOUR_CLICKHOUSE_SECRET>
      description: "ClickHouse sink for Dataset: apex.erc1155_transfers"
      schema_override:
        amount: UInt256
        token_id: UInt256
      from: apex.erc1155_transfers
  ```
</Warning>

You can start the pipeline by running:

```bash
goldsky pipeline start apex-erc1155-pipeline.yaml
```

Or

```bash
goldsky pipeline apply apex-erc1155-pipeline.yaml --status ACTIVE
```

That's it! You should soon start seeing ERC-20 token transfers in your database.

## Building ERC-1155 Transfers from scratch using logs

In the previous method we just explored, the ERC-20 datasets that we used as source to the pipeline encapsulates all the decoding logic that's explained in this section.
Read on if you are interested in learning how it's implemented in case you want to consider extending or modifying this logic yourself.

There are two ways that we can go about building these token transfers pipeline from scratch:

1. Use the `raw_logs` Direct Indexing dataset for that chain in combination with [Decoding Transform Functions](/reference/mirror-functions/decoding-functions) using the ABI of a specific ERC-1155 Contract.
2. Use the `decoded_logs` Direct Indexing dataset for that chain in which the decoding process has already been done by Goldsky. This is only available for certain chains as you can check in [this list](/mirror/sources/direct-indexing).

We'll primarily focus on the first decoding method using `raw_logs` and decoding functions as it's the default and most used way of decoding;  we'll also present an example using `decoded_logs` and highlight the differences between the two.

### Building ERC-1155 Tranfers using Decoding Transform Functions

In this example, we will stream all the `Transfer` events of all the ERC-1155 tokens for the [Scroll chain](https://scroll.io/). To that end, we will dinamically fetch the ABI of the [Rubyscore\_Scroll](https://scrollscan.com/token/0xdc3d8318fbaec2de49281843f5bba22e78338146) token from the Scrollscan API (available [here](https://api.scrollscan.com/api?module=contract\&action=getabi\&address=0xdc3d8318fbaec2de49281843f5bba22e78338146))
and use it to identify all the same events for the tokens in the chain. We have decided to use the ABI of this token for this example but any other ERC-1155 compliant token would also work.

ERC-1155 combines the features of ERC-20 and ERC-721 contracts and adds a few features.
Each transfer has both a token ID and a value representing the quantity being transfered for funglible tokens, the number `1` for tokens intended to represent NFTs, but how these work depends on how the contract is implemented.

ERC-1155 also introduces new event signatures for transfers: `TransferSingle(address,address,address,uint256,uint256)` and `TransferBatch(address,address,address,uint256[],uint256[])` which lets the contract transfer multiple tokens at once to a single recipient.
This causes us some trouble since we want one row per transfer in our database, so we'll need some extra SQL logic in our pipeline to deal with this. To mitigate this complexity we have created two different transforms, each dealing with Single and Batch transfers separately.
We then aggregate both tables into a single view using a third transform.

Let's now see all these concepts applied in an example pipeline definition:

#### Pipeline Definition

<Tabs>
  <Tab title="v3">
    ```yaml scroll-erc1155-transfers.yaml
    name: scroll-erc1155-transfers
    apiVersion: 3
    sources:
      my_scroll_mainnet_raw_logs:
        type: dataset
        dataset_name: scroll_mainnet.raw_logs
        version: 1.0.0
    transforms:
      scroll_decoded:
        primary_key: id
        # Fetch the ABI from scrollscan for Rubyscore_Scroll token
        sql: >
          SELECT
            *,
            _gs_log_decode(
                _gs_fetch_abi('https://api.scrollscan.com/api?module=contract&action=getabi&address=0xdc3d8318fbaec2de49281843f5bba22e78338146', 'etherscan'),
                `topics`,
                `data`
            ) AS `decoded`
            FROM my_scroll_mainnet_raw_logs
      scroll_clean:
        primary_key: id
        # Clean up the previous transform, unnest the values from the `decoded` object
        sql: >
          SELECT
            *,
            decoded.event_params AS `event_params`,
            decoded.event_signature AS `event_name`
            FROM scroll_decoded
            WHERE decoded IS NOT NULL
      erc1155_transfer_single:
        primary_key: id
        sql: >
          SELECT
            id,
            address AS contract_address,
            lower(event_params[2]) AS sender,
            lower(event_params[3]) AS recipient,
            COALESCE(TRY_CAST(event_params[4] AS DECIMAL(78)), 0) AS token_id,
            COALESCE(TRY_CAST(event_params[5] AS DECIMAL(78)), 0) AS amount,
            block_number,
            block_hash,
            log_index,
            transaction_hash,
            transaction_index
            FROM scroll_clean WHERE topics LIKE '0xc3d58168c5ae7397731d063d5bbf3d657854427343f4c083240f7aacaa2d0f62%'
      erc1155_transfer_batch:
        primary_key: id
        sql: >
          WITH transfer_batch_logs AS (
            SELECT
              *,
              _gs_split_string_by(
                REPLACE(TRIM(LEADING '[' FROM TRIM(TRAILING ']' FROM event_params[4])), ',', ' ')
              ) AS token_ids,
              _gs_split_string_by(
                REPLACE(TRIM(LEADING '[' FROM TRIM(TRAILING ']' FROM event_params[5])), ',', ' ')
              ) AS amounts
            FROM
              scroll_clean
            WHERE topics LIKE '0x4a39dc06d4c0dbc64b70af90fd698a233a518aa5d07e595d983b8c0526c8f7fb%'
            )
          SELECT
            id || '_' || CAST(t.idx AS STRING) AS `id`,
            address AS contract_address,
            lower(event_params[2]) AS sender,
            lower(event_params[3]) AS recipient,
            COALESCE(TRY_CAST(token_ids[t.idx] AS DECIMAL(78)),0) AS token_id,
            COALESCE(TRY_CAST(amounts[t.idx] AS DECIMAL(78)),0) AS amount,
            block_number,
            block_hash,
            log_index,
            transaction_hash,
            transaction_index
            FROM transfer_batch_logs
              CROSS JOIN UNNEST(
                CAST(
                  _gs_generate_series(
                    CAST(1 AS BIGINT),
                    CAST(COALESCE(CARDINALITY(token_ids), 0) AS BIGINT)
                ) AS ARRAY<INTEGER>
              )
              ) AS t (idx)
      scroll_1155_transfers:
        primary_key: id
        sql: >
          SELECT * FROM erc1155_transfer_single
          UNION ALL
          SELECT * FROM erc1155_transfer_batch WHERE amount > 0

    sinks:
      scroll_1155_sink:
        type: postgres
        table: erc1155_transfers
        schema: mirror
        secret_name: <YOUR_SECRET>
        description: Postgres sink for ERC1155 transfers
        from: scroll_1155_transfers
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml scroll-erc1155-transfers.yaml
    sources:
      - type: dataset
        referenceName: scroll_mainnet.raw_logs
        version: 1.0.0
    transforms:
      - referenceName: scroll_decoded
        type: sql
        primaryKey: id
        # Fetch the ABI from scrollscan for Rubyscore_Scroll token
        sql: >
          SELECT
            *,
            _gs_log_decode(
                _gs_fetch_abi('https://api.scrollscan.com/api?module=contract&action=getabi&address=0xdc3d8318fbaec2de49281843f5bba22e78338146', 'etherscan'),
                `topics`,
                `data`
            ) AS `decoded`
            FROM scroll_mainnet.raw_logs

      - referenceName: scroll_clean
        primaryKey: id
        type: sql
        # Clean up the previous transform, unnest the values from the `decoded` object
        sql: >
          SELECT
            *,
            decoded.event_params AS `event_params`,
            decoded.event_signature AS `event_name`
            FROM scroll_decoded
            WHERE decoded IS NOT NULL

      - referenceName: erc1155_transfer_single
        primaryKey: id
        type: sql
        sql: >
          SELECT
            id,
            address AS contract_address,
            lower(event_params[2]) AS sender,
            lower(event_params[3]) AS recipient,
            COALESCE(TRY_CAST(event_params[4] AS NUMERIC), -999) AS token_id,
            COALESCE(TRY_CAST(event_params[5] AS NUMERIC), -999) AS amount,
            block_number,
            block_hash,
            log_index,
            transaction_hash,
            transaction_index
            FROM scroll_clean WHERE topics LIKE '0xc3d58168c5ae7397731d063d5bbf3d657854427343f4c083240f7aacaa2d0f62%'
      
      - referenceName: erc1155_transfer_batch
        primaryKey: id
        type: sql
        sql: >
          WITH transfer_batch_logs AS (
            SELECT
              *,
              _gs_split_string_by(
                REPLACE(TRIM(LEADING '[' FROM TRIM(TRAILING ']' FROM event_params[4])), ',', ' ')
              ) AS token_ids,
              _gs_split_string_by(
                REPLACE(TRIM(LEADING '[' FROM TRIM(TRAILING ']' FROM event_params[5])), ',', ' ')
              ) AS amounts
            FROM
              scroll_clean
            WHERE topics LIKE '0x4a39dc06d4c0dbc64b70af90fd698a233a518aa5d07e595d983b8c0526c8f7fb%'
            )
          SELECT
            id || '_' || CAST(t.idx AS STRING) AS `id`,
            address AS contract_address,
            lower(event_params[2]) AS sender,
            lower(event_params[3]) AS recipient,
            CAST(token_ids[t.idx] AS NUMERIC(78)) as token_id,
            CAST(amounts[t.idx] AS NUMERIC(78)) as amount,
            block_number,
            block_hash,
            log_index,
            transaction_hash,
            transaction_index
            FROM transfer_batch_logs
              CROSS JOIN UNNEST(
                CAST(
                  _gs_generate_series(
                    CAST(1 AS BIGINT),
                    CAST(COALESCE(CARDINALITY(token_ids), 0) AS BIGINT)
                ) AS ARRAY<INTEGER>
              )
              ) AS t (idx)

      - type: sql
        referenceName: scroll_1155_transfers
        primaryKey: id
        sql: >
          SELECT * FROM erc1155_transfer_single
          UNION ALL
          SELECT * FROM erc1155_transfer_batch WHERE amount > 0

    sinks:
      - type: postgres
        table: erc1155_transfers
        schema: mirror
        secretName: <YOUR_SECRET>
        description: Postgres sink for ERC1155 transfers
        referenceName: scroll_1155_sink
        sourceStreamName: scroll_1155_transfers
    ```
  </Tab>
</Tabs>

<Note>
  If you copy and use this configuration file, make sure to update:

  1. Your `secretName`. If you already [created a secret](/mirror/manage-secrets), you can find it via the [CLI command](/reference/cli#secret) `goldsky secret list`.
  2. The schema and table you want the data written to, by default it writes to `mirror.erc1155_transfers`.
</Note>

There are 5 transforms in this pipeline definition which we'll explain how they work. We'll start from the top:

##### Decoding Transforms

```sql Transform: scroll_decoded 
SELECT
  *,
  _gs_log_decode(
      _gs_fetch_abi('https://api.scrollscan.com/api?module=contract&action=getabi&address=0xc7d86908ccf644db7c69437d5852cedbc1ad3f69', 'etherscan'),
      `topics`,
      `data`
  ) AS `decoded`
  FROM scroll_mainnet.raw_logs
```

As explained in the [Decoding Contract Events guide](/mirror/guides/decoding-contract-events) we first make use of the `_gs_fetch_abi` function to get the ABI from Scrollscan and pass it as first argument
to the function `_gs_log_decode` to decode its topics and data. We store the result in a `decoded` [ROW](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/types/#row) which we unnest on the next transform.

```sql Transform: scroll_clean 
SELECT
  *,
  decoded.event_params AS `event_params`,
  decoded.event_signature AS `event_name`
  FROM scroll_decoded
  WHERE decoded IS NOT NULL
```

In this second transform, we take the `event_params` and `event_signature` from the result of the decoding. We then filter the query on `decoded IS NOT NULL` to leave out potential null results from the decoder.

#### SingleTransfer Transform

```sql Transform: erc1155_transfer_single
SELECT
  id,
  address AS contract_address,
  lower(event_params[2]) AS sender,
  lower(event_params[3]) AS recipient,
  COALESCE(TRY_CAST(event_params[4] AS DECIMAL(78)), 0) AS token_id,
  COALESCE(TRY_CAST(event_params[5] AS DECIMAL(78)), 0) AS amount,
  block_number,
  block_hash,
  log_index,
  transaction_hash,
  transaction_index
  FROM scroll_clean WHERE topics LIKE '0xc3d58168c5ae7397731d063d5bbf3d657854427343f4c083240f7aacaa2d0f62%'
```

In this transform we focus on SingleTransfer events.

Similar to the [ERC-721 example](/mirror/guides/token-transfers/ERC-721-transfers), we use `event_params` we pull out the sender, recipient and token ID, note the indexes we use are different since ERC-1155 tokens have a different `event_signature`.

```sql
COALESCE(TRY_CAST(event_params[4] AS DECIMAL(78)), 0) AS token_id,
COALESCE(TRY_CAST(event_params[5] AS DECIMAL(78)), 0) AS amount,
```

1. `event_params[4]` is the fourth element of the `event_params` array, and for ERC-1155 this is the token ID.
2. `TRY_CAST(event_params[4] AS NUMERIC)` is casting the string element `event_params[4]` to `NUMERIC` - token IDs can be as large as an unsigned 256 bit integer, so make sure your database can handle that, if not, you can cast it to a different data type that your sink can handle. We use `TRY_CAST` because it will prevent the pipeline from failing in case the cast fails returning a `NULL` value instead.
3. `COALESCE(TRY_CAST(event_params[4] AS DECIMAL(78)), 0)`: `COALESCE` can take an arbitrary number of arguments and returns the first non-NULL value. Since `TRY_CAST` can return a `NULL` we're returning `0` in case it does. This isn't strictly necessary but is useful to do in case you want to find offending values that were unable to be cast.

We repeat this process for `event_params[5]` which represents the amount of a token.

```sql
AND raw_log.topics LIKE '0xc3d58168c5ae7397731d063d5bbf3d657854427343f4c083240f7aacaa2d0f62%'
```

We filter for a specific topic to get ERC-1155 single transfers, the above topic is for the `event_signature` `TransferSingle(address,address,address,uint256,uint256)`. As with [ERC-721](/mirror/guides/token-transfers/ERC-721-transfers), we could use the event signature as a filter instead.

##### BatchTransfers Transform

Now, let's look at the BatchTransfer events:

```Transform: erc1155_transfer_single (subquery)
WITH transfer_batch_logs AS (
    SELECT
      *,
      _gs_split_string_by(
        REPLACE(TRIM(LEADING '[' FROM TRIM(TRAILING ']' FROM event_params[4])), ',', ' ')
      ) AS token_ids,
      _gs_split_string_by(
        REPLACE(TRIM(LEADING '[' FROM TRIM(TRAILING ']' FROM event_params[5])), ',', ' ')
      ) AS amounts
    FROM
      ethereum.decoded_logs
    WHERE scroll_clean LIKE '0x4a39dc06d4c0dbc64b70af90fd698a233a518aa5d07e595d983b8c0526c8f7fb%'
    )
```

The first thing we want to achieve is to decompose the string representation of tokens and their respective amounts into separate rows that we
can add as columns to each transaction. This will allow us to index on tokenId-amount pairs much more easily as a second step.

This is the trickiest part of the transformation and involves some functionality that is niche to both Goldsky and Flink v1.17.
Well start from the inside and work our way out again.

1. `TRIM(LEADING '[' FROM TRIM(TRAILING ']' FROM event_params[4]))`: Similar to the [ERC-721 example](/mirror/guides/token-transfers/ERC-721-transfers),
   we use `event_params` to access the token\_id information. For ERC-1155, the string for batch transfers in element 4 looks like this when decoded: \[1 2 3 4 5 6]. We need to trim the leading and trailing \[ and ] characters before splitting it out into individual token IDs.
2. `_gs_split_string_by(...)`: This is a Goldsky UDF which splits strings by the space character only. If you need to split by another character, for now you can use `REGEXP_REPLACE(column, ',', ' ')` to replace commas with spaces.
3. `CROSS JOIN UNNEST ... AS token_ids (token_id)`: This works like `UNNEST` in most other SQL dialects, but is a special case in Flink. It may be confusing that we have two separate CROSS JOINs, but they don't work like CROSS JOIN in other SQL dialects, we'll get a single row with a `token_id` and `token_value` that map correctly to each other.

Lastly, we filter on topic:

```sql
raw_log.topics LIKE '0x4a39dc06d4c0dbc64b70af90fd698a233a518aa5d07e595d983b8c0526c8f7fb%'
```

This is the same as the other topic filters but it is using the topic hash of the batch transfer event signature.

Next, onto creating an index for each tokenId - amount pair:

```Transform: erc1155_transfer_single (time series)
FROM transfer_batch_logs
    CROSS JOIN UNNEST(
      CAST(
        _gs_generate_series(
          CAST(1 AS BIGINT),
          CAST(COALESCE(CARDINALITY(token_ids), 0) AS BIGINT)
      ) AS ARRAY<INTEGER>
    ) 
    ) AS t (idx)
```

In this step we generate a series of indexes that we can use to access each individual tokenId - amount pair within a transfer.
We do this by definining a Goldsky UDF called `_gs_generate_series` which will generate an array of indexes for as many tokens there are in the batch.
We combine this indexes with our existing table and use to access each token - amount pair:

```
CAST(token_ids[t.idx] AS NUMERIC(78)) as token_id,
CAST(amounts[t.idx] AS NUMERIC(78)) as amount,
```

We also use this logic to generate the resulting ID Primary Key for batch transfers:

```sql
id || '_' || CAST(t.idx AS STRING) AS `id`
```

The `id` coming from the source represents an entire batch transfer event, which can contain multiple tokens, so we concatenate the token\_id to the `id` to make the unnested rows unique.

#### Combining Single and Batch Transfers

```Transform: scroll_1155_transfers
SELECT * FROM erc1155_transfer_single
UNION ALL
SELECT * FROM erc1155_transfer_batch
WHERE amount > 0
```

This final directive in the third transform creates a combined stream of all single transfers and batch transfers.

### Deploying the pipeline

Our last step is to deploy this pipeline and start sinking ERC-1155 transfer data into our database. Assuming we are using the same file name for the pipeline configuration as in this example,
we can use the [CLI pipeline create command](/reference/cli#pipeline-create) like this:

`goldsky pipeline create scroll-erc1155-transfers --definition-path scroll-erc1155-transfers.yaml`

After some time, you should see the pipeline start streaming Transfer data into your sink.

<Note>
  Remember that you can always speed up the streaming process by [updating](/reference/cli#pipeline-update) the resourceSize of the pipeline
</Note>

Here's an example transfer record from our sink:

| id                                                                         | contract\_address                          | sender                                     | recipient                                  | token\_id | event\_name | block\_number | block\_hash                                                        | log\_index | transaction\_hash                                                  | transaction\_index |
| -------------------------------------------------------------------------- | ------------------------------------------ | ------------------------------------------ | ------------------------------------------ | --------- | ----------- | ------------- | ------------------------------------------------------------------ | ---------- | ------------------------------------------------------------------ | ------------------ |
| log\_0x360fcd6ca8c684039c45642d748735645fac639099d8a89ec57ad2b274407c25\_7 | 0x7de37842bcf314c83afe83a8dab87f85ca3a2cee | 0x0000000000000000000000000000000000000000 | 0x16f6aff7a2d84b802b2ddf0f0aed49033b69f4f9 | 6         | 1           | 105651        | 0x360fcd6ca8c684039c45642d748735645fac639099d8a89ec57ad2b274407c25 | 7          | 0x5907ba72e32434938f45539b2792e4eacf0d141db7c4c101e207c1fb26c99274 | 5                  |

We can find this [transaction in Scrollscan](https://scrollscan.com/tx/0x0a968c797271d18420261d22f1cef08b040c45b6dd219c9a53f76c1545e592ce). We see that it corresponds to a mint of 60 tokens:

<img className="block mx-auto" width="450" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/guides/token-transfers/erc1155-transfer.png" />

This concludes our successful deployment of a Mirror pipeline streaming ERC-1155 Tokens from Scroll chain into our database using inline decoders. Congrats! 

### ERC-1155 Transfers using decoded datasets

As explained in the Introduction, Goldsky provides decoded datasets for Raw Logs and Raw Traces for a number of different chains. You can check [this list](/mirror/sources/direct-indexing) to see if the chain you are interested in has these decoded datasets.
In these cases, there is no need for us to run Decoding Transform Functions as the dataset itself will already contain the event signature and event params decoded.

Click on the button below to see an example pipeline definition for streaming ERC-1155 tokens on the Ethereum chain using the `decoded_logs` dataset.

<Accordion title="Decoded Logs Pipeline Definition">
  ```yaml ethereum-decoded-logs-erc1155-transfers.yaml
  sources:
    - referenceName: ethereum.decoded_logs
      version: 1.0.0
      type: dataset
      startAt: earliest
      description: Decoded logs for events emitted from contracts. Contains the
        decoded event signature and event parameters, contract address, data,
        topics, and metadata for the block and transaction.
  transforms:
    - type: sql
      referenceName: erc1155_transfer_single
      primaryKey: id
      sql: >-
        SELECT
            address AS contract_address,
            lower(event_params[2]) AS sender,
            lower(event_params[3]) AS recipient,
            COALESCE(TRY_CAST(event_params[4] AS DECIMAL(78)), 0) AS token_id,
            COALESCE(TRY_CAST(event_params[5] AS DECIMAL(78)), 0) AS amount,
            raw_log.block_number       AS block_number,
            raw_log.block_hash         AS block_hash,
            raw_log.log_index          AS log_index,
            raw_log.transaction_hash   AS transaction_hash,
            raw_log.transaction_index  AS transaction_index,
            id
            FROM ethereum.decoded_logs WHERE raw_log.topics LIKE '0xc3d58168c5ae7397731d063d5bbf3d657854427343f4c083240f7aacaa2d0f62%'
            AND address = '0xc36cf0cfcb5d905b8b513860db0cfe63f6cf9f5c'
    - type: sql
      referenceName: erc1155_transfer_batch
      primaryKey: id
      description: ERC1155 Transform
      sql: >-
        WITH transfer_batch_logs AS (
          SELECT
            *,
            _gs_split_string_by(
              REPLACE(TRIM(LEADING '[' FROM TRIM(TRAILING ']' FROM event_params[4])), ',', ' ')
            ) AS token_ids,
            _gs_split_string_by(
              REPLACE(TRIM(LEADING '[' FROM TRIM(TRAILING ']' FROM event_params[5])), ',', ' ')
            ) AS amounts
          FROM
            ethereum.decoded_logs
          WHERE raw_log.topics LIKE '0x4a39dc06d4c0dbc64b70af90fd698a233a518aa5d07e595d983b8c0526c8f7fb%'
          AND address = '0xc36cf0cfcb5d905b8b513860db0cfe63f6cf9f5c'
          )
        SELECT
          address AS contract_address,
          lower(event_params[2]) AS sender,
          lower(event_params[3]) AS recipient,
          CAST(token_ids[t.idx] AS NUMERIC(78)) as token_id,
          CAST(amounts[t.idx] AS NUMERIC(78)) as amount,
          raw_log.block_number       AS block_number,
          raw_log.block_hash         AS block_hash,
          raw_log.log_index          AS log_index,
          raw_log.transaction_hash   AS transaction_hash,
          raw_log.transaction_index  AS transaction_index,
          id || '_' || CAST(t.idx AS STRING) AS `id`
          FROM transfer_batch_logs
            CROSS JOIN UNNEST(
              CAST(
                _gs_generate_series(
                  CAST(1 AS BIGINT),
                  CAST(COALESCE(CARDINALITY(token_ids), 0) AS BIGINT)
              ) AS ARRAY<INTEGER>
            )
            ) AS t (idx)
    - type: sql
      referenceName: ethereum_1155_transfers
      primaryKey: id
      sql: SELECT * FROM erc1155_transfer_single UNION ALL SELECT * FROM
        erc1155_transfer_batch WHERE amount > 0
  sinks:
    - type: postgres
      table: erc1155_transfers
      schema: mirror
      secretName: <YOUR_SECRET>
      description: Postgres sink for 1155 transfers
      referenceName: transfers
      sourceStreamName: ethereum_1155_transfers
  ```

  <Note>
    If you copy and use this configuration file, make sure to update:

    1. Your `secretName`. If you already [created a secret](/mirror/manage-secrets), you can find it via the [CLI command](/reference/cli#secret) `goldsky secret list`.
    2. The schema and table you want the data written to, by default it writes to `mirror.erc1155_transfers`.
  </Note>
</Accordion>

You can appreciate that it's pretty similar to the inline decoding pipeline method but here we simply create a transform which does the filtering based on the `raw_log.topics` just as we did on the previous method.

Assuming we are using the same filename for the pipeline configuration as in this example we can deploy this pipeline with the [CLI pipeline create command](/reference/cli#pipeline-create):

`goldsky pipeline create ethereum-erc1155-transfers --definition-path ethereum-decoded-logs-erc1155-transfers.yaml`

## Conclusion

In this guide, we have learnt how Mirror simplifies streaming ERC-1155 Transfer events into your database.

We have first looked into the easy way of achieving this, simply by making use of the readily available ERC-1155 dataset of the EVM chaina and using its as the source to our pipeline.

We have deep dived into the standard decoding method using Decoding Transform Functions, implementing an example on Scroll chain.
We have also looked into an example implementation using the decoded\_logs dataset for Ethereum. Both are great decoding methods and depending on your use case and dataset availability you might prefer one over the other.

With Mirror, developers gain flexibility and efficiency in integrating blockchain data, opening up new possibilities for applications and insights. Experience the transformative power of Mirror today and redefine your approach to blockchain data integration.

<Snippet file="getting-help.mdx" />


# ERC-20 Transfers
Source: https://docs.goldsky.com/mirror/guides/token-transfers/ERC-20-transfers

Create a table containing ERC-20 Transfers for several or all token contracts

ERC-20 tokens provide a standardized format for fungible digital assets within EVM ecosystems. The process of transferring ERC-20 tokens into a database is fundamental, unlocking opportunities for data analysis, tracking, and the development of innovative solutions.

This guide is part of a series of tutorials on how you can stream transfer data into your datawarehouse using Mirror pipelines. Here we will be focusing on ERC-20 Transfers, visit the following guides for other types of Transfers:

* [Native Transfers](/mirror/guides/token-transfers/Native-transfers)
* [ERC-721 Transfers](/mirror/guides/token-transfers/ERC-721-transfers)
* [ERC-1155 Transfers](/mirror/guides/token-transfers/ERC-1155-transfers)

## What you'll need

1. A Goldky account and the CLI installed

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

2. A basic understanding of the [Mirror product](/mirror)
3. A destination sink to write your data to. In this example, we will use [the PostgreSQL Sink](/mirror/sinks/postgres)

## Introduction

In order to stream all the ERC-20 Transfers of a chain there are two potential methods available:

1. Use the readily available ERC-20 dataset for the chain you are interested in: this is the easiest and quickest method to get you streaming token transfers into your sink of choice with minimum code.
2. Build the ERC-20 Transfers pipeline from scratch using raw or decoded logs: this method takes more code and time to implement but it's a great way to learn about how you can use decoding functions in case you
   want to build more customized pipelines.

Let's explore both method below with more detail:

## Using the ERC-20 Transfers Source Dataset

Every EVM chain has its own ERC-20 dataset available for you to use as source in your pipelines. You can check this by running the `goldsky dataset list` command and finding the EVM of your choice.
For this example, let's use `apex` chain and create a simple pipeline definition using its ERC-20 dataset that writes the data into a PostgreSQL instance:

```yaml apex-erc20-transfers.yaml
name: apex-erc20-pipeline
resource_size: s
apiVersion: 3
sources:
  apex.erc20_transfers:
    dataset_name: apex.erc20_transfers
    version: 1.0.0
    type: dataset
    start_at: earliest
transforms: {}
sinks:
  postgres_apex.erc20_transfers_public_apex_erc20_transfers:
    type: postgres
    table: apex_erc20_transfers
    schema: public
    secret_name: <YOUR_SECRET>
    description: 'Postgres sink for Dataset: apex.erc20_transfers'
    from: apex.erc20_transfers
```

<Note>
  If you copy and use this configuration file, make sure to update:

  1. Your `secretName`. If you already [created a secret](/mirror/manage-secrets), you can find it via the [CLI command](/reference/cli#secret) `goldsky secret list`.
  2. The schema and table you want the data written to, by default it writes to `public.apex_erc20_transfers`.
</Note>

You can start above pipeline by running:

```bash
goldsky pipeline start apex-erc20-pipeline.yaml
```

Or

```bash
goldsky pipeline apply apex-erc20-pipeline.yaml --status ACTIVE
```

That's it! You should soon start seeing ERC-20 token transfers in your database.

## Building ERC-20 Transfers from scratch using logs

In the previous method we just explored, the ERC-20 datasets that we used as source to the pipeline encapsulates all the decoding logic that's explained in this section.
Read on if you are interested in learning how it's implemented in case you want to consider extending or modifying this logic yourself.

There are two ways that we can go about building this token transfers pipeline from scratch:

1. Use the `raw_logs` Direct Indexing dataset for that chain in combination with [Decoding Transform Functions](/reference/mirror-functions/decoding-functions) using the ABI of a specific ERC-20 Contract.
2. Use the `decoded_logs` Direct Indexing dataset for that chain in which the decoding process has already been done by Goldsky. This is only available for certain chains as you can check in [this list](/mirror/sources/direct-indexing).

We'll primarily focus on the first decoding method using `raw_logs` and decoding functions as it's the default and most used way of decoding;  we'll also present an example using `decoded_logs` and highlight the differences between the two.

### Building ERC-20 Tranfers using Decoding Transform Functions

In this example, we will stream all the `Transfer` events of all the ERC-20 Tokens for the [Scroll chain](https://scroll.io/). To that end, we will dinamically fetch the ABI of the USDT token from the Scrollscan API (available [here](https://api.scrollscan.com/api?module=contract\&action=getabi\&address=0xc7d86908ccf644db7c69437d5852cedbc1ad3f69))
and use it to identify all the same events for the tokens in the chain. We have decided to use the ABI of USDT token contract for this example but any other ERC-20 compliant token would also work.

We need to differentiate ERC-20 token transfers from ERC-721 (NFT) transfers since they have the same event signature in decoded data: `Transfer(address,address,uint256)`.
However, if we look closely at their event definitions we can appreciate that the number of topics differ:

* [ERC-20](https://ethereum.org/en/developers/docs/standards/tokens/erc-20/): `event Transfer(address indexed _from, address indexed _to, uint256 _value)`
* [ERC-721](https://ethereum.org/en/developers/docs/standards/tokens/erc-721/): `event Transfer(address indexed _from, address indexed _to, uint256 indexed _tokenId)`

ERC-20 Transfer events have three topics (one topic for event signature + 2 topics for the indexed params).
NFTs on the other hand have four topics as they have one more indexed param in the event signature.
We will use this as a filter in our pipeline transform to only transfer ERC-20 Transfer events.

Let's now see all these concepts applied in an example pipeline definition:

#### Pipeline Definition

<Tabs>
  <Tab title="v3">
    ```yaml scroll-erc20-transfers.yaml
    name: scroll-erc20-transfers
    apiVersion: 3
    sources:
      my_scroll_mainnet_raw_logs:
        type: dataset
        dataset_name: scroll_mainnet.raw_logs
        version: 1.0.0
    transforms:
      scroll_decoded:
        primary_key: id
        # Fetch the ABI from scrollscan for USDT
        sql: >
          SELECT
            *,
            _gs_log_decode(
                _gs_fetch_abi('https://api.scrollscan.com/api?module=contract&action=getabi&address=0xc7d86908ccf644db7c69437d5852cedbc1ad3f69', 'etherscan'),
                `topics`,
                `data`
            ) AS `decoded`
            FROM my_scroll_mainnet_raw_logs
            WHERE topics LIKE '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef%'
            AND SPLIT_INDEX(topics, ',', 3) IS NULL
      scroll_clean:
        primary_key: id
        # Clean up the previous transform, unnest the values from the `decoded` object
        sql: >
          SELECT
            *,
            decoded.event_params AS `event_params`,
            decoded.event_signature AS `event_name`
            FROM scroll_decoded
            WHERE decoded IS NOT NULL
            AND decoded.event_signature = 'Transfer'
      scroll_20_transfers:
        primary_key: id
        sql: >
          SELECT
            id,
            address AS token_id,
            lower(event_params[1]) AS sender,
            lower(event_params[2]) AS recipient,
            lower(event_params[3]) AS `value`,
            event_name,
            block_number,
            block_hash,
            log_index,
            transaction_hash,
            transaction_index
            FROM scroll_clean
    sinks:
      scroll_20_sink:
        type: postgres
        table: erc20_transfers
        schema: mirror
        secret_name: <YOUR_SECRET>
        description: Postgres sink for ERC20 transfers
        from: scroll_20_transfers
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml scroll-erc20-transfers.yaml
    sources:
      - type: dataset
        referenceName: scroll_mainnet.raw_logs
        version: 1.0.0
    transforms:
      - referenceName: scroll_decoded
        type: sql
        primaryKey: id
        # Fetch the ABI from scrollscan for USDT
        sql: >
          SELECT
            *,
            _gs_log_decode(
                _gs_fetch_abi('https://api.scrollscan.com/api?module=contract&action=getabi&address=0xc7d86908ccf644db7c69437d5852cedbc1ad3f69', 'etherscan'),
                `topics`,
                `data`
            ) AS `decoded`
            WHERE topics LIKE '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef%'
            AND SPLIT_INDEX(topics, ',', 3) IS NULL
            FROM scroll_mainnet.raw_logs

      - referenceName: scroll_clean
        primaryKey: id
        type: sql
        # Clean up the previous transform, unnest the values from the `decoded` object
        sql: >
          SELECT
            *,
            decoded.event_params AS `event_params`,
            decoded.event_signature AS `event_name`
            FROM scroll_decoded
            WHERE decoded IS NOT NULL
            AND decoded.event_signature = 'Transfer'

      - referenceName: scroll_20_transfers
        primaryKey: id
        type: sql
        sql: >
          SELECT
            id,
            address AS token_id,
            lower(event_params[1]) AS sender,
            lower(event_params[2]) AS recipient,
            lower(event_params[3]) AS `value`,
            event_name,
            block_number,
            block_hash,
            log_index,
            transaction_hash,
            transaction_index
            FROM scroll_clean
    sinks:
      - type: postgres
        table: erc20_transfers
        schema: mirror
        secretName: <YOUR_SECRET>
        description: Postgres sink for ERC20 transfers
        referenceName: scroll_20_sink
        sourceStreamName: scroll_20_transfers
    ```
  </Tab>
</Tabs>

<Note>
  If you copy and use this configuration file, make sure to update:

  1. Your `secretName`. If you already [created a secret](/mirror/manage-secrets), you can find it via the [CLI command](/reference/cli#secret) `goldsky secret list`.
  2. The schema and table you want the data written to, by default it writes to `mirror.erc20_transfers`.
</Note>

There are three transforms in this pipeline definition which we'll explain how they work:

```sql Transform: scroll_decoded 
SELECT
  *,
  _gs_log_decode(
      _gs_fetch_abi('https://api.scrollscan.com/api?module=contract&action=getabi&address=0xc7d86908ccf644db7c69437d5852cedbc1ad3f69', 'etherscan'),
      `topics`,
      `data`
  ) AS `decoded`
  FROM scroll_mainnet.raw_logs
  WHERE topics LIKE '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef%'
  AND SPLIT_INDEX(topics, ',', 3) IS NULL
```

As explained in the [Decoding Contract Events guide](/mirror/guides/decoding-contract-events) we first make use of the `_gs_fetch_abi` function to get the ABI from Scrollscan and pass it as first argument
to the function `_gs_log_decode` to decode its topics and data. We store the result in a `decoded` [ROW](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/types/#row) which we unnest on the next transform.
We also limit the decoding to the relevent events using the topic filter and `SPLIT_INDEX` to only include ERC-20 transfers.

* `topics LIKE '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef%'`: `topics` is a comma separated string. Each value in the string is a hash. The first is the hash of the full event\_signature (including arguments), in our case `Transfer(address,address,uint256)` for ERC-20, which is hashed to `0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef`. We use `LIKE` to only consider the first signature, with a `%` at the end, which acts as a wildcard.
* `SPLIT_INDEX(topics, ',', 3) IS NULL`: as mentioned in the introduction, ERC-20 transfers share the same `event_signature` as ERC-721 transfers. The difference between them is the number of topics associated with the event. ERC-721 transfers have four topics, and ERC-20 transfers have three.

```sql Transform: scroll_clean 
SELECT
  *,
  decoded.event_params AS `event_params`,
  decoded.event_signature AS `event_name`
  FROM scroll_decoded
  WHERE decoded IS NOT NULL
  AND decoded.event_signature = 'Transfer'
```

In this second transform, we take the `event_params` and `event_signature` from the result of the decoding. We then filter the query on:

* `decoded IS NOT NULL`: to leave out potential null results from the decoder
* `decoded.event_signature = 'Transfer'`: the decoder will output the event name as event\_signature, excluding its arguments. We use it to filter only for Transfer events.

```sql Transform: scroll_20_transfers 
SELECT
  id,
  address AS token_id,
  lower(event_params[1]) AS sender,
  lower(event_params[2]) AS recipient,
  lower(event_params[3]) AS `value`,
  event_name,
  block_number,
  block_hash,
  log_index,
  transaction_hash,
  transaction_index
  FROM scroll_clean
```

In this last transform we are essentially selecting all the Transfer information we are interested in having in our database.
We've included a number of columns that you may or may not need, the main columns needed for most purposes are: `id`, `address` (if you are syncing multiple contract addresses), `sender`, `recipient`, `token_id`, and `value`.

* `id`: This is the Goldsky provided `id`, it is a string composed of the dataset name, block hash, and log index, which is unique per event, here's an example: `log_0x60eaf5a2ab37c73cf1f3bbd32fc17f2709953192b530d75aadc521111f476d6c_18`
* `address AS token_id`: We use the `lower` function here to lower-case the address to make using this data simpler downstream, we also rename the column to `token_id` to make it more explicit.
* `lower(event_params[1]) AS sender`: Here we continue to lower-case values for consistency. In this case we're using the first element of the `event_params` array (using a 1-based index), and renaming it to `sender`. Each event parameter maps to an argument to the `event_signature`.
* `lower(event_params[2]) AS recipient`: Like the previous column, we're pulling the second element in the `event_params` array and renaming it to `recipient`.
* `lower(event_params[3]) AS value`: We're pulling the third element in the `event_params` array and renaming it to `value` to represent the amount of the token\_id sent in the transfer.

Lastly, we are also adding more block metadata to the query to add context to each transaction:

```
event_name,
block_number,
block_hash,
log_index,
transaction_hash,
transaction_index
```

It's worth mentioning that in this example we are interested in all the ERC-20 Transfer events but if you would like to filter for specific contract addresses you could simply add a `WHERE` filter to this query with address you are interested in, like: `WHERE address IN ('0xBC4CA0EdA7647A8aB7C2061c2E118A18a936f13D', '0xdac17f958d2ee523a2206206994597c13d831ec7')`

#### Deploying the pipeline

Our last step is to deploy this pipeline and start sinking ERC-20 transfer data into our database. Assuming we are using the same file name for the pipeline configuration as in this example,
we can use the [CLI pipeline create command](/reference/cli#pipeline-create) like this:

`goldsky pipeline create scroll-erc20-transfers --definition-path scroll-erc20-transfers.yaml`

After some time, you should see the pipeline start streaming Transfer data into your sink.

<Note>
  Remember that you can always speed up the streaming process by [updating](/reference/cli#pipeline-update) the resourceSize of the pipeline
</Note>

Here's an example transfer record from our sink:

| id                                                                         | token\_id                                  | sender                                     | recipient                                  | value             | event\_name | block\_number | block\_hash                                                        | log\_index | transaction\_hash                                                  | transaction\_index |
| -------------------------------------------------------------------------- | ------------------------------------------ | ------------------------------------------ | ------------------------------------------ | ----------------- | ----------- | ------------- | ------------------------------------------------------------------ | ---------- | ------------------------------------------------------------------ | ------------------ |
| log\_0x666622ad5c04eb5a335364d9268e24c64d67d005949570061d6c150271b0da12\_2 | 0x5300000000000000000000000000000000000004 | 0xefeb222f8046aaa032c56290416c3192111c0085 | 0x8c5c4595df2b398a16aa39105b07518466db1e5e | 22000000000000006 | Transfer    | 5136          | 0x666622ad5c04eb5a335364d9268e24c64d67d005949570061d6c150271b0da12 | 2          | 0x63097d8bd16e34caacfa812d7b608c29eb9dd261f1b334aa4cfc31a2dab2f271 | 0                  |

We can find this [transaction in Scrollscan](https://scrollscan.com/tx/0x63097d8bd16e34caacfa812d7b608c29eb9dd261f1b334aa4cfc31a2dab2f271). We see that it corresponds to the second internal transfers of Wrapped ETH (WETH):

<img className="block mx-auto" width="450" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/guides/token-transfers/erc20-transaction.png" />

This concludes our successful deployment of a Mirror pipeline streaming ERC-20 Tokens from Scroll chain into our database using inline decoders. Congrats! 

### ERC-20 Transfers using decoded datasets

As explained in the Introduction, Goldsky provides decoded datasets for Raw Logs and Raw Traces for a number of different chains. You can check [this list](/mirror/sources/direct-indexing) to see if the chain you are interested in has these decoded datasets.
In these cases, there is no need for us to run Decoding Transform Functions as the dataset itself will already contain the event signature and event params decoded.

Click on the button below to see an example pipeline definition for streaming ERC-20 tokens on the Ethereum chain using the `decoded_logs` dataset.

<Accordion title="Decoded Logs Pipeline Definition">
  ```yaml ethereum-decoded-logs-erc20-transfers.yaml
  sources:
    - referenceName: ethereum.decoded_logs
      version: 1.0.0
      type: dataset
      startAt: earliest
      description: Decoded logs for events emitted from contracts. Contains the
        decoded event signature and event parameters, contract address, data,
        topics, and metadata for the block and transaction.
  transforms:
    - type: sql
      referenceName: ethereum_20_transfers
      primaryKey: id
      description: ERC20 Transfers
      sql: >-
        SELECT
                address AS token_id,
                lower(event_params[1]) AS sender,
                lower(event_params[2]) AS recipient,
                lower(event_params[3]) AS `value`,
                raw_log.block_number       AS block_number,
                raw_log.block_hash         AS block_hash,
                raw_log.log_index          AS log_index,
                raw_log.transaction_hash   AS transaction_hash,
                raw_log.transaction_index  AS transaction_index,
                id
                FROM ethereum.decoded_logs WHERE raw_log.topics LIKE '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef%'
                AND SPLIT_INDEX(raw_log.topics, ',', 3) IS NULL
  sinks:
    - type: postgres
      table: erc20_transfers
      schema: mirror
      secretName: <YOUR SECRET>
      description: Postgres sink for ERC20 transfers
      referenceName: ethereum_20_sink
      sourceStreamName: ethereum_20_transfers
  ```

  <Note>
    If you copy and use this configuration file, make sure to update:

    1. Your `secretName`. If you already [created a secret](/mirror/manage-secrets), you can find it via the [CLI command](/reference/cli#secret) `goldsky secret list`.
    2. The schema and table you want the data written to, by default it writes to `mirror.erc20_transfers`.
  </Note>
</Accordion>

You can appreciate that it's pretty similar to the inline decoding pipeline method but here we simply create a transform which does the filtering based on the `raw_log.topics` just as we did on the previous method.

Assuming we are using the same filename for the pipeline configuration as in this example and that you have added your own [secret](/mirror/manage-secrets)
we can deploy this pipeline with the [CLI pipeline create command](/reference/cli#pipeline-create):

`goldsky pipeline create ethereum-erc20-transfers --definition-path ethereum-decoded-logs-erc20-transfers.yaml`

## Conclusion

In this guide, we have learnt how Mirror simplifies streaming ERC-20 Transfer events into your database.

We have first looked into the easy way of achieving this, simply by making use of the readily available ERC-20 dataset of the EVM chaina and using its as the source to our pipeline.

Next, we have deep dived into the standard decoding method using Decoding Transform Functions, implementing an example on Scroll chain.
We have also looked into an example implementation using the decoded\_logs dataset for Ethereum. Both are great decoding methods and depending on your use case and dataset availability you might prefer one over the other.

With Mirror, developers gain flexibility and efficiency in integrating blockchain data, opening up new possibilities for applications and insights. Experience the transformative power of Mirror today and redefine your approach to blockchain data integration.

<Snippet file="getting-help.mdx" />


# ERC-721 Transfers
Source: https://docs.goldsky.com/mirror/guides/token-transfers/ERC-721-transfers

Create a table containing ERC-721 Transfers for several or all token contracts

ERC-721 tokens, also known as NFTs, provide a standardized format for non-fungible digital assets within EVM ecosystems. The process of transferring ERC-721 tokens into a database is fundamental, unlocking opportunities for data analysis, tracking, and the development of innovative solutions.

This guide is part of a series of tutorials on how you can stream transfer data into your datawarehouse using Mirror pipelines. Here we will be focusing on ERC-721 Transfers, visit the following two other guides for other types of Transfers:

* [Native Transfers](/mirror/guides/token-transfers/Native-transfers)
* [ERC-20 Transfers](/mirror/guides/token-transfers/ERC-20-transfers)
* [ERC-1155 Transfers](/mirror/guides/token-transfers/ERC-1155-transfers)

## What you'll need

1. A Goldky account and the CLI installed

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

2. A basic understanding of the [Mirror product](/mirror)
3. A destination sink to write your data to. In this example, we will use [the PostgreSQL Sink](/mirror/sinks/postgres)

## Introduction

In order to stream all the ERC-721 Transfers of a chain there are two potential methods available:

1. Use the readily available ERC-20 dataset for the chain you are interested in: this is the easiest and quickest method to get you streaming token transfers into your sink of choice with minimum code.
2. Build the ERC-20 Transfers pipeline from scratch using raw or decoded logs: this method takes more code and time to implement but it's a great way to learn about how you can use decoding functions in case you
   want to build more customized pipelines.

Let's explore both method below with more detail:

## Using the ERC-20 Transfers Source Dataset

Every EVM chain has its own ERC-20 dataset available for you to use as source in your pipelines. You can check this by running the `goldsky dataset list` command and finding the EVM of your choice.
For this example, let's use `apex` chain and create a simple pipeline definition using its ERC-20 dataset that writes the data into a PostgreSQL instance:

```yaml apex-erc721-tokens.yaml
name: apex-erc721-pipeline
resource_size: s
apiVersion: 3
sources:
  apex.erc721_transfers:
    dataset_name: apex.erc721_transfers
    version: 1.0.0
    type: dataset
    start_at: earliest
transforms: {}
sinks:
  postgres_apex.erc721_transfers_public_apex_erc721_transfers:
    type: postgres
    table: apex_erc721_transfers
    schema: public
    secret_name: <YOUR_SECRET>
    description: 'Postgres sink for Dataset: apex.erc721_transfers'
    from: apex.erc721_transfers
```

<Note>
  If you copy and use this configuration file, make sure to update:

  1. Your `secretName`. If you already [created a secret](/mirror/manage-secrets), you can find it via the [CLI command](/reference/cli#secret) `goldsky secret list`.
  2. The schema and table you want the data written to, by default it writes to `public.apex_erc721_transfers`.
</Note>

You can start the pipeline by running:

```bash
goldsky pipeline start apex-erc721-pipeline.yaml
```

Or

```bash
goldsky pipeline apply apex-erc721-pipeline.yaml --status ACTIVE
```

That's it! You should soon start seeing ERC-721 token transfers in your database.

## Building ERC-721 Transfers from scratch using logs

In the previous method we just explored, the ERC-721 datasets that we used as source to the pipeline encapsulates all the decoding logic that's explained in this section.
Read on if you are interested in learning how it's implemented in case you want to consider extending or modifying this logic yourself.

There are two ways that we can go about building this token transfers pipeline from scratch:

1. Use the `raw_logs` Direct Indexing dataset for that chain in combination with [Decoding Transform Functions](/reference/mirror-functions/decoding-functions) using the ABI of a specific ERC-721 Contract.
2. Use the `decoded_logs` Direct Indexing dataset for that chain in which the decoding process has already been done by Goldsky. This is only available for certain chains as you can check in [this list](/mirror/sources/direct-indexing).

We'll primarily focus on the first decoding method using `raw_logs` and decoding functions as it's the default and most used way of decoding;  we'll also present an example using `decoded_logs` and highlight the differences between the two.

### Building ERC-721 Tranfers using Decoding Transform Functions

In this example, we will stream all the `Transfer` events of all the ERC-721 tokens for the [Scroll chain](https://scroll.io/). To that end, we will dinamically fetch the ABI of the [Cosmic Surprise](https://scrollscan.com/token/0xcf7f37b4916ac5c530c863f8c8bb26ec1e8d2ccb) token from the Scrollscan API (available [here](https://api.scrollscan.com/api?module=contract\&action=getabi\&address=0xcf7f37b4916ac5c530c863f8c8bb26ec1e8d2ccb))
and use it to identify all the same events for the tokens in the chain. We have decided to use the ABI of this NFT contract for this example but any other ERC-721 compliant token would also work.

We need to differentiate ERC-20 token transfers from ERC-721 (NFT) transfers since they have the same event signature in decoded data: `Transfer(address,address,uint256)`.
However, if we look closely at their event definitions we can appreciate that the number of topics differ:

* [ERC-20](https://ethereum.org/en/developers/docs/standards/tokens/erc-20/): `event Transfer(address indexed _from, address indexed _to, uint256 _value)`
* [ERC-721](https://ethereum.org/en/developers/docs/standards/tokens/erc-721/): `event Transfer(address indexed _from, address indexed _to, uint256 indexed _tokenId)`

ERC-20 Transfer events have three topics (one topic for event signature + 2 topics for the indexed params).
NFTs on the other hand have four topics as they have one more indexed param in the event signature.
We will use this as a filter in our pipeline transform to only index ERC-721 Transfer events.

Let's now see all these concepts applied in an example pipeline definition:

#### Pipeline Definition

<Tabs>
  <Tab title="v3">
    ```yaml scroll-erc721-transfers.yaml
    name: scroll-erc721-transfers
    apiVersion: 3
    sources:
      my_scroll_mainnet_raw_logs:
        type: dataset
        dataset_name: scroll_mainnet.raw_logs
        version: 1.0.0
    transforms:
      scroll_decoded:
        primary_key: id
        # Fetch the ABI from scrollscan for Cosmic Surprise token
        sql: >
          SELECT
            *,
            _gs_log_decode(
                _gs_fetch_abi('https://api.scrollscan.com/api?module=contract&action=getabi&address=0xcf7f37b4916ac5c530c863f8c8bb26ec1e8d2ccb', 'etherscan'),
                `topics`,
                `data`
            ) AS `decoded`
            WHERE topics LIKE '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef%'
            AND SPLIT_INDEX(topics, ',', 3) IS NOT NULL
            FROM my_scroll_mainnet_raw_logs
      scroll_clean:
        primary_key: id
        # Clean up the previous transform, unnest the values from the `decoded` object
        sql: >
          SELECT
            *,
            decoded.event_params AS `event_params`,
            decoded.event_signature AS `event_name`
            FROM scroll_decoded
            WHERE decoded IS NOT NULL
            AND decoded.event_signature = 'Transfer'
      scroll_721_transfers:
        primary_key: id
        sql: >
          SELECT
            id,
            address AS contract_address,
            lower(event_params[1]) AS sender,
            lower(event_params[2]) AS recipient,
            COALESCE(TRY_CAST(event_params[3] AS NUMERIC), -999) AS token_id,
            event_name,
            block_number,
            block_hash,
            log_index,
            transaction_hash,
            transaction_index
            FROM scroll_clean
    sinks:
      scroll_721_sink:
        type: postgres
        table: erc721_transfers
        schema: mirror
        secret_name: <YOUR SECRET>
        description: Postgres sink for ERC721 transfers
        from: scroll_721_transfers
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml scroll-erc721-transfers.yaml
    sources:
      - type: dataset
        referenceName: scroll_mainnet.raw_logs
        version: 1.0.0
    transforms:
      - referenceName: scroll_decoded
        type: sql
        primaryKey: id
        # Fetch the ABI from scrollscan for Cosmic Surprise token
        sql: >
          SELECT
            *,
            _gs_log_decode(
                _gs_fetch_abi('https://api.scrollscan.com/api?module=contract&action=getabi&address=0xcf7f37b4916ac5c530c863f8c8bb26ec1e8d2ccb', 'etherscan'),
                `topics`,
                `data`
            ) AS `decoded`
            FROM scroll_mainnet.raw_logs
            WHERE topics LIKE '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef%'
            AND SPLIT_INDEX(topics, ',', 3) IS NOT NULL

      - referenceName: scroll_clean
        primaryKey: id
        type: sql
        # Clean up the previous transform, unnest the values from the `decoded` object
        sql: >
          SELECT
            *,
            decoded.event_params AS `event_params`,
            decoded.event_signature AS `event_name`
            FROM scroll_decoded
            WHERE decoded IS NOT NULL
            AND decoded.event_signature = 'Transfer'

      - referenceName: scroll_721_transfers
        primaryKey: id
        type: sql
        sql: >
          SELECT
            id,
            address AS contract_address,
            lower(event_params[1]) AS sender,
            lower(event_params[2]) AS recipient,
            COALESCE(TRY_CAST(event_params[3] AS NUMERIC), -999) AS token_id,
            event_name,
            block_number,
            block_hash,
            log_index,
            transaction_hash,
            transaction_index
            FROM scroll_clean
    sinks:
      - type: postgres
        table: erc721_transfers
        schema: mirror
        secretName: <YOUR SECRET>
        description: Postgres sink for ERC721 transfers
        referenceName: scroll_721_sink
        sourceStreamName: scroll_721_transfers
    ```
  </Tab>
</Tabs>

<Note>
  If you copy and use this configuration file, make sure to update:

  1. Your `secretName`. If you already [created a secret](/mirror/manage-secrets), you can find it via the [CLI command](/reference/cli#secret) `goldsky secret list`.
  2. The schema and table you want the data written to, by default it writes to `mirror.erc721_transfers`.
</Note>

There are 3 transforms in this pipeline definition which we'll explain how they work:

```sql Transform: scroll_decoded 
SELECT
  *,
  _gs_log_decode(
      _gs_fetch_abi('https://api.scrollscan.com/api?module=contract&action=getabi&address=0xc7d86908ccf644db7c69437d5852cedbc1ad3f69', 'etherscan'),
      `topics`,
      `data`
  ) AS `decoded`
  FROM scroll_mainnet.raw_logs
  WHERE topics LIKE '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef%'
  AND SPLIT_INDEX(topics, ',', 3) IS NOT NULL
```

As explained in the [Decoding Contract Events guide](/mirror/guides/decoding-contract-events) we first make use of the `_gs_fetch_abi` function to get the ABI from Scrollscan and pass it as first argument
to the function `_gs_log_decode` to decode its topics and data. We store the result in a `decoded` [ROW](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/types/#row) which we unnest on the next transform.

We include the topic and `SPLIT_INDEX` filters here to limit decoding only to the relevant events.

* `topics LIKE '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef%'`: `topics` is a comma separated string. Each value in the string is a hash. The first is the hash of the full event\_signature (including arguments), in our case `Transfer(address,address,uint256)` for ERC-721, which is hashed to `0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef`. We use `LIKE` to only consider the first signature, with a `%` at the end, which acts as a wildcard.
* `SPLIT_INDEX(topics, ',', 3) IS NOT NULL`: as mentioned in the introduction, ERC-20 transfers share the same `event_signature` as ERC-721 transfers. The difference between them is the number of topics associated with the event. ERC-721 transfers have four topics, and ERC-20 transfers have three.

```sql Transform: scroll_clean 
SELECT
  *,
  decoded.event_params AS `event_params`,
  decoded.event_signature AS `event_name`
  FROM scroll_decoded
  WHERE decoded IS NOT NULL
  AND decoded.event_signature = 'Transfer'
```

In this second transform, we take the `event_params` and `event_signature` from the result of the decoding. We then filter the query on:

* `decoded IS NOT NULL`: to leave out potential null results from the decoder
* `decoded.event_signature = 'Transfer'`: the decoder will output the event name as event\_signature, excluding its arguments. We use it to filter only for Transfer events.

```sql Transform: scroll_721_transfers 
SELECT
  id,
  address AS contract_address,
  lower(event_params[1]) AS sender,
  lower(event_params[2]) AS recipient,
  COALESCE(TRY_CAST(event_params[3] AS NUMERIC), -999) AS token_id,
  event_name,
  block_number,
  block_hash,
  log_index,
  transaction_hash,
  transaction_index
  FROM scroll_clean
```

In this last transform we are essentially selecting all the Transfer information we are interested in having in our database.
We've included a number of columns that you may or may not need, the main columns needed for most purposes are: `id`, `contract_address` (if you are syncing multiple contract addresses), `sender`, `recipient` and `token_id`.

* `id`: This is the Goldsky provided `id`, it is a string composed of the dataset name, block hash, and log index, which is unique per event, here's an example: `log_0x60eaf5a2ab37c73cf1f3bbd32fc17f2709953192b530d75aadc521111f476d6c_18`
* `address AS contract_address`: We use the lower function here to lower-case the address to make using this data simpler downstream, we also rename the column to contract\_address to make it more explicit.
* `lower(event_params[1]) AS sender`: Here we continue to lower-case values for consistency. In this case we're using the first element of the `event_params` array (using a 1-based index), and renaming it to `sender`. Each event parameter maps to an argument to the `event_signature`.
* `lower(event_params[2]) AS recipient`: Like the previous column, we're pulling the second element in the `event_params` array and renaming it to `recipient`.

For the token\_id we introduce a few SQL functions `COALESCE(TRY_CAST(event_params[3] AS NUMERIC), -999) AS token_id`. Well start from the inside and work our way out.

1. `event_params[3]` is the third element of the `event_params` array, and for ERC-721 this is the token ID. Although not covered in this example, since ERC-20 shares the same signature, this element represents a token balance rather than token ID if you're decoding ERC-20 transfers.
2. `TRY_CAST(event_params[3] AS NUMERIC)` is casting the string element `event_params[3]` to `NUMERIC` - token IDs can be as large as an unsigned 256 bit integer, so make sure your database can handle that, if not, you can cast it to a different data type that your sink can handle. We use `TRY_CAST` because it will prevent the pipeline from failing in case the cast fails returning a `NULL` value instead.
3. `COALESCE(TRY_CAST(event_params[3] AS NUMERIC), -999)`: `COALESCE` can take an arbitrary number of arguments and returns the first non-NULL value. Since `TRY_CAST` can return a `NULL` we're returning `-999` in case it does. This isn't strictly necessary but is useful to do in case you want to find offending values that were unable to be cast.

Lastly, we are also adding more block metadata to the query to add context to each transaction:

```
event_name,
block_number,
block_hash,
log_index,
transaction_hash,
transaction_index
```

It's worth mentioning that in this example we are interested in all the ERC-721 Transfer events but if you would like to filter for specific contract addresses you could simply add a `WHERE` filter to this query with address you are interested in, like: `WHERE address IN ('0xBC4CA0EdA7647A8aB7C2061c2E118A18a936f13D', '0xdac17f958d2ee523a2206206994597c13d831ec7')`

#### Deploying the pipeline

Our last step is to deploy this pipeline and start sinking ERC-721 transfer data into our database. Assuming we are using the same file name for the pipeline configuration as in this example,
we can use the [CLI pipeline create command](/reference/cli#pipeline-create) like this:

`goldsky pipeline create scroll-erc721-transfers --definition-path scroll-erc721-transfers.yaml`

After some time, you should see the pipeline start streaming Transfer data into your sink.

<Note>
  Remember that you can always speed up the streaming process by [updating](/reference/cli#pipeline-update) the resourceSize of the pipeline
</Note>

Here's an example transfer record from our sink:

| id                                                                          | contract\_address                          | sender                                     | recipient                                  | token\_id | event\_name | block\_number | block\_hash                                                        | log\_index | transaction\_hash                                                  | transaction\_index |
| --------------------------------------------------------------------------- | ------------------------------------------ | ------------------------------------------ | ------------------------------------------ | --------- | ----------- | ------------- | ------------------------------------------------------------------ | ---------- | ------------------------------------------------------------------ | ------------------ |
| log\_0x5e3225c40254dd5b1b709152feafaa8437e505ae54c028b6d433362150f99986\_34 | 0x6e55472109e6abe4054a8e8b8d9edffcb31032c5 | 0xd2cda3fa01d34878bbe6496c7327b3781d4422bc | 0x6e55472109e6abe4054a8e8b8d9edffcb31032c5 | 38087399  | Transfer    | 4057598       | 0x5e3225c40254dd5b1b709152feafaa8437e505ae54c028b6d433362150f99986 | 34         | 0xf06c42ffd407bb9abba8f00d4a42cb7f1acc1725c604b8895cdb5f785f827967 | 11                 |

We can find this [transaction in Scrollscan](https://scrollscan.com/tx/0xf06c42ffd407bb9abba8f00d4a42cb7f1acc1725c604b8895cdb5f785f827967). We see that it corresponds to the transfer of MERK token:

<img className="block mx-auto" width="450" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/mirror/guides/token-transfers/erc721-transfer.png" />

This concludes our successful deployment of a Mirror pipeline streaming ERC-721 Tokens from Scroll chain into our database using inline decoders. Congrats! 

### ERC-721 Transfers using decoded datasets

As explained in the Introduction, Goldsky provides decoded datasets for Raw Logs and Raw Traces for a number of different chains. You can check [this list](/mirror/sources/direct-indexing) to see if the chain you are interested in has these decoded datasets.
In these cases, there is no need for us to run Decoding Transform Functions as the dataset itself will already contain the event signature and event params decoded.

Click on the button below to see an example pipeline definition for streaming ERC-1155 tokens on the Ethereum chain using the `decoded_logs` dataset.

<Accordion title="Decoded Logs Pipeline Definition">
  ```yaml ethereum-decoded-logs-erc721-transfers.yaml
  sources:
    - referenceName: ethereum.decoded_logs
      version: 1.0.0
      type: dataset
      startAt: earliest
      description: Decoded logs for events emitted from contracts. Contains the
        decoded event signature and event parameters, contract address, data,
        topics, and metadata for the block and transaction.
  transforms:
    - type: sql
      referenceName: ethereum_721_transfers
      primaryKey: id
      description: ERC721 Transfers
      sql: >-
        SELECT
                address AS contract_address,
                lower(event_params[1]) AS sender,
                lower(event_params[2]) AS recipient,
                COALESCE(TRY_CAST(event_params[3] AS NUMERIC), -999) AS token_id,
                raw_log.block_number       AS block_number,
                raw_log.block_hash         AS block_hash,
                raw_log.log_index          AS log_index,
                raw_log.transaction_hash   AS transaction_hash,
                raw_log.transaction_index  AS transaction_index,
                id
                FROM ethereum.decoded_logs WHERE raw_log.topics LIKE '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef%'
                AND SPLIT_INDEX(raw_log.topics, ',', 3) IS NOT NULL
  sinks:
    - type: postgres
      table: ecr721_transfers
      schema: mirror
      secretName: <YOUR SECRET>
      description: Postgres sink for 721 NFT transfers
      referenceName: ethereum_721_sink
      sourceStreamName: ethereum_721_transfers
  ```

  <Note>
    If you copy and use this configuration file, make sure to update:

    1. Your `secretName`. If you already [created a secret](/mirror/manage-secrets), you can find it via the [CLI command](/reference/cli#secret) `goldsky secret list`.
    2. The schema and table you want the data written to, by default it writes to `mirror.erc721_transfers`.
  </Note>
</Accordion>

You can appreciate that it's pretty similar to the inline decoding pipeline method but here we simply create a transform which does the filtering based on the `raw_log.topics` just as we did on the previous method.

Assuming we are using the same filename for the pipeline configuration as in this example we can deploy this pipeline with the [CLI pipeline create command](/reference/cli#pipeline-create):

`goldsky pipeline create ethereum-erc721-transfers --definition-path ethereum-decoded-logs-erc721-transfers.yaml`

## Conclusion

In this guide, we have learnt how Mirror simplifies streaming NFT Transfer events into your database.

We have first looked into the easy way of achieving this, simply by making use of the readily available ERC-721 dataset of the EVM chaina and using its as the source to our pipeline.

We have deep dived into the standard decoding method using Decoding Transform Functions, implementing an example on Scroll chain.
We have also looked into an example implementation using the decoded\_logs dataset for Ethereum. Both are great decoding methods and depending on your use case and dataset availability you might prefer one over the other.

With Mirror, developers gain flexibility and efficiency in integrating blockchain data, opening up new possibilities for applications and insights. Experience the transformative power of Mirror today and redefine your approach to blockchain data integration.

<Snippet file="getting-help.mdx" />


# Native Transfers
Source: https://docs.goldsky.com/mirror/guides/token-transfers/Native-transfers

Create a table containing transfers for the native token of a chain

This guide explains how to use [Raw Logs Direct Indexing sources](/mirror/sources/direct-indexing) to create a Mirror pipeline that allows you to stream all native transactions for a chain into your database. In the example below, we will focus on ETH transfers on the Ethereum network but the same
logic applies to any EVM-compatible chain which has this source available.

This guide is part of a series of tutorials on how you can export transfer data into your datawarehouse. Here we will be focusing on ERC-20 Transfers, visit the following guides for other types of Transfers:

* [ERC-20 Transfers](/mirror/guides/token-transfers/ERC-20-transfers)
* [ERC-721 Transfers](/mirror/guides/token-transfers/ERC-721-transfers)
* [ERC-1155 Transfers](/mirror/guides/token-transfers/ERC-1155-transfers)

## What you'll need

1. A basic understanding of the Mirror product's more [basic ETL use case](/mirror/guides/export-events-to-database).
2. A basic understanding of SQL, though we use the syntax and functionality of [Flink v1.17](https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/functions/systemfunctions/).
3. A destination sink to write your data to.

## Preface

When it comes to identifying native transfers on a chain it's important to highlight that there are [two types of accounts](https://ethereum.org/en/developers/docs/accounts/) that can interact with transactions:

* Externally Owned Accounts (EOA): controlled by an actual user.
* Contract Accounts: controlled by code.

Currently, transactions in a block can only be initiated by a EOAs (this is something that could change in the future with the introduction of[Account Abstraction](https://ethereum.org/en/roadmap/account-abstraction/)).
For instance, take [block 16240000](https://etherscan.io/block/16240000); you will see all transactions initiated belong to EOAs.

A transaction initiated by an EOA can send value to another EOA as in [this transaction](https://etherscan.io/tx/0x7498065db91e8543c6eafed286687fe8006b9ff90081153f769ad47ce115afc8).
Alternatively, this EOA can call a smart contract's method and optionally send value with it as in [this transaction](https://etherscan.io/tx/0x7856bfef7e5da7b22fbdc2fa923bf29d040b2d1b3dbdb3b834dffdc06f4f0a17/advanced).

Smart contracts can then call other smart contracts. They can alternatively send value directly to another EOA. These internal transactions initiated by smart contracts can optionally send native value along so it is important to consider them.
In most chain explorers you can identify these internal transactions and their corresponding value transfers [accessing Advanced view mode](https://etherscan.io/tx/0x7856bfef7e5da7b22fbdc2fa923bf29d040b2d1b3dbdb3b834dffdc06f4f0a17#internal).

All of these types of transactions (EOA initiated & internal transactions) are available in our Raw Logs dataset so we will use it as the source for our Mirror pipeline. You can see its data schema [here](/reference/schema/EVM-schemas#raw-traces).

## Pipeline YAML

There is one transform in this configuration and we'll explain how it works. If you copy and use this configuration file, make sure to update:

1. Your `secret_name` (v2: `secretName`). If you already [created a secret](https://docs.goldsky.com/mirror/manage-secrets), you can find it via the [CLI command](https://docs.goldsky.com/reference/cli#secret) `goldsky secret list`.
2. The schema and table you want the data written to, by default it writes to `public.eth_transfers`.

<Tabs>
  <Tab title="v3">
    ```yaml native-transfers.yaml
    name: native-transfers
    apiVersion: 3
    sources:
      my_ethereum_raw_traces:
        dataset_name: ethereum.raw_traces
        version: 1.1.0
        type: dataset
        start_at: earliest
    transforms:
      ethereum_eth_transfers_transform:
        primary_key: id
        description: ETH Transfers transform
        sql: >
          SELECT
            id,
            block_number,
            block_hash,
            block_timestamp,
            transaction_hash,
            transaction_index,
            from_address,
            to_address,
            CASE
                WHEN trace_address <> '' THEN 'Internal TX'
                ELSE 'EOA TX'
            END AS tx_type,
            CASE
                WHEN block_number <= 17999551 THEN COALESCE(TRY_CAST(`value` AS DECIMAL(100)) / 1e9, 0)
                ELSE COALESCE(TRY_CAST(`value` AS DECIMAL(100)), 0)
            END AS `value`,
            call_type,
            trace_address,
            status
          FROM
            my_ethereum_raw_traces
          WHERE
            call_type <> 'delegatecall' and `value` > 0 and status = 1;
    sinks:
      postgres_ethereum.eth_transfers:
        type: postgres
        table: eth_transfers
        schema: public
        secret_name: <YOUR_SECRET>
        description: "Postgres sink for ethereum.eth_transfers"
        from: ethereum_eth_transfers_transform
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml
    sources:
    - referenceName: ethereum.raw_traces
        version: 1.1.0
        type: dataset
        startAt: earliest
        description: Traces of all function calls made on the chain including metadata
        for block, trace, transaction, and gas.
    transforms:
    - type: sql
        referenceName: ethereum_eth_transfers_transform
        primaryKey: id
        description: ETH Transfers transform
        sql: >
        SELECT
            id,
            block_number,
            block_hash,
            block_timestamp,
            transaction_hash,
            transaction_index,
            from_address,
            to_address,
            CASE
                WHEN trace_address <> '' THEN 'Internal TX'
                ELSE 'EOA TX'
            END AS tx_type,
            CASE
                WHEN block_number <= 17999551 THEN COALESCE(TRY_CAST(`value` AS DECIMAL(100)) / 1e9, 0)
                ELSE COALESCE(TRY_CAST(`value` AS DECIMAL(100)), 0)
            END AS `value`,
            call_type,
            trace_address,
            status
        FROM
            ethereum.raw_traces
        WHERE
            call_type <> 'delegatecall' and `value` > 0 and status = 1;
    sinks:
    - type: postgres
        table: eth_transfers
        schema: public
        secretName: <YOUR_SECRET>
        description: "Postgres sink for ethereum.eth_transfers"
        referenceName: postgres_ethereum.eth_transfers
        sourceStreamName: ethereum_eth_transfers_transform
    ```
  </Tab>
</Tabs>

### Native Transfers Transform

We'll start at the top.

#### Traces context columns

```sql
SELECT
    id,
    block_number,
    block_hash,
    block_timestamp,
    transaction_hash,
    transaction_index,
    from_address,
    to_address,
```

These are optional columns from this dataset which we include to give us some context around the actual transfer.

#### Transaction Type

```sql
CASE
    WHEN trace_address <> '' THEN 'Internal TX'
    ELSE 'EOA TX'
END AS tx_type,
```

Here we look into `trace_address` column to identify whether this an initial EOA transaction or an internal one. This is also optional to include.

#### Token Value

```sql
CASE
    WHEN block_number <= 17999551 THEN COALESCE(TRY_CAST(`value` AS DECIMAL(100)) / 1e9, 0)
    ELSE COALESCE(TRY_CAST(`value` AS DECIMAL(100)), 0)
END AS `value`,
```

<Warning>
  Due to the nature of the dataset, we need to make this conversion as values before 17999551 block were wrongly multiplied by 1e9, this is only the case for the Ethereum dataset. Other datasets do not need this adjustment.
</Warning>

<Note>Some columns are surrounded by backticks, this is because they are reserved words in Flink SQL. Common columns that need backticks are: data, output, value, and a full list can be found [here](https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/table/sql/overview/#reserved-keywords).</Note>

#### Filter

```sql
call_type,
trace_address,
status
```

We include these values in the SELECT statement as we will making use of them in the filter explained below:

```sql
WHERE
    call_type <> 'delegatecall' and `value` > 0 and status = 1;
```

Here we filter based on:

* `call_type <> 'delegatecall'`: [delegatecall](https://www.educative.io/answers/what-is-delegatecall-in-ethereum) is a type of function call where the called contract's code is executed with the state of the calling contract, including storage and balance. In some cases, it can mistakenly carry over the value transfer of the original
  calling contract which would compromise our data quality due to value transfer duplications. As a result, we can safely leave them out of our resulting dataset as delegatecalls can never send value with them.
* `value > 0`: we want to make sure we track transactions with actual native value.
* `status = 1`: the Raw Traces dataset can contain traces which got reverted. With this filter, we make sure to consider only successful transactions.

## Deploying the pipeline

To deploy this pipeline and start sinking ERC-20 transfer data into your database simply execute:

`goldsky pipeline create <pipeline_name> --definition-path <yaml_file>`

<Snippet file="getting-help.mdx" />


# Introduction
Source: https://docs.goldsky.com/mirror/introduction



# Stream Onchain Data with Pipelines

Mirror streams **onchain data directly to your database**, with \<1s latency.

Using a database offers unlimited queries and the flexibility to easily combine onchain and offchain data together in one place.

<Steps>
  <Step title="Choose a chain">
    <Card title="Mirror supports over 130+ chains." icon="arrow-right-from-bracket" iconType="duotone" href="/mirror/sources/supported-sources">
      You can [source](/mirror/sources/supported-sources) the data you want via
      a subgraph or direct indexing, then use
      [transforms](/mirror/transforms/transforms-overview) to further filter or map that
      data.
    </Card>
  </Step>

  <Step title="Choose a database">
    <Card title="Optimize your DB for what you do." icon="arrow-right-to-bracket" iconType="duotone" href="/mirror/sinks/supported-sinks">
      Mirror can minimize your latency if you're [running an
      app](/mirror/sinks/supported-sinks#for-apis-for-apps), or maximize your
      efficiency if you're [calculating
      analytics](/mirror/sinks/supported-sinks#for-analytics). You can even send
      data to a [channel](/mirror/extensions/channels/overview) to level up your
      data team.
    </Card>
  </Step>
</Steps>

Behind the scenes, Mirror automatically creates and runs data pipelines for you off a `.yaml` config file.  Pipelines:

1. Are reorg-aware and update your datastores with the latest information
2. Fully manage backfills + edge streaming so you can focus on your product
3. Benefit from quality checks and automated fixes & improvements
4. Work with data across chains, harmonizing timestamps, etc. automatically

Set up your first database by [creating a pipeline](/mirror/create-a-pipeline) in 5 minutes.

<Snippet file="getting-help.mdx" />


# Database secrets
Source: https://docs.goldsky.com/mirror/manage-secrets



## Overview

In order for Goldsky to connect to your sink, you have to configure secrets. Secrets refer to your datastore credentials which are securely stored in your Goldsky account.

You can create and manage your secrets with the `goldsky secret` command. To see a list of available commands and how to use them, please refer to the output of `goldsky secret -h`.

For sink-specific secret information, please refer to the [individual sink pages](/mirror/sinks).

## Guided CLI experience

If you create a pipeline with `goldsky pipeline create <pipeline-name>`, there is no need to create a secret beforehand. The CLI will list existing secrets and offer you the option of creating a new secret as part of the pipeline creation flow.


# Example Pipelines Repo
Source: https://docs.goldsky.com/mirror/mirror-github





# Performance benchmark
Source: https://docs.goldsky.com/mirror/performance-benchmark

A test of Mirror's write speeds into a ClickHouse database.

## Overview

As a quick test of Mirror's write performance, we ran a pipeline of each size backfilling Ethereum blocks data (\~18.5M rows as of October 23, 2023) with an [unmodified schema](/reference/schema/EVM-schemas#raw-blocks).

This test was performed on a 720 GiB RAM, 180 vCPU [ClickHouse Cloud](https://clickhouse.com/) instance to ensure that non-pipeline factors (eg. disk IO, available memory, CPU cores) do not act as a bottleneck.

Each test was run on a completely clean instance (ie. no existing tables) with no other queries or commands running on the database.

## Results

| Pipeline size | Peak (rows/m) | Time to backfill |
| ------------- | ------------- | ---------------- |
| S             | 350,000       | 53min            |
| M             | 950,000       | 20min            |
| L             | 2,100,000     | 9min             |
| XL            | 3,500,000     | \~5min           |
| XXL           | 6,300,000     | 3min             |


# ClickHouse
Source: https://docs.goldsky.com/mirror/sinks/clickhouse



[ClickHouse](https://clickhouse.com/) is a highly performant and cost-effective OLAP database that can support real-time inserts. Mirror pipelines can write subgraph or blockchain data directly into ClickHouse with full data guarantees and reorganization handling.

Mirror can work with any ClickHouse setup, but we have several strong defaults. From our experimentation, the `ReplacingMergeTree` table engine with `append_only_mode` offers the best real-time data performance for large datasets.

[ReplacingMergeTree](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replacingmergetree) engine is used for all sink tables by default. If you don't want to use a ReplacingMergeTree, you can pre-create the table with any data engine you'd like. If you don't want to use a ReplacingMergeTree, you can disable `append_only_mode`.

Full configuration details for Clickhouse sink is available in the [reference](/reference/config-file/pipeline#clickhouse) page.

## Secrets

<Warning>
  **Use HTTP**
  Mirror writes to ClickHouse via the `http`  interface (often port `8443`), rather than the `tcp` interface (often port `9000`).
</Warning>

```shell

goldsky secret create --name A_CLICKHOUSE_SECRET --value '{
  "url": "clickhouse://blah.host.com:8443?ssl=true",
  "type": "clickHouse",
  "username": "default",
  "password": "qwerty123",
  "databaseName": "myDatabase"
}'
```

## Required permissions

The user will need the following permissions for the target database.

* CREATE DATABASE permissions for that database
* INSERT, SELECT, CREATE, DROP table permissions for tables within that database

```sql
CREATE USER 'username' IDENTIFIED WITH password 'user_password';

GRANT CREATE DATABASE ON goldsky.* TO 'username';
GRANT SELECT, INSERT, DROP, CREATE ON goldsky.* TO 'username';
```

It's highly recommended to assign a ROLE to the user as well, and restrict the amount of total memory and CPU the pipeline has access to. The pipeline will take what it needs to insert as fast as possible, and while that may be desired for a backfill, in a production scenario you may want to isolate those resources.

## Data consistency with ReplacingMergeTrees

With `ReplacingMergeTree` tables, we can write, overwrite, and flag rows with the same primary key for deletes without actually mutating. As a result, the actual raw data in the table may contain duplicates.

ClickHouse allows you to clean up duplicates and deletes from the table by running

```sql
OPTIMIZE <tablename> FINAL;
```

which will merge rows with the same primary key into one. This may not be deterministic and fully clean all data up, so it's recommended to also add the `FINAL` keyword after the table name for queries.

```SQL
SELECT <columns>
FROM <table name> FINAL
```

This will run a clean-up process, though there may be performance considerations.

## Append-Only Mode

<Warning>
  **Proceed with Caution**

  Without `append_only_mode=true` (v2: `appendOnlyMode=true`), the pipeline may hit ClickHouse mutation flush limits. Write speed will also be slower due to mutations.
</Warning>

Append-only mode means the pipeline will only *write* and not *update* or *delete* tables. There will be no mutations, only inserts.

This drastically increases insert speed and reduces Flush exceptions (which happen when too many mutations are queued up).

It's highly recommended as it can help you operate a large dataset with many writes with a small ClickHouse instance.

When `append_only_mode` (v2: `appendOnlyMode`) is `true` (default and recommended for ReplacingMergeTrees), the sink behaves the following way:

* All updates and deletes are converted to inserts.
* `is_deleted` column is automatically added to a table. It contains `1` in case of deletes, `0` otherwise.
* If `versionColumnName` is specified, it's used as a [version number column](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replacingmergetree#ver) for deduplication. If it's not specified, `insert_time` column is automatically added to a table. It contains insertion time and is used for deduplication.
* Primary key is used in the `ORDER BY` clause.

This allows us to handle blockchain reorganizations natively while providing high insert speeds.

When `append_only_mode` (v2: `appendOnlyMode`) is `false`:

* All updates and deletes are propagated as is.
* No extra columns are added.
* Primary key is used in the `PRIMARY KEY` clause.


# Elasticsearch
Source: https://docs.goldsky.com/mirror/sinks/elasticsearch



Give your users blazing-fast auto-complete suggestions, full-text fuzzy searches, and scored recommendations based off of on-chain data.

[Elasticsearch](https://www.elastic.co/) is the leading search datastore, used for a wide variety of usecase for billions of datapoints a day, including search, roll-up aggregations, and ultra-fast lookups on text data.

Goldsky supports real-time insertion into Elasticsearch, with event data updating in Elasticsearch indexes as soon as it gets finalized on-chain.

See the [Elasticsearch docs](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/elasticsearch-intro.html) to see more of what it can do!

Full configuration details for Elasticsearch sink is available in the [reference](/reference/config-file/pipeline#elasticsearch) page.

Contact us at [sales@goldsky.com](mailto:sales@goldsky.com) to learn more about how we can power search for your on-chain data!

## Secrets

Create an Elasticsearch secret with the following CLI command:

```shell
goldsky secret create --name AN_ELASTICSEARCH_SECRET --value '{
  "host": "Type.String()",
  "username": "Type.String()",
  "password": "Type.String()",
  "type": "elasticsearch" 
}'
```


# MySQL
Source: https://docs.goldsky.com/mirror/sinks/mysql



[MySQL](https://www.mysql.com/) is a powerful, open source object-relational database system used for OLTP workloads.

Mirror supports MySQL as a sink, allowing you to write data directly into MySQL. This provides a robust and flexible solution for both mid-sized analytical workloads and high performance REST and GraphQL APIs.

When you create a new pipeline, a table will be automatically created with columns from the source dataset. If a table is already created, the pipeline will write to it. As an example, you can set up partitions before you setup the pipeline, allowing you to scale MySQL even further.

MySQL also supports Timescale hypertables, if the hypertable is already setup. We have a separate Timescale sink in technical preview that will automatically setup hypertables for you - contact [support@goldsky.com](mailto:support@goldsky.com) for access.

Full configuration details for MySQL sink is available in the [reference](/reference/config-file/pipeline#mysql) page.

## Role Creation

Here is an example snippet to give the permissions needed for pipelines.

```sql

CREATE ROLE goldsky_writer WITH LOGIN PASSWORD 'supersecurepassword';

-- Allow the pipeline to create schemas.
-- This is needed even if the schemas already exist
GRANT CREATE ON DATABASE mysql TO goldsky_writer;

-- For existing schemas that you want the pipeline to write to:
GRANT USAGE, CREATE ON SCHEMA <schemaName> TO goldsky_writer;

```

## Secret Creation

Create a MySQL secret with the following CLI command:

```shell
goldsky secret create --name A_MYSQL_SECRET --value '{
  "type": "jdbc",
  "protocol": "mysql",
  "host": "db.host.com",
  "port": 5432,
  "databaseName": "myDatabase",
  "user": "myUser",
  "password": "myPassword"
}'
```

## Examples

### Getting an edge-only stream of decoded logs

This definition gets real-time edge stream of decoded logs straight into a MySQL table named `eth_logs` in the `goldsky` schema, with the secret `A_MYSQL_SECRET` created above.

<Tabs>
  <Tab title="v3">
    ```yaml
    name: ethereum-decoded-logs-to-mysql
    apiVersion: 3
    sources:
      my_ethereum_decoded_logs:
        dataset_name: ethereum.decoded_logs
        version: 1.0.0
        type: dataset
        start_at: latest
    transforms:
      logs:
        sql: |
          SELECT
              id,
              address,
              event_signature,
              event_params,
              raw_log.block_number as block_number,
              raw_log.block_hash as block_hash,
              raw_log.transaction_hash as transaction_hash
          FROM
              my_ethereum_decoded_logs
        primary_key: id
    sinks:
      my_mysql_sink:
        type: mysql
        table: eth_logs
        schema: goldsky
        secret_name: A_MYSQL_SECRET
        from: logs
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml
    sources:
      - name: ethereum.decoded_logs
        version: 1.0.0
        type: dataset
        startAt: latest

    transforms:
      - sql: |
          SELECT
              id,
              address,
              event_signature,
              event_params,
              raw_log.block_number as block_number,
              raw_log.block_hash as block_hash,
              raw_log.transaction_hash as transaction_hash
          FROM
              ethereum.decoded_logs
        name: logs
        type: sql
        primaryKey: id

    sinks:
      - type: mysql
        table: eth_logs
        schema: goldsky
        secretName: A_MYSQL_SECRET
        sourceStreamName: logs
    ```
  </Tab>
</Tabs>

## Tips for backfilling large datasets into MySQL

While MySQL offers fast access of data, writing large backfills into MySQL can sometimes be hard to scale.

Often, pipelines are bottlenecked against sinks.

Here are some things to try:

### Avoid indexes on tables until *after* the backfill

Indexes increase the amount of writes needed for each insert. When doing many writes, inserts can slow down the process significantly if we're hitting resources limitations.

### Bigger batch\_sizes for the inserts

The `sink_buffer_max_rows` setting controls how many rows are batched into a single insert statement. Depending on the size of the events, you can increase this to help with write performance. `1000` is a good number to start with. The pipeline will collect data until the batch is full, or until the `sink_buffer_interval` is met.

### Temporarily scale up the database

Take a look at your database stats like CPU and Memory to see where the bottlenecks are. Often, big writes aren't blocked on CPU or RAM, but rather on network or disk I/O.

For Google Cloud SQL, there are I/O burst limits that you can surpass by increasing the amount of CPU.

For AWS RDS instances (including Aurora), the network burst limits are documented for each instance. A rule of thumb is to look at the `EBS baseline I/O` performance as burst credits are easily used up in a backfill scenario.

### Aurora Tips

When using Aurora, for large datasets, make sure to use `Aurora I/O optimized`, which charges for more storage, but gives you immense savings on I/O credits. If you're streaming the entire chain into your database, or have a very active subgraph, these savings can be considerable, and the disk performance is significantly more stable and results in more stable CPU usage pattern.


# PostgreSQL
Source: https://docs.goldsky.com/mirror/sinks/postgres



[PostgreSQL](https://www.postgresql.org/) is a powerful, open source object-relational database system used for OLTP workloads.

Mirror supports PostgreSQL as a sink, allowing you to write data directly into PostgreSQL. This provides a robust and flexible solution for both mid-sized analytical workloads and high performance REST and GraphQL APIs.

When you create a new pipeline, a table will be automatically created with columns from the source dataset. If a table is already created, the pipeline will write to it. As an example, you can set up partitions before you setup the pipeline, allowing you to scale PostgreSQL even further.

The PostgreSQL also supports Timescale hypertables, if the hypertable is already setup. We have a separate Timescale sink in technical preview that will automatically setup hypertables for you - contact [support@goldsky.com](mailto:support@goldsky.com) for access.

Full configuration details for PostgreSQL sink is available in the [reference](/reference/config-file/pipeline#postgresql) page.

## Role Creation

Here is an example snippet to give the permissions needed for pipelines.

```sql

CREATE ROLE goldsky_writer WITH LOGIN PASSWORD 'supersecurepassword';

-- Allow the pipeline to create schemas.
-- This is needed even if the schemas already exist
GRANT CREATE ON DATABASE postgres TO goldsky_writer;

-- For existing schemas that you want the pipeline to write to:
GRANT USAGE, CREATE ON SCHEMA <schemaName> TO goldsky_writer;

```

## Secret Creation

Create a PostgreSQL secret with the following CLI command:

```shell
goldsky secret create --name A_POSTGRESQL_SECRET --value '{
  "type": "jdbc",
  "protocol": "postgresql",
  "host": "db.host.com",
  "port": 5432,
  "databaseName": "myDatabase",
  "user": "myUser",
  "password": "myPassword"
}'
```

## Examples

### Getting an edge-only stream of decoded logs

This definition gets real-time edge stream of decoded logs straight into a postgres table named `eth_logs` in the `goldsky` schema, with the secret `A_POSTGRESQL_SECRET` created above.

<Tabs>
  <Tab title="v3">
    ```yaml
    name: ethereum-decoded-logs-to-postgres
    apiVersion: 3
    sources:
      my_ethereum_decoded_logs:
        dataset_name: ethereum.decoded_logs
        version: 1.0.0
        type: dataset
        start_at: latest
    transforms:
      logs:
        sql: |
          SELECT
              id,
              address,
              event_signature,
              event_params,
              raw_log.block_number as block_number,
              raw_log.block_hash as block_hash,
              raw_log.transaction_hash as transaction_hash
          FROM
              my_ethereum_decoded_logs
        primary_key: id
    sinks:
      my_postgres_sink:
        type: postgres
        table: eth_logs
        schema: goldsky
        secret_name: A_POSTGRESQL_SECRET
        from: logs
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml
    sources:
      - name: ethereum.decoded_logs
        version: 1.0.0
        type: dataset
        startAt: latest

    transforms:
      - sql: |
          SELECT
              id,
              address,
              event_signature,
              event_params,
              raw_log.block_number as block_number,
              raw_log.block_hash as block_hash,
              raw_log.transaction_hash as transaction_hash
          FROM
              ethereum.decoded_logs
        name: logs
        type: sql
        primaryKey: id

    sinks:
      - type: postgres
        table: eth_logs
        schema: goldsky
        secretName: A_POSTGRESQL_SECRET
        sourceStreamName: logs
    ```
  </Tab>
</Tabs>

## Tips for backfilling large datasets into PostgreSQL

While PostgreSQL offers fast access of data, writing large backfills into PostgreSQL can sometimes be hard to scale.

Often, pipelines are bottlenecked against sinks.

Here are some things to try:

### Avoid indexes on tables until *after* the backfill

Indexes increase the amount of writes needed for each insert. When doing many writes, inserts can slow down the process significantly if we're hitting resources limitations.

### Bigger batch\_sizes for the inserts

The `sink_buffer_max_rows` setting controls how many rows are batched into a single insert statement. Depending on the size of the events, you can increase this to help with write performance. `1000` is a good number to start with. The pipeline will collect data until the batch is full, or until the `sink_buffer_interval` is met.

### Temporarily scale up the database

Take a look at your database stats like CPU and Memory to see where the bottlenecks are. Often, big writes aren't blocked on CPU or RAM, but rather on network or disk I/O.

For Google Cloud SQL, there are I/O burst limits that you can surpass by increasing the amount of CPU.

For AWS RDS instances (including Aurora), the network burst limits are documented for each instance. A rule of thumb is to look at the `EBS baseline I/O` performance as burst credits are easily used up in a backfill scenario.

# Provider Specific Notes

### AWS Aurora Postgres

When using Aurora, for large datasets, make sure to use `Aurora I/O optimized`, which charges for more storage, but gives you immense savings on I/O credits. If you're streaming the entire chain into your database, or have a very active subgraph, these savings can be considerable, and the disk performance is significantly more stable and results in more stable CPU usage pattern.

### Supabase

Supabase's direct connection URLs only support IPv6 connections and will not work with our default validation. There are two solutions

1. Use `Session Pooling`. In the connection screen, scroll down to see the connection string for the session pooler. This will be included in all Supabase plans and will work for most people. However, sessions will expire, and may lead to some warning logs in your pipeline logs. These will be dealt with gracefully and no action is needed. No data will be lost due to a session disconnection.&#x20;

   ![](https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/image.png)

2. Alternatively, buy the IPv4 add-on, if session pooling doesn't fit your needs. It can lead to more persistent direct connections,&#x20;


# Mirror - Supported sinks
Source: https://docs.goldsky.com/mirror/sinks/supported-sinks



Sinks define the destination of your data. We support two broad categories of sinks based on their functionality and applicability:

* Standard Sinks: These sinks are destinations readily available for querying and analysis, such as traditional databases.

* Channel Sinks: These sinks serve as intermediate storage layers, facilitating further integration into your data stack. Examples: Kafka, AWS S3, or AWS SQS.

## Standard Sinks

Standard Sinks are the default and most popular type of sinks for Mirror. They are optimized for immediate querying and analysis, providing a seamless experience for real-time data access and operations. These sinks are:

<CardGroup cols={2}>
  <Card title="Postgres" icon="database" href="/mirror/sinks/postgres" iconType="duotone">
    Postgres stands out with its advanced features, extensibility, and strong
    ACID compliance.
  </Card>

  <Card title="MySQL" icon="database" href="/mirror/sinks/mysql" iconType="duotone">
    MySQL stands out with its advanced features, extensibility, and strong
    ACID compliance.
  </Card>

  <Card title="ClickHouse" icon="chart-mixed" href="/mirror/sinks/clickhouse" iconType="duotone">
    ClickHouse delivers exceptional performance for OLAP queries with its
    columnar storage format.
  </Card>

  <Card title="Elasticsearch" icon="magnifying-glass" href="/mirror/sinks/elasticsearch" iconType="duotone">
    Elasticsearch is a powerful tool for real-time search and analytics on large
    datasets.
  </Card>

  <Card title="Timescale" icon="timeline" href="/mirror/sinks/timescale" iconType="duotone">
    Timescale offers powerful time-series data management and analytics with
    PostgreSQL compatibility.
  </Card>

  <Card title="Goldsky Channels" icon="plug" href="/mirror/extensions/channels/overview" iconType="duotone">
    Goldsky Channels are intermediate storage layers designed to absorb the Goldsky firehose and let you stream the data into alternative sinks. These channels are AWS S3, AWS SQS and Kafka.
  </Card>

  <Card title="Webhook" icon="sink" href="/mirror/sinks/webhook">
    A Webhook sink enables sending data to an external service via HTTP. This allows you to output pipeline results to your application server, to a third-party API, or a bot.
  </Card>
</CardGroup>

### What should I use?

#### For APIs for apps

For sub-second queries, typically you would choose a database that has row-based storage (i.e. it stores each row as it is instead of applying any sort of compression).

The drawbacks are that they take more space. This means large, non-indexed scans can take longer and storage costs can be higher.

1. [Postgres](/mirror/sinks/postgres) is the gold standard for application databases. It can scale almost infinitely with some management, and can support very fast point-lookups with proper indexing. <br /><br />
   If you require super fast lookups by `transaction_hash` or a specific column, Postgres is a very safe choice to start with. Its great as a backend for live data APIs. <br /><br />
   However, it can be slow for analytics queries with a lot of aggregations. For that, you may want to look for an analytical database. <br /><br />
   Great hosted solutions for Postgres include [NeonDB](https://neon.tech/), [AWS Aurora](https://aws.amazon.com/rds/aurora/), and [GCP CloudSQL](https://cloud.google.com/sql).

2. [Elasticsearch](/mirror/sinks/elasticsearch) is a no-sql database that allows for blazing fast lookups and searches. Elasticsearch is built around super-fast non-indexed scanning, meaning it can look at every single record to find the one you want. As a result, you can do queries like fuzzy matches and wildcard lookups with millisecond latency. <br /><br />
   Common applications include search on multiple columns, instant auto-complete, and more.

#### For Analytics

1. [ClickHouse](/mirror/sinks/clickhouse) is a very efficient choice for storage. You can store the entire Ethereum blockchain and pay around \$50 in storage. <br /><br />
   We recommend considering ClickHouse as an alternative to Snowflake or BigQuery - it supports many of the same use cases, and has additional features such as materialized views. Weve seen our customers save tens of thousands of dollars using Goldsky and ClickHouse as a solution. <br /><br />
   The pricing for managing ClickHouse is based on storage cost, then compute cost. The compute cost is constant and isnt based on the amount of data scanned, so you can run concurrent queries without increasing cost.

## Channel Sinks

Channel Sinks act as an extension of the default sinks, providing intermediate storage for more complex data integration scenarios. They are designed to handle high-throughput data streams and enable further processing within your data stack. Examples include:

* AWS S3: A scalable object storage service.

* AWS SQS: A fully managed message queue for microservices, distributed systems, and serverless applications.

* Kafka: A distributed event streaming platform.

For more information on Channel Sinks and how to integrate them, visit our [Channels documentation](/mirror/extensions/channels/overview).


# Timescale
Source: https://docs.goldsky.com/mirror/sinks/timescale



<Warning>
  **Closed Beta**

  This feature is in closed beta and only available for our enterprise customers.

  Please contact us at [support@goldsky.com](mailto:support@goldsky.com) to request access to this feature.
</Warning>

We partner with [Timescale](https://www.timescale.com) to provide teams with real-time data access on on-chain data, using a database powerful enough for time series analytical queries and fast enough for transactional workloads like APIs.

Timescale support is in the form of hypertables - any dataset that has a `timestamp`-like field can be used to create a Timescale hypertable.

You can also use the traditional JDBC/postgres sink with Timecale - you would just need to create the hypertable yourself.

You use TimescaleDB for anything you would use PostgreSQL for, including directly serving APIs and other simple indexed table look-ups. With Timescale Hypertables, you can also make complex database queries like time-windowed aggregations, continuous group-bys, and more.

Learn more about Timescale here: [https://docs.timescale.com/api/latest/](https://docs.timescale.com/api/latest/)


# Webhook
Source: https://docs.goldsky.com/mirror/sinks/webhook



A Webhook sink allows you to send data to an external service via HTTP. This provides considerable flexibility for forwarding pipeline results to your application server, a third-party API, or a bot.

Webhook sinks ensure at least once delivery and manage back-pressure, meaning data delivery adapts based on the responsiveness of your endpoints. The pipeline sends a POST request with a JSON payload to a specified URL, and the receiver only needs to return a 200 status code to confirm successful delivery.

Here is a snippet of YAML that specifies a Webhook sink:

## Pipeline configuration

<Tabs>
  <Tab title="v3">
    ```yaml
    sinks:
      my_webhook_sink:
        type: webhook
        
        # The webhook url
        url: Type.String()

        # The object key coming from either a source or transform. 
        # Example: ethereum.raw_blocks.
        from: Type.String()

        # The name of a goldsky httpauth secret you created which contains a header that can be used for authentication. More on how to create these in the section below.
        secret_name: Type.Optional(Type.String())

        # Optional metadata that you want to send on every request.
        headers:
          SOME-HEADER-KEY: Type.Optional(Type.String())
        
        # Whether to send only one row per http request (better for compatibility with third-party integrations - e.g bots) or to mini-batch it (better for throughput).  
        one_row_per_request: Type.Optional(Type.Boolean())
        
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml
    sinks:
      myWebhookSink:
        type: webhook
        
        # The webhook url
        url: Type.String()

        # The object key coming from either a source or transform. 
        # Example: ethereum.raw_blocks.
        from: Type.String()

        # The name of a goldsky httpauth secret you created which contains a header that can be used for authentication. More on how to create these in the section below.
        secretName: Type.Optional(Type.String())

        # Optional metadata that you want to send on every request.
        headers:
          SOME-HEADER-KEY: Type.Optional(Type.String())
        
        # Whether to send only one row per http request (better for compatibility with third-party integrations - e.g bots) or to mini-batch it (better for throughput).  
        oneRowPerRequest: Type.Optional(Type.Boolean())
        
    ```
  </Tab>
</Tabs>

## Secret creation

Create a httpauth secret with the following CLI command:

```shell
goldsky secret create
```

Select `httpauth` as the secret type and then follow the prompts to finish creating your httpauth secret.

## Example Webhook sink configuration

```yaml
sinks:
  my_webhook_sink:
    type: webhook
    url: https://my-webhook-service.com/webhook-1
    from: ethereum.raw_blocks
    secret_name: ETH_BLOCKS_SECRET
```


# Direct indexing
Source: https://docs.goldsky.com/mirror/sources/direct-indexing



With mirror pipelines, you can access to indexed on-chain data. Define them as a source and pipe them into any sink we support.

## Use-cases

* Mirror specific logs and traces from a set of contracts into a postgres database to build an API for your protocol
* ETL data into a data warehouse to run analytics
* Push the full blockchain into Kafka or S3 to build a datalake for ML

## Supported Chains

<Snippet file="supported-chains-mirror.mdx" />

## Schema

The schema for each of these datasets can be found [here](/reference/schema/EVM-schemas).

## Backfill vs Fast Scan

Goldsky allows you either backfill the entire datasets or alternatively pre-filter the data based on specific attributes.
This allows for an optimal cost and time efficient streaming experience based on your specific use case.

For more information on how to enable each streaming mode in your pipelines visit our [reference documentation](/reference/config-file/pipeline#backfill-vs-fast-scan).


# NFT datasets
Source: https://docs.goldsky.com/mirror/sources/nft-data



<Note>
  **Technical Preview**
  This dataset is in technical preview - it's being used in production by customers already but we are onboarding new users slowly as we scale up our infrastructure.

  [Email us](mailto:sales@goldsky.com) if you'd like to join our technical preview program.
</Note>

Our NFT Metadata dataset includes:

* Image metadata
* Royalty metadata
* Floor price and previous sales
* Transfers
* Off-chain and on-chain Bids/Sales
* Rarity

As a mirror source, you'd be able to sink it into any of our supported sinks, execute transformations and aggregations, as well as join it with other datasets.

This dataset is in technical preview - it's being used in production by customers already but we are onboarding new users slowly as we scale up our infrastructure.


# Subgraphs
Source: https://docs.goldsky.com/mirror/sources/subgraphs



You can use subgraphs as a pipeline source, allowing you to combined the flexibility of subgraph indexing with the expressiveness of the database of your choice.

This enables a lot of powerful use-cases:

* Reuse all your existing subgraph entities.
* Increase querying speeds drastically compared to graphql-engines.
* Flexible aggregations that weren't possible with just GraphQL.
* Analytics on protocols through Clickhouse, and more.
* Plug into BI tools, train AI, and export data for your users

Full configuration details for Subgraph Entity source is available in the [reference](/reference/config-file/pipeline#subgraph-entity) page.

## Automatic Deduplication

Subgraphs natively support time travel queries. This means every historical version of every entity is stored. To do this, each row has an `id`, `vid`, and `block_range`.

When you update an entity in a subgraph mapping handler, a new row in the database is created with the same ID, but new VID and block\_range, and the old row's `block_range` is updated to have an end.

By default, pipelines **deduplicate** on `id`, to show only the latest row per `id`. In other words, historical entity state is not kept in the sink database. This saves a lot of database space and makes for easier querying, as additional deduplication logic is not needed for simple queries. In a postgres database for example, the pipeline will update existing rows with the values from the newest block.

This deduplication happens through setting the primary key in the data going through the pipeline. By default, the primary key is `id`.

If historical data is desired, you can set the primary key to `vid` through a transform.

<Tabs>
  <Tab title="v3">
    ```yaml
    name: qidao-optimism-subgraph-to-postgrse
    apiVersion: 3
    sources:
      subgraph_account:
          type: subgraph_entity
          name: account
          subgraphs:
          - name: qidao-optimism
            version: 1.1.0
    transforms:
      historical_accounts:
        sql: >-
          select * from subgraph_account
        primary_key: vid
    sinks:
      postgres_account:
        type: postgres
        table: historical_accounts
        schema: goldsky
        secret_name: A_POSTGRESQL_SECRET
        from: historical_accounts
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml
    sources:
      - type: subgraphEntity
        # The deployment IDs you gathered above. If you put multiple,
        # they must have the same schema
        deployments:
          - id: QmPuXT3poo1T4rS6agZfT51ZZkiN3zQr6n5F2o1v9dRnnr
        # A name, referred to later in the `sourceStreamName` of a transformation or sink
        referenceName: account
        entity:
          # The name of the entities
          name: account

    transforms:
      - referenceName: historical_accounts
        type: sql
        # The `account` referenced here is the referenceName set in the source
        sql: >-
          select * from account
        primaryKey: vid


    sinks:
      - type: postgres
        table: historical_accounts
        schema: goldsky
        secretName: A_POSTGRESQL_SECRET
        # the `historical_accounts` is the referenceKey of the transformation made above
        sourceStreamName: historical_accounts
    ```
  </Tab>
</Tabs>

In this case, all historical versions of the entity will be retained in the pipeline sink. If there was no table, tables will be automatically created as well.

## Using the wizard

### Subgraphs from your project

Use any of your own subgraphs as a pipeline source. Use `goldsky pipeline create <pipeline-name>` and select `Project Subgraph`, and push subgraph data into any of our supported sinks.

### Community subgraphs

When you create a new pipeline with `goldsky pipeline create <your-pipeline-name>`, select **Community Subgraphs** as the source type. This will display a list of available subgraphs to choose from. Select the one you are interested in and follow the prompts to complete the pipeline creation.

This will get load the subgraph into your project and create a pipeline with that subgraph as the source.


# Mirror - Supported sources
Source: https://docs.goldsky.com/mirror/sources/supported-sources



<CardGroup cols={1}>
  <Card title="Subgraphs" icon="code" href="/mirror/sources/subgraphs" iconType="duotone">
    Mirror data from community subgraphs or from your own custom subgraphs into any sink.
  </Card>

  <Card title="Direct indexing" icon="hive" href="/mirror/sources/direct-indexing" iconType="duotone">
    Mirror entire blockchains into your database for analysis, or filter/transform them to what you need.
  </Card>

  <Card title="NFT datasets" icon="hexagon-vertical-nft" href="/mirror/sources/nft-data" iconType="duotone">
    Curated, NFT-specific datasets for token metadata, sales/listings/transfers activity, and more.
  </Card>
</CardGroup>


# Static IPs
Source: https://docs.goldsky.com/mirror/static-ips



## Overview

Goldsky can connect to your sinks using static IPs. This can be helpful if you want to further restrict access to your sinks and ensure that only Goldsky owned services can access them.

This feature is currently in a closed beta and only available for our enterprise customers. Please contact us at [support@goldsky.com](mailto:support@goldsky.com) (or through Slack/Telegram) to request access to this feature.

### Usage

1. Reach out to us to have this feature enabled for your account.
2. Whitelist the following IPs: `100.21.15.214`, `44.229.26.196`, `44.230.239.184`, `52.38.124.121`
3. Make sure you're using the latest version of the CLI: `curl  | sh`
4. Deploy your pipelines with the flag for enabling static IPs by setting the `dedicated_egress_ip: true|false` in the YAML config of your pipeline

Example:

```
name: private-ip-pipeline
apiVersion: 3
resource_size: s
dedicated_egress_ip: true
sources:
  reward:
    type: subgraph_entity
    subgraphs:
      - name: rewards-subgraph
        version: 1.0.1
    name: reward_payout
transforms: {}
sinks:
  rewards_payout_sink:
    type: postgres
    table: reward_payout
    schema: rewards
    secret_name: DB_WITH_IP_WHITELISTED
    from: reward
```


# External Handler Transforms
Source: https://docs.goldsky.com/mirror/transforms/external-handlers

Transforming data with an external http service.

With external handler transforms, you can send data from your Mirror pipeline to an external service via HTTP and return the processed results back into the pipeline. This opens up a world of possibilities by allowing you to bring your own custom logic, programming languages, and external services into the transformation process.

[In this repo](https://github.com/goldsky-io/documentation-examples/tree/main/mirror-pipelines/goldsky-enriched-erc20-pipeline) you can see an example implementation of enriching ERC-20 Transfer Events with an HTTP service.

**Key Features of External Handler Transforms:**

* Send data to external services via HTTP.
* Supports a wide variety of programming languages and external libraries.
* Handle complex processing outside the pipeline and return results in real time.
* Guaranteed at least once delivery and back-pressure control to ensure data integrity.

### How External Handlers work

1. The pipeline sends a POST request to the external handler with a mini-batch of JSON rows.
2. The external handler processes the data and returns the transformed rows in the same format and order as received.

### Example workflow

1. The pipeline sends data to an external service (e.g. a custom API).
2. The service processes the data and returns the results to the pipeline.
3. The pipeline continues processing the enriched data downstream.

### Example HTTP Request

```json
    POST /external-handler
    [
      {"id": 1, "value": "abc"},
      {"id": 2, "value": "def"}
    ]
```

### Example HTTP Response

```json
    [
      {"id": 1, "transformed_value": "xyz"},
      {"id": 2, "transformed_value": "uvw"}
    ]
```

### YAML config with an external transform

<Tab title="example.yaml">
  ```YAML
  transforms:
    my_external_handler_transform:
      type: handler # the transform type. [required]
      primary_key: hash # [required]
      url: http://example-url/example-transform-route # url that your external handler is bound to. [required]
      headers: # [optional]
  	    Some-Header: some_value # use http headers to pass any tokens your server requires for authentication or any metadata that you think is useful.
      from: ethereum.raw_blocks # the input for the handler. Data sent to your handler will have the same schema as this source/transform. [required]
      # A schema override signals to the pipeline that the handler will respond with a schema that differs from the upstream source/transform (in this case ethereum.raw_blocks).
      # No override means that the handler will do some processing, but that its output will maintain the upstream schema.
      # The return type of the handler is equal to the upstream schema after the override is applied. Make sure that your handler returns a response with rows that follow this schema.
      schema_override: # [optional]
        new_column_name: datatype # if you want to add a new column, do so by including its name and datatype. 
        existing_column_name: new_datatype # if you want to change the type of an existing column (e.g. cast an int to string), do so by including its name and the new datatype
        other_existing_column_name: null # if you want to drop an existing column, do so by including its name and setting its datatype to null
  ```
</Tab>

### Schema override datatypes

When overriding the schema of the data returned by the handler its important to get the datatypes for each column right. The schema\_override property is a map of column names to Flink SQL datatypes.

<Accordion title="Complete list of supported datatypes">
  | Data Type      | Notes                               |
  | :------------- | :---------------------------------- |
  | STRING         |                                     |
  | BOOLEAN        |                                     |
  | BYTE           |                                     |
  | DECIMAL        | Supports fixed precision and scale. |
  | SMALLINT       |                                     |
  | INTEGER        |                                     |
  | BIGINT         |                                     |
  | FLOAT          |                                     |
  | DOUBLE         |                                     |
  | TIME           | Supports only a precision of 0.     |
  | TIMESTAMP      |                                     |
  | TIMESTAMP\_LTZ |                                     |
  | ARRAY          |                                     |
  | ROW            |                                     |
</Accordion>

### Key considerations

* **Schema Changes:** If the external handlers output schema changes, you will need to redeploy the pipeline with the relevant schema\_override.
* **Failure Handling:** In case of failures, the pipeline retries requests indefinitely with exponential backoff.
* **Networking & Performance:** For optimal performance, deploy your handler in a region close to where the pipelines are deployed (we use aws `us-west-2`). Aim to keep p95 latency under 100 milliseconds for best results.
* **Connection & Response times**: The maximum allowed response time is 5 minutes and the maximum allowed time to establish a connection is 1 minute.

### In-order mode for external handlers

In-Order mode allows for subgraph-style processing inside mirror. Records are emitted to the handler in the order that they appear on-chain.

**How to get started**

1. Make sure that the sources that you want to use currently support [Fast Scan](/mirror/sources/direct-indexing). If they dont, submit a request to support.
2. In your pipeline definition specify the `filter` and `in_order` attributes for your source.
3. Declare a transform of type handler or a sink of type webhook.

Simple transforms (e.g filtering) in between the source and the handler/webhook are allowed, but other complex transforms (e.g. aggregations, joins) can cause loss of ordering.

**Example YAML config, with in-order mode**

<Tab title="in-order-mode-example.yaml">
  ```YAML
  name: in-order-pipeline
  sources:
    ethereum.raw_transactions:
      dataset_name: ethereum.raw_transactions
      version: 1.1.0
      type: dataset
      filter: block_number > 21875698 # [required]
      in_order: true # [required] enables in-order mode on the given source and its downstream transforms and sinks.
  sinks:
    my_in_order_sink:
      type: webhook
      url: https://my-handler.com/process-in-order
      headers:
        WEBHOOK-SECRET: secret_two
        secret_name: HTTPAUTH_SECRET_TWO
      from: another_transform
    my_sink:
      type: webhook
      url: https://python-handler.fly.dev/echo
      from: ethereum.raw_transactions
  ```
</Tab>

**Example in-order webhook sink**

```javascript
const express = require('express');
const { Pool } = require('pg');

const app = express();
app.use(express.json());

// Database connection settings
const pool = new Pool({
    user: 'your_user',
    host: 'localhost',
    database: 'your_database',
    password: 'your_password',
    port: 5432,
});

async function isDuplicate(client, key) {
    const res = await client.query("SELECT 1 FROM processed_messages WHERE key = $1", [key]);
    return res.rowCount > 0;
}

app.post('/webhook', async (req, res) => {
    const client = await pool.connect();
    try {
        await client.query('BEGIN');
        
        const payload = req.body;
        const metadata = payload.metadata || {};
        const data = payload.data || {};
        const op = metadata.op;
        const key = metadata.key;
        
        if (!key || !op || !data) {
            await client.query('ROLLBACK');
            return res.status(400).json({ error: "Invalid payload" });
        }

        if (await isDuplicate(client, key)) {
            await client.query('ROLLBACK');
            return res.status(200).json({ message: "Duplicate request processed without write side effects" });
        }

        if (op === "INSERT") {
            const fields = Object.keys(data);
            const values = Object.values(data);
            const placeholders = fields.map((_, i) => `$${i + 1}`).join(', ');
            const query = `INSERT INTO my_table (${fields.join(', ')}) VALUES (${placeholders})`;
            await client.query(query, values);
        } else if (op === "DELETE") {
            const conditions = Object.keys(data).map((key, i) => `${key} = $${i + 1}`).join(' AND ');
            const values = Object.values(data);
            const query = `DELETE FROM my_table WHERE ${conditions}`;
            await client.query(query, values);
        } else {
            await client.query('ROLLBACK');
            return res.status(400).json({ error: "Invalid operation" });
        }
        
        await client.query("INSERT INTO processed_messages (key) VALUES ($1)", [key]);
        await client.query('COMMIT');
        return res.status(200).json({ message: "Success" });
    } catch (e) {
        await client.query('ROLLBACK');
        return res.status(500).json({ error: e.message });
    } finally {
        client.release();
    }
});

app.listen(5000, () => {
    console.log('Server running on port 5000');
});
```

**In-order mode tips**

* To observe records in order, either have a single instance of your handler responding to requests OR introduce some coordination mechanism to make sure that only one replica of the service can answer at a time.
* When deploying your service, avoid having old and new instances running at the same time. Instead, discard the current instance and incur a little downtime to preserve ordering.
* When receiving messages that have already been processed in the handler (pre-existing idempotency key or previous index (e.g already seen block number)) **don't** introduce any side effects on your side, but **do** respond to the message as usual (i.e., processed messages for handlers, success code for webhook sink) so that the pipeline knows to keep going.

### Useful tips

Schema Changes: A change in the output schema of the external handler requires redeployment with schema\_override.

* **Failure Handling:** The pipeline retries indefinitely with exponential backoff.
* **Networking:** Deploy the handler close to where the pipeline runs for better performance.
* **Latency:** Keep handler response times under 100ms to ensure smooth operation.


# SQL Transforms
Source: https://docs.goldsky.com/mirror/transforms/sql-transforms

Transforming blockchain data with Streaming SQL

## SQL Transforms

SQL transforms allow you to write SQL queries to modify and shape data from multiple sources within the pipeline. This is ideal for operations that need to be performed within the data pipeline itself, such as filtering, aggregating, or joining datasets.

Depending on how you choose to [source](/mirror/sources/supported-sources) your data, you might find that you run into 1 of 2 challenges:

1. **You only care about a few contracts**

   Rather than fill up your database with a ton of extra data, you'd rather ***filter*** down your data to a smaller set.
2. **The data is still a bit raw**

   Maybe you'd rather track gwei rounded to the nearest whole number instead of wei.  You're looking to ***map*** data to a different format so you don't have to run this calculation over and over again.

### The SQL Solution

You can use SQL-based transforms to solve both of these challenges that normally would have you writing your own indexer or data pipeline. Instead, Goldsky can automatically run these for you using just 3 pieces of info:

* `name`: **A shortname for this transform**

  You can refer to this from sinks via `from` or treat it as a table in SQL from other transforms.
* `sql`: **The actual SQL**

  To filter your data, use a `WHERE` clause, e.g. `WHERE liquidity > 1000`.

  To map your data, use an `AS` clause combined with `SELECT`, e.g. `SELECT wei / 1000000000 AS gwei`.
* `primary_key`: **A unique ID**

  This should be unique, but you can also use this to intentionally de-duplicate data - the latest row with the same ID will replace all the others.

Combine them together into your [config](/reference/config-file/pipeline):

<Tabs>
  <Tab title="v3">
    ```yaml
    transforms:
      negative_fpmm_scaled_liquidity_parameter:
        sql: SELECT id FROM polymarket.fixed_product_market_maker WHERE scaled_liquidity_parameter < 0
        primary_key: id
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml
    transforms:
      - referenceName: negative_fpmm_scaled_liquidity_parameter
        type: sql
        sql: SELECT id FROM polygon.fixed_product_market_maker WHERE scaled_liquidity_parameter < 0
        primaryKey: id
    ```
  </Tab>
</Tabs>

That's it. You can now filter and map data to exactly what you need.


# Overview
Source: https://docs.goldsky.com/mirror/transforms/transforms-overview

Learn about Mirror's powerful transformation capabilities.

While the simple pipelines let you get real-time data from one of our data sets into your own destination, most teams also do enrichment and filtering using transforms.

With transforms, you can decode check external API is call contracts storage and more. You can even call your own APIs in order to tie the pipeline into your existing system seamlessly.

## [SQL Transforms](/mirror/transforms/sql-transforms)

SQL transforms allow you to write SQL queries to modify and shape data from multiple sources within the pipeline. This is ideal for operations that need to be performed within the data pipeline itself, such as filtering, aggregating, or joining datasets.

Depending on how you choose to [source](/mirror/sources/supported-sources) your data, you might find that you run into 1 of 2 challenges:

1. **You only care about a few contracts**

   Rather than fill up your database with a ton of extra data, you'd rather ***filter*** down your data to a smaller set.
2. **The data is still a bit raw**

   Maybe you'd rather track gwei rounded to the nearest whole number instead of wei.  You're looking to ***map*** data to a different format so you don't have to run this calculation over and over again.

## [External Handler Transforms](/mirror/transforms/external-handlers)

With external handler transforms, you can send data from your Mirror pipeline to an external service via HTTP and return the processed results back into the pipeline. This opens up a world of possibilities by allowing you to bring your own custom logic, programming languages, and external services into the transformation process.

Key Features of External Handler Transforms:

* Send data to external services via HTTP.
* Supports a wide variety of programming languages and external libraries.
* Handle complex processing outside the pipeline and return results in real time.
* Guaranteed at least once delivery and back-pressure control to ensure data integrity.

### How External Handlers work

1. The pipeline sends a POST request to the external handler with a mini-batch of JSON rows.
2. The external handler processes the data and returns the transformed rows in the same format and order as received


# Pricing
Source: https://docs.goldsky.com/pricing/summary

Understand how metered billing works on Goldsky

<Info>
  Our prices are quoted on a monthly basis for simpler presentation, but metered and billed on an hourly basis. This has a few key implications.

  1. To account for the varying number of days in each month of the year, we conservatively estimate that each month has 730 hours. This means that estimated monthly pricing shown is higher than what you would typically pay for the specified usage in most months.

  2. All estimations of the number of subgraphs or Mirror pipelines assume "always-on" capacity. In practice, you can run double the number of subgraph workers or pipeline workers for half the time and pay the same price. This similarly holds for the "entities stored" metric in subgraphs.
</Info>

## Subgraphs

We track usage based on two metrics: (1) the number of active subgraphs, and (2) the amount of data stored across all subgraphs in your project.

### Metering

#### Active Subgraphs

The number of active subgraph workers, tracked hourly. If you pause or delete a subgraph, it is no longer billed.

Examples:

1. If you have 10 active subgraphs, you use **10** *subgraph worker hours* per hour. At 730 hours per month, you  incur **7,300** *subgraph worker hours*.
2. If you begin a period with 10 active subgraphs and delete all of them halfway through the period, you are billed the equivalent of 5 subgraphs for that period.

#### Subgraph Entities Stored

The number of entities stored across all subgraphs in your project, tracked hourly. If you delete a subgraph, stored entities are no longer tracked. All entities in a project count toward the project's usage on a cumulative basis.

Examples:

1. If you have 3 active subgraphs that cumulatively store 30,000 entities, you use **30,000** *subgraph entity storage hours* per hour.

   At 730 hours per month, you incur `30,000 * 730 = 21,900,000` *subgraph entity storage hours* in that month.

2. If you begin a period with 3 active subgraphs, each with 10,000 entities, and you delete 2 of them after 10 days, you use **30,000** *entity subgraph hours* for the first 10 days, then **10,000** *entity subgraph hours* thereafter.

### Starter Plan

#### Active Subgraphs

Up to 3 active subgraphs per month.

#### Subgraph Storage

Up to 100,000 entities stored per month.

You incur usage for each hour that each subgraph in your project is deployed and active. If you have 2 subgraphs deployed and active for 2 hours each, you will accumulate 4 hours of usage.

When you exceed Starter Plan usage limits, subgraph indexing will be paused, but subgraphs will remain queryable.

### Scale Plan

| Active Subgraphs <br />(subgraph worker-hours) |                                                         |
| ---------------------------------------------- | ------------------------------------------------------- |
| First 2,250 worker-hours                       | Free (i.e., 3 always-on subgraphs)                      |
| Above 2,250 worker-hours                       | \$0.05/hour (i.e., \~\$36.50/month/additional subgraph) |

| Subgraph Storage <br />(subgraph entity storage-hours)                   |                                                                                               |
| ------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------- |
| First 100k entities stored <br /> (i.e., up to 75,000,000 storage-hours) | Free                                                                                          |
| Up to 10M entities stored <br /> (i.e., up to 7.5B storage-hours)        | \~\$4.00/month per 100k entities stored <br /> (i.e., \$0.0053 per 100k entities stored/hour) |
| Above 10M entities stored <br /> (i.e., >7.5B storage-hours)             | \~\$1.05/month per 100k entities stored <br /> (i.e., \$0.0014 per 100k entities stored/hour) |

## Mirror

### Metering

#### Active Pipeline Workers

The number of active workers, billed hourly. Pipeline resources can have multiple parallel workers, and each worker incurs usage separately.

| Resource Size   | Workers |
| --------------- | ------- |
| small (default) | 1       |
| medium          | 4       |
| large           | 10      |
| x-large         | 20      |
| xx-large        | 40      |

If you have one small pipeline and one large pipeline each deployed for 2 hours, you will accumulate `1*2*1 + 1*2*10 = 2 + 20 = 22` hours of usage.

Note: Pipelines that use a single subgraph as a source, and webhooks or GraphQL APIs as sink(s), are not metered as pipelines. However, you still accumulate hourly subgraph usage.

Examples:

1. If you have **1** small pipeline, you use **1** *pipeline worker-hour* every hour. At 730 hours in the average month, you would incur **730** *pipeline worker-hours* for that month.
2. If you start with **10** small pipelines in a billing period and delete all of them halfway through the billing period, you are charged the equivalent of 5 pipeline workers for the full billing period.
3. If you have **2** large pipelines, you will be using **20** *pipeline worker-hours* every hour, equating to **14,600** *pipeline worker-hours* if you run them the entire month.

#### Pipeline Event Writes

The number of records written by pipelines in your project. For example, for a PostgreSQL sink, every row created, updated, or deleted, counts as a write. For a Kafka sink, every message counts as write.

Examples:

1. If you have a pipeline that writes **20,000** records per day for 10 days, and then **20** records per day for 10 days, you will be using **200,200** pipeline event writes.
2. If you have two pipelines that each write 1 million events in one month, then you are not charged for the first one million events, but you are charged \$1 for the next one million, as per the Starter Plan limits below.

### Starter Plan

#### Active Pipeline Workers

Each billing cycle, you can run 1 small pipeline free of charge (\~730 pipeline worker-hours).

#### Pipeline Event Writes

You can write up to 1 million events to a sink, cumulatively across all pipelines, per billing cycle.

When you exceed Starter Plan limits, pipelines will be paused, but pipelines will remain queryable.

### Scale Plan

You will incur usage for each hour that each pipeline in your project is deployed and active.

Note: The pipeline `resource size` maps to the underlying VM size and acts as a multiplier on hourly usage.

| Active Pipelines <br />(pipeline worker-hours) |                                                |
| ---------------------------------------------- | ---------------------------------------------- |
| First 750 worker-hours                         | Free (i.e., 1 always-on pipeline worker/month) |
| 751+ worker-hours                              | \$0.10 (i.e., \$73.00/month per worker)        |

| Pipeline Throughput <br />(pipeline events written) |                           |
| --------------------------------------------------- | ------------------------- |
| First 1M events written                             | Free                      |
| Up to 100M events written                           | \$1.00 per 100,000 events |
| Above 100M events written                           | \$0.10 per 100,000 events |


# Role-based access control
Source: https://docs.goldsky.com/rbac

Use RBAC to determine who can do what on your Goldsky project

## Overview

Goldsky supports Role Based Access Control (RBAC) to help you restrict what actions can be taken by different members of the team.

We support 4 different roles: `Owner`, `Admin`, `Editor` and `Viewer`. The permissions are listed below:

* `Owner`
  * Can do everything an `Admin` can do
  * Can add other `Owner`s to the project
  * Can remove other `Owner`s from the project
  * Can update the role of teammates to `Owner`
  * Can change the subscription and billing information of the project
* `Admin`
  * Can do everything an `Editor` can do
  * Can invite non-`Owner` teammates to a project
  * Can remove non-`Owner` teammates from a project
  * Can update the role of non-`Owner` teammates on a project
* `Editor`
  * Can do everything a `Viewer` can do
  * Can create, update and delete API keys
  * Can create, update and delete subgraphs
  * Can create, update and delete pipelines
  * Can create, update and delete secrets
  * Can create, update and delete webhooks
  * Can edit the name of a project
* `Viewer`
  * Can view subgraphs
  * Can view pipelines
  * Can view secrets
  * Can view webhooks
  * Can view metrics
  * Can view teammates
  * Can leave a project
  * Can create new projects

## Using the Webapp

### Adding a teammate to your project

When adding a teammate you will be prompted to select the desired role for the new teammate(s). The default selected role is `Viewer`

### Changing the role of teammates

You must be an `Admin` to change the role of your teammate(s).

To manage the RBAC settings for the team members of a given project, select the project and navigate to the [Settings](https://app.goldsky.com/dashboard/settings#team) menu.
Click on the overflow menu and click on `Update Role`

## Using the Command Line

### Adding a teammate to your project

Use the `--role` flag of `goldsky project users invite` to select which role the invited users will have. The default role is `Viewer`.

```
goldsky project users invite --emails "<user1email>" "<user2email>" (passing as many emails as you want) --role <role>
```

### Changing the role of teammates

Use the `--role` flag of `goldsky project users update` to change the role a user defined by the `--email`

```
goldsky project users update --email "<userToRemoveEmail>" --role <role>
```


# CLI Reference
Source: https://docs.goldsky.com/reference/cli

Goldsky's command line interface reference

{/*}
  This file is generated. Do not modify.

  To update the file:
  1. Navigate to the goldsky-io/goldsky monorepo
  2. cd packages/cli && pnpm docs:reference:generate
  3. Use the cli-reference.md content
  {*/}

```
goldsky <cmd> args
```

How to use:

```
goldsky <cmd> args

Commands:
  goldsky            Get started with Goldsky  [default]
  goldsky login      Log in to Goldsky to enable authenticated CLI commands
  goldsky logout     Log out of Goldsky on this computer
  goldsky subgraph   Commands related to subgraphs
  goldsky project    Commands related to project management
  goldsky pipeline   Commands related to Goldsky pipelines
  goldsky dataset    Commands related to Goldsky datasets
  goldsky indexed    Analyze blockchain data with indexed.xyz
  goldsky secret     Commands related to secret management
  goldsky telemetry  Commands related to CLI telemetry

Options:
      --token    CLI Auth Token  [string] [default: ""]
      --color    Colorize output  [boolean] [default: true]
  -v, --version  Show version number  [boolean]
  -h, --help     Show help  [boolean]

```

## login

```
goldsky login
```

How to use:

```
goldsky login

Log in to Goldsky to enable authenticated CLI commands

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

## logout

```
goldsky logout
```

How to use:

```
goldsky logout

Log out of Goldsky on this computer

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

## subgraph

```
goldsky subgraph
```

How to use:

```
goldsky subgraph

Commands related to subgraphs

Commands:
  goldsky subgraph deploy <nameAndVersion>  Deploy a subgraph to Goldsky
  goldsky subgraph list [nameAndVersion]    View deployed subgraphs and tags
  goldsky subgraph delete <nameAndVersion>  Delete a subgraph from Goldsky
  goldsky subgraph tag                      Commands related to tags
  goldsky subgraph webhook                  Commands related to webhooks
  goldsky subgraph log <nameAndVersion>     Tail a subgraph's logs
  goldsky subgraph pause <nameAndVersion>   Pause a subgraph
  goldsky subgraph start <nameAndVersion>   Start a subgraph
  goldsky subgraph update <nameAndVersion>  Update a subgraph
  goldsky subgraph init [nameAndVersion]    Initialize a new subgraph project with basic scaffolding

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### subgraph deploy

```
goldsky subgraph deploy <nameAndVersion>
```

How to use:

```
goldsky subgraph deploy <nameAndVersion>

Deploy a subgraph to Goldsky

Positionals:
  nameAndVersion  Name and version of the subgraph, e.g. 'my-subgraph/1.0.0'  [string] [required]

Options:
      --token                 CLI Auth Token  [string] [default: ""]
      --color                 Colorize output  [boolean] [default: true]
      --path                  Path to subgraph  [string]
      --description           Description/notes for the subgraph  [string]
      --from-ipfs-hash        IPFS hash of a publicly deployed subgraph  [string]
      --ipfs-gateway          IPFS gateway to use if downloading the subgraph from IPFS  [string] [default: "https://ipfs.network.thegraph.com"]
      --from-abi              Generate a subgraph from an ABI  [string]
      --from-url              GraphQL endpoint for a publicly deployed subgraph  [string]
      --remove-graft          Remove grafts from the subgraph prior to deployment  [boolean] [default: false]
      --start-block           Change start block of your subgraph prior to deployment. If used in conjunction with --graft-from, this will be the graft block as well.  [number]
      --graft-from            Graft from the latest block of an existing subgraph in the format <name>/<version>  [string]
      --enable-call-handlers  Generate a subgraph from an ABI with call handlers enabled. Only meaningful when used with --from-abi  [boolean] [default: false]
      --tag                   Tag the subgraph after deployment, comma separated for multiple tags  [string]
  -h, --help                  Show help  [boolean]

```

### subgraph list

```
goldsky subgraph list [nameAndVersion]
```

How to use:

```
goldsky subgraph list [nameAndVersion]

View deployed subgraphs and tags

Positionals:
  nameAndVersion  Name and version of the subgraph, e.g. 'my-subgraph/1.0.0'  [string]

Options:
      --token    CLI Auth Token  [string] [default: ""]
      --color    Colorize output  [boolean] [default: true]
      --filter   Limit results to just tags or deployments  [choices: "tags", "deployments"]
      --summary  Summarize subgraphs & versions without all their details  [boolean] [default: false]
  -h, --help     Show help  [boolean]

```

### subgraph delete

```
goldsky subgraph delete <nameAndVersion>
```

How to use:

```
goldsky subgraph delete <nameAndVersion>

Delete a subgraph from Goldsky

Positionals:
  nameAndVersion  Name and version of the subgraph, e.g. 'my-subgraph/1.0.0'  [string] [required]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -f, --force  Force the deletion without prompting for confirmation  [boolean] [default: false]
  -h, --help   Show help  [boolean]

```

### subgraph tag

```
goldsky subgraph tag
```

How to use:

```
goldsky subgraph tag

Commands related to tags

Commands:
  goldsky subgraph tag create <nameAndVersion>  Create a new tag
  goldsky subgraph tag delete <nameAndVersion>  Delete a tag

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

#### subgraph tag create

```
goldsky subgraph tag create <nameAndVersion>
```

How to use:

```
goldsky subgraph tag create <nameAndVersion>

Create a new tag

Positionals:
  nameAndVersion  Name and version of the subgraph, e.g. 'my-subgraph/1.0.0'  [string] [required]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -t, --tag    The name of the tag  [string] [required]
  -h, --help   Show help  [boolean]

```

#### subgraph tag delete

```
goldsky subgraph tag delete <nameAndVersion>
```

How to use:

```
goldsky subgraph tag delete <nameAndVersion>

Delete a tag

Positionals:
  nameAndVersion  Name and version of the subgraph, e.g. 'my-subgraph/1.0.0'  [string] [required]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -t, --tag    The name of the tag to delete  [string] [required]
  -f, --force  Force the deletion without prompting for confirmation  [boolean] [default: false]
  -h, --help   Show help  [boolean]

```

### subgraph webhook

```
goldsky subgraph webhook
```

How to use:

```
goldsky subgraph webhook

Commands related to webhooks

Commands:
  goldsky subgraph webhook create <nameAndVersion>         Create a webhook
  goldsky subgraph webhook delete [webhook-name]           Delete a webhook
  goldsky subgraph webhook list                            List webhooks
  goldsky subgraph webhook list-entities <nameAndVersion>  List possible webhook entities for a subgraph

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

#### subgraph webhook create

```
goldsky subgraph webhook create <nameAndVersion>
```

How to use:

```
goldsky subgraph webhook create <nameAndVersion>

Create a webhook

Positionals:
  nameAndVersion  Name and version of the subgraph, e.g. 'my-subgraph/1.0.0'  [string] [required]

Options:
      --token           CLI Auth Token  [string] [default: ""]
      --color           Colorize output  [boolean] [default: true]
      --name            Name of the webhook, must be unique  [string] [required]
      --url             URL to send events to  [string] [required]
      --entity          Subgraph entity to send events for  [string] [required]
      --num-retries     Number of times to retry sending an event  [number] [default: 10]
      --retry-interval  Number of seconds to wait between retries  [number] [default: 60]
      --retry-timeout   Number of seconds to wait for a response before retrying  [number] [default: 30]
      --secret          The secret you will receive with each webhook request Goldsky sends  [string]
  -h, --help            Show help  [boolean]

```

#### subgraph webhook delete

```
goldsky subgraph webhook delete [webhook-name]
```

How to use:

```
goldsky subgraph webhook delete [webhook-name]

Delete a webhook

Positionals:
  webhook-name  Name of the webhook to delete  [string]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
      --name   Name of the webhook to delete  [deprecated: Please use the positional argument <webhook-name> instead.] [string]
  -f, --force  Force the deletion without prompting for confirmation  [boolean] [default: false]
  -h, --help   Show help  [boolean]

```

#### subgraph webhook list

```
goldsky subgraph webhook list
```

How to use:

```
goldsky subgraph webhook list

List webhooks

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

#### subgraph webhook list-entities

```
goldsky subgraph webhook list-entities <nameAndVersion>
```

How to use:

```
goldsky subgraph webhook list-entities <nameAndVersion>

List possible webhook entities for a subgraph

Positionals:
  nameAndVersion  Name and version of the subgraph, e.g. 'my-subgraph/1.0.0'  [string] [required]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### subgraph log

```
goldsky subgraph log <nameAndVersion>
```

How to use:

```
goldsky subgraph log <nameAndVersion>

Tail a subgraph's logs

Positionals:
  nameAndVersion  Name and version of the subgraph, e.g. 'my-subgraph/1.0.0'  [string] [required]

Options:
      --token     CLI Auth Token  [string] [default: ""]
      --color     Colorize output  [boolean] [default: true]
      --since     Return logs newer than a relative duration like now, 5s, 2m, or 3h  [default: "1m"]
      --format    The format used to output logs, use text or json for easier parsed output, use pretty for more readable console output  [choices: "pretty", "json", "text"] [default: "text"]
      --filter    The minimum log level to output  [choices: "error", "warn", "info", "debug"] [default: "info"]
      --levels    The explicit comma separated log levels to include (error, warn, info, debug)
      --interval  The time in seconds to wait between checking for new logs  [number] [default: 5]
  -h, --help      Show help  [boolean]

```

### subgraph pause

```
goldsky subgraph pause <nameAndVersion>
```

How to use:

```
goldsky subgraph pause <nameAndVersion>

Pause a subgraph

Positionals:
  nameAndVersion  Name and version of the subgraph, e.g. 'my-subgraph/1.0.0'  [string] [required]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### subgraph start

```
goldsky subgraph start <nameAndVersion>
```

How to use:

```
goldsky subgraph start <nameAndVersion>

Start a subgraph

Positionals:
  nameAndVersion  Name and version of the subgraph, e.g. 'my-subgraph/1.0.0'  [string] [required]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### subgraph update

```
goldsky subgraph update <nameAndVersion>
```

How to use:

```
goldsky subgraph update <nameAndVersion>

Update a subgraph

Positionals:
  nameAndVersion  Name and version of the subgraph, e.g. 'my-subgraph/1.0.0'  [string] [required]

Options:
      --token             CLI Auth Token  [string] [default: ""]
      --color             Colorize output  [boolean] [default: true]
      --public-endpoint   Toggle public endpoint for the subgraph  [string] [choices: "enabled", "disabled"]
      --private-endpoint  Toggle private endpoint for the subgraph  [string] [choices: "enabled", "disabled"]
      --description       Description/notes for the subgraph  [string]
  -h, --help              Show help  [boolean]

```

### subgraph init

```
goldsky subgraph init [nameAndVersion]
```

How to use:

```
goldsky subgraph init [nameAndVersion]

Initialize a new subgraph project with basic scaffolding

Positionals:
  nameAndVersion  Name and version of the subgraph, e.g. 'my-subgraph/1.0.0'  [string]

Options:
      --token            CLI Auth Token  [string] [default: ""]
      --color            Colorize output  [boolean] [default: true]
      --target-path      Target path to write subgraph files to  [string]
      --force            Overwrite existing files at the target path  [boolean] [default: false]
      --from-config      Path to instant subgraph JSON configuration file  [string]
      --abi              ABI source(s) for contract(s)  [string]
      --contract         Contract address(es) to watch for events  [string]
      --contract-events  Event names to index for the contract(s)  [string]
      --contract-calls   Call names to index for the contract(s)  [string]
      --network          Network(s) to use for contract(s)                        reference our docs for supported subgraph networks:      https://docs.goldsky.com/chains/supported-networks  [string]
      --contract-name    Name of the contract(s)  [string]
      --start-block      Block to start at for a contract on a specific network  [string]
      --description      Subgraph description  [string]
      --call-handlers    Enable call handlers for the subgraph  [boolean]
      --build            Build the subgraph after writing files  [boolean]
      --deploy           Deploy the subgraph after build  [boolean]
  -h, --help             Show help  [boolean]

```

## project

```
goldsky project
```

How to use:

```
goldsky project

Commands related to project management

Commands:
  goldsky project users   Commands related to the users of a project
  goldsky project leave   Leave a project
  goldsky project list    List all of the projects you belong to
  goldsky project update  Update a project
  goldsky project create  Create a project

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### project users

```
goldsky project users
```

How to use:

```
goldsky project users

Commands related to the users of a project

Commands:
  goldsky project users list    List all users for this project
  goldsky project users invite  Invite a user to your project
  goldsky project users remove  Remove a user from your project
  goldsky project users update  Update a user's project permissions

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

#### project users list

```
goldsky project users list
```

How to use:

```
goldsky project users list

List all users for this project

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

#### project users invite

```
goldsky project users invite
```

How to use:

```
goldsky project users invite

Invite a user to your project

Options:
      --token   CLI Auth Token  [string] [default: ""]
      --color   Colorize output  [boolean] [default: true]
      --emails  emails of users to invite  [array] [required]
      --role    desired role of invited user(s)  [string] [required] [choices: "Owner", "Admin", "Editor", "Viewer"] [default: "Viewer"]
  -h, --help    Show help  [boolean]

```

#### project users remove

```
goldsky project users remove
```

How to use:

```
goldsky project users remove

Remove a user from your project

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
      --email  email of user to remove  [string] [required]
  -h, --help   Show help  [boolean]

```

#### project users update

```
goldsky project users update
```

How to use:

```
goldsky project users update

Update a user's project permissions

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
      --email  email of user to remove  [string] [required]
      --role   role of user to update  [string] [required] [choices: "Owner", "Admin", "Editor", "Viewer"]
  -h, --help   Show help  [boolean]

```

### project leave

```
goldsky project leave
```

How to use:

```
goldsky project leave

Leave a project

Options:
      --token      CLI Auth Token  [string] [default: ""]
      --color      Colorize output  [boolean] [default: true]
      --projectId  the ID of the project you want to leave  [string] [required]
  -h, --help       Show help  [boolean]

```

### project list

```
goldsky project list
```

How to use:

```
goldsky project list

List all of the projects you belong to

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### project update

```
goldsky project update
```

How to use:

```
goldsky project update

Update a project

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
      --name   the new name of the project  [string] [required]
  -h, --help   Show help  [boolean]

```

### project create

```
goldsky project create
```

How to use:

```
goldsky project create

Create a project

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
      --name   the name of the new project  [string] [required]
  -h, --help   Show help  [boolean]

```

## pipeline

```
goldsky pipeline
```

How to use:

```
goldsky pipeline

Commands related to Goldsky pipelines

Commands:
  goldsky pipeline get <nameOrConfigPath>                    Get a pipeline
  goldsky pipeline export [name]                             Export pipeline configurations
  goldsky pipeline apply <config-path>                       Apply the provided pipeline yaml config. This command creates the pipeline if it doesn't exist or updates the existing pipeline. This command is idempotent.
  goldsky pipeline get-definition <name>                     [deprecated] Get a shareable pipeline definition. Use "pipeline get <name> --definition" instead.
  goldsky pipeline create <name>                             Create a pipeline
  goldsky pipeline update <name>                             [deprecated] Update a pipeline. Use "pipeline apply" instead.
  goldsky pipeline delete <nameOrConfigPath>                 Delete a pipeline
  goldsky pipeline list                                      List all pipelines
  goldsky pipeline monitor <nameOrConfigPath>                Monitor a pipeline runtime
  goldsky pipeline pause <nameOrConfigPath>                  Pause a pipeline
  goldsky pipeline start <nameOrConfigPath>                  Start a pipeline
  goldsky pipeline stop <nameOrConfigPath>                   Stop a pipeline
  goldsky pipeline info <nameOrConfigPath>                   Display pipeline information
  goldsky pipeline resize <nameOrConfigPath> <resourceSize>  Resize a pipeline
  goldsky pipeline validate [config-path]                    Validate a pipeline definition or config.
  goldsky pipeline cancel-update <nameOrConfigPath>          Cancel in-flight update request
  goldsky pipeline restart <nameOrConfigPath>                Restart a pipeline. Useful in scenarios where pipeline needs to be restarted without any configuration changes.
  goldsky pipeline snapshots                                 Commands related to snapshots

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### pipeline get

```
goldsky pipeline get <nameOrConfigPath>
```

How to use:

```
goldsky pipeline get <nameOrConfigPath>

Get a pipeline

Positionals:
  nameOrConfigPath  pipeline name or config file path  [string] [required]

Options:
      --token                   CLI Auth Token  [string] [default: ""]
      --color                   Colorize output  [boolean] [default: true]
      --outputFormat, --output  format of the output. Either json or table. Defaults to json.  [deprecated] [string] [choices: "json", "table", "yaml"] [default: "yaml"]
      --definition              print the pipeline's definition only (sources, transforms, sinks)  [boolean]
  -v, --version                 pipeline version. Returns latest version of the pipeline if not set.  [string]
  -h, --help                    Show help  [boolean]

```

### pipeline export

```
goldsky pipeline export [name]
```

How to use:

```
goldsky pipeline export [name]

Export pipeline configurations

Positionals:
  name  pipeline name  [string]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
      --all    Export pipeline configurations for all available pipelines  [boolean]
  -h, --help   Show help  [boolean]

```

### pipeline apply

```
goldsky pipeline apply <config-path>
```

How to use:

```
goldsky pipeline apply <config-path>

Apply the provided pipeline yaml config. This command creates the pipeline if it doesn't exist or updates the existing pipeline. This command is idempotent.

Positionals:
  config-path  path to the yaml pipeline config file.  [string] [required]

Options:
      --token                      CLI Auth Token  [string] [default: ""]
      --color                      Colorize output  [boolean] [default: true]
      --from-snapshot              Snapshot that will be used to start the pipeline. Applicable values are: 'last', 'new', 'none' or a snapshot-id. 'last' uses latest available snapshot. 'new' creates a new snapshot to use. 'none': does not use any snapshot aka starts from scratch. Including the option without any argument will start an interactive mode to select from a list of available snapshots. Defaults to 'new'  [string]
      --save-progress              Attempt a snapshot of the pipeline before applying the update. Only applies if the pipeline already has status: ACTIVE and is running without issues. Defaults to saving progress unless pipeline is being updated to status=INACTIVE.  [deprecated: Use '--from-snapshot'] [boolean]
      --skip-transform-validation  skips the validation of the transforms when updating the pipeline. Defaults to false  [boolean]
      --skip-validation            skips the validation of the transforms when updating the pipeline. Defaults to false  [deprecated] [boolean]
      --use-latest-snapshot        attempts to use the latest available snapshot.  [deprecated: Use '--from-snapshot'] [boolean]
      --status                     Status of the pipeline  [string] [choices: "ACTIVE", "INACTIVE", "PAUSED"]
  -h, --help                       Show help  [boolean]

```

### pipeline get-definition

```
goldsky pipeline get-definition <name>
```

How to use:

```
goldsky pipeline get-definition <name>

[deprecated] Get a shareable pipeline definition. Use "pipeline get <name> --definition" instead.

Positionals:
  name  pipeline name  [string] [required]

Options:
      --token                   CLI Auth Token  [string] [default: ""]
      --color                   Colorize output  [boolean] [default: true]
      --outputFormat, --output  format of the output. Either json or yaml. Defaults to yaml.  [deprecated] [string] [choices: "json", "yaml"] [default: "yaml"]
  -h, --help                    Show help  [boolean]

```

### pipeline create

```
goldsky pipeline create <name>
```

How to use:

```
goldsky pipeline create <name>

Create a pipeline

Positionals:
  name  name of the new pipeline  [string] [required]

Options:
      --token                          CLI Auth Token  [string] [default: ""]
      --color                          Colorize output  [boolean] [default: true]
      --output, --outputFormat         format of the output. Either json or table. Defaults to table.  [string] [choices: "json", "table", "yaml"] [default: "yaml"]
      --resource-size, --resourceSize  runtime resource size for when the pipeline runs  [deprecated: Use 'pipeline resize'] [string] [required] [choices: "s", "m", "l", "xl", "xxl", "mem.l", "mem.xl", "mem.xxl"] [default: "s"]
      --skip-transform-validation      skips the validation of the transforms when creating the pipeline.  [boolean]
      --description                    the description of the new pipeline  [deprecated: Use 'pipeline apply'] [string]
      --definition                     definition of the pipeline that includes sources, transforms, sinks. Provided as json eg: `{sources: [], transforms: [], sinks:[]}`  [deprecated: Use 'pipeline apply'] [string]
      --definition-path                path to a json/yaml file with the definition of the pipeline that includes sources, transforms, sinks.  [deprecated: Use 'pipeline apply'] [string]
      --status                         the desired status of the pipeline  [deprecated: Use 'pipeline start/stop/pause'] [string] [choices: "ACTIVE", "INACTIVE"] [default: "ACTIVE"]
      --use-dedicated-ip               Whether the pipeline should use dedicated egress IPs  [boolean] [required] [default: false]
  -h, --help                           Show help  [boolean]

```

### pipeline update

```
goldsky pipeline update <name>
```

How to use:

```
goldsky pipeline update <name>

[deprecated] Update a pipeline. Use "pipeline apply" instead.

Positionals:
  name  name of the pipeline to update.  [string] [required]

Options:
      --token                          CLI Auth Token  [string] [default: ""]
      --color                          Colorize output  [boolean] [default: true]
      --outputFormat, --output         format of the output. Either json or table. Defaults to json.  [deprecated] [string] [required] [choices: "json", "table", "yaml"] [default: "yaml"]
      --resource-size, --resourceSize  runtime resource size for when the pipeline runs  [string] [choices: "s", "m", "l", "xl", "xxl", "mem.l", "mem.xl", "mem.xxl"]
      --status                         status of the pipeline  [string] [choices: "ACTIVE", "INACTIVE", "PAUSED"]
      --save-progress                  takes a snapshot of the pipeline before applying the update. Only applies if the pipeline already has status: ACTIVE. Defaults to saving progress unless pipeline is being updated to status=INACTIVE.  [boolean]
      --skip-transform-validation      skips the validation of the transforms when updating the pipeline.  [boolean]
      --use-latest-snapshot            attempts to use the latest available snapshot.  [boolean]
      --definition                     definition of the pipeline that includes sources, transforms, sinks. Provided as json eg: `{sources: [], transforms: [], sinks:[]}`  [string]
      --definition-path                path to a json/yaml file with the definition of the pipeline that includes sources, transforms, sinks.  [string]
      --description                    description of the pipeline`  [string]
  -h, --help                           Show help  [boolean]

```

### pipeline delete

```
goldsky pipeline delete <nameOrConfigPath>
```

How to use:

```
goldsky pipeline delete <nameOrConfigPath>

Delete a pipeline

Positionals:
  nameOrConfigPath  pipeline name or config file path  [string] [required]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -f, --force  Force the deletion without prompting for confirmation  [boolean] [default: false]
  -h, --help   Show help  [boolean]

```

### pipeline list

```
goldsky pipeline list
```

How to use:

```
goldsky pipeline list

List all pipelines

Options:
      --token                    CLI Auth Token  [string] [default: ""]
      --color                    Colorize output  [boolean] [default: true]
      --output, --outputFormat   format of the output. Either json or table. Defaults to json.  [string] [choices: "json", "table", "yaml"] [default: "table"]
      --outputVerbosity          Either summary or all. Defaults to summary.  [string] [choices: "summary", "usablewithapplycmd", "all"] [default: "summary"]
      --include-runtime-details  includes runtime details for each pipeline like runtime status and errors. Defaults to false.  [boolean] [default: false]
  -h, --help                     Show help  [boolean]

```

### pipeline monitor

```
goldsky pipeline monitor <nameOrConfigPath>
```

How to use:

```
goldsky pipeline monitor <nameOrConfigPath>

Monitor a pipeline runtime

Positionals:
  nameOrConfigPath  pipeline name or config file path  [string] [required]

Options:
      --token                          CLI Auth Token  [string] [default: ""]
      --color                          Colorize output  [boolean] [default: true]
      --update-request                 monitor update request  [boolean]
      --max-refreshes, --maxRefreshes  max. number of data refreshes.  [number]
  -v, --version                        pipeline version, uses latest version if not set.  [string]
  -h, --help                           Show help  [boolean]

```

### pipeline pause

```
goldsky pipeline pause <nameOrConfigPath>
```

How to use:

```
goldsky pipeline pause <nameOrConfigPath>

Pause a pipeline

Positionals:
  nameOrConfigPath  pipeline name or config file path  [string] [required]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### pipeline start

```
goldsky pipeline start <nameOrConfigPath>
```

How to use:

```
goldsky pipeline start <nameOrConfigPath>

Start a pipeline

Positionals:
  nameOrConfigPath  pipeline name or config path  [string] [required]

Options:
      --token                CLI Auth Token  [string] [default: ""]
      --color                Colorize output  [boolean] [default: true]
      --use-latest-snapshot  attempts to use the latest available snapshot.  [deprecated: Use '--from-snapshot'] [boolean]
      --from-snapshot        Snapshot that will be used to start the pipeline. Applicable values are: 'last', 'new', 'none' or a snapshot-id. 'last' uses latest available snapshot. 'new' creates a new snapshot to use. 'none': does not use any snapshot aka starts from scratch. Including the option without any argument will start an interactive mode to select from a list of available snapshots. Defaults to 'new'  [string]
  -h, --help                 Show help  [boolean]

```

### pipeline stop

```
goldsky pipeline stop <nameOrConfigPath>
```

How to use:

```
goldsky pipeline stop <nameOrConfigPath>

Stop a pipeline

Positionals:
  nameOrConfigPath  pipeline name or config file path  [string] [required]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### pipeline info

```
goldsky pipeline info <nameOrConfigPath>
```

How to use:

```
goldsky pipeline info <nameOrConfigPath>

Display pipeline information

Positionals:
  nameOrConfigPath  pipeline name or config file path  [string] [required]

Options:
      --token    CLI Auth Token  [string] [default: ""]
      --color    Colorize output  [boolean] [default: true]
  -v, --version  pipeline version. Returns latest version of the pipeline if not set.  [string]
  -h, --help     Show help  [boolean]

```

### pipeline resize

```
goldsky pipeline resize <nameOrConfigPath> <resourceSize>
```

How to use:

```
goldsky pipeline resize <nameOrConfigPath> <resourceSize>

Resize a pipeline

Positionals:
  nameOrConfigPath             pipeline name or config file path  [string] [required]
  resource-size, resourceSize  runtime resource size  [string] [choices: "s", "m", "l", "xl", "xxl", "mem.l", "mem.xl", "mem.xxl"] [default: "s"]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### pipeline validate

```
goldsky pipeline validate [config-path]
```

How to use:

```
goldsky pipeline validate [config-path]

Validate a pipeline definition or config.

Positionals:
  config-path  path to the yaml pipeline config file.  [string]

Options:
      --token            CLI Auth Token  [string] [default: ""]
      --color            Colorize output  [boolean] [default: true]
      --definition       definition of the pipeline that includes sources, transforms, sinks. Provided as json eg: `{sources: [], transforms: [], sinks:[]}`  [deprecated: use config-path positional instead.] [string]
      --definition-path  path to a json/yaml file with the definition of the pipeline that includes sources, transforms, sinks.  [deprecated: use config-path positional instead.] [string]
  -h, --help             Show help  [boolean]

```

### pipeline cancel-update

```
goldsky pipeline cancel-update <nameOrConfigPath>
```

How to use:

```
goldsky pipeline cancel-update <nameOrConfigPath>

Cancel in-flight update request

Positionals:
  nameOrConfigPath  pipeline name or config file path  [string] [required]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### pipeline restart

```
goldsky pipeline restart <nameOrConfigPath>
```

How to use:

```
goldsky pipeline restart <nameOrConfigPath>

Restart a pipeline. Useful in scenarios where pipeline needs to be restarted without any configuration changes.

Positionals:
  nameOrConfigPath  pipeline name or config path  [string] [required]

Options:
      --token               CLI Auth Token  [string] [default: ""]
      --color               Colorize output  [boolean] [default: true]
      --from-snapshot       Snapshot that will be used to start the pipeline. Applicable values are: 'last', 'new', 'none' or a snapshot-id. 'last' uses latest available snapshot. 'new' creates a new snapshot to use. 'none': does not use any snapshot aka starts from scratch. Including the option without any argument will start an interactive mode to select from a list of available snapshots. Defaults to 'new'  [string] [required]
      --disable-monitoring  Disables monitoring after the command is run. Defaults to false.  [boolean] [default: false]
  -h, --help                Show help  [boolean]

```

### pipeline snapshots

```
goldsky pipeline snapshots
```

How to use:

```
goldsky pipeline snapshots

Commands related to snapshots

Commands:
  goldsky pipeline snapshots list <nameOrConfigPath>    List snapshots in a pipeline
  goldsky pipeline snapshots create <nameOrConfigPath>  Attempts to take a snapshot of the pipeline

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

#### pipeline snapshots list

```
goldsky pipeline snapshots list <nameOrConfigPath>
```

How to use:

```
goldsky pipeline snapshots list <nameOrConfigPath>

List snapshots in a pipeline

Positionals:
  nameOrConfigPath  pipeline name or config file path  [string] [required]

Options:
      --token    CLI Auth Token  [string] [default: ""]
      --color    Colorize output  [boolean] [default: true]
  -v, --version  pipeline version. Returns snapshots across all versions if not set.  [string]
  -h, --help     Show help  [boolean]

```

#### pipeline snapshots create

```
goldsky pipeline snapshots create <nameOrConfigPath>
```

How to use:

```
goldsky pipeline snapshots create <nameOrConfigPath>

Attempts to take a snapshot of the pipeline

Positionals:
  nameOrConfigPath  pipeline name or config file path  [string] [required]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

## dataset

```
goldsky dataset
```

How to use:

```
goldsky dataset

Commands related to Goldsky datasets

Commands:
  goldsky dataset get <name>  Get a dataset
  goldsky dataset list        List datasets

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### dataset get

```
goldsky dataset get <name>
```

How to use:

```
goldsky dataset get <name>

Get a dataset

Positionals:
  name  dataset name  [string] [required]

Options:
      --token         CLI Auth Token  [string] [default: ""]
      --color         Colorize output  [boolean] [default: true]
      --outputFormat  the output format. Either json or yaml. Defaults to yaml  [string]
  -v, --version       dataset version  [string]
  -h, --help          Show help  [boolean]

```

### dataset list

```
goldsky dataset list
```

How to use:

```
goldsky dataset list

List datasets

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

## indexed

```
goldsky indexed
```

How to use:

```
goldsky indexed

Analyze blockchain data with indexed.xyz

Commands:
  goldsky indexed sync  Commands related to syncing indexed.xyz real-time raw & decoded crypto datasets

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### indexed sync

```
goldsky indexed sync
```

How to use:

```
goldsky indexed sync

Commands related to syncing indexed.xyz real-time raw & decoded crypto datasets

Commands:
  goldsky indexed sync decoded-logs      Sync decoded logs for a smart contract from a network to this computer
  goldsky indexed sync raw-blocks        Sync all blocks from a network
  goldsky indexed sync raw-logs          Sync all logs from a network
  goldsky indexed sync raw-transactions  Sync all transactions from a network

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

#### indexed sync decoded-logs

```
goldsky indexed sync decoded-logs
```

How to use:

```
goldsky indexed sync decoded-logs

Sync decoded logs for a smart contract from a network to this computer

Options:
      --token             CLI Auth Token  [string] [default: ""]
      --color             Colorize output  [boolean] [default: true]
      --contract-address  The contract address you are interested in  [string] [default: ""]
      --network           The network of indexed.xyz data to synchronize  [string] [default: "ethereum"]
  -h, --help              Show help  [boolean]

```

#### indexed sync raw-blocks

```
goldsky indexed sync raw-blocks
```

How to use:

```
goldsky indexed sync raw-blocks

Sync all blocks from a network

Options:
      --token    CLI Auth Token  [string] [default: ""]
      --color    Colorize output  [boolean] [default: true]
      --network  The network of indexed.xyz data to synchronize  [string] [default: "ethereum"]
  -h, --help     Show help  [boolean]

```

#### indexed sync raw-logs

```
goldsky indexed sync raw-logs
```

How to use:

```
goldsky indexed sync raw-logs

Sync all logs from a network

Options:
      --token             CLI Auth Token  [string] [default: ""]
      --color             Colorize output  [boolean] [default: true]
      --contract-address  The contract address you are interested in  [string] [default: ""]
      --network           The network of indexed.xyz data to synchronize  [string] [default: "ethereum"]
  -h, --help              Show help  [boolean]

```

#### indexed sync raw-transactions

```
goldsky indexed sync raw-transactions
```

How to use:

```
goldsky indexed sync raw-transactions

Sync all transactions from a network

Options:
      --token    CLI Auth Token  [string] [default: ""]
      --color    Colorize output  [boolean] [default: true]
      --network  The network of indexed.xyz data to synchronize  [string] [default: "ethereum"]
  -h, --help     Show help  [boolean]

```

## secret

```
goldsky secret
```

How to use:

```
goldsky secret

Commands related to secret management

Commands:
  goldsky secret create         create a secret
  goldsky secret list           list all secrets
  goldsky secret reveal <name>  reveal a secret
  goldsky secret update <name>  update a secret
  goldsky secret delete <name>  delete a secret

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### secret create

```
goldsky secret create
```

How to use:

```
goldsky secret create

create a secret

Options:
      --token        CLI Auth Token  [string] [default: ""]
      --color        Colorize output  [boolean] [default: true]
      --name         the name of the new secret  [string]
      --value        the value of the new secret in json  [string]
      --description  the description of the new secret  [string]
  -h, --help         Show help  [boolean]

```

### secret list

```
goldsky secret list
```

How to use:

```
goldsky secret list

list all secrets

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### secret reveal

```
goldsky secret reveal <name>
```

How to use:

```
goldsky secret reveal <name>

reveal a secret

Positionals:
  name  the name of the secret  [string] [required]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### secret update

```
goldsky secret update <name>
```

How to use:

```
goldsky secret update <name>

update a secret

Positionals:
  name  the name of the secret  [string] [required]

Options:
      --token        CLI Auth Token  [string] [default: ""]
      --color        Colorize output  [boolean] [default: true]
      --value        the new value of the secret  [string]
      --description  the new description of the secret  [string]
  -h, --help         Show help  [boolean]

```

### secret delete

```
goldsky secret delete <name>
```

How to use:

```
goldsky secret delete <name>

delete a secret

Positionals:
  name  the name of the secret to delete  [string] [required]

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -f, --force  Force the deletion without prompting for confirmation  [boolean] [default: false]
  -h, --help   Show help  [boolean]

```

## telemetry

```
goldsky telemetry
```

How to use:

```
goldsky telemetry

Commands related to CLI telemetry

Commands:
  goldsky telemetry status   Display the CLI telemetry status
  goldsky telemetry enable   Enable anonymous CLI telemetry
  goldsky telemetry disable  Disable anonymous CLI telemetry

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### telemetry status

```
goldsky telemetry status
```

How to use:

```
goldsky telemetry status

Display the CLI telemetry status

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### telemetry enable

```
goldsky telemetry enable
```

How to use:

```
goldsky telemetry enable

Enable anonymous CLI telemetry

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```

### telemetry disable

```
goldsky telemetry disable
```

How to use:

```
goldsky telemetry disable

Disable anonymous CLI telemetry

Options:
      --token  CLI Auth Token  [string] [default: ""]
      --color  Colorize output  [boolean] [default: true]
  -h, --help   Show help  [boolean]

```


# Instant subgraph configuration
Source: https://docs.goldsky.com/reference/config-file/instant-subgraph



## Configuration schemas

Currently there is only a single configuration schema, [version 1](#version-1). This configuration file is required for [instant / no-code subgraphs](/subgraphs/guides/create-a-no-code-subgraph).

### Version 1

* **\[REQUIRED]** `version` (`string`, must be `"1"`) - The version of the configuration schema.
* ***\[OPTIONAL]*** `name` (`string`) - The name of the subgraph.
* **\[REQUIRED]** `abis` (map of `object`) - A map of ABI names to ABI source configurations.
  * **\[REQUIRED]** `path` (`string`) - The path to the ABI source, relative to the configuration file.
* **\[REQUIRED]** `instances` (array of `object`) - A list of data source or data template instances to index.
* ***\[OPTIONAL]*** `enableCallHandlers` (`boolean`) - Whether to enable call handler indexing for the subgraph

<Tip>
  *Note that `abis` also supports inline ABI definitions, either as the raw ABI array or as the JSON string.*
</Tip>

#### Data source instance

Data sources are instances derived from a single contract address.

* **\[REQUIRED]** `abi` (`string`) - The name of the ABI source.
* **\[REQUIRED]** `address` (`string`) - The contract address to index.
* **\[REQUIRED]** `startBlock` (`number`) - The block to start indexing from.
* **\[REQUIRED]** `chain` (`string`) - The chain to index on.
* ***\[OPTIONAL]*** `enrich` (`object`) - An object containing enrichment configurations.

#### Data template instance

Data templates are instances derived from an event emitted by a contract. The event signature must include an address parameter that contains the contract address that will be indexed.

* **\[REQUIRED]** `abi` (`string`) - The name of the ABI data template instance (e.g., the pool).
* **\[REQUIRED]** `source` (`object`) - The source event details to create a new data template instance.
  * **\[REQUIRED]** `abi` (`string`) - The name of the ABI data template source (e.g., the factory).
  * **\[REQUIRED]** `eventSignature` (`string`) - The event signature to listen for.
  * **\[REQUIRED]** `addressParam` (`string`) - The parameter to extract the contract address from.
* ***\[OPTIONAL]*** `enrich` (`object`) - An object containing enrichment configurations.

#### Instance enrichment

Enrichments allow data source and template instances to be enriched by performing eth calls and mapping the outputs to one or more fields and/or entities.

* ***\[OPTIONAL]*** `options` (`object`) - enrichment options.
  * ***\[OPTIONAL]*** `debugging` (`boolean`) - Flag to emit debugging logs.
  * ***\[OPTIONAL]*** `imports` (array of `string`) - List of additional imports to include in the generated mapping file. You only need to include additional imports if you are using those types within your configuration.
* **\[REQUIRED]** `handlers` (map of `object`) - A map of trigger signatures to enrichment handler configurations (signature must be defined in the instance abi).
  * ***\[OPTIONAL]*** `calls` (map of `object`) - A map of call reference names to eth call configurations. This can be omitted if mapping expressions do not require any eth calls.
  * **\[REQUIRED]** `entities` (map of `object`) - A map of entity names to entity configurations.

#### Enrichment call configuration

Enrichment call configurations capture all information required to perform an eth call within the context of an existing event or call handler mapping function.

* ***\[OPTIONAL]*** `abi` (`string`) - The name of the abi defining the call to perform (if omitted then we'll use the instance abi).
* ***\[OPTIONAL]*** `source` (`string`) - The contract address source [expression](#enrichment-expressions) to use for the call (if omitted then we'll use the current instance source).
* **\[REQUIRED]** `name` (`string`) - The name of the eth call to perform. Note that this must be the exact name as defined in the ABI. The eth call invoker will actually call the `try_<name>` function to safely handle a potential revert and prevent any errors in the subgraph due to an invalid eth call. If the eth call is required then the subgraph will result in an error state.
* ***\[OPTIONAL]*** `params` (`string`) - The parameter [expression](#enrichment-expressions) to use when performing the eth call (this can be omitted if the eth call requires no parameters, and must include all parameters separated by commas otherwise). e.g., `"event.params.owner, event.params.tokenId"`.
* ***\[OPTIONAL]*** `depends_on` (array of `string`) - List of call reference names that this call depends on (this should be used if a parameter is derived from a previously defined call).
* ***\[OPTIONAL]*** `required` (`boolean`) - Flag to indicate that the call must succeed for the enrichment to take place (if the call does not succeed then the enrichment is aborted and no enrichment entity mapping will take place).
* ***\[OPTIONAL]*** `declared` (`boolean`) - Flag to indicate that the call should be marked as declared, meaning that the call will be executed and the result cached prior to the mapping handler function being invoked.
* ***\[OPTIONAL]*** `conditions` (`object`) - Optional condition [expressions](#enrichment-expressions) to test before and after performing the call (if either condition fails then the enrichment is aborted and no enrichment entity mapping will take place).
  * ***\[OPTIONAL]*** `pre` (`string`) - The condition to test before performing the call.
  * ***\[OPTIONAL]*** `post` (`string`) - The condition to test after performing the call.

#### Enrichment entity configuration

Enrichement entity configurations are a map of field name and type to field value expressions. The configuration supports both a simplified single configuration and a multi-instance configuration. The single configuration is most likely all that is needed for most use cases, but if the need arises to describe an enriched entity where multiple instances are created within a single mapping (think of creating the same entity with different ids for the same event or call handler), then we can describe the entity as an array of configurations where each also includes an `id` expression for determining the unique `id` suffix.

* An entity field mapping key looks like `<field_name> <field_type>`, e.g., `tokenId uint256`
  * the field name can be any valid GraphQL schema field identifier, typically this would either be a *camelCase* or *snake\_case* string
  * the field type can be any valid ABI type name
* An entity field mapping value is an [expression](#enrichment-expressions), e.g., `calls.owner.toHexString()`
  * it must return a value of the type specified in the field mapping key (i.e., `address` must be converted to `string` using `.toHexString()`)

When configuring an entity for multiple instances, the configuration takes the following form

* **\[REQUIRED]** `id` (`string`) - The [expression](#enrichment-expressions) to determine the unique id suffix for the entity instance.
* ***\[OPTIONAL]*** `explicit_id` (`boolean`) - Flag to indicate that the id expression should be used as the explicit id for the entity instance (if omitted then the `id` expression will be appended to the parent entity `id`).
* **\[REQUIRED]** `mapping` (map of `object`) - A map of field name and type to field value expressions (as described above).

#### Enrichment expressions

Enrichment expressions are AssemblyScript expressions that can be used to produce static or dynamic values based on the available runtime context. The expression runtime context includes the `event` object (or the `call` object for call handlers), the (parent) `entity` object, and the `calls` object reference to all previously executed eth calls. Expressions can include any combination of string concatenation, type transformation, math result, or logical branching, meaning that there is a lot of customization available to the configuration when declaring an expression. Note however that static expressions may often be the most appropriate for simple enrichments.

Below each of the runtime context elements are described in more detail:

* `event` and `call` - The incoming event/call object to the mapping handler function. The parameters to this object will already be converted to the entity fields, one for each parameter defined in the corresponding ABI file.
* `entity` - The parent entity object to the mapping handler function, this entity will have already been saved before enrichment begins.
* `calls` - The object containing all previously executed eth calls. This object is used to reference the results of previous calls in the current call configuration. Calls not yet executed can still be referenced but they will be `null` until the call is invoked. Any calls that are marked `required` (or marked as a dependency of another call) will throw an error if accessed before the call is invoked.

## Explanation of common patterns

### Single source pattern

```json5
{
  "version": "1",
  "name": "TokenDeployed",
  "abis": {
    "TokenRegistry": {
      "path": "./path/to/your/abi.json"
    }
  },
  "instances": [
    {
      "abi": "TokenRegistry",
      "address": "0x...",
      "startBlock": 13983724,
      "chain": "your_chain"
    }
  ]
}
```

* `"version": "1"`: The version of this config, we only support a value of "1" right now.
* `"name": "TokenDeployed"`: The name of the event you want to track as specified in the ABI file.
* `"abis": { "TokenRegistry": { "path": "./path/to/your/abi.json" } }`: Mapping of ABIs names (can be anything you want) to ABI files.
* `"abi": "TokenRegistry"`: The ABI you want to track. This name must match a key in the `abis` object above.
* `"address": "0x...",`: The address of the contract.
* `"startBlock": 13983724`: The block from which you want your subgraph to start indexing (in most cases, this is the block that deployed your contract)
* `"chain": "your_chain"`: The chain you want to track this contract on

### Factory pattern

Some contracts create other child contracts, which then emit events that you need to track. The configuration here can handle that by allowing you specify a `source` inside an `instance` entry. The `source` tells the indexer which Factory contract event creates a new contract, and the address of the new contract as inferred from the event argument.

```json5
{
  "version": "1",
  "name": "TokenDeployed",
  "abis": {
    "Factory": {
      "path": "./abis/factory.json"
    },
    "Pool": {
      "path": "./abis/pool.json"
    }
  },
  "instances": [
    {
      "abi": "Factory",
      "address": "0xa98242820EBF3a405D265CCd22A4Ea8F64AFb281",
      "startBlock": 16748800,
      "chain": "bsc"
    },
    {
      "abi": "Pool",
      "source": {
        "abi": "Factory",
        "eventSignature": "PoolCreated(address,address,bool)",
        "addressParam": "pool"
      }
    }
  ]
}
```

* `"Factory": { "path": "./abis/factory.json" }`: The path to the ABI for the Factory contract
* `"Pool": { "path": "./abis/pool.json"` }: The path the ABI for the contract deployed by the Factory contract
* `{ "abi": "Pool" }`: This is the main difference between the configuration for factory vs non-factory applications. In this example, the Factory contract makes new Pool contracts and the below configuration specifies that with this `source` object.
* `"source": { "abi": "Factory" }`: The ABI name which creates this contract.
* `"eventSignature": "PoolCreated(address,address,bool)",`: This is the signature of the event from the Factory contract which indicates that this contract was created.
* `"addressParam": "pool"`: The name of the parameter from the Factory contract's event that contains the new address to track.

In this pattern, there is a defined factory contract that makes many pools, and each pool needs to be tracked. We have two ABIs and the last `instance` entry looks for any `PoolCreated` event in the `Factory` ABI, gets a parameter from it, and uses that as a data source to watch for future `Pool` events in the `Pool` ABI.

### Enrichment pattern

```json5
{
  "version": "1",
  "name": "TokenDeployed",
  "abis": {
    "TokenRegistry": {
      "path": "./path/to/your/abi.json"
    }
  },
  "instances": [
    {
      "abi": "TokenRegistry",
      "address": "0x...",
      "startBlock": 13983724,
      "chain": "your_chain"
      "enrich": {
        "Minted(address)": {
          "calls": {
            "balance": {
              "name": "balanceOf",
              "params": "event.params.owner"
            },
          },
          "entities": {
            "Balance": {
              "owner address": "event.params.owner.toHexString()",
              "balance uint256": "calls.balance"
            }
          }
        }
      }
    }
  ]
}
```

* `"Minted(address)"`: the event signature (as defined in the `TokenRegistry` ABI) to perform the enrichment within.
* `"balance"`: the name of the call reference.
* `"name": "balanceOf"`: the name of the eth call to perform.
* `"params": "event.params.owner"`: the parameter to pass to the `balanceOf` eth call. `event` represents the incoming event object to the `Minted(address)` mapping handler function.
* `"Balance"`: the new enrichment entity name to create.
* `"owner address"`: the first field name and type for the entity. In this case we would see `Balance.owner` defined as a `String` in the generated schema because the `address` type serializes to a `String`.
* `"event.params.owner.toHexString()"`: the expression to determine the value for the `owner` field. `event` represents the incoming event object to the `Minted(address)` mapping handler function. Since `event.params.owner` is an `address` type, we need to convert it to a `String` using the `.toHexString()` method.
* `"balance uint256"`: the second field name and type for the entity. In this case we would see `Balance.balance` defined as a `BigInt` in the generated schema.
* `"calls.balance"`: the expression to determine the value for the `balance` field. `calls` represents the object containing all previously executed eth calls and `balance` refers to our call reference name.

## Examples

### Multi-chain

This example shows how to define multiple chains with many addresses.

```json
{
  "name": "TokenDeployed",
  "abis": {
    "TokenRegistry": {
      "path": "./abis/tokenRegistryAbi.json"
    }
  },
  "instances": [
    {
      "abi": "TokenRegistry",
      "address": "0x0A6f564C5c9BeBD66F1595f1B51D1F3de6Ef3b79",
      "startBlock": 13983724,
      "chain": "mainnet"
    },
    {
      "abi": "TokenRegistry",
      "address": "0x2d6775C1673d4cE55e1f827A0D53e62C43d1F304",
      "startBlock": 13718798,
      "chain": "avalanche"
    },
    {
      "abi": "TokenRegistry",
      "address": "0x10B84C73001745D969e7056D7ca474ce1D959FE8",
      "startBlock": 59533,
      "chain": "evmos"
    },
    {
      "abi": "TokenRegistry",
      "address": "0xa7E4Fea3c1468D6C1A3A77e21e6e43Daed855C1b",
      "startBlock": 171256,
      "chain": "moonbeam"
    },
    {
      "abi": "TokenRegistry",
      "address": "0x19d4b0F5871913c714554Bbb457F2a1549f52E04",
      "startBlock": 1356181,
      "chain": "milkomedac1"
    }
  ]
}
```

This configuration results in multiple deployed subgraphs, each with an identical GraphQL schema for you to fetch data. If you prefer a combined view of the data across all deployed subgraphs, please have a look at [cross-chain subgraphs](/subgraphs/guides/create-a-multi-chain-subgraph).

### Nouns enrichment with balances on transfer

```json5
{
  "version": "1",
  "name": "nouns/1.0.0",
  "abis": {
    "nouns": {
      "path": "./abis/nouns.json"
    }
  },
  "instances": [
    {
      "abi": "nouns",
      "address": "0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03",
      "startBlock": 12985438,
      "chain": "mainnet",
      "enrich": {
        "handlers": {
          "Transfer(indexed address,indexed address,indexed uint256)": {
            "calls": {
              "nouns_balance": {
                "name": "balanceOf",
                "params": "event.params.to"
              }
            },
            "entities": {
              "EnrichmentBalance": {
                "tokenId uint256": "event.params.tokenId",
                "previousOwner address": "event.params.from.toHexString()",
                "owner address": "event.params.to.toHexString()",
                "nouns uint256": "calls.nouns_balance"
              }
            }
          }
        }
      }
    }
  ]
}
```

This configuration will create a new `EnrichmentBalance` entity that contains a `nouns` balance field for each `Transfer` event that occurs on the `nouns` contract. `Transfer` entities will automatically define an `enrichmentBalances` field that will yield an array of enrichment balances for each transfer event. Similarly, all `EnrichmentBalance` entities will define a `transfer` field that will yield the `Transfer` entity that triggered the enrichment. Below is an example GraphQL query to fetch transfers and enrichment balances in various ways.

```graphql
query NounsTransfersAndBalancesDemo {
  enrichmentBalances(first:1, orderBy:timestamp_, orderDirection:desc) {
    id
    timestamp_
    tokenId
    previousOwner
    owner
    nouns
    transfer {
      id
      transactionHash_
    }
  }
  transfers(first:1, orderBy:timestamp_, orderDirection:desc) {
    id
    transactionHash_
    timestamp_
    tokenId
    from
    to
    enrichmentBalances {
      id
      nouns
    }
  }
}
```


# Mirror Pipeline Configuration Schema
Source: https://docs.goldsky.com/reference/config-file/pipeline

Schema details for pipeline configurations

<Note>
  We recently released v3 of pipeline configurations which uses a more intuitive
  and user friendly format to define and configure pipelines using a yaml file.
  For backward compatibility purposes, we will still support the previous v2
  format. This is why you will find references to each format in each yaml file
  presented across the documentation. Feel free to use whichever is more
  comfortable for you but we encourage you to start migrating to v3 format.
</Note>

This page includes info on the full Pipeline configuration schema. For conceuptal learning about Pipelines, please refer to the [about Pipeline](/mirror/about-pipeline) page.

<ParamField path="name" type="string" required>
  Name of the pipeline. Must only contain lowercase letters, numbers, hyphens
  and should be less than 50 characters.
</ParamField>

<ParamField path="sources" type="object" required>
  [Sources](/reference/config-file#sources) represent origin of the data into the pipeline.

  Supported source types:

  * [Subgraph Entities](/reference/config-file/pipeline#subgraphentity)
  * [Datasets](/reference/config-file/pipeline#dataset)
</ParamField>

<ParamField path="transforms" type="object" required>
  [Transforms](/reference/config-file#transforms) represent data transformation logic to be applied to either a source and/or transform in the pipeline.
  If your pipeline does not need to transform data, this attribute can be an empty object.

  Supported transform types:

  * [SQL](/reference/config-file/pipeline#sql)
  * [Handler](/reference/config-file/pipeline#handler)
</ParamField>

<ParamField path="sinks" type="object" required>
  [Sinks](/reference/config-file#sinks) represent destination for source and/or transform data out of the pipeline.

  Supported sink types:

  * [PostgreSQL](/reference/config-file/pipeline#postgresql)
  * [Clickhouse](/reference/config-file/pipeline#clickhouse)
  * [MySQL](/reference/config-file/pipeline#mysql)
  * [Elastic Search](/reference/config-file/pipeline#elasticsearch)
  * [Open Search](/reference/config-file/pipeline#opensearch)
  * [Kafka](/reference/config-file/pipeline#kafka)
  * [File](/reference/config-file/pipeline#file)
  * [SQS](/reference/config-file/pipeline#sqs)
  * [DynamoDb](/reference/config-file/pipeline#dynamodb)
  * [Webhook](/reference/config-file/pipeline#webhook)
</ParamField>

<ParamField path="resource_size" type="string" optional>
  It defines the amount of compute power to add to the pipeline. It can take one
  of the following values: "s", "m", "l", "xl", "xxl". For new pipeline
  creation, it defaults to "s". For updates, it defaults to the current
  resource\_size of the pipeline.
</ParamField>

<ParamField path="description" type="string" optional>
  Description of the pipeline.
</ParamField>

## Sources

Represents the origin of the data into the pipeline. Each source has a unique name to be used as a reference in transforms/sinks.

<Tabs>
  <Tab title="v3 name">
    `sources.<key_name>` is used as the referenceable name in other transforms and sinks.
  </Tab>

  <Tab title="v2 name (deprecated)">
    `definition.sources[idx].referenceName` is used as the referenceable name in other transforms and sinks.
  </Tab>
</Tabs>

### Subgraph Entity

Use your [subgraph](/mirror/sources/subgraphs) as a source for your pipeline.

#### Example

<Tabs>
  <Tab title="v3">
    In the sources section of your pipeline configuration, you can add a `subgraph_entity` per subgraph entity that you want to use.

    <Tabs>
      <Tab title="example.yaml">
        ```yaml
        sources:
          subgraph_account:
              type: subgraph_entity
              name: account
              subgraphs:
              - name: qidao-optimism
                version: 1.1.0
          subgraph_market_daily_snapshot:
              type: subgraph_entity
              name: market_daily_snapshot
              subgraphs:
              - name: qidao-optimism
                version: 1.1.0
        ```
      </Tab>
    </Tabs>
  </Tab>

  <Tab title="v2 (deprecated)">
    In the sources section of your pipeline definition, you can add a `subgraphEntity` per subgraph entity that you want to use.

    <Tabs>
      <Tab title="example-v2.yaml">
        ```yaml
        sources:
          - type: subgraphEntity
            # The deployment IDs you gathered above. If you put multiple,
            # they must have the same schema
            deployments:
              - id: QmPuXT3poo1T4rS6agZfT51ZZkiN3zQr6n5F2o1v9dRnnr
            # A name, referred to later in the `sourceStreamName` of a transformation or sink
            referenceName: account
            entity:
              # The name of the entities
              name: account
          - type: subgraphEntity
            deployments:
              - id: QmPuXT3poo1T4rS6agZfT51ZZkiN3zQr6n5F2o1v9dRnnr
            referenceName: market_daily_snapshot
            entity:
              name: market_daily_snapshot
        ```
      </Tab>
    </Tabs>
  </Tab>
</Tabs>

#### Schema

<Tabs>
  <Tab title="v3">
    <ParamField path="sources.<key_name>" type="string" required>
      Unique name of the source. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the source, for Subgraph Entity sources, it is always `subgraph_entity`.
    </ParamField>

    <ParamField path="description" type="string" optional>
      Description of the source
    </ParamField>

    <ParamField path="name" type="string" required>
      Entity `name` in your subgraph.
    </ParamField>

    <ParamField path="start_at" type="string" optional>
      `earliest` processes data from the first block.

      `latest` processes data from the latest block at pipeline start time.

      Defaults to `latest`
    </ParamField>

    <ParamField path="filter" type="string" optional>
      Filter expression that does a [fast scan](/reference/config-file/pipeline#fast-scan) on the dataset. Only useful when `start_at` is set to `earliest`.

      Expression follows the SQL standard for what comes after the WHERE clause. Few examples:

      ```yaml
      address = '0x21552aeb494579c772a601f655e9b3c514fda960'
      address = '0xb794f5ea0ba39494ce839613ff2qasdf34353dga' OR address = '0x21552aeb494579c772a601f655e9b3c514fda960'
      address = '0xb794f5ea0ba39494ce839613ff2qasdf34353dga' AND amount > 500
      ```
    </ParamField>

    <ParamField path="subgraphs" type="subgraphReference[]" required>
      References deployed subgraphs(s) that have the entity mentioned in the `name` attribute.

      ```yaml
      subgraphs:
        - name: polymarket
          version: 1.0.0
      ```

      Supports subgraphs deployed across multiple chains aka cross-chain usecase.

      ```yaml
      subgraphs:
        - name: polymarket
          version: 1.0.0
        - name: base
          version: 1.1.0
      ```

      [Cross-chain subgraph full example](/mirror/guides/merging-crosschain-subgraphs)
    </ParamField>
  </Tab>

  <Tab title="v2 (deprecated)">
    <ParamField path="referenceName" type="string" required>
      Unique name of the source. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the source, for Subgraph Entity sources, it is always `subgraphEntity`.
    </ParamField>

    <ParamField path="description" type="string" optional>
      Description of the source
    </ParamField>

    <ParamField path="start_at" type="string" optional>
      `earliest` processes data from the first block.

      `latest` processes data from the latest block at pipeline start time.

      Defaults to `latest`
    </ParamField>

    <ParamField path="filter" type="string" optional>
      Filter expression that does a [fast scan](/reference/config-file/pipeline#fast-scan) on the dataset. Only useful when `start_at` is set to `earliest`.

      Expression follows the SQL standard for what comes after the WHERE clause. Few examples:

      ```yaml
      address = '0x21552aeb494579c772a601f655e9b3c514fda960'
      address = '0xb794f5ea0ba39494ce839613ff2qasdf34353dga' OR address = '0x21552aeb494579c772a601f655e9b3c514fda960'
      address = '0xb794f5ea0ba39494ce839613ff2qasdf34353dga' AND amount > 500
      ```
    </ParamField>

    <ParamField path="entity" type="entityReference" required>
      References the entity of the deployed subgraph.

      ```yaml
      entity:
        name: fixed_product_market_maker
      ```
    </ParamField>

    <ParamField path="deployments" type="[{id: string}]" required>
      References deployed subgraphs(s) that have the entity mentioned in the `entity.name` attribute.

      The value for the `id` is the ipfs hash of the subgraph.

      ```yaml
      deployments:
        - id: QmVcgRByfiFSzZfi7RZ21gkJoGKG2jeRA1DrpvCQ6ficNb
      ```

      Supports subgraphs deployed across multiple chains aka cross-chain usecase:

      ```yaml
      deployments:
       - id: QmVcgRByfiFSzZfi7RZ21gkJoGKG2jeRA1DrpvCQ6ficNb
       - id: QmaA9c8QcavxHJ7iZw6om2GHnmisBJFrnRm8E1ihBoAYjX
      ```

      [Cross-chain subgraph full example](/mirror/guides/merging-crosschain-subgraphs)
    </ParamField>
  </Tab>
</Tabs>

### Dataset

Dataset lets you define [Direct Indexing](/mirror/sources/direct-indexing) sources. These data sources are curated by the Goldsky team, with automated QA guaranteeing correctness.

#### Example

<Tabs>
  <Tab title="v3">
    ```yaml
    sources:
      base_logs:
        type: dataset
        dataset_name: base.logs
        version: 1.0.0
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```
    sources:
      - type: dataset
        referenceName: base.logs
        version: 1.0.0
    ```
  </Tab>
</Tabs>

#### Schema

<Tabs>
  <Tab title="v3">
    <ParamField path="sources.<key_name>" type="string" required>
      Unique name of the source. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the source, for Dataset sources, it is always `dataset`
    </ParamField>

    <ParamField path="description" type="string" optional>
      Description of the source
    </ParamField>

    <ParamField path="dataset_name" type="string" required>
      Name of a goldsky dataset. Please use `goldsky dataset list` and select your chain of choice.

      Please refer to [supported chains](/mirror/sources/direct-indexing#supported-chains) for an overview of what data is available for individual chains.
    </ParamField>

    <ParamField path="version" type="string" required>
      Version of the goldsky dataset in `dataset_name`.
    </ParamField>

    <ParamField path="start_at" type="string" optional>
      `earliest` processes data from the first block.

      `latest` processes data from the latest block at pipeline start time.

      Defaults to `latest`
    </ParamField>

    <ParamField path="filter" type="string" optional>
      Filter expression that does a [fast scan](/reference/config-file/pipeline#fast-scan) on the dataset. Only useful when `start_at` is set to `earliest`.

      Expression follows the SQL standard for what comes after the WHERE clause. Few examples:

      ```yaml
      address = '0x21552aeb494579c772a601f655e9b3c514fda960'
      address = '0xb794f5ea0ba39494ce839613ff2qasdf34353dga' OR address = '0x21552aeb494579c772a601f655e9b3c514fda960'
      address = '0xb794f5ea0ba39494ce839613ff2qasdf34353dga' AND amount > 500
      ```
    </ParamField>
  </Tab>

  <Tab title="v2 (deprecated)">
    <ParamField path="referenceName" type="string" required>
      Unique name of the source. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the source, for Dataset sources, it is always `dataset`
    </ParamField>

    <ParamField path="description" type="string" optional>
      Description of the source
    </ParamField>

    <ParamField path="version" type="string" required>
      Version of the goldsky dataset in `dataset_name`.
    </ParamField>

    <ParamField path="start_at" type="string" optional>
      `earliest` processes data from the first block.

      `latest` processes data from the latest block at pipeline start time.

      Defaults to `latest`
    </ParamField>

    <ParamField path="filter" type="string" optional>
      Filter expression that does a [fast scan](/reference/config-file/pipeline#fast-scan) on the dataset. Only useful when `start_at` is set to `earliest`.

      Expression follows the SQL standard for what comes after the WHERE clause. Few examples:

      * `address = '0x21552aeb494579c772a601f655e9b3c514fda960'`
      * `address = '0xb794f5ea0ba39494ce839613ff2qasdf34353dga' OR address = '0x21552aeb494579c772a601f655e9b3c514fda960'`
      * `address = '0xb794f5ea0ba39494ce839613ff2qasdf34353dga' AND amount > 500`
    </ParamField>
  </Tab>
</Tabs>

#### Fast Scan

Processing full datasets (starting from `earliest`) (aka doing a **Backfill**) requires the pipeline to process significant amount of data which affects how quickly it reaches at edge (latest record in the dataset). This is especially true for datasets for larger chains.

However, in many use-cases, pipeline may only be interested in a small-subset of the historical data. In such cases, you can enable **Fast Scan** on your pipeline by defining the `filter` attribute in the `dataset` source.

The filter is pre-applied at the source level; making the initial ingestion of historical data much faster. When defining a `filter` please be sure to use attributes that exist in the dataset. You can get the schema of the dataset by running `goldsky dataset get <dataset_name>`.

See example below where we pre-apply a filter based on contract address:

<Tabs>
  <Tab title="v3">
    ```yaml
    sources:
      base_logs:
        type: dataset
        dataset_name: base.logs
        version: 1.0.0
        filter: address = '0x21552aeb494579c772a601f655e9b3c514fda960'
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```
    sources:
      - type: dataset
        referenceName: base.logs
        version: 1.0.0
        filter: address = '0x21552aeb494579c772a601f655e9b3c514fda960'
    ```
  </Tab>
</Tabs>

## Transforms

Represents data transformation logic to be applied to either a source and/or transform in the pipeline. Each transform has a unique name to be used as a reference in transforms/sinks.

<Tabs>
  <Tab title="v3 name">
    `transforms.<key_name>` is used as the referenceable name in other transforms and sinks.
  </Tab>

  <Tab title="v2 name (deprecated)">
    `definition.transforms[idx].referenceName` is used as the referenceable name in other transforms and sinks.
  </Tab>
</Tabs>

### SQL

SQL query that transforms or filters the data from a `source` or another `transform`.

#### Example

<Tabs>
  <Tab title="v3">
    ```yaml
    transforms:
      negative_fpmm_scaled_liquidity_parameter:
        sql: SELECT id FROM polymarket.fixed_product_market_maker WHERE scaled_liquidity_parameter < 0
        primary_key: id
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```
    transforms:
      - referenceName: negative_fpmm_scaled_liquidity_parameter
        type: sql
        sql: SELECT id FROM polygon.fixed_product_market_maker WHERE scaled_liquidity_parameter < 0
        primaryKey: id
    ```
  </Tab>
</Tabs>

#### Schema

<Tabs>
  <Tab title="v3">
    <ParamField path="transforms.<key_name>" type="string" required>
      Unique name of the transform. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the transform, for SQL transforms it is always `sql`
    </ParamField>

    <ParamField path="sql" type="string" required>
      The SQL query to be executed on either source or transform in the pipeline.

      The source data for sql transform is determined by the `FROM <table_name>` part of the query. Any source or transform can be referenced as SQL table.
    </ParamField>

    <ParamField path="primary_key" type="string" required>
      The primary key for the transformation. If there are any two rows with the same primary\_key, the pipeline will override it with the latest value.
    </ParamField>
  </Tab>

  <Tab title="v2 (deprecated)">
    <ParamField path="referenceName" type="string" required>
      Unique name of the transform. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the transform, for SQL transforms it is always `sql`
    </ParamField>

    <ParamField path="sql" type="string" required>
      The SQL query to be executed on either source or transform in the pipeline.

      The source data for sql transform is determined by the `FROM <table_name>` part of the query. Any source or transform can be referenced as SQL table.
    </ParamField>

    <ParamField path="primaryKey" type="string" required>
      The primary key for the transformation. If there are any two rows with the same primaryKey, the pipeline will override it with the latest value.
    </ParamField>
  </Tab>
</Tabs>

### Handler

Lets you transform data by sending data to a [handler](/mirror/transforms/external-handlers) endpoint.

#### Example

<Tab title="v3">
  ```yaml
  transforms:
    my_external_handler_transform:
      type: handler
      primary_key: id
      url: http://example-url/example-transform-route
      from: ethereum.raw_blocks
  ```
</Tab>

#### Schema

<Tabs>
  <Tab title="v3">
    <ParamField path="transforms.<key_name>" type="string" required>
      Unique name of the transform. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the transform, for Handler transforms it is always `handler`
    </ParamField>

    <ParamField path="url" type="string" required>
      Endpoint to send the data for transformation.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the transform. Reference a source/transform defined in this pipeline.

      Data sent to your handler will have the same schema as this source/transform.
    </ParamField>

    <ParamField path="primary_key" type="string" required>
      The primary key for the transformation. If there are any two rows with the same primary\_key, the pipeline will override it with the latest value.
    </ParamField>

    <ParamField path="payload_columns" type="string[]" required>
      The primary key for the transformation. If there are any two rows with the same primary\_key, the pipeline will override it with the latest value.
    </ParamField>

    <ParamField path="schema_override" type="object">
      Allows overriding the schema of the response data returned by the handler. Default is to expect the same schema as `source|transform` referenced in the `from` attribute.

      A map of column names to Flink SQL datatypes. If the handler response schema changes the pipeline needs to be re-deployed with this attribute updated.

      To add a new attribute: `new_attribute_name: datatype`
      To remove an existing attribute: `existing_attribute_name: null`
      To change an existing attribute's datatype: `existing_attribute_name: datatype`

      <Accordion title="Complete list of supported datatypes">
        | Data Type      | Notes                               |
        | -------------- | ----------------------------------- |
        | STRING         |                                     |
        | BOOLEAN        |                                     |
        | BYTE           |                                     |
        | DECIMAL        | Supports fixed precision and scale. |
        | SMALLINT       |                                     |
        | INTEGER        |                                     |
        | BIGINT         |                                     |
        | FLOAT          |                                     |
        | DOUBLE         |                                     |
        | TIME           | Supports only a precision of 0.     |
        | TIMESTAMP      |                                     |
        | TIMESTAMP\_LTZ |                                     |
        | ARRAY          |                                     |
        | ROW            |                                     |
      </Accordion>
    </ParamField>

    <ParamField path="headers" type="object">
      Headers to be sent in the request from the pipeline to the handler endpoint.

      A common use case is to pass any tokens your server requires for authentication or any metadata.
    </ParamField>

    <ParamField path="secret_name" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the handler.
      For handler transform, use the `httpauth` secret type.
    </ParamField>
  </Tab>

  <Tab title="v2 (deprecated)">
    <ParamField path="referenceName" type="string" required>
      Unique name of the transform. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the transform, for SQL transforms it is always `sql`
    </ParamField>

    <ParamField path="sql" type="string" required>
      The SQL query to be executed on either source or transform in the pipeline.

      The source data for sql transform is determined by the `FROM <table_name>` part of the query. Any source or transform can be referenced as SQL table.
    </ParamField>

    <ParamField path="primaryKey" type="string" required>
      The primary key for the transformation. If there are any two rows with the same primaryKey, the pipeline will override it with the latest value.
    </ParamField>
  </Tab>
</Tabs>

## Sinks

Represents destination for source and/or transform data out of the pipeline. Since sinks represent the end of the dataflow in the pipeline, unlike source and transform, it does not need to be referenced elsewhere in the configuration.

Most sinks are either databases such as `postgresql`, `dynamodb` etc. Or channels such as `kafka`, `sqs` etc.

Also, most sinks are provided by the user, hence the pipeline needs credentials to be able to write data to a sink. Thus, users need to create a Goldsky Secret and reference it in the sink.

### PostgreSQL

Lets you sink data to a [PostgreSQL](/mirror/sinks/postgres) table.

#### Example

<Tabs>
  <Tab title="v3">
    ```yaml
    sinks:
      postgres_test_negative_fpmm_scaled_liquidity_parameter:
        type: postgres
        from: negative_fpmm_scaled_liquidity_parameter
        table: test_negative_fpmm_scaled_liquidity_parameter
        schema: public
        secret_name: API_POSTGRES_CREDENTIALS
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```
    sinks:
      - type: postgres
        sourceStreamName: negative_fpmm_scaled_liquidity_parameter
        referenceName: postgres_test_negative_fpmm_scaled_liquidity_parameter
        table: test_negative_fpmm_scaled_liquidity_parameter
        schema: public
        secretName: API_POSTGRES_CREDENTIALS
    ```
  </Tab>
</Tabs>

#### Schema

<Tabs>
  <Tab title="v3">
    <ParamField path="sinks.<key_name>" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for postgresql it is always `postgressql`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="table" type="string" required>
      The destination table. It will be created if it doesn't exist. Schema is defined in the secret credentials.
    </ParamField>

    <ParamField path="secret_name" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      For postgres sink, use the `jdbc` secret type.
    </ParamField>

    <ParamField path="batch_size" type="integer" optional>
      The number of records the pipeline will send together in a batch. Default `100`
    </ParamField>

    <ParamField path="batch_flush_interval" type="string" optional>
      The maximum time the pipeline will batch records before flushing to sink. Default: '1s'
    </ParamField>

    <ParamField path="scan_autocommit" type="boolean" optional>
      Enables auto commit. Default: `true`
    </ParamField>

    <ParamField path="rewrite_batched_inserts" type="boolean" optional>
      Rewrite individual insert statements into multi-value insert statements. Default `true`
    </ParamField>

    <ParamField path="conditional_upsert_column" type="boolean" optional>
      Optional column that will be used to select the 'correct' row in case of conflict using the 'greater' wins strategy: - ie later date, higher number.
      The column must be numeric.
    </ParamField>
  </Tab>

  <Tab title="v2 (deprecated)">
    <ParamField path="referenceName" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for postgresql it is always `postgressql`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="table" type="string" required>
      The destination table. It will be created if it doesn't exist. Schema is defined in the secret credentials.
    </ParamField>

    <ParamField path="secretName" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      For postgres sink, use the `jdbc` secret type.
    </ParamField>

    <ParamField path="batchSize" type="integer" optional>
      The maximum time (in milliseconds) the pipeline will batch events. Default `100`
    </ParamField>

    <ParamField path="batchFlushInterval" type="string" optional>
      The maximum time the pipeline will batch events before flushing to sink. Default: '1s'
    </ParamField>

    <ParamField path="scanAutocommit" type="boolean" optional>
      Enables auto commit. Default: `true`
    </ParamField>

    <ParamField path="rewriteBatchedInserts" type="boolean" optional>
      Rewrite individual insert statements into multi-value insert statements. Default `true`
    </ParamField>

    <ParamField path="conditionalUpsertColumn" type="boolean" optional>
      Optional column that will be used to select the 'correct' row in case of conflict using the 'greater' wins strategy: - ie later date, higher number.
      The column must be numeric.
    </ParamField>
  </Tab>
</Tabs>

### Clickhouse

Lets you sink data to a [Clickhouse](/mirror/sinks/clickhouse) table.

#### Example

<Tabs>
  <Tab title="v3">
    v3 example
  </Tab>

  <Tab title="v2 (deprecated)">
    to do v2 example
  </Tab>
</Tabs>

#### Schema

<Tabs>
  <Tab title="v3">
    <ParamField path="sinks.<key_name>" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for Clickhouse it is always `clickhouse`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="secret_name" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      For postgres sink, use the `jdbc` secret type.
    </ParamField>

    <ParamField path="table" type="string" required>
      The destination table. It will be created if it doesn't exist. Schema is defined in the secret credentials.
    </ParamField>

    <ParamField path="batch_size" type="integer" optional>
      The maximum time (in milliseconds) the pipeline will batch records. Default `1000`
    </ParamField>

    <ParamField path="batch_flush_interval" type="string" optional>
      The maximum time the pipeline will batch records before flushing to sink. Default: '1s'
    </ParamField>

    <ParamField path="append_only_mode" type="boolean" optional>
      Only do inserts on the table and not update or delete.
      Increases insert speed and reduces Flush exceptions (which happen when too many mutations are queued up).
      More details in the [Clickhouse](/mirror/sinks/clickhouse#append-only-mode) guide. Default `true`.
    </ParamField>

    <ParamField path="version_column_name" type="string" optional>
      Column name to be used as a version number. Only used in `append_only_mode = true`.
    </ParamField>

    <ParamField path="primary_key_override" type="string" optional>
      Use a different primary key than the one that automatically inferred from the source and/or transform.
    </ParamField>

    <ParamField path="schema_override" type="object" optional>
      Ability to override the automatic schema propagation from the pipeline to Clickhouse. Map of `column_name -> clickhouse_datatype`

      Useful in situations when data type is incompatible between the pipeline and Clickhouse. Or when wanting to use specific type for a column.
    </ParamField>
  </Tab>

  <Tab title="v2 (deprecated)">
    <ParamField path="referenceName" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for Clickhouse it is always `clickhouse`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="secretName" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      For postgres sink, use the `jdbc` secret type.
    </ParamField>

    <ParamField path="table" type="string" required>
      The destination table. It will be created if it doesn't exist. Schema is defined in the secret credentials.
    </ParamField>

    <ParamField path="batchSize" type="integer" optional>
      The maximum time (in milliseconds) the pipeline will batch records. Default `1000`
    </ParamField>

    <ParamField path="batchFlushInterval" type="string" optional>
      The maximum time the pipeline will batch records before flushing to sink. Default: '1s'
    </ParamField>

    <ParamField path="appendOnlyMode" type="boolean" optional>
      Only do inserts on the table and not update or delete.
      Increases insert speed and reduces Flush exceptions (which happen when too many mutations are queued up).
      More details in the [Clickhouse](/mirror/sinks/clickhouse#append-only-mode) guide. Default `true`.
    </ParamField>

    <ParamField path="versionColumnName" type="string" optional>
      Column name to be used as a version number. Only used in `append_only_mode = true`.
    </ParamField>

    <ParamField path="primaryKeyOverride" type="string" optional>
      Use a different primary key than the one that automatically inferred from the source and/or transform.
    </ParamField>

    <ParamField path="schemaOverride" type="object" optional>
      Ability to override the automatic schema propagation from the pipeline to Clickhouse. Map of `column_name -> clickhouse_datatype`

      Useful in situations when data type is incompatible between the pipeline and Clickhouse. Or when wanting to use specific type for a column.
    </ParamField>
  </Tab>
</Tabs>

### MySQL

Lets you sink data to a [MySQL](/mirror/sinks/mysql) table.

#### Example

<Tabs>
  <Tab title="v3">
    ```yaml
    sinks:
      postgres_test_negative_fpmm_scaled_liquidity_parameter:
        type: postgres
        from: negative_fpmm_scaled_liquidity_parameter
        table: test_negative_fpmm_scaled_liquidity_parameter
        schema: public
        secret_name: API_POSTGRES_CREDENTIALS
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```
    sinks:
      - type: postgres
        sourceStreamName: negative_fpmm_scaled_liquidity_parameter
        referenceName: postgres_test_negative_fpmm_scaled_liquidity_parameter
        table: test_negative_fpmm_scaled_liquidity_parameter
        schema: public
        secretName: API_POSTGRES_CREDENTIALS
    ```
  </Tab>
</Tabs>

#### Schema

<Tabs>
  <Tab title="v3">
    <ParamField path="sinks.<key_name>" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for postgresql it is always `postgressql`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="schema" type="string" required>
      Database name
    </ParamField>

    <ParamField path="table" type="string" required>
      The destination table. It will be created if it doesn't exist. Schema is defined in the secret credentials.
    </ParamField>

    <ParamField path="secret_name" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      For postgres sink, use the `jdbc` secret type.
    </ParamField>

    <ParamField path="batch_size" type="integer" optional>
      The maximum time (in milliseconds) the pipeline will batch events. Default `100`
    </ParamField>

    <ParamField path="batch_flush_interval" type="string" optional>
      The maximum time the pipeline will batch events before flushing to sink. Default: '1s'
    </ParamField>

    <ParamField path="scan_autocommit" type="boolean" optional>
      Enables auto commit. Default: `true`
    </ParamField>

    <ParamField path="rewrite_batched_inserts" type="boolean" optional>
      Rewrite individual insert statements into multi-value insert statements. Default `true`
    </ParamField>

    <ParamField path="conditional_upsert_column" type="boolean" optional>
      Optional column that will be used to select the 'correct' row in case of conflict using the 'greater' wins strategy: - ie later date, higher number.
      The column must be numeric.
    </ParamField>
  </Tab>

  <Tab title="v2 (deprecated)">
    <ParamField path="sinks.<key_name>" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for postgresql it is always `postgressql`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="schema" type="string" required>
      Database name
    </ParamField>

    <ParamField path="table" type="string" required>
      The destination table. It will be created if it doesn't exist. Schema is defined in the secret credentials.
    </ParamField>

    <ParamField path="secretName" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      For postgres sink, use the `jdbc` secret type.
    </ParamField>

    <ParamField path="batchSize" type="integer" optional>
      The maximum time (in milliseconds) the pipeline will batch events. Default `100`
    </ParamField>

    <ParamField path="batchFlushInterval" type="string" optional>
      The maximum time the pipeline will batch events before flushing to sink. Default: '1s'
    </ParamField>

    <ParamField path="scanAutocommit" type="boolean" optional>
      Enables auto commit. Default: `true`
    </ParamField>

    <ParamField path="rewriteBatchedInserts" type="boolean" optional>
      Rewrite individual insert statements into multi-value insert statements. Default `true`
    </ParamField>

    <ParamField path="conditionalUpsertColumn" type="boolean" optional>
      Optional column that will be used to select the 'correct' row in case of conflict using the 'greater' wins strategy: - ie later date, higher number.
      The column must be numeric.
    </ParamField>
  </Tab>
</Tabs>

### Elastic Search

Lets you sink data to a [Elastic Search](/mirror/sinks/postgres) index.

#### Example

<Tabs>
  <Tab title="v3">
    v3 example
  </Tab>

  <Tab title="v2 (deprecated)">
    v2 example
  </Tab>
</Tabs>

#### Schema

<Tabs>
  <Tab title="v3">
    <ParamField path="sinks.<key_name>" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for Elastic Search it is always `elasticsearch`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="index" type="string" required>
      Elastic search index to write to.
    </ParamField>

    <ParamField path="secretName" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      For Elastic Search sink, use the `elasticSearch` secret type.
    </ParamField>
  </Tab>

  <Tab title="v2 (deprecated)">
    <ParamField path="referenceName" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for Elastic Search it is always `elasticsearch`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="index" type="string" required>
      Elastic search index to write to.
    </ParamField>

    <ParamField path="secretName" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      For Elastic Search sink, use the `elasticSearch` secret type.
    </ParamField>
  </Tab>
</Tabs>

### Open Search

#### Example

<Tabs>
  <Tab title="v3">
    ```yaml
     sinks:
        my_elasticsearch_sink:
          description: Type.Optional(Type.String())
          type: elasticsearch
          from: Type.String()
          index: Type.String()
          secret_name: Type.String()
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```
    sinks:
      - type: elasticsearch
        sourceStreamName: negative_fpmm_scaled_liquidity_parameter
        referenceName: postgres_test_negative_fpmm_scaled_liquidity_parameter
        index: test_negative_fpmm_scaled_liquidity_parameter
        secretName: API_POSTGRES_CREDENTIALS
    ```
  </Tab>
</Tabs>

#### Schema

<Tabs>
  <Tab title="v3">
    <ParamField path="sinks.<key_name>" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for Elastic Search it is always `elasticsearch`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="index" type="string" required>
      Elastic search index to write to.
    </ParamField>

    <ParamField path="secretName" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      For Elastic Search sink, use the `elasticSearch` secret type.
    </ParamField>
  </Tab>

  <Tab title="v2 (deprecated)">
    <ParamField path="referenceName" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for Elastic Search it is always `elasticsearch`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="index" type="string" required>
      Elastic search index to write to.
    </ParamField>

    <ParamField path="secretName" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      For Elastic Search sink, use the `elasticSearch` secret type.
    </ParamField>
  </Tab>
</Tabs>

### Kafka

Lets you sink data to a [Kafka](/mirror/extensions/channels/kafka) topic.

#### Example

<Tabs>
  <Tab title="v3">
    ```yaml
    sinks:
      kafka_topic_sink:
        type: kafka
        from: my_source
        topic: accounts
        secret_name: KAFKA_SINK_SECRET_CR343D
        topic_partitions: 2
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml
    sinks:
      - type: kafka
        sourceStreamName: my_source
        topic: accounts
        secretName: KAFKA_SINK_SECRET_CR343D
        topicPartitions: 2
    ```
  </Tab>
</Tabs>

#### Schema

<Tabs>
  <Tab title="v3">
    <ParamField path="sinks.<key_name>" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for Kafka sink it is always `kafka`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="topic" type="string" required>
      Kafka topic name to write to. Will be created if it does not exist.
    </ParamField>

    <ParamField path="topic_partitions" type="string">
      Number of paritions to be set in the topic. Only applicable if topic does not exists.
    </ParamField>

    <ParamField path="upsert_mode" type="boolean">
      When set to `true`, the sink will emit tombstone messages (null values) for DELETE operations instead of the actual payload. This is useful for maintaining the state in Kafka topics where the latest state of a key is required, and older states should be logically deleted. Default `false`
    </ParamField>

    <ParamField path="format" type="string">
      Format of the record in the topic. Supported types: `json`, `avro`. Requires Schema Registry credentials in the secret for `avro` type.
    </ParamField>

    <ParamField path="secret_name" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      For Kafka sink, use the `kafka` secret type.
    </ParamField>
  </Tab>

  <Tab title="v2 (deprecated)">
    <ParamField path="referenceName" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for Kafka sink it is always `kafka`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="topic" type="string" required>
      Kafka topic name to write to.
    </ParamField>

    <ParamField path="topicPartitions" type="string">
      To be used when creating the topic, in case it does not exist.
    </ParamField>

    <ParamField path="upsertMode" type="boolean">
      When set to `true`, the sink will emit tombstone messages (null values) for DELETE operations instead of the actual payload. This is useful for maintaining the state in Kafka topics where the latest state of a key is required, and older states should be logically deleted. Default `false`
    </ParamField>

    <ParamField path="format" type="string" optional>
      Format of the record in the topic. Supported types: `json`, `avro`. Requires Schema Registry credentials in the secret for `avro` type.

      Default: `avro`
    </ParamField>

    <ParamField path="secretName" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      For Kafka sink, use the `kafka` secret type.
    </ParamField>
  </Tab>
</Tabs>

### File

#### Example

<Tabs>
  <Tab title="v3">
    ```yaml
    sinks:
      s3_write:
        type: file
        path: s3://goldsky/linea/traces/
        format: parquet
        from: linea.traces
        secret_name: GOLDSKY_S3_CREDS
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```
    sinks:
      - type: file
        sourceStreamName: linea.traces
        referenceName: s3_write
        path: s3://goldsky/linea/traces/
        secretName: GOLDSKY_S3_CREDS
    ```
  </Tab>
</Tabs>

#### Schema

<Tabs>
  <Tab title="v3">
    <ParamField path="transforms.<key_name>" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for File sink it is always `file`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="path" type="string" required>
      Path to write to. Use prefix `s3://`. Currently, only `S3` is supported.
    </ParamField>

    <ParamField path="format" type="string" required>
      Format of the output file. Supported types: `parquet`, `csv`.
    </ParamField>

    <ParamField path="auto_compaction" type="boolean">
      Enables auto-compaction which helps optimize the output file size. Default `false`
    </ParamField>

    <ParamField path="partition_columns" type="boolean">
      Columns to be used for partitioning. Multiple columns are comma separated. For eg: `"col1,col2"`
    </ParamField>

    <ParamField path="batch_size" type="string">
      The maximum sink file size before creating a new one. Default: `128MB`
    </ParamField>

    <ParamField path="batch_flush_interval" type="string">
      The maximum time the pipeline will batch records before flushing to sink. Default: `30min`
    </ParamField>
  </Tab>

  <Tab title="v2 (deprecated)">
    <ParamField path="referenceName" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for File sink it is always `file`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="sourceStreamName" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="path" type="string" required>
      Path to write to. Use prefix `s3://`. Currently, only `S3` is supported.
    </ParamField>

    <ParamField path="format" type="string" required>
      Format of the output file. Supported types: `parquet`, `csv`.
    </ParamField>

    <ParamField path="autoCompaction" type="boolean">
      Enables auto-compaction which helps optimize the output file size. Default `false`
    </ParamField>

    <ParamField path="partitionColumns" type="boolean">
      Columns to be used for partitioning. Multiple columns are comma separated. For eg: `"col1,col2"`
    </ParamField>

    <ParamField path="rollingPolicyFileSize" type="string">
      The maximum sink file size before creating a new one. Default: `128MB`
    </ParamField>

    <ParamField path="rollingPolicyRolloverInterval" type="string">
      The maximum time the pipeline will batch records before flushing to sink. Default: `30min`
    </ParamField>
  </Tab>
</Tabs>

### DynamoDB

#### Example

<Tabs>
  <Tab title="v3">
    ```yaml
    sinks:
      postgres_test_negative_fpmm_scaled_liquidity_parameter:
        type: postgres
        from: negative_fpmm_scaled_liquidity_parameter
        table: test_negative_fpmm_scaled_liquidity_parameter
        schema: public
        secret_name: API_POSTGRES_CREDENTIALS
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```
    sinks:
      - type: postgres
        sourceStreamName: negative_fpmm_scaled_liquidity_parameter
        referenceName: postgres_test_negative_fpmm_scaled_liquidity_parameter
        table: test_negative_fpmm_scaled_liquidity_parameter
        schema: public
        secretName: API_POSTGRES_CREDENTIALS
    ```
  </Tab>
</Tabs>

#### Schema

<Tabs>
  <Tab title="v3">
    <ParamField path="sinks.<key_name>" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for Clickhouse it is always `clickhouse`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="secret_name" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      For DynamoDB sink, use the `dynamodb` secret type.
    </ParamField>

    <ParamField path="table" type="string" required>
      The destination table. It will be created if it doesn't exist.
    </ParamField>

    <ParamField path="endpoint" type="integer" optional>
      Endpoint override, useful when writing to a DynamoDB VPC
    </ParamField>

    <ParamField path="request_max_in_flight" type="integer" optional>
      Maximum number of requests in flight. Default `50`
    </ParamField>

    <ParamField path="batch_max_size" type="integer" optional>
      Batch max size. Default: `25`
    </ParamField>

    <ParamField path="request_max_buffered" type="string" optional>
      Maximum number of records to buffer. Default: `10000`
    </ParamField>

    <ParamField path="fail_on_error" type="boolean" optional>
      Fail the sink on write error. Default `false`
    </ParamField>
  </Tab>

  <Tab title="v2 (deprecated)">
    <ParamField path="referenceName" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for Clickhouse it is always `clickhouse`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="secretName" type="string" required>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      For DynamoDB sink, use the `dynamodb` secret type.
    </ParamField>

    <ParamField path="table" type="string" required>
      The destination table. It will be created if it doesn't exist.
    </ParamField>

    <ParamField path="endpoint" type="string" optional>
      Endpoint override, useful when writing to a DynamoDB VPC
    </ParamField>

    <ParamField path="requestMaxInFlight" type="integer" optional>
      Maximum number of requests in flight. Default `50`
    </ParamField>

    <ParamField path="batchMaxSize" type="integer" optional>
      Batch max size. Default: `25`
    </ParamField>

    <ParamField path="requestMaxBuffered" type="string" optional>
      Maximum number of records to buffer. Default: `10000`
    </ParamField>

    <ParamField path="failOnError" type="boolean" optional>
      Fail the sink on write error. Default `false`
    </ParamField>
  </Tab>
</Tabs>

### Webhook

#### Example

<Tabs>
  <Tab title="v3">
    ```yaml
    sinks:
      webhook_publish:
        type: webhook
        from: base.logs
        url: https://webhook.site/d06324e8-d273-45b4-a18b-c4ad69c6e7e6
        secret_name: WEBHOOK_SECRET_CM3UPDBJC0
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```
    sinks:
      - type: webhook
        sourceStreamName: base.logs
        referenceName: webhook_publish
        url: https://webhook.site/d06324e8-d273-45b4-a18b-c4ad69c6e7e6
        secretName: WEBHOOK_SECRET_CM3UPDBJC0
    ```
  </Tab>
</Tabs>

#### Schema

<Tabs>
  <Tab title="v3">
    <ParamField path="sinks.<key_name>" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for Webhook sinks it is always `webhook`
    </ParamField>

    <ParamField path="url" type="string" required>
      Defines the URL to send the record(s) to.
    </ParamField>

    <ParamField path="one_row_per_request" type="boolean" optional>
      Send only one record per call to the provided url
    </ParamField>

    <ParamField path="secret_name" type="boolean" optional>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      Use this if you do not want to expose authenciation details in plain text in the `headers` attribute.

      For webhook sink, use the `httpauth` secret type.
    </ParamField>

    <ParamField path="headers" type="object" optional>
      Headers to be sent in the request from the pipeline to the url
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>
  </Tab>

  <Tab title="v2 (deprecated)">
    <ParamField path="referenceName" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for Webhook sinks it is always `webhook`
    </ParamField>

    <ParamField path="url" type="string" required>
      Defines the URL to send the record(s) to.
    </ParamField>

    <ParamField path="oneRowPerRequest" type="boolean" optional>
      Send only one record per call to the provided url
    </ParamField>

    <ParamField path="secretName" type="boolean" optional>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      Use this if you do not want to expose authenciation details in plain text in the `headers` attribute.

      Use `httpauth` secret type.
    </ParamField>

    <ParamField path="headers" type="object" optional>
      Headers to be sent in the request from the pipeline to the url
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>
  </Tab>
</Tabs>

### SQS

Lets you sink data to a [AWS SQS](/mirror/extensions/channels/aws-sqs) topic.

#### Example

<Tabs>
  <Tab title="v3">
    ```yaml
    sinks:
      my_sqs_sink:
        type: sqs
        url: https://sqs.us-east-1.amazonaws.com/335342423/dev-logs
        secret_name: SQS_SECRET_IAM
        from: my_transform
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml
    sinks:
      - type: sqs
        referenceName: my_sqs_sink
        url: https://sqs.us-east-1.amazonaws.com/335342423/dev-logs
        secretName: SQS_SECRET_IAM
        sourceStreamName: my_transform
    ```
  </Tab>
</Tabs>

#### Schema

<Tabs>
  <Tab title="v3">
    <ParamField path="sinks.<key_name>" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for postgresql it is always `postgressql`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="secret_name" type="boolean" optional>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      Use this if you do not want to expose authenciation details in plain text in the `headers` attribute.

      For sqs sink, use the `sqs` secret type.
    </ParamField>

    <ParamField path="url" type="object" optional>
      SQS topic URL
    </ParamField>

    <ParamField path="fail_on_error" type="boolean" optional>
      Fail the sink on write error. Default `false`
    </ParamField>
  </Tab>

  <Tab title="v2 (deprecated)">
    <ParamField path="referenceName" type="string" required>
      Unique name of the sink. This is a user provided value.
    </ParamField>

    <ParamField path="type" type="string" required>
      Defines the type of the sink, for postgresql it is always `postgressql`
    </ParamField>

    <ParamField path="description" type="string" optional>
      User provided description.
    </ParamField>

    <ParamField path="from" type="string" required>
      Data source for the sink. Reference to either a source or a transform defined in this pipeline.
    </ParamField>

    <ParamField path="secretName" type="boolean" optional>
      Goldksy secret name that contains credentials for calls between the pipeline and the sink.
      Use this if you do not want to expose authenciation details in plain text in the `headers` attribute.

      For sqs sink, use the `sqs` secret type.
    </ParamField>

    <ParamField path="url" type="object" optional>
      SQS topic URL
    </ParamField>

    <ParamField path="failOnError" type="boolean" optional>
      Fail the sink on write error. Default `false`
    </ParamField>
  </Tab>
</Tabs>

## Pipeline runtime attributes

While sources, transforms and sinks define the business logic of your pipeline. There are attributes that change the pipeline execution/runtime.

If you need a refresher on the of pipelines make sure to check out [About Pipeline](/mirror/about-pipeline), here we'll just focus on specific attributes.

Following are request-level attributes that only controls the behavior of a particular request on the pipeline. These attributes should be passed via arguments to the `goldsky pipeline apply <config_file> <arguments/flags>` command.

<ParamField path="status" type="string" optional>
  Defines the desired status for the pipeline which can be one of the three: "ACTIVE", "INACTIVE", "PAUSED". If not provided it will default to the current status of the pipeline.
</ParamField>

<ParamField path="save_progress" type="boolean" optional>
  Defines whether the pipeline should attempt to create a fresh snapshot before this configuration is applied. The pipeline needs to be in a healthy state for snapshot to be created successfully. It defaults to `true`.
</ParamField>

<ParamField path="use_latest_snapshot" type="boolean" optional>
  Defines whether the pipeline should be started from the latest available snapshot. This attribute is useful in restarting scenarios.
  To restart a pipeline from scratch, use `--use_latest_snapshot false`.  It defaults to `true`.
</ParamField>

<ParamField path="restart" type="boolean" optional>
  Instructs the pipeline to restart. Useful in scenarios where the pipeline needs to be restarted but no configuration change is needed. It defaults to `undefined`.
</ParamField>

## Pipeline Runtime Commands

Commands that change the pipeline runtime. Many commands aim to abstract away the above attributes into meaningful actions.

#### Start

There are multiple ways to do this:

* `goldsky pipeline start <name_or_path_to_config_file>`
* `goldsky pipeline apply <name_or_path_to_config_file> --status ACTIVE`

This command will have no effect on pipeline that already has a desired status of `ACTIVE`.

#### Pause

Pause will attempt to take a snapshot and stop the pipeline so that it can be resumed later.

There are multiple ways to do this:

* `goldsky pipeline pause <name_or_path_to_config_file>`
* `goldsky pipeline apply <name_or_path_to_config_file> --status PAUSED`

#### Stop

Stopping a pipeline does not attempt to take a snapshot.

There are multiple ways to do this:

* `goldsky pipeline stop <pipeline_name(if exists) or path_to_config>`
* `goldsky pipeline apply <path_to_config> --status INACTIVE --from-snapshot none`
* `goldsky pipeline apply <path_to_config> --status INACTIVE --save-progress false` (prior to CLI version `11.0.0`)

#### Update

Make any needed changes to the pipeline configuration file and run `goldsky pipeline apply <name_or_path_to_config_file>`.

By default any update on a `RUNNING` pipeline will attempt to take a snapshot before applying the update.

If you'd like to avoid taking snapshot as part of the update, run:

* `goldsky pipeline apply <name_or_path_to_config_file> --from-snapshot last`
* `goldsky pipeline apply <name_or_path_to_config_file> --save-progress false` (prior to CLI version `11.0.0`)

This is useful in a situations where the pipeline is running into issues, hence the snapshot will not succeed, blocking the update that is to fix the issue.

#### Resize

Useful in scenarios where the pipeline is running into resource constraints.

There are multiple ways to do this:

* `goldsky pipeline resize <resource_size>`
* `goldsky pipeline apply <name_or_path_to_config_file>` with the config file having the attribute:

```
resource_size: xl
```

#### Restart

Useful in the scenarios where a restart is needed but there are no changes in the configuration. For example, pipeline sink's database connection got stuck because the database has restarted.

There are multiple ways to restart a RUNNING pipeline without any configuration changes:

1. `goldsky pipeline restart <path_to_config_or_name> --from-snapshot last|none`

The above command will attempt to restart the pipeline.

To restart with no snapshot aka from scratch, provide the `--from-snapshot none` option.
To restart with last available snapshot, provide the `--from-snapshot last` option.

2. `goldsky pipeline apply <path_to_configuration> --restart` (CLI version below 10.0.0)

By default, the above command will will attempt a new snapshot and start the pipeline from that particular snapshot.

To avoid using any existing snapshot or triggering a new one (aka starting from scratch) add the `--from-snapshot none` or `--save-progress false --use-latest-snapshot false` if you are using CLI version older than `11.0.0`.

#### Monitor

Provides pipeline runtime information that is helpful for monitoring/developing a pipeline. Although this command does not change the runtime, it provides info like status, metrics, logs etc. that helps with devleloping a pipeline.

`goldsky pipeline monitor <name_or_path_to_config_file>`


# Event Decoding Functions
Source: https://docs.goldsky.com/reference/mirror-functions/decoding-functions



Mirror provides 3 custom functions which can be in used within transforms to decode raw contract events during processing: `_gs_log_decode`, `_gs_tx_decode` and `_gs_fetch_abi`.

## \_gs\_log\_decode

This function decodes [Raw Logs](/reference/schema/EVM-schemas#raw-logs) data given a json string representing the ABI string. Since it is stateless it will scale very well across different pipeline sizes.

It will automatically use the matching event in the ABI, but partial ABIs that only contain the target event will also work.

```Function Function definition
_gs_log_decode(string abi, string topics, string data)
```

#### Params

<ParamField path="abi" type="string" required>
  A string representing a json array of events and functions
</ParamField>

<ParamField path="topics" type="string" required>
  Name of the column the dataset which contains a string containing the topics, comma separated.
</ParamField>

<ParamField path="data" type="string" required>
  Name of the column the dataset which contains a string containing the payload of the event.
</ParamField>

#### Return

<ResponseField name="ROW ( event_param TEXT[], event_signature TEXT )" type="ROW">
  The function will output a [nested ROW](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/types/#row) type with `event_param::TEXT[]` and `event_signature::TEXT`. If youre planning on using a sink that doesnt support nested ROWs, you may want to do a further transformation to unnest the result.
</ResponseField>

#### Examples

<ResponseField name="Function Call">
  If you were using the [Raw Logs](/reference/schema/EVM-schemas#raw-logs) source dataset, you would call this function passing your ABI and using the 'topics' and 'data' columns. For example:

  ```sql
  SELECT
   _gs_log_decode('[
    {
      "anonymous": false,
      "inputs": [
        {
          "indexed": true,
          "name": "src",
          "type": "address"
        },
        {
          "indexed": true,
          "name": "dst",
          "type": "address"
        },
        {
          "indexed": false,
          "name": "wad",
          "type": "uint256"
        }
      ],
      "name": "Transfer",
      "type": "event"
    }
  ]', `topics`, `data`) as decoded
  from base.raw_logs
  ```

  You would then able to access both topics and data from the decoded column as `decoded.event_params` and `decoded.event_signature` in a second transform. See below for a complete example pipeline.
</ResponseField>

## \_gs\_tx\_decode

This function decodes [Raw Traces](/reference/schema/EVM-schemas#raw-traces) data given a json string representing the ABI string. Since it is stateless it will scale very well across different pipeline sizes.

It will automatically use the matching function in the ABI, but partial ABIs that only contain the target function will also work.

```Function Function definition
_gs_tx_decode(string abi, string input, string output)
```

#### Params

<ParamField path="abi" type="string" required>
  A string representing a json array of events and functions
</ParamField>

<ParamField path="input" type="string" required>
  Name of the column the dataset which contains a string containing the data sent along with the message call.
</ParamField>

<ParamField path="data" type="string" required>
  Name of the column the dataset which contains a string containing the data returned by the message call.
</ParamField>

#### Return

<ResponseField name="ROW ( function TEXT, decoded_inputs TEXT[], decoded_outputs TEXT[] )" type="ROW">
  The function will output a [nested ROW](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/types/#row) type with the name of the function along with its inputs and outputs as arrays of strings. If youre planning on using a sink that doesnt support nested ROWs, you may want to do a further transformation to unnest the result.
</ResponseField>

#### Examples

<ResponseField name="Function Call">
  If you were using the [Raw Traces](/reference/schema/EVM-schemas#raw-traces) source dataset, you would call this function passing your ABI and using the 'input' and 'output' columns. For example:

  ```sql
  SELECT
   _gs_tx_decode('[
     {
        "inputs": [
          {
            "internalType": "address",
            "name": "sharesSubject",
            "type": "address"
          },
          {
            "internalType": "uint256",
            "name": "amount",
            "type": "uint256"
          }
        ],
        "name": "getBuyPriceAfterFee",
        "outputs": [
          {
            "internalType": "uint256",
            "name": "",
            "type": "uint256"
          }
        ],
        "stateMutability": "view",
        "type": "function"
      }
  ]', `input`, `output`) as decoded
  from base.raw_logs
  ```

  You would then able to access both function and its inputs and outputs from the decoded columns as `decoded.function`, `decoded.decoded_inputs` and `decoded.decoded_outputs` in a second transform. You can see an example in [this guide](/mirror/guides/decoding-traces).
</ResponseField>

## \_gs\_fetch\_abi

We provide a convenient function for fetching ABIs, as often they are too big to copy and paste into the yaml definition.

<Warning>The ABI will be fetched once when a pipeline starts. If a pipeline is updated to a new version, or restarted, the ABI will be fetched again. It will not be re-read at any point while a pipeline is running.</Warning>

```Function Function definition
_gs_fetch_abi(string url, string type)
```

#### Params

<ParamField path="url" type="string" required>
  The URL from where the ABI will be fetched
</ParamField>

<ParamField path="type" type="string" required>
  The type of url. Two types of value are accepted:

  * `etherscan` for etherscan or etherscan-compatible APIs
  * `raw` for json array ABIs.

  If you use etherscan-compatible APIs it's highly recommended to include your own API key if you are using this function with a large pipeline. The amount of workers may result in the
  API limits being surpassed.
</ParamField>

#### Examples

<ResponseField name="Function Call">
  Following up on the previous example, we can replace the raw ABI definition by a call to basescan using the \_gs\_log\_decode function:

  ```sql
  select
      # Call the EVM decoding function
      _gs_log_decode(
          # This fetches the ABI from basescan, a `etherscan` compatible site.
          _gs_fetch_abi('https://api.basescan.org/api?module=contract&action=getabi&address=0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4', 'etherscan'),
          `topics`,
          `data`
      ) as `decoded`,
  from base.raw_logs
  ```

  In some cases, you may prefer to host the ABI yourself and retrieve it from your a separate server such as Github Gist:

  ```sql
  select
      # Call the EVM decoding function
      _gs_log_decode(
          # This fetches the ABI from Github Gist
          _gs_fetch_abi('https://gist.githubusercontent.com/JavierTrujilloG/bde43d5079ea5d03edcc68b4516fd297/raw/7b32cf313cd4810f65e726e531ad065eecc47dc1/friendtech_base.json', 'raw'),
          `topics`,
          `data`
      ) as `decoded`,
  from base.raw_logs
  ```
</ResponseField>

## Pipeline Example

Below is an example pipeline definition for decoding events for Friendtech contract in Base, make sure to visit [this guide](/mirror/guides/decoding-contract-events) for a more in-depth explanation of this pipeline:

<Tabs>
  <Tab title="v3">
    ```yaml
    name: friendtech-decoded-events
    apiVersion: 3
    sources:
      my_base_raw_logs:
        type: dataset
        dataset_name: base.raw_logs
        version: 1.0.0
    transforms:
      friendtech_decoded:
        primary_key: id
        # Fetch the ABI from basescan, then use it to decode from the friendtech address.
        sql: >
          select 
            `id`,
            _gs_log_decode(
                _gs_fetch_abi('https://api.basescan.org/api?module=contract&action=getabi&address=0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4', 'etherscan'), 
                `topics`, 
                `data`
            ) as `decoded`, 
            block_number, 
            transaction_hash 
          from my_base_raw_logs
          where address='0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4'
      friendtech_clean:
        primary_key: id
        # Clean up the previous transform, unnest the values from the `decoded` object.
        sql: >
          select 
            `id`, 
            decoded.event_params as `event_params`, 
            decoded.event_signature as `event_signature`,
            block_number,
            transaction_hash
          from friendtech_decoded 
          where decoded is not null
    sinks:
      friendtech_events:
        secret_name: EXAMPLE_SECRET
        type: postgres
        from: friendtech_clean
        schema: decoded_events
        table: friendtech
    ```
  </Tab>

  <Tab title="v2 (deprecated)">
    ```yaml
    sources:
      - type: dataset
        referenceName: base.raw_logs
        version: 1.0.0
    transforms:
      - referenceName: friendtech_decoded
        type: sql
        primaryKey: id
        # Fetch the ABI from basescan, then use it to decode from the friendtech address.
        sql: >
          select 
            `id`,
            _gs_log_decode(
                _gs_fetch_abi('https://api.basescan.org/api?module=contract&action=getabi&address=0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4', 'etherscan'), 
                `topics`, 
                `data`
            ) as `decoded`, 
            block_number, 
            transaction_hash 
          from base.raw_logs
          where address='0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4'
      - referenceName: friendtech_clean
        primaryKey: id
        type: sql
        # Clean up the previous transform, unnest the values from the `decoded` object.
        sql: >
          select 
            `id`, 
            decoded.event_params as `event_params`, 
            decoded.event_signature as `event_signature`,
            block_number,
            transaction_hash
          from friendtech_decoded 
          where decoded is not null
    sinks:
      - referenceName: friendtech_events
        secretName: EXAMPLE_SECRET
        type: postgres
        sourceStreamName: friendtech_clean
        schema: decoded_events
        table: friendtech
    ```
  </Tab>
</Tabs>


# EVM chains' data schemas
Source: https://docs.goldsky.com/reference/schema/EVM-schemas



### Blocks

| Field               |   | Type   |
| ------------------- | - | ------ |
| id                  |   | STRING |
| number              |   | LONG   |
| hash                |   | STRING |
| parent\_hash        |   | STRING |
| nonce               |   | STRING |
| sha3\_uncles        |   | STRING |
| logs\_bloom         |   | STRING |
| transactions\_root  |   | STRING |
| state\_root         |   | STRING |
| receipts\_root      |   | STRING |
| miner               |   | STRING |
| difficulty          |   | DOUBLE |
| total\_difficulty   |   | DOUBLE |
| size                |   | LONG   |
| extra\_data         |   | STRING |
| gas\_limit          |   | LONG   |
| gas\_used           |   | LONG   |
| timestamp           |   | LONG   |
| transaction\_count  |   | LONG   |
| base\_fee\_per\_gas |   | LONG   |
| withdrawals\_root   |   | STRING |

### Enriched Transactions

| Field                                |   | Type    |
| ------------------------------------ | - | ------- |
| id                                   |   | STRING  |
| hash                                 |   | STRING  |
| nonce                                |   | LONG    |
| block\_hash                          |   | STRING  |
| block\_number                        |   | LONG    |
| transaction\_index                   |   | LONG    |
| from\_address                        |   | STRING  |
| to\_address                          |   | STRING  |
| value                                |   | DECIMAL |
| gas                                  |   | DECIMAL |
| gas\_price                           |   | DECIMAL |
| input                                |   | STRING  |
| max\_fee\_per\_gas                   |   | DECIMAL |
| max\_priority\_fee\_per\_gas         |   | DECIMAL |
| transaction\_type                    |   | LONG    |
| block\_timestamp                     |   | LONG    |
| receipt\_cumulative\_gas\_used       |   | DECIMAL |
| receipt\_gas\_used                   |   | DECIMAL |
| receipt\_contract\_address           |   | STRING  |
| receipt\_status                      |   | LONG    |
| receipt\_effective\_gas\_price       |   | DECIMAL |
| receipt\_root\_hash                  |   | STRING  |
| receipt\_l1\_fee                     |   | DECIMAL |
| receipt\_l1\_gas\_used               |   | DECIMAL |
| receipt\_l1\_gas\_price              |   | DECIMAL |
| receipt\_l1\_fee\_scalar             |   | DOUBLE  |
| receipt\_l1\_blob\_base\_fee         |   | DECIMAL |
| receipt\_l1\_blob\_base\_fee\_scalar |   | LONG    |
| blob\_versioned\_hashes              |   | ARRAY   |
| max\_fee\_per\_blob\_gas             |   | DECIMAL |
| receipt\_l1\_block\_number           |   | LONG    |
| receipt\_l1\_base\_fee\_scalar       |   | LONG    |
| gateway\_fee                         |   | LONG    |
| fee\_currency                        |   | STRING  |
| gateway\_fee\_recipient              |   | STRING  |

### Logs

| Field              |   | Type   |
| ------------------ | - | ------ |
| id                 |   | STRING |
| block\_number      |   | LONG   |
| block\_hash        |   | STRING |
| transaction\_hash  |   | STRING |
| transaction\_index |   | LONG   |
| log\_index         |   | LONG   |
| address            |   | STRING |
| data               |   | STRING |
| topics             |   | STRING |
| block\_timestamp   |   | LONG   |

### Raw traces

| Field              |   | Type    |
| ------------------ | - | ------- |
| id                 |   | STRING  |
| block\_number      |   | LONG    |
| block\_hash        |   | STRING  |
| transaction\_hash  |   | STRING  |
| transaction\_index |   | LONG    |
| from\_address      |   | STRING  |
| to\_address        |   | STRING  |
| value              |   | DECIMAL |
| input              |   | STRING  |
| output             |   | STRING  |
| trace\_type        |   | STRING  |
| call\_type         |   | STRING  |
| reward\_type       |   | STRING  |
| gas                |   | LONG    |
| gas\_used          |   | LONG    |
| subtraces          |   | LONG    |
| trace\_address     |   | STRING  |
| error              |   | STRING  |
| status             |   | LONG    |
| trace\_id          |   | STRING  |
| block\_timestamp   |   | LONG    |


# Curated data schemas
Source: https://docs.goldsky.com/reference/schema/curated-schemas



## Token Transfers

### ERC-20

| Field              |   | Type    |
| ------------------ | - | ------- |
| id                 |   | STRING  |
| contract\_address  |   | STRING  |
| sender             |   | STRING  |
| recipient          |   | STRING  |
| amount             |   | DECIMAL |
| transaction\_hash  |   | STRING  |
| block\_hash        |   | STRING  |
| block\_number      |   | LONG    |
| block\_timestamp   |   | LONG    |
| transaction\_index |   | LONG    |
| log\_index         |   | LONG    |

### ERC-721

| Field              |   | Type    |
| ------------------ | - | ------- |
| id                 |   | STRING  |
| contract\_address  |   | STRING  |
| sender             |   | STRING  |
| recipient          |   | STRING  |
| token\_id          |   | DECIMAL |
| transaction\_hash  |   | STRING  |
| block\_hash        |   | STRING  |
| block\_number      |   | LONG    |
| block\_timestamp   |   | LONG    |
| transaction\_index |   | LONG    |
| log\_index         |   | LONG    |

### ERC-1155

| Field              |   | Type    |
| ------------------ | - | ------- |
| id                 |   | STRING  |
| contract\_address  |   | STRING  |
| sender             |   | STRING  |
| recipient          |   | STRING  |
| token\_id          |   | DECIMAL |
| amount             |   | DECIMAL |
| transaction\_hash  |   | STRING  |
| block\_hash        |   | STRING  |
| block\_number      |   | LONG    |
| block\_timestamp   |   | LONG    |
| transaction\_index |   | LONG    |
| log\_index         |   | LONG    |

## Polymarket Datasets

### Global Open Interest

| Field        |   | Type    |
| ------------ | - | ------- |
| vid          |   | LONG    |
| block\_range |   | STRING  |
| id           |   | STRING  |
| amount       |   | DECIMAL |
| \_gs\_chain  |   | STRING  |
| \_gs\_gid    |   | STRING  |

### User Positions

| Field         |   | Type   |
| ------------- | - | ------ |
| vid           |   | LONG   |
| block\_range  |   | STRING |
| id            |   | STRING |
| user          |   | STRING |
| token\_id     |   | STRING |
| amount        |   | LONG   |
| avg\_price    |   | LONG   |
| realized\_pnl |   | LONG   |
| total\_bought |   | LONG   |
| \_gs\_chain   |   | STRING |
| \_gs\_gid     |   | STRING |

### User Balances

| Field        |   | Type    |
| ------------ | - | ------- |
| vid          |   | LONG    |
| block\_range |   | STRING  |
| id           |   | STRING  |
| user         |   | STRING  |
| asset        |   | STRING  |
| balance      |   | DECIMAL |
| \_gs\_chain  |   | STRING  |
| \_gs\_gid    |   | STRING  |

### Market Open Interest

| Field        |   | Type    |
| ------------ | - | ------- |
| vid          |   | LONG    |
| block\_range |   | STRING  |
| id           |   | STRING  |
| amount       |   | DECIMAL |
| \_gs\_chain  |   | STRING  |
| \_gs\_gid    |   | STRING  |

### Order Filled

| Field                 |   | Type   |
| --------------------- | - | ------ |
| vid                   |   | LONG   |
| block\_range          |   | STRING |
| id                    |   | STRING |
| transaction\_hash     |   | STRING |
| timestamp             |   | LONG   |
| order\_hash           |   | STRING |
| maker                 |   | STRING |
| taker                 |   | STRING |
| maker\_asset\_id      |   | STRING |
| taker\_asset\_id      |   | STRING |
| maker\_amount\_filled |   | LONG   |
| taker\_amount\_filled |   | LONG   |
| fee                   |   | LONG   |
| \_gs\_chain           |   | STRING |
| \_gs\_gid             |   | STRING |

### Orders Matched

| Field        |   | Type    |
| ------------ | - | ------- |
| vid          |   | LONG    |
| block\_range |   | STRING  |
| id           |   | STRING  |
| amount       |   | DECIMAL |
| \_gs\_chain  |   | STRING  |
| \_gs\_gid    |   | STRING  |


# NFT data schemas
Source: https://docs.goldsky.com/reference/schema/nft-data



Eight pre-curated, enriched NFT data streams are available, encompassing all 721, 1155, and major nonstandard collections (eg. CryptoPunks).

| Data                    | Description                                                                           |
| ----------------------- | ------------------------------------------------------------------------------------- |
| **Tokens**              | Core, per-token metadata such as traits, token media links, and more.                 |
| **Owners**              | Realtime wallet holdings for individual NFTs.                                         |
| **Collections**         | Summary information including social links, realtime floor price, and more.           |
| **Sales and transfers** | All historical & realtime transfer events with marketplace sale data if applicable.   |
| **Bids**                | Current open bids at the individual NFT level.                                        |
| **Bid events**          | Realtime bid events (added, modified, removed) at the individual NFT level.           |
| **Listings**            | Currently active listings at the individual NFT level.                                |
| **Listing events**      | Realtime listing events (added, modified, removed, sold) at the individual NFT level. |

{/*}
  The schema for each of these pipelines is below.
  ## Tokens
  <Snippet file="schemas/nft-tokens.mdx" />
  ## Owners
  <Snippet file="schemas/nft-owners.mdx" />
  ## Collections
  <Snippet file="schemas/nft-collections.mdx" />
  ## Sales and transfers
  <Snippet file="schemas/nft-sales-and-transfers.mdx" />
  ## Bids
  <Snippet file="schemas/nft-listings.mdx" />
  ## Bid events
  <Snippet file="schemas/nft-listings.mdx" />
  ## Listings
  <Snippet file="schemas/nft-listings.mdx" />
  ## Listings events
  <Snippet file="schemas/nft-listing-events.mdx" />
  {*/}


# Non-EVM chains' data schemas
Source: https://docs.goldsky.com/reference/schema/non-EVM-schemas



## Beacon

### Attestations

| Field               |   | Type   |
| ------------------- | - | ------ |
| id                  |   | STRING |
| block\_root         |   | STRING |
| block\_slot         |   | LONG   |
| block\_epoch        |   | LONG   |
| block\_timestamp    |   | LONG   |
| aggregation\_bits   |   | STRING |
| slot                |   | LONG   |
| index               |   | LONG   |
| beacon\_block\_root |   | STRING |
| source\_epoch       |   | LONG   |
| source\_root        |   | STRING |
| target\_epoch       |   | LONG   |
| target\_root        |   | STRING |
| signature           |   | STRING |

### Attester Slashing

| Field                               |   | Type   |
| ----------------------------------- | - | ------ |
| id                                  |   | STRING |
| block\_timestamp                    |   | LONG   |
| attestation\_1\_attesting\_indices  |   | ARRAY  |
| attestation\_1\_slot                |   | LONG   |
| attestation\_1\_index               |   | LONG   |
| attestation\_1\_beacon\_block\_root |   | STRING |
| attestation\_1\_source\_epoch       |   | LONG   |
| attestation\_1\_source\_root        |   | STRING |
| attestation\_1\_target\_epoch       |   | LONG   |
| attestation\_1\_target\_root        |   | STRING |
| attestation\_1\_signature           |   | STRING |
| attestation\_2\_attesting\_indices  |   | ARRAY  |
| attestation\_2\_slot                |   | LONG   |
| attestation\_2\_index               |   | LONG   |
| attestation\_2\_beacon\_block\_root |   | STRING |
| attestation\_2\_source\_epoch       |   | LONG   |
| attestation\_2\_source\_root        |   | STRING |
| attestation\_2\_target\_epoch       |   | LONG   |
| attestation\_2\_target\_root        |   | STRING |
| attestation\_2\_signature           |   | STRING |

### Blocks

| Field                             |   | Type    |
| --------------------------------- | - | ------- |
| id                                |   | STRING  |
| block\_slot                       |   | LONG    |
| block\_epoch                      |   | LONG    |
| block\_timestamp                  |   | LONG    |
| block\_root                       |   | STRING  |
| proposer\_index                   |   | LONG    |
| skipped                           |   | BOOLEAN |
| parent\_root                      |   | STRING  |
| state\_root                       |   | STRING  |
| randao\_reveal                    |   | STRING  |
| graffiti                          |   | STRING  |
| eth1\_block\_hash                 |   | STRING  |
| eth1\_deposit\_root               |   | STRING  |
| eth1\_deposit\_count              |   | LONG    |
| signature                         |   | STRING  |
| execution\_payload\_block\_number |   | LONG    |
| execution\_payload\_block\_hash   |   | STRING  |
| blob\_gas\_used                   |   | DECIMAL |
| excess\_blob\_gas                 |   | DECIMAL |
| blob\_kzg\_commitments            |   | STRING  |
| sync\_committee\_bits             |   | STRING  |
| sync\_committee\_signature        |   | STRING  |

### BLS Signature to Execution Address Changes

| Field                  |   | Type   |
| ---------------------- | - | ------ |
| id                     |   | STRING |
| block\_slot            |   | LONG   |
| block\_epoch           |   | LONG   |
| block\_timestamp       |   | LONG   |
| proposer\_index        |   | LONG   |
| block\_root            |   | STRING |
| validator\_index       |   | LONG   |
| from\_bls\_pubkey      |   | STRING |
| to\_execution\_address |   | STRING |
| signature              |   | STRING |

### Deposits

| Field                   |   | Type   |
| ----------------------- | - | ------ |
| id                      |   | STRING |
| block\_slot             |   | LONG   |
| block\_epoch            |   | LONG   |
| block\_timestamp        |   | LONG   |
| block\_root             |   | STRING |
| pubkey                  |   | STRING |
| withdrawal\_credentials |   | STRING |
| amount                  |   | LONG   |
| signature               |   | STRING |

### Proposer Slashing

| Field                      |   | Type   |
| -------------------------- | - | ------ |
| id                         |   | STRING |
| block\_timestamp           |   | LONG   |
| header\_1\_slot            |   | LONG   |
| header\_1\_proposer\_index |   | LONG   |
| header\_1\_parent\_root    |   | STRING |
| header\_1\_state\_root     |   | STRING |
| header\_1\_body\_root      |   | STRING |
| header\_1\_signature       |   | STRING |
| header\_2\_slot            |   | LONG   |
| header\_2\_proposer\_index |   | LONG   |
| header\_2\_parent\_root    |   | STRING |
| header\_2\_state\_root     |   | STRING |
| header\_2\_body\_root      |   | STRING |
| header\_2\_signature       |   | STRING |

### Voluntary Exits

| Field            |   | Type   |
| ---------------- | - | ------ |
| id               |   | STRING |
| block\_timestamp |   | LONG   |

### Withdrawls

| Field             |   | Type   |
| ----------------- | - | ------ |
| id                |   | STRING |
| block\_slot       |   | LONG   |
| block\_epoch      |   | LONG   |
| block\_timestamp  |   | LONG   |
| block\_root       |   | STRING |
| index             |   | LONG   |
| normalized\_index |   | LONG   |
| validator\_index  |   | LONG   |
| address           |   | STRING |
| amount            |   | LONG   |

## Solana

### Edge Accounts

| Field                   |   | Type    |
| ----------------------- | - | ------- |
| id                      |   | STRING  |
| block\_slot             |   | LONG    |
| block\_hash             |   | STRING  |
| block\_timestamp        |   | LONG    |
| pubkey                  |   | STRING  |
| tx\_signature           |   | STRING  |
| executable              |   | BOOLEAN |
| lamports                |   | DOUBLE  |
| owner                   |   | STRING  |
| rent\_epoch             |   | DECIMAL |
| data                    |   | STRING  |
| program                 |   | STRING  |
| space                   |   | LONG    |
| account\_type           |   | STRING  |
| is\_native              |   | BOOLEAN |
| mint                    |   | STRING  |
| mint\_authority         |   | STRING  |
| state                   |   | STRING  |
| token\_amount           |   | DOUBLE  |
| token\_amount\_decimals |   | LONG    |
| supply                  |   | LONG    |
| program\_data           |   | STRING  |
| authorized\_voters      |   | STRING  |
| authorized\_withdrawer  |   | STRING  |
| prior\_voters           |   | STRING  |
| node\_pubkey            |   | STRING  |
| commission              |   | LONG    |
| epoch\_credits          |   | STRING  |
| votes                   |   | STRING  |
| root\_slot              |   | LONG    |
| last\_timestamp         |   | STRING  |

### Edge Blocks

| Field                 |   | Type    |
| --------------------- | - | ------- |
| id                    |   | STRING  |
| skipped               |   | BOOLEAN |
| slot                  |   | LONG    |
| parent\_slot          |   | LONG    |
| hash                  |   | STRING  |
| timestamp             |   | LONG    |
| height                |   | LONG    |
| previous\_block\_hash |   | STRING  |
| transaction\_count    |   | LONG    |
| leader\_reward        |   | LONG    |
| leader                |   | STRING  |

### Edge Instructions

| Field             |   | Type   |
| ----------------- | - | ------ |
| id                |   | STRING |
| block\_slot       |   | LONG   |
| block\_hash       |   | STRING |
| block\_timestamp  |   | LONG   |
| tx\_signature     |   | STRING |
| index             |   | LONG   |
| parent\_index     |   | LONG   |
| accounts          |   | STRING |
| data              |   | STRING |
| program           |   | STRING |
| program\_id       |   | STRING |
| instruction\_type |   | STRING |
| params            |   | STRING |
| parsed            |   | STRING |

### Edge Rewards

| Field            |   | Type   |
| ---------------- | - | ------ |
| id               |   | STRING |
| block\_slot      |   | LONG   |
| block\_hash      |   | STRING |
| block\_timestamp |   | LONG   |
| commission       |   | LONG   |
| lamports         |   | LONG   |
| post\_balance    |   | LONG   |
| pub\_key         |   | STRING |
| reward\_type     |   | STRING |

### Edge Token Transfers

| Field            |   | Type    |
| ---------------- | - | ------- |
| id               |   | STRING  |
| block\_slot      |   | LONG    |
| block\_hash      |   | STRING  |
| block\_timestamp |   | LONG    |
| source           |   | STRING  |
| destination      |   | STRING  |
| authority        |   | STRING  |
| value            |   | DECIMAL |
| decimals         |   | DECIMAL |
| mint             |   | STRING  |
| mint\_authority  |   | STRING  |
| transfer\_type   |   | STRING  |
| tx\_signature    |   | STRING  |

### Edge Tokens

| Field                      |   | Type    |
| -------------------------- | - | ------- |
| id                         |   | STRING  |
| block\_slot                |   | LONG    |
| block\_hash                |   | STRING  |
| block\_timestamp           |   | LONG    |
| tx\_signature              |   | STRING  |
| mint                       |   | STRING  |
| update\_authority          |   | STRING  |
| name                       |   | STRING  |
| symbol                     |   | STRING  |
| uri                        |   | STRING  |
| seller\_fee\_basis\_points |   | LONG    |
| creators                   |   | STRING  |
| primary\_sale\_happened    |   | BOOLEAN |
| is\_mutable                |   | BOOLEAN |
| is\_nft                    |   | BOOLEAN |
| token\_type                |   | STRING  |
| retrieval\_timestamp       |   | LONG    |

### Edge Transactions

| Field                    |   | Type    |
| ------------------------ | - | ------- |
| id                       |   | STRING  |
| index                    |   | LONG    |
| signature                |   | STRING  |
| block\_hash              |   | STRING  |
| recent\_block\_hash      |   | STRING  |
| block\_slot              |   | LONG    |
| block\_timestamp         |   | LONG    |
| fee                      |   | LONG    |
| status                   |   | LONG    |
| err                      |   | STRING  |
| accounts                 |   | STRING  |
| log\_messages            |   | STRING  |
| balance\_changes         |   | STRING  |
| pre\_token\_balances     |   | STRING  |
| post\_token\_balances    |   | STRING  |
| compute\_units\_consumed |   | DECIMAL |

### Edge Transactions with Instructions

| Field                    |   | Type    |
| ------------------------ | - | ------- |
| id                       |   | STRING  |
| index                    |   | LONG    |
| signature                |   | STRING  |
| block\_hash              |   | STRING  |
| recent\_block\_hash      |   | STRING  |
| block\_slot              |   | LONG    |
| block\_timestamp         |   | LONG    |
| fee                      |   | LONG    |
| status                   |   | LONG    |
| err                      |   | STRING  |
| accounts                 |   | STRING  |
| log\_messages            |   | STRING  |
| balance\_changes         |   | STRING  |
| pre\_token\_balances     |   | STRING  |
| post\_token\_balances    |   | STRING  |
| compute\_units\_consumed |   | DECIMAL |
| instructions             |   | STRING  |

## Starknet

### Blocks

| Field                         |   | Type    |
| ----------------------------- | - | ------- |
| id                            |   | STRING  |
| number                        |   | LONG    |
| hash                          |   | STRING  |
| parent\_hash                  |   | STRING  |
| new\_root                     |   | STRING  |
| status                        |   | STRING  |
| sequencer\_address            |   | STRING  |
| timestamp                     |   | LONG    |
| transaction\_count            |   | LONG    |
| l1\_da\_mode                  |   | STRING  |
| l1\_gas\_price\_in\_fri       |   | DECIMAL |
| l1\_gas\_price\_in\_wei       |   | DECIMAL |
| l1\_data\_gas\_price\_in\_fri |   | DECIMAL |
| l1\_data\_gas\_price\_in\_wei |   | DECIMAL |
| starknet\_version             |   | STRING  |

### Events

| Field                          |   | Type   |
| ------------------------------ | - | ------ |
| id                             |   | STRING |
| block\_number                  |   | LONG   |
| block\_hash                    |   | STRING |
| block\_timestamp               |   | LONG   |
| transaction\_hash              |   | STRING |
| transaction\_execution\_status |   | STRING |
| transaction\_type              |   | STRING |
| transaction\_version           |   | LONG   |
| from\_address                  |   | STRING |
| data                           |   | ARRAY  |
| keys                           |   | ARRAY  |

### Messages

| Field                          |   | Type   |
| ------------------------------ | - | ------ |
| id                             |   | STRING |
| block\_number                  |   | LONG   |
| block\_hash                    |   | STRING |
| block\_timestamp               |   | LONG   |
| transaction\_hash              |   | STRING |
| transaction\_execution\_status |   | STRING |
| transaction\_type              |   | STRING |
| transaction\_version           |   | LONG   |
| message\_index                 |   | LONG   |
| from\_address                  |   | STRING |
| to\_address                    |   | STRING |
| payload                        |   | ARRAY  |

### Enriched Transactions

| Field                                            |   | Type    |
| ------------------------------------------------ | - | ------- |
| id                                               |   | STRING  |
| block\_number                                    |   | LONG    |
| block\_hash                                      |   | STRING  |
| block\_timestamp                                 |   | LONG    |
| version                                          |   | LONG    |
| hash                                             |   | STRING  |
| transaction\_type                                |   | STRING  |
| execution\_status                                |   | STRING  |
| finality\_status                                 |   | STRING  |
| nonce                                            |   | LONG    |
| contract\_address                                |   | STRING  |
| entry\_point\_selector                           |   | STRING  |
| contract\_address\_salt                          |   | STRING  |
| class\_hash                                      |   | STRING  |
| calldata                                         |   | ARRAY   |
| constructor\_calldata                            |   | ARRAY   |
| signature                                        |   | ARRAY   |
| sender\_address                                  |   | STRING  |
| max\_fee                                         |   | DECIMAL |
| transaction\_index                               |   | LONG    |
| actual\_fee\_amount                              |   | DECIMAL |
| actual\_fee\_unit                                |   | STRING  |
| resource\_bounds\_l1\_gas\_max\_amount           |   | LONG    |
| resource\_bounds\_l1\_gas\_max\_price\_per\_unit |   | LONG    |
| resource\_bounds\_l2\_gas\_max\_amount           |   | LONG    |
| resource\_bounds\_l2\_gas\_max\_price\_per\_unit |   | LONG    |
| message\_count                                   |   | LONG    |
| event\_count                                     |   | LONG    |

## Stellar

### Assets

| Field         |   | Type   |
| ------------- | - | ------ |
| asset\_code   |   | STRING |
| asset\_issuer |   | STRING |
| asset\_type   |   | STRING |
| id            |   | STRING |

### Contract Events

| Field                          |   | Type    |
| ------------------------------ | - | ------- |
| id                             |   | STRING  |
| transaction\_hash              |   | STRING  |
| transaction\_id                |   | LONG    |
| successful                     |   | BOOLEAN |
| ledger\_sequence               |   | LONG    |
| closed\_at                     |   | LONG    |
| in\_successful\_contract\_call |   | BOOLEAN |
| contract\_id                   |   | STRING  |
| type                           |   | INT     |
| type\_string                   |   | STRING  |
| topics                         |   | STRING  |
| topics\_decoded                |   | STRING  |
| data                           |   | STRING  |
| data\_decoded                  |   | STRING  |
| contract\_event\_xdr           |   | STRING  |

### Raw Effects

| Field          |   | Type   |
| -------------- | - | ------ |
| id             |   | STRING |
| address        |   | STRING |
| address\_muxed |   | STRING |
| operation\_id  |   | LONG   |
| type           |   | INT    |
| type\_string   |   | STRING |
| closed\_at     |   | LONG   |
| details        |   | STRING |

### Ledgers

| Field                          |   | Type   |
| ------------------------------ | - | ------ |
| id                             |   | STRING |
| sequence                       |   | LONG   |
| ledger\_hash                   |   | STRING |
| previous\_ledger\_hash         |   | STRING |
| ledger\_header                 |   | STRING |
| transaction\_count             |   | INT    |
| operation\_count               |   | INT    |
| successful\_transaction\_count |   | INT    |
| failed\_transaction\_count     |   | INT    |
| tx\_set\_operation\_count      |   | STRING |
| closed\_at                     |   | LONG   |
| total\_coins                   |   | LONG   |
| fee\_pool                      |   | LONG   |
| base\_fee                      |   | LONG   |
| base\_reserve                  |   | LONG   |
| max\_tx\_set\_size             |   | LONG   |
| protocol\_version              |   | LONG   |
| ledger\_id                     |   | LONG   |
| soroban\_fee\_write\_1kb       |   | LONG   |

### Operations

| Field                   |   | Type             |
| ----------------------- | - | ---------------- |
| source\_account         |   | STRING           |
| source\_account\_muxed  |   | STRING           |
| type                    |   | INT              |
| type\_string            |   | STRING           |
| details                 |   | STRING           |
| transaction\_id         |   | LONG             |
| id                      |   | LONG             |
| closed\_at              |   | TIMESTAMP-MICROS |
| operation\_result\_code |   | STRING           |
| operation\_trace\_code  |   | STRING           |

### Transactions

| Field                                   |   | Type             |
| --------------------------------------- | - | ---------------- |
| id                                      |   | STRING           |
| transaction\_hash                       |   | STRING           |
| ledger\_sequence                        |   | LONG             |
| account                                 |   | STRING           |
| account\_muxed                          |   | STRING           |
| account\_sequence                       |   | LONG             |
| max\_fee                                |   | LONG             |
| fee\_charged                            |   | LONG             |
| operation\_count                        |   | INT              |
| tx\_envelope                            |   | STRING           |
| tx\_result                              |   | STRING           |
| tx\_meta                                |   | STRING           |
| tx\_fee\_meta                           |   | STRING           |
| created\_at                             |   | TIMESTAMP-MICROS |
| memo\_type                              |   | STRING           |
| memo                                    |   | STRING           |
| time\_bounds                            |   | STRING           |
| successful                              |   | BOOLEAN          |
| transaction\_id                         |   | LONG             |
| fee\_account                            |   | STRING           |
| fee\_account\_muxed                     |   | STRING           |
| inner\_transaction\_hash                |   | STRING           |
| new\_max\_fee                           |   | LONG             |
| ledger\_bounds                          |   | STRING           |
| min\_account\_sequence                  |   | LONG             |
| min\_account\_sequence\_age             |   | LONG             |
| min\_account\_sequence\_ledger\_gap     |   | LONG             |
| extra\_signers                          |   | ARRAY            |
| closed\_at                              |   | TIMESTAMP-MICROS |
| resource\_fee                           |   | LONG             |
| soroban\_resources\_instructions        |   | STRING           |
| soroban\_resources\_read\_bytes         |   | STRING           |
| soroban\_resources\_write\_bytes        |   | STRING           |
| transaction\_result\_code               |   | STRING           |
| inclusion\_fee\_bid                     |   | LONG             |
| inclusion\_fee\_charged                 |   | LONG             |
| resource\_fee\_refund                   |   | LONG             |
| non\_refundable\_resource\_fee\_charged |   | LONG             |
| refundable\_resource\_fee\_charged      |   | LONG             |
| rent\_fee\_charged                      |   | LONG             |

### Trades

| Field                        |   | Type    |
| ---------------------------- | - | ------- |
| id                           |   | STRING  |
| order                        |   | INT     |
| ledger\_closed\_at           |   | LONG    |
| selling\_account\_address    |   | STRING  |
| selling\_asset\_code         |   | STRING  |
| selling\_asset\_issuer       |   | STRING  |
| selling\_asset\_type         |   | STRING  |
| selling\_asset\_id           |   | LONG    |
| selling\_amount              |   | DOUBLE  |
| buying\_account\_address     |   | STRING  |
| buying\_asset\_code          |   | STRING  |
| buying\_asset\_issuer        |   | STRING  |
| buying\_asset\_type          |   | STRING  |
| buying\_asset\_id            |   | LONG    |
| buying\_amount               |   | DOUBLE  |
| price\_n                     |   | LONG    |
| price\_d                     |   | LONG    |
| selling\_offer\_id           |   | LONG    |
| buying\_offer\_id            |   | LONG    |
| selling\_liquidity\_pool\_id |   | STRING  |
| liquidity\_pool\_fee         |   | LONG    |
| history\_operation\_id       |   | LONG    |
| trade\_type                  |   | INT     |
| rounding\_slippage           |   | LONG    |
| seller\_is\_exact            |   | BOOLEAN |

## Sui

### Checkpoints

| Field                         |   | Type    |
| ----------------------------- | - | ------- |
| sequence\_number              |   | LONG    |
| checkpoint\_digest            |   | STRING  |
| epoch                         |   | LONG    |
| tx\_digests                   |   | STRING  |
| network\_total\_transactions  |   | LONG    |
| timestamp\_ms                 |   | LONG    |
| previous\_checkpoint\_digest  |   | STRING  |
| total\_gas\_cost              |   | LONG    |
| computation\_cost             |   | LONG    |
| storage\_cost                 |   | LONG    |
| storage\_rebate               |   | LONG    |
| non\_refundable\_storage\_fee |   | LONG    |
| checkpoint\_commitments       |   | STRING  |
| validator\_signature          |   | STRING  |
| successful\_tx\_num           |   | LONG    |
| end\_of\_epoch\_data          |   | STRING  |
| end\_of\_epoch                |   | BOOLEAN |

### Raw Transactions

| Field                |   | Type   |
| -------------------- | - | ------ |
| tx\_sequence\_number |   | LONG   |
| tx\_digest           |   | STRING |
| sender\_signed\_data |   | STRING |
| effects              |   | STRING |

### Events

| Field                        |   | Type   |
| ---------------------------- | - | ------ |
| tx\_sequence\_number         |   | LONG   |
| event\_sequence\_number      |   | LONG   |
| checkpoint\_sequence\_number |   | LONG   |
| transaction\_digest          |   | STRING |
| senders                      |   | STRING |
| package                      |   | STRING |
| module                       |   | STRING |
| event\_type                  |   | STRING |
| event\_type\_package         |   | STRING |
| event\_type\_module          |   | STRING |
| event\_type\_name            |   | STRING |
| bcs                          |   | STRING |
| timestamp\_ms                |   | LONG   |

### Epochs

| Field                              |   | Type   |
| ---------------------------------- | - | ------ |
| id                                 |   | STRING |
| epoch                              |   | LONG   |
| first\_checkpoint\_id              |   | LONG   |
| epoch\_start\_timestamp            |   | LONG   |
| reference\_gas\_price              |   | LONG   |
| protocol\_version                  |   | LONG   |
| total\_stake                       |   | LONG   |
| storage\_fund\_balance             |   | LONG   |
| system\_state                      |   | STRING |
| epoch\_total\_transactions         |   | LONG   |
| last\_checkpoint\_id               |   | LONG   |
| epoch\_end\_timestamp              |   | LONG   |
| storage\_fund\_reinvestment        |   | LONG   |
| storage\_charge                    |   | LONG   |
| storage\_rebate                    |   | LONG   |
| stake\_subsidy\_amount             |   | LONG   |
| total\_gas\_fees                   |   | LONG   |
| total\_stake\_rewards\_distributed |   | LONG   |
| leftover\_storage\_fund\_inflow    |   | LONG   |
| epoch\_commitments                 |   | STRING |

### Packages

| Field                        |   | Type   |
| ---------------------------- | - | ------ |
| package\_id                  |   | STRING |
| checkpoint\_sequence\_number |   | LONG   |
| move\_package                |   | STRING |


# Subgraphs vs Mirror
Source: https://docs.goldsky.com/subgraph-vs-mirror

Wondering how to best leverage Goldsky's products? This guide explains the differences between Subgraphs and Mirror and how you can use them individually or together to meet your needs.

Goldsky offers two flagship products, [Subgraphs](/subgraphs/introduction) and [Mirror](/mirror/introduction), designed to help developers interact with blockchain data more efficiently. While both tools serve similar goals - retrieving, processing, and querying blockchain data - they differ significantly in how they handle data management, scalability, and customization.

Goldsky Subgraphs are a powerful abstraction built on top of blockchain indexing technology. They allow developers to define data sources, transformation logic, and queries using GraphQL against an instant API endpoint, making it easier to retrieve structured blockchain data. Subgraphs are particularly useful for dApps (decentralized applications) that need to query specific pieces of on-chain data efficiently.

Goldsky Mirror provide a different approach to managing blockchain and off-chain data, focusing on real-time data streaming directly to a database, offering full control and customization over the data pipeline. Instead of providing an API endpoint for querying data like subgraphs, Mirror Pipelines stream the raw or processed data directly to a database managed by the user. This setup allows users to have more flexibility in how they store, manipulate, and query this data.

Lets now look into the difference between both products from the perspective of important functional dimensions:

### 1. **Data Design**

* **Subgraphs**:
  * The data model is optimized specifically for on-chain data:
    * The entities and data types you define in subgraphs are tailored to represent blockchain data efficiently.
    * You can create relationships and aggregations between on-chain entities (e.g. track the balance of a user for specific tokens)
    * You can enrich entity data with other on-chain sources using `eth_call`. However, integrating off-chain data is not supported.
    * On-chain data is restricted to EVM-compatible chains.
* **Mirror**:
  * The data model is flexible and open to any type of data:
    * You can combine blockchain data with your own data seamlessly, offering more complex and customizable use cases.
    * Using Mirror the recommendation is to get data into your database and do more aggregations/enrichments downstream. You can also perform some pre-processing using SQL dialect before writing to your database.
    * On-chain data not restricted to EVMs. Mirror currently support alternative L1s such as [Solana](https://docs.goldsky.com/mirror/cryptohouse), Sui and many others.

### 2. **Infrastructure**

* **Subgraphs**:
  * Provide an instant GraphQL API thats ready to use right out of the box for querying blockchain data.
  * The entire infrastructure (including indexing, database and querying layer) is managed by Goldsky, minimizing setup time but limiting control over data handling.
* **Mirror**:
  * Fully runs on Goldskys infrastructure to stream data into your database in real-time, but ultimately, you need to set up and manage your own data storage and querying infrastructure.
  * Offers full control over data, allowing you to optimize infrastructure and scale as needed. This way, you can colocate the data with other data realms of your business and offer greater privacy and UX.

### 3. **Ecosystem & Development**

* **Subgraphs**:
  * Established technology with a rich ecosystem. Numerous open-source repositories are available for reference, and [Goldsky's Instant Subgraphs](https://docs.goldsky.com/subgraphs/guides/create-a-no-code-subgraph) allow you to quickly create subgraphs with no code.
  * There is a vast community around subgraph development, making it easier to find support, tutorials, and pre-built examples.
* **Mirror**:
  * As a newer product, Mirror Pipelines doesnt have as many public examples or pre-built repositories to reference. However, users can benefit from Goldskys support to set up and optimize their pipelines. We also create [curated datasets](https://docs.goldsky.com/chains/supported-networks#curated-datasets) for specific use cases that are readily available for deployment.

### 4. **Scalability & Performance**

* **Subgraphs**:
  * Perform well under low throughput conditions (less than 40-50 events per second). However, as event frequency increases, latency grows, and maintaining subgraphs becomes more complex.
  * Goldsky offers the ability to fine-tune subgraphs, providing [custom indexers](https://docs.goldsky.com/subgraphs/serverless-vs-dedicated) that help optimize performance and efficiency. However, it's important to note that there's a limit to how much fine-tuning can be done within the subgraph framework
  * Multi-chain setups often require reindexing from scratch, which can be time-consuming when switching between chains. This can slow down applications that rely on frequent updates from multiple chains.
* **Mirror**:
  * Designed for scalability. You can expand your infrastructure horizontally (adding more servers, optimizing queries, etc.) as the data load increases. A default Mirror pipeline writes about 2,000 rows a second, but you can scale up to 40 workers with an XXL Mirror pipeline. With that, you can see speeds of over 100,000 rows a second; backfilling the entire Ethereum blocks table in under 4 minutes.
  * Thanks to its fast backfill capabilities and the fact that you can colocate data as you see fit, Mirror is optimized to multi-chain applications as represented in [this article by Splits Engineering team](https://splits.org/blog/engineering-multichain/)
  * Real-time streaming ensures that query latency is only limited by the performance of your own database, not by external API calls or indexing limitations.

### 5. **Expressiveness on data transformation**

* **Subgraphs**:
  * Data transformation in subgraph mappings is very expressive due to the fact thats defined on Javascript which is a very popular language with lots of support and customization possibilities.
* **Mirror**:
  * Data transformation in Mirror can be done in 2 ways:
    * [SQL transforms](/mirror/transforms/sql-transforms): this can be advantageous for users proficient on SQL but it can feel a bit more rigid for developers with not as much experience with this language.
    * [External Handlers](/mirror/transforms/external-handlers): you own the processing layer and have full flexibility on how you would like transform the data using the technology and framework of your choice.

### **Common Use Cases**

Now that we have covered the most important functional differences, lets look at some practical scenarios where it makes more sense choosing one technology over the other:

* **Subgraphs**:
  * Best suited for applications that deal exclusively with on-chain data and dont need integration with off-chain sources.
  * Ideal for predefined data models, such as dApps that need to query specific smart contract events or execute standard blockchain queries.
  * Great for low to moderate traffic scenarios with relatively straightforward data structures and querying needs.
* **Mirror Pipelines**:
  * A better fit for applications that require both on-chain and off-chain data, offering the flexibility to combine data sources for advanced analytics or decision-making.
  * Ideal for multi-chain applications, as it simplifies the process of managing data across different blockchains without the need for reindexing. This is specially true if your application needs non-EVM data like Solana.
  * Perfect for high-traffic applications, where low latency and real-time data access are critical to the performance and user experience.

### Subgraph + Mirror

Fortunately, you are not restricted to choose one technology over the other: Subgraphs and Mirror can be combined to leverage the strengths of both technologies by [definining  subgraphs as the data source](https://docs.goldsky.com/mirror/sources/subgraphs) to your pipelines. This dual approach ensures that applications can benefit from the speed and convenience of instant APIs while also gaining full control over data storage and integration through Mirror 

### **Conclusion**

While both Subgraphs and Mirror Pipelines offer powerful solutions for interacting with blockchain data, choosing the right tool depends on your specific needs. In some cases, either technology may seem like a viable option, but it's important to carefully evaluate your requirements. If you're working with simpler on-chain data queries or need quick setup and ease of use, Subgraphs might be the best fit. On the other hand, for more complex applications that require real-time data streaming, multi-chain support, or the integration of off-chain data, Mirror Pipelines provides the flexibility and control you need. Remember that you are not constrained to one technology but that you can combine them to get the best of both worlds.

Ultimately, selecting the right solutionor a combination of bothdepends on aligning with your projects performance, scalability, and infrastructure goals to ensure long-term success.


# Deploy a subgraph
Source: https://docs.goldsky.com/subgraphs/deploying-subgraphs



<video controls className="w-full aspect-video" src="https://gssupport.s3.us-east-2.amazonaws.com/docs/Subgraph+Deployment+Walkthrough.mp4" />

There are three primary ways to deploy a subgraph on Goldsky:

1. From source code
2. Migrating from The Graph or any other subgraph host
3. Via instant, no-code subgraphs

For any of the above, you'll need to have the Goldsky CLI installed and be logged in; you can do this by following the instructions below.

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

For these examples we'll use the Ethereum contract for [POAP](https://poap.xyz).

# From source code

If youve developed your own subgraph, you can deploy it from the source. In our example well work off of a clone of the [POAP subgraph](https://github.com/goldsky-io/poap-subgraph).

First we need to clone the Git repo.

```shell
git clone https://github.com/goldsky-io/poap-subgraph
```

Now change into that directory. From here, we'll build the subgraph from templates. Open source subgraphs have different instructions to get them to build, so check the `README.md` or look at the `package.json` for hints as to the correct build commands. Usually it's a two step process, but since POAP is deployed on multiple chains, there's one extra step at the start to generate the correct data from templates.

```shell
yarn prepare:mainnet
yarn codegen
yarn build
```

Then you can deploy the subgraph to Goldsky using the following command.

```shell
goldsky subgraph deploy poap-subgraph/1.0.0 --path .
```

# From The Graph or another host

<Info>
  For a detailed walkthrough, follow our [dedicated guide](/subgraphs/migrate-from-the-graph).
</Info>

# Via instant, no-code subgraphs

<Info>
  For a detailed walkthrough, follow our [dedicated guide](/subgraphs/guides/create-a-no-code-subgraph).
</Info>


# GraphQL Endpoints
Source: https://docs.goldsky.com/subgraphs/graphql-endpoints



All subgraphs come with a GraphQL interface that allows you to query the data in the subgraph. Traditionally these GraphQL
interfaces are completely public and can be accessed by anyone. Goldsky supports public GraphQL endpoints for both
subgraphs and their tags.

## Public endpoints

For example, in the Goldsky managed community project there exists the `uniswap-v3-base/1.0.0` subgraph with a tag of `prod`.
This subgraph has a [public endpoint](https://api.goldsky.com/api/public/project_cl8ylkiw00krx0hvza0qw17vn/subgraphs/uniswap-v3-base/1.0.0/gn)
and the tag `prod` also has a [public endpoint](https://api.goldsky.com/api/public/project_cl8ylkiw00krx0hvza0qw17vn/subgraphs/uniswap-v3-base/prod/gn).

![Uniswap public subgraph endpoint for prod tag](https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/uniswap-base-public.png)

In general, public endpoints come in the form of `https://api.goldsky.com/api/public/<project_id>/subgraphs/<subgraph name>/<version or tag>/gn`

Goldsky adds rate limiting to all public endpoints to prevent abuse. We currently have a default rate limit of 50 requests per 10 seconds.
This can be unlocked by contacting us at [support@goldsky.com](mailto:support@goldsky.com).

One major downside of public endpoints is that they are completely public and can be accessed by anyone. This means that
anyone can query the data in the subgraph and potentially abuse the endpoint. This is why we also support private endpoints.

## \[*BETA*] Private endpoints

Private endpoints are only accessible by authenticated users. This means that you can control who can access the data in
your subgraph. Private endpoints are only available to users who have been granted access to the subgraph. Accessing
a private endpoint requires sending an `Authorization` header with the GraphQL request. The value of the `Authorization`
header should be in the form of `Bearer <token>` where the `token` is an API token that has been generated through
[Goldsky project general settings](https://app.goldsky.com/dashboard/settings#general). Remember that API tokens are scoped to specific projects. This means an API
token for `projectA` cannot be used to access the private endpoints of subgraphs in `projectB`.

Private endpoints can be toggled on and off for each subgraph and tag. This means that you can have a mix of public and
private endpoints for your subgraph. For example, you can have a public endpoint for your subgraph and a private endpoint
for a specific tag.

Here's an example of how to access a private endpoint using the GraphiQL interface:

![GraphiQL query with Authorization header](https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/query-private-endpoint-graphiql.png)

Private subgraphs endpoints follow the same format as public subgraph endpoints except they start with `/api/private`
instead of `/api/public`. For example, the private endpoint for the `prod` tag of the `uniswap-v3-base/1.0.0` subgraph
would be `https://api.goldsky.com/api/private/project_cl8ylkiw00krx0hvza0qw17vn/subgraphs/uniswap-v3-base/1.0.0/gn`.

### Revoking access

To revoke access to a private endpoint you can simply delete the API token that was used to access the endpoint. If you
don't know which key is used to access the endpoint, you'll have to revoke all API tokens for all users that have access
to the project. While this step is not ideal during this beta, this step will be addressed before this feature reaches general availability.

## Enabling and disabling public and private endpoints

By default, all new subgraphs and their tags come with the public endpoint enabled and the private endpoint disabled.
Both of these settings can be changed using the CLI and the webapp. To change either setting, you must have [`Editor` permissions](../rbac).

### CLI

To toggle one of these settings using the CLI you can use the `goldsky subgraph update` command with the
`--public-endpoint <disabled|enabled>` flag and/or the `--private-endpoint <disabled|enabled>` flag. Here's a complete example
disabling the public endpoint and enabling the private endpoint for the `prod` tag of the `uniswap-v3-base/1.0.0` subgraph:

```bash
goldsky subgraph update uniswap-v3-base/prod --public-endpoint disabled --private-endpoint enabled
```

### Dashboard

To toggle one of these settings using the dashboard webapp you can navigate to the subgraph detail page and use the relevant
toggles to enable or disable the public or private endpoints of the subgraph or its tags.

[//]: # "TODO: add a screenshot of this once the implementation and design are complete"

### Errors

Goldsky does not enforce CORS on our GraphQL endpoints. If you see an error that references CORS, or an error with the response code 429, you're likely seeing an issue with rate limiting. Rate limits can be unlocked on a case-by-case basis on the Scale plan and above. Please [reach out to us](mailto:support@goldsky.com?subject=Rate%20limits%20or%20errors) if you need help with rate limits or any GraphQL response errors.


# Create a multi-chain subgraph
Source: https://docs.goldsky.com/subgraphs/guides/create-a-multi-chain-subgraph

Use Mirror to sync multiple subgraphs to one table.

You can use subgraphs as a pipeline source, allowing you to combine the flexibility of subgraph indexing with the expressiveness of the database of your choice. **You can also push data from *multiple subgraphs* with the same schema into the same sink, allowing you to merge subgraphs across chains.**

## What you'll need

1. One or more subgraphs in your project - this can be from community subgraphs, a deployed subgraph, or a [no-code subgraph](/subgraphs/guides/create-a-no-code-subgraph).

   <Note> If more than one subgraph is desired, they need to have the same graphql schema. You can use [this tool](https://www.npmjs.com/package/graphql-schema-diff) to compare schemas.</Note>
2. A working database supported by Mirror. For more information on setting up a sink, see the [sink documentation](/mirror/sinks/).

## Walkthrough

<Steps>
  <Step title="Prepare a database">
    `goldsky secret list` will show you the database secrets available on your active project.

    If you need to setup a secret, you can use `goldsky secret create -h`. [Here](/mirror/manage-secrets) is an example.
  </Step>

  <Step title="Select the subgraphs to combine">
    Open the [Subgraphs Dashboard](https://app.goldsky.com/subgraphs) and find the deployment IDs of each subgraph you would like to use as a source.

    Run the following query against the subgraph to get the deployment ID.

    ```graphql
    query {
      _meta {
        deployment
      }
    }
    ```
  </Step>

  <Step title="Create the pipeline definition">
    Open a text editor create your definition, using the `subgraphEntity` source. In this example we will use subgraphs on Optimism and on BSC:

    * `qidao-optimism` (`QmPuXT3poo1T4rS6agZfT51ZZkiN3zQr6n5F2o1v9dRnnr`)
    * `qidao-bsc` (`QmWgW69CaTwJYwcSdu36mkXgwWY11RjvX1oMGykrxT3wDS`)

    They have the same schema, and we will be syncing the `account` and `event` entities from each.

    <Warning>
      Entities may be camelCased in the GraphQL API, but here they must be snake\_cased. For example, `dailySnapshot` will be `daily_snapshot` here.
    </Warning>

    <CodeGroup>
      ```yaml qidao-crosschain.yaml
      sources:
        - type: subgraphEntity
          # The deployment IDs you gathered above. If you put multiple,
          # they must have the same schema
          deployments:
            - id: QmPuXT3poo1T4rS6agZfT51ZZkiN3zQr6n5F2o1v9dRnnr
            - id: QmWgW69CaTwJYwcSdu36mkXgwWY11RjvX1oMGykrxT3wDS
          # A reference name, referred to later in the `sourceStreamName` of either a transformation or a sink.
          referenceName: account
          entity:
            # The name of the entities
            name: account
        - type: subgraphEntity
          deployments:
            - id: QmPuXT3poo1T4rS6agZfT51ZZkiN3zQr6n5F2o1v9dRnnr
            - id: QmWgW69CaTwJYwcSdu36mkXgwWY11RjvX1oMGykrxT3wDS
          referenceName: market_daily_snapshot
          entity:
            name: market_daily_snapshot
      # We are just replicating data, so we don't need any SQL transforms.
      transforms: []
      sinks:
        # In this example, we're using a postgres secret called SUPER_SECRET_SECRET.
        # Feel free to change this out with any other type of sink.
        - type: postgres
          # The sourceStreamName matches the above `referenceNames`
          sourceStreamName: account
          table: qidao_accounts
          schema: public
          secretName: SUPER_SECRET_SECRET
        - type: postgres
          sourceStreamName: market_daily_snapshot
          table: qidao_market_daily_snapshot
          schema: public
          secretName: SUPER_SECRET_SECRET
      ```
    </CodeGroup>
  </Step>

  <Step title="Create the pipeline">
    ```shell
    goldsky pipeline create qidao-crosschain --definition-path qidao-crosschain.yaml --status ACTIVE
    ```

    You should see a response from the server like:

    ```
      Successfully validated --definition-path file
     Created pipeline with name: qidao-crosschain
    name: qidao-crosschain
    version: 1
    project_id: project_cl8ylkiw00krx0hvza0qw17vn
    status: INACTIVE
    resource_size: s
    is_deleted: false
    created_at: 1697696162607
    updated_at: 1697696162607
    definition:
      sources:
        - type: subgraphEntity
          entity:
            name: account
          referenceName: account
          deployments:
            - id: QmPuXT3poo1T4rS6agZfT51ZZkiN3zQr6n5F2o1v9dRnnr
            - id: QmWgW69CaTwJYwcSdu36mkXgwWY11RjvX1oMGykrxT3wDS
        - type: subgraphEntity
          entity:
            name: market_daily_snapshot
          referenceName: market_daily_snapshot
          deployments:
            - id: QmPuXT3poo1T4rS6agZfT51ZZkiN3zQr6n5F2o1v9dRnnr
            - id: QmWgW69CaTwJYwcSdu36mkXgwWY11RjvX1oMGykrxT3wDS
    ...
    ```
  </Step>

  <Step title="Monitor the pipeline">
    Monitor the pipeline with `goldsky pipeline monitor qidao-crosschain`. The status should change from `STARTING` to `RUNNING` in a minute or so, and data will start appearing in your postgresql database.
  </Step>

  <Step title="Create API server">
    Once you have multiple subgraphs being written into one destination database, you can set up a GraphQL API server with this database as a source; there are many options to do this:

    * [Apollo Server](https://www.apollographql.com/docs/apollo-server/)
    * [Express GraphQL](https://graphql.org/graphql-js/running-an-express-graphql-server/)
    * [Hasura](https://hasura.io/) \[recommended for quick-start]
  </Step>
</Steps>

<Snippet file="getting-help.mdx" />


# Create no-code subgraphs
Source: https://docs.goldsky.com/subgraphs/guides/create-a-no-code-subgraph



## What you'll need

1. The contract address you're interested in indexing.
2. The ABI (Application Binary Interface) of the contract.

## Walkthrough

<Accordion title="Video walkthrough" icon="video">
  <iframe src="https://jumpshare.com/embed/Qs5rjmavZWFDI9pBCa7g?hideTitle=true" frameBorder="0" webkitallowfullscreen="true" mozallowfullscreen="true" allowFullScreen width="100%" height="450px" />
</Accordion>

<Steps>
  <Step title="Getting the ABI">
    If the contract youre interested in indexing is a contract you deployed, then youll have the contract address and ABI handy. Otherwise, you can use a mix of public explorer tools to find this information. For example, if were interested in indexing the [friend.tech](http://friend.tech) contract

    1. Find the contract address from [Dappradar](https://dappradar.com/)
    2. Click through to the [block explorer](https://basescan.org/address/0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4#code) where the ABI can be found under the `Contract ABI` section. You can also [click here](https://api.basescan.org/api?module=contract\&action=getabi\&address=0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4\&format=raw) to download it.

    Save the ABI to your local file system and make a note of the contract address. Also make a note of the block number the contract was deployed at, youll need this at a later step.
  </Step>

  <Step title="Creating the configuration file">
    The next step is to create the Instant Subgraph configuration file (e.g. `friendtech-config.json`). This file consists of five key sections:

    1. Config version number
    2. Config name
    3. ABIs
    4. Chains
    5. Contract instances

    ### Version number

    <Info>
      As of October 2023, our Instant Subgraph configuration system is on version 1. This may change in the future. This is **not the version number of your subgraph**, but of Goldsky's configuration file format.
    </Info>

    ### Config name

    This is a name of your choice that helps you understand what this config is for. It is only used for internal debugging. For this guide, we'll use`friendtech`.

    ### ABIs, chains, and contract instances

    These three sections are interconnected.

    1. Name your ABI and enter the path to the ABI file you saved earlier (relative to where this config file is located). In this case, `ftshares` and `abi.json`.
    2. Write out the contract instance, referencing the ABI you named earlier, address it's deployed at, chain it's on, the start block.

    ```json friendtech-config.json
    {
      "version": "1",
      "name": "friendtech",
      "abis": {
        "ftshares": {
          "path": "./abi.json"
        }
      },
      "instances": [
        {
          "abi": "ftshares",
          "address": "0xCF205808Ed36593aa40a44F10c7f7C2F67d4A4d4",
          "startBlock": 2430440,
          "chain": "base"
        }
      ]
    }
    ```

    <Info>
      The abi name in `instances` should match a key in `abis`, in this example, `ftshares`. It is possible to have more than one `chains` and more than one ABI. Multiple chains will result in multiple subgraphs. The file `abi.json` in this example should contain the friendtech ABI [downloaded from here](https://api.basescan.org/api?module=contract\&action=getabi\&address=0xcf205808ed36593aa40a44f10c7f7c2f67d4a4d4\&format=raw).
    </Info>

    This configuration can handle multiple contracts with distinct ABIs, the same contract across multiple chains, or multiple contracts with distinct ABIs on multiple chains.

    **For a complete reference of the various properties, please see the[Instant Subgraphs references docs](/reference/config-file/instant-subgraph)**
  </Step>

  <Step title="Deploying the subgraph">
    With your configuration file ready, it's time to deploy the subgraph.

    1. Open the CLI and log in to your Goldsky account with the command:`goldsky login`.
    2. Deploy the subgraph using the command:`goldsky subgraph deploy name/version --from-abi <path-to-config-file>`, then pass in the path to the config file you created. Note - do NOT pass in the ABI itself, but rather the config file defined above. Example: `goldsky subgraph deploy friendtech/1.0 --from-abi friendtech-config.json`

    Goldsky will generate all the necessary subgraph code, deploy it, and return an endpoint that you can start querying.

    Clicking the endpoint link takes you to a web client where you can browse the schema and draft queries to integrate into your app.
  </Step>
</Steps>

## Extending your subgraph with enrichments

Enrichments are a powerful way to add additional data to your subgraph by performing eth calls in the middle of an event or call handler.

See the [enrichments configuration reference](/reference/config-file/instant-subgraph#instance-enrichment) for more information on how to define these enrichments, and for an [example configuration with enrichments](/reference/config-file/instant-subgraph#nouns-enrichment-with-balances-on-transfer).

### Concepts

* Enrichments are defined at the instance level, and executed at the trigger handler level. This means that you can have different enrichments for different data sources or templates and that all enrichment executions are isolated to the handler they are being called from.
  * any additional imports from `@graphprotocol/graph-ts` beyond `BigInt`, `Bytes`, and `store` can be declared in the `options.imports` field of the enrichment (e.g., `BigDecimal`).
* Enrichments always begin by performing all eth calls first, if any eth calls are aborted then the enrichment as a whole is aborted.
  * calls marked as `required` or having another call declare them as a `depends_on` dependency will abort if the call is not successful, otherwise the call output value will remain as `null`.
  * calls marked as `declared` will configure the subgraph to execute the call prior to invoking the mapping handler. This can be useful for performance reasons, but only works for eth calls that have no mapping handler dependencies.
  * calls support `pre` and `post` expressions for `conditions` to test before and after the call, if either fails the call is aborted. Since these are expressions, they can be dynamic or constant values.
  * call `source` is an expression and therefore allows for dynamic values using math or concatenations. If the `source` is simply a contract address then it will be automatically converted to an `Address` type.
  * call `params` is an expression list and can also be dynamic values or constants.
* Enrichments support defining new entities as well as updating existing entities. If the entity name matches the trigger entity name, then the entity field mappings will be applied to the existing entity.
  * entity names should be singular and capitalized, this will ensure that the generated does not produce naming conflicts.
  * entity field mapping values are expressions and can be dynamic or constant values.
  * new enrichment entities are linked to the parent (trigger) entity that created them, with the parent (trigger) entity also linking to the new entity or entities in the opposite direction (always a collection type).
  * note that while you can define existing entities that are not the trigger entity, you may not update existing entities only create new instances of that entity.
  * entities support being created multiple times in a single enrichment, but require a unique `id` expression to be defined for each entity, `id` can by a dynamic value or a constant. this `id` is appended to the parent entity `id` to create a unique `id` for each enrichment entity in the list.
  * entities can be made mutable by setting the `explicit_id` flag to `true`, this will use the value of `id` without appending it to the parent entity `id`, creating an addressable entity that can be updated.

### Snippets

Below are some various examples of configurations for different scenarios. To keep each example brief, we will only show the `enrich` section of the configuration, and in most cases only the part of the `enrich` section that is relevant. See the [enrichments configuration reference](/reference/config-file/instant-subgraph#instance-enrichment) for the full configuration reference.

#### Options

Here we are enabling debugging for the enrichment (this will output the enrichment steps to the subgraph log), as well as importing `BigDecimal` for use in a `calls` or `entities` section.

```json
"enrich": {
  "options": {
    "imports": ["BigDecimal"],
    "debugging": true
  }
}
```

#### Call self

Here we are calling a function on the same contract as the trigger event. This means we can omit the `abi` and `source` configuration fields, as they are implied in this scenario, we only need to include the `name` and `params` fields (if the function declares paramters). We can refer to the result of this call using `calls.balance`.

```json
"calls": {
  "balance": {
    "name": "balanceOf",
    "params": "event.params.owner"
  }
}
```

#### Call dependency

Here we are creating a 2-call dependency, where the second call depends on the first call (the params are `calls.owner` meaning we need the value of the `owner` call before we can invoke `balanceOf`). This means that if the first call fails, the second call will not be executed. Calls are always executed in the order they are configured, so the second call will have access to the output of the first call (in this example, we use that output as a parameter to the second call). We can list multiple calls in the `depends_on` array to create a dependency graph (if needed). Adding a call to the `depends_on` array will not automatically re-order the calls, so be sure to list them in the correct order.

```json
"calls": {
  "owner": {
    "name": "ownerOf",
    "params": "event.params.id"
  },
  "balance": {
    "depends_on": ["owner"],
    "name": "balanceOf",
    "params": "calls.owner"
  }
}
```

#### External contract call for known address

Here we are calling a function on an external contract, where we know the address of the contract ahead of time. In this case, we need to include the `abi` and `source` configuration fields.

```json
"calls": {
  "usdc_balance": {
    "abi": "erc20",
    "source": "0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48",
    "name": "balanceOf",
    "params": "event.params.owner"
  }
}
```

#### External contract call for dynamic address

Here we are setting up a 2 call chain to first determine the contract address, then call a function on that contract. In our example, the `contractAddress` function is returning an `Address` type so we can use the call result directly in the `source` field of the second call. If `contractAddress` was instead returning a `string` type, then we would use `"source": "Address.fromString(calls.contract_address)"`, though this would be an unusual case to observe.

```json
"calls": {
  "contract_address": {
    "name": "contractAddress",
    "params": "event.params.id"
  },
  "balance": {
    "depends_on": ["contract_address"],
    "abi": "erc20",
    "source": "calls.contract_address",
    "name": "balanceOf",
    "params": "event.params.owner"
  }
}
```

#### Required call

Here we are marking a call as required, meaning that if the call fails then the enrichment as a whole will be aborted. This is useful when you do not want to create a new entity (or enrich an existing entity) if the call does not return any meaningful data. Also note that when using `depends_on`, the dependency call is automatically marked as required. This should be used when the address of the contract being called may not always implement the function being called.

```json
"calls": {
  "balance": {
    "abi": "erc20",
    "name": "balanceOf",
    "source": "event.params.address",
    "params": "event.params.owner",
    "required": true
  }
}
```

#### Pre and post conditions

Here we are using conditions to prevent a call from being executed or to abort the enrichment if the call result is not satisfactory. Avoiding an eth call can have a big performance impact if the inputs to the call are often invalid. Avoiding the creation of an entity can save on entity counts if the entity is not needed or useful for various call results. Conditions are simply checked at their target site in the enrichment, and evaluated to its negation to check if an abort is necessary (e.g., `true` becomes `!(true)`, which is always false and therefore never aborts). In this example, we're excluding the call if the `owner` is in a deny list, and we're aborting the enrichment if the balance is `0`.

```json
"calls": {
  "balance": {
    "name": "balanceOf",
    "params": "event.params.owner",
    "conditions": {
      "pre": "![Address.zero().toHexString(), \"0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03\"].includes(event.params.owner.toHexString())",
      "post": "result.value.gt(BigInt.zero())"
    }
  }
}
```

#### Simple entity field mapping constant

Here were are simply replicating the `id` field from the event params into our enrichment entity. This can be useful if you want to filter or sort the enrichment entities by this field.

```json
  "MyEntity": {
    "id uint256": "event.params.id"
  },
```

#### Simple entity field mapping expression

Here we are applying a serialization function to the value of a call result. This is necessary as the enrichment code generator does not resolve the effective type of an expression, so if there is a type mismatch a serialization function must be applied (in this case `String` vs `Address`).

```json
  "MyEntity": {
    "owner address": "calls.owner.toHexString()"
  },
```

#### Complex entity field mapping expression

Here we are conditionally setting the value of `usd_balance` on whether or not the `usdc_balance` call was successful. If the call was not successful, then we set the value to `BigDecimal.zero()`, otherwise we divide the call result by `10^6` (USDC decimals) to convert the balance to a `USD` value.

```json
  "MyEntity": {
    "usd_balance fixed": "calls.usdc_balance === null ? BigDecimal.zero() : calls.usdc_balance!.divDecimal(BigInt.fromU32(10).pow(6).toBigDecimal())"
  },
```

<Snippet file="getting-help.mdx" />

#### Multiple entity instances

Here we are creating multiple instances of an entity in a single enrichment. Each entity id will be suffixed with the provided `id` value.

```json
  "MyEntity": [
    {
      "id": "'sender'",
      "mapping": {
        "balance fixed": "calls.balance"
      }
    },
    {
      "id": "'receiver'",
      "mapping": {
        "balance fixed": "calls.balance"
      }
    }
  ]
```

#### Addressable entity

Here we are creating an entity that is addressable by an explicit id. This means that we can update this entity with new values.

```json
  "MyEntity": [
    {
      "id": "calls.owner.toHexString()",
      "explicit_id": true,
      "mapping": {
        "current_balance fixed": "calls.balance"
      }
    }
  ]
```

<Tip>
  *We must use an array for our entity definition to allow setting the `explicit_id` flag.*
</Tip>

## Examples

Here are some examples of various instant subgraph configurations. Each example builds on the previous example.

Each of these examples can be saved locally to a file (e.g., `subgraph.json`) and deployed using `goldsky subgraph deploy nouns/1.0.0 --from-abi subgraph.json`.

### Simple NOUNS example

This is a basic instant subgraph configuration, a great starting point for learning about instant subgraphs.

```json5 simple-nouns-config.json
{
  "version": "1",
  "name": "nouns/1.0.0",
  "abis": {
    "nouns": [
      {
        "anonymous": false,
        "inputs": [
          {
            "indexed": true,
            "internalType": "address",
            "name": "from",
            "type": "address"
          },
          {
            "indexed": true,
            "internalType": "address",
            "name": "to",
            "type": "address"
          },
          {
            "indexed": true,
            "internalType": "uint256",
            "name": "tokenId",
            "type": "uint256"
          }
        ],
        "name": "Transfer",
        "type": "event"
      }
    ]
  },
  "instances": [
    {
      "abi": "nouns",
      "address": "0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03",
      "startBlock": 12985438,
      "chain": "mainnet"
    }
  ]
}
```

### NOUNS enrichment with receiver balance on transfer

This example describes a very simple enrichment that adds a `balance` field to a `Balance` enrichment entity. This `balance` field is populated by calling the `balanceOf` function on the `to` address of the `Transfer` event.

```json5 nouns-balance-config.json
{
  "version": "1",
  "name": "nouns/1.0.0",
  "abis": {
    "nouns": [
      {
        "anonymous": false,
        "inputs": [
          {
            "indexed": true,
            "internalType": "address",
            "name": "from",
            "type": "address"
          },
          {
            "indexed": true,
            "internalType": "address",
            "name": "to",
            "type": "address"
          },
          {
            "indexed": true,
            "internalType": "uint256",
            "name": "tokenId",
            "type": "uint256"
          }
        ],
        "name": "Transfer",
        "type": "event"
      },
      {
        "inputs": [
          { "internalType": "address", "name": "owner", "type": "address" }
        ],
        "name": "balanceOf",
        "outputs": [
          { "internalType": "uint256", "name": "", "type": "uint256" }
        ],
        "stateMutability": "view",
        "type": "function"
      }
    ]
  },
  "instances": [
    {
      "abi": "nouns",
      "address": "0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03",
      "startBlock": 12985438,
      "chain": "mainnet",
      "enrich": {
        "handlers": {
          "Transfer(indexed address,indexed address,indexed uint256)": {
            "calls": {
              "balance": {
                "name": "balanceOf",
                "params": "event.params.to",
                "required": true
              }
            },
            "entities": {
              "Balance": {
                "owner address": "event.params.to.toHexString()",
                "balance uint256": "calls.balance"
              }
            }
          }
        }
      }
    }
  ]
}
```

### NOUNS enrichment with sender & receiver balance on transfer entities

This example alters our previous example by capturing the `balance` field on both `FromBalance` and `ToBalance` enrichment entities. This `balance` field is populated by calling the `balanceOf` function on both the `from` and `to` address of the `Transfer` event.

```json5 nouns-balance-config-2.json
{
  "version": "1",
  "name": "nouns/1.0.0",
  "abis": {
    "nouns": [
      {
        "anonymous": false,
        "inputs": [
          {
            "indexed": true,
            "internalType": "address",
            "name": "from",
            "type": "address"
          },
          {
            "indexed": true,
            "internalType": "address",
            "name": "to",
            "type": "address"
          },
          {
            "indexed": true,
            "internalType": "uint256",
            "name": "tokenId",
            "type": "uint256"
          }
        ],
        "name": "Transfer",
        "type": "event"
      },
      {
        "inputs": [
          { "internalType": "address", "name": "owner", "type": "address" }
        ],
        "name": "balanceOf",
        "outputs": [
          { "internalType": "uint256", "name": "", "type": "uint256" }
        ],
        "stateMutability": "view",
        "type": "function"
      }
    ]
  },
  "instances": [
    {
      "abi": "nouns",
      "address": "0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03",
      "startBlock": 12985438,
      "chain": "mainnet",
      "enrich": {
        "handlers": {
          "Transfer(indexed address,indexed address,indexed uint256)": {
            "calls": {
              "from_balance": {
                "name": "balanceOf",
                "params": "event.params.from",
                "required": true
              },
              "to_balance": {
                "name": "balanceOf",
                "params": "event.params.to",
                "required": true
              }
            },
            "entities": {
              "FromBalance": {
                "owner address": "event.params.from.toHexString()",
                "balance uint256": "calls.from_balance"
              },
              "ToBalance": {
                "owner address": "event.params.to.toHexString()",
                "balance uint256": "calls.to_balance"
              }
            }
          }
        }
      }
    }
  ]
}
```

### NOUNS enrichment with mutable current balance on transfer for both sender & receiver

This example alters our previous example balance entities to become a single mutable `Balance` entity, so that both sender and receiver use the same entity.

```json5 nouns-mutable-balance-config.json
{
  "version": "1",
  "name": "nouns/1.0.0",
  "abis": {
    "nouns": [
      {
        "anonymous": false,
        "inputs": [
          {
            "indexed": true,
            "internalType": "address",
            "name": "from",
            "type": "address"
          },
          {
            "indexed": true,
            "internalType": "address",
            "name": "to",
            "type": "address"
          },
          {
            "indexed": true,
            "internalType": "uint256",
            "name": "tokenId",
            "type": "uint256"
          }
        ],
        "name": "Transfer",
        "type": "event"
      },
      {
        "inputs": [
          { "internalType": "address", "name": "owner", "type": "address" }
        ],
        "name": "balanceOf",
        "outputs": [
          { "internalType": "uint256", "name": "", "type": "uint256" }
        ],
        "stateMutability": "view",
        "type": "function"
      }
    ]
  },
  "instances": [
    {
      "abi": "nouns",
      "address": "0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03",
      "startBlock": 12985438,
      "chain": "mainnet",
      "enrich": {
        "handlers": {
          "Transfer(indexed address,indexed address,indexed uint256)": {
            "calls": {
              "from_balance": {
                "name": "balanceOf",
                "params": "event.params.from",
                "required": true
              },
              "to_balance": {
                "name": "balanceOf",
                "params": "event.params.to",
                "required": true
              }
            },
            "entities": {
              "Balance": [
                {
                  "id": "event.params.from.toHexString()",
                  "explicit_id": true,
                  "mapping": {
                    "balance uint256": "calls.from_balance"
                  }
                },
                {
                  "id": "event.params.to.toHexString()",
                  "explicit_id": true,
                  "mapping": {
                    "balance uint256": "calls.to_balance"
                  }
                }
              ]
            }
          }
        }
      }
    }
  ]
}
```

<Tip>
  We can now query the `Balance` entity by the owner address (`id`) to see the current balance.

  ```graphql
  {
    balance(id: "0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03") {
      id
      balance
    }
  }
  ```
</Tip>

### NOUNS enrichment with declared eth call

This example alters our previous example by adding the `declared` flag to boost performance of the `balanceOf` eth calls. declared calls only work for eth calls that have no mapping handler dependencies, in other words the call can be executed from the event params only. Also note that call handlers do not support delcared calls (yet), if `declared` is set on a call handler enrichment it will be ignored.

```json5 nouns-declared-calls-config.json
{
  "version": "1",
  "name": "nouns/1.0.0",
  "abis": {
    "nouns": [
      {
        "anonymous": false,
        "inputs": [
          {
            "indexed": true,
            "internalType": "address",
            "name": "from",
            "type": "address"
          },
          {
            "indexed": true,
            "internalType": "address",
            "name": "to",
            "type": "address"
          },
          {
            "indexed": true,
            "internalType": "uint256",
            "name": "tokenId",
            "type": "uint256"
          }
        ],
        "name": "Transfer",
        "type": "event"
      },
      {
        "inputs": [
          { "internalType": "address", "name": "owner", "type": "address" }
        ],
        "name": "balanceOf",
        "outputs": [
          { "internalType": "uint256", "name": "", "type": "uint256" }
        ],
        "stateMutability": "view",
        "type": "function"
      }
    ]
  },
  "instances": [
    {
      "abi": "nouns",
      "address": "0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03",
      "startBlock": 12985438,
      "chain": "mainnet",
      "enrich": {
        "handlers": {
          "Transfer(indexed address,indexed address,indexed uint256)": {
            "calls": {
              "from_balance": {
                "name": "balanceOf",
                "params": "event.params.from",
                "required": true,
                "declared": true
              },
              "to_balance": {
                "name": "balanceOf",
                "params": "event.params.to",
                "required": true,
                "declared": true
              }
            },
            "entities": {
              "Balance": [
                {
                  "id": "event.params.from.toHexString()",
                  "explicit_id": true,
                  "mapping": {
                    "balance uint256": "calls.from_balance"
                  }
                },
                {
                  "id": "event.params.to.toHexString()",
                  "explicit_id": true,
                  "mapping": {
                    "balance uint256": "calls.to_balance"
                  }
                }
              ]
            }
          }
        }
      }
    }
  ]
}
```


# Declared eth-calls
Source: https://docs.goldsky.com/subgraphs/guides/declared-eth-calls

Improve your subgraph performance by declaring eth_calls

[Declarative eth\_calls](https://thegraph.com/docs/en/developing/creating-a-subgraph/#declared-eth_call) are a valuable subgraph feature that allows eth\_calls to be executed ahead of time which significantly improves the indexing performance.
Check out [this example](https://github.com/goldsky-io/documentation-examples/tree/main/goldsky-subgraphs/declared-eth-call-subgraph) for a practical implementation of this method using an ERC-20 subgraph on the Taiko network


# Send subgraph-driven webhooks
Source: https://docs.goldsky.com/subgraphs/guides/send-subgraph-driven-webhooks

Receive real-time HTTP POST requests based on  your subgraphs.

Power Discord notifications, back-end operations, orderbooks, and more with webhooks for subgraphs. Receive real-time HTTP POST requests to your backends whenever a subgraph indexes a new event. Every project has webhooks enabled by default for free.

Let's speed-run a simple example of a webhook. We'll create a webhook that sends a POST request to a URL of your choice whenever a trade occurs on the X2Y2 exchange.

## What you'll need

1. One or more subgraphs in your project - this can be from community subgraphs, a deployed subgraph, or a [no-code subgraph](/subgraphs/guides/create-a-no-code-subgraph).

2. A webhook handler; making a fully functional webhook handler is out of scope for this walkthrough so we'll be using a test platform called [Webhook.site](https://webhook.site/).

## Walkthrough

<Steps>
  <Step title="Deploy subgraph">
    Use Messari's [x2y2](https://thegraph.com/hosted-service/subgraph/messari/x2y2-ethereum) subgraph to the x2y2 exchange.

    ```shell
    > goldsky subgraph deploy x2y2/v1 --from-ipfs-hash Qmaj3MHPQ5AecbPuzUyLo9rFvuQwcAYpkXrf3dTUPV8rRu
    Deploying Subgraph:
      Downloading subgraph from IPFS (This can take a while)
      Validating build path
      Packaging deployment bundle from /var/folders/p5/7qc7spd57jbfv00n84yzc97h0000gn/T/goldsky-deploy-Qmaj3MHPQ5AecbPuzUyLo9rFvuQwcAYpkXrf3dTUPV8rRu
    ```
  </Step>

  <Step title="Create webhook handler">
    Let's use a pre-made webhook handler by going to [webhook.site](https://webhook.site) and copying the URL. It may look like something like `https://webhook.site/<YOUR-UNIQUE-WEBHOOK-SITE-ID>`

    <Info>
      Don't use format `https://webhook.site/#!/<YOUR-UNIQUE-WEBHOOK-SITE-ID>`
    </Info>

    Any new webhook can be sent to this URL and we'll be able to see and inspect the request body.
  </Step>

  <Step title="Create webhook">
    Create a webhook to receive x2y2 trades.

    ```shell
    > goldsky subgraph webhook create x2y2/v1 --name x2y2-trade-webhook --entity trade --url https://webhook.site/<YOUR-UNIQUE-WEBHOOK-URL>
     Creating webhook

    Webhook 'x2y2-trade-webhook' created.
    Make sure calls to your endpoint have the following value for the 'goldsky-webhook-secret' header: whs_01GNV4RMJCFVH14S4YAFW7RGQK
    ```

    A secret will be generated for you to use in your webhook handler. This secret is used to authenticate the webhook request. You can ignore it for the purposes for this speed run.
  </Step>

  <Step title="Test webhook">
    Inspect the webhook.site URL (or your custom handler) again, you should see events start to stream in.
  </Step>
</Steps>

<Snippet file="getting-help.mdx" />


# Subgraph deploy wizard
Source: https://docs.goldsky.com/subgraphs/guides/subgraph-deploy-wizard



## What you'll need

1. The contract address(es) you're interested in indexing.
2. That's it! 

## Walkthrough

We're going to build a subgraph to track the [Nouns contract](https://etherscan.io/address/0x9c8ff314c9bc7f6e59a9d9225fb22946427edc03) on `mainnet`.

<Accordion title="Video walkthrough" icon="video">
  <video controls className="w-full aspect-video" src="https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/subgraphs/guides/subgraph-deploy-wizard-demo.mp4" />
</Accordion>

<Steps>
  <Step title="Launching the wizard CLI">
    ```
    goldsky subgraph init
    ```

    <Info>
      *Remember to run `goldsky login` first if you haven't already authenticated with Goldsky.*
    </Info>

    This will launch the wizard and guide you through the process of deploying a subgraph on Goldsky.

    ```
      Goldsky Subgraph configuration wizard
    ```
  </Step>

  <Step title="Choose a subgraph name">
    The name must start with a letter and contain only letters, numbers, underscores, and hyphens.

    e.g., `nouns-demo`

    ```
    
      Subgraph name
      nouns-demo
    
    ```

    <Tip>
      *see [related argument documentation](#nameandversion-positional-argument)*
    </Tip>
  </Step>

  <Step title="Define a subgraph version">
    This will default to `1.0.0`, but you can change this to anything as long as it starts with a letter or number and contains only letters, numbers, underscores, hyphens, pluses, and periods.

    e.g., `1.0.0-demo+docs`

    ```
    
      Subgraph version
      1.0.0-demo+docs
    
    ```

    <Tip>
      *see [related argument documentation](#nameandversion-positional-argument)*
    </Tip>
  </Step>

  <Step title="Set your target path">
    This must be any valid path on your system, and will default to subgraph name and version as parent and child directories respectively. The target path is where the no-code subgraph configuration will be written, as well as where any remotely fetched files will be saved. Target path is expanded, with `~` (user home directory) and environment variables being replaced accordingly.

    <Info>
      *If you have already run through this guide, or you already have created `~/my-subgraphs/nouns-demo/1.0.0-demo+docs` then this step will be followed with a prompt to confirm overwriting existing files.*
    </Info>

    e.g., `~/my-subgraphs/nouns-demo/1.0.0-demo+docs`

    ```
    
      Subgraph path
      ~/my-subgraphs/nouns-demo/1.0.0-demo+docs
    
    ```

    <Tip>
      *see [related argument documentation](#target-path)*
    </Tip>
  </Step>

  <Step title="Setup ABI sources">
    In most cases this can be left blank so that we automatically source ABIs from local and remote sources. If you have local path(s) that contain various ABIs, you can specify them here.

    e.g., `~/my-subgraphs/abis`

    <Info>
      *In this case, we'll leave this blank here because we haven't saved any ABIs locally to `~/my-subgraphs/abis` yet.*
    </Info>

    ```
    
      Contract ABI source
      path/to/abi, leave blank to skip
    
    ```

    <Tip>
      *see [related argument documentation](#abi)*
    </Tip>
  </Step>

  <Step title="Add contract addresses">
    You can add any number of contract addresses here (as long as you add at least one). After entering all details about a contract, you'll be asked if you want to add another contract. Contract addresses must begin with a `0x` and be exactly `42` characters long.

    e.g., `0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03`

    ```
    
      Contract address
      0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03
    
    ```

    <Tip>
      *see [related argument documentation](#contract)*
    </Tip>
  </Step>

  <Step title="Choose which network to index contract on">
    Decide which network you would like to index for this contract, refer to our [supported networks](/chains/supported-networks) for the full list of available options. If the wrong network is selected, your contract may not exist on that network and no data will indexed.

    e.g., `mainnet`

    ```
    
      Contract network
      mainnet
    
    ```

    <Tip>
      *see [related argument documentation](#network)*
    </Tip>
  </Step>

  <Step title="Choose which block to start indexing on">
    The start block will be automatically determined based on the network you specified in the previous step. A remote source is interrogated to determine this start block, but not all remote sources are able to respond with a valid start block value. If the remote source is unable to acquire a valid start block then the prompt will fallback to `0` and you'll be able to manually enter a start block. If you are unsure what the start block might be, using `0` is a safe bet but may result in a longer indexing time before any data is available.

    e.g., `12985438`

    <Info>
      *In this case, the wizard should have automatically determined the start block for our contract on `mainnet`. If there is a networking issue and the start block is not fetched automatically, please enter `12985438` manually.*
    </Info>

    <Warning>
      *On some networks, contracts deployed more than a year ago may not be possible to automatically determine the start block due to a default configuration option in a common RPC provider software.*
    </Warning>

    ```
    
      Found start block: 12985438
    
      Start block
      12985438
    
    ```

    <Tip>
      *see [related argument documentation](#start-block)*
    </Tip>
  </Step>

  <Step title="Add another network for this contract?">
    In some cases, you may want to index the same contract on multiple networks. If this is the case, you can choose `Yes` and add another network here to repeat the past `2` steps for another network. If you only want to index this contract on one network, you can choose `No` and move on to the next step.

    <Info>
      *In this case, we only want to index this contract on the `mainnet` network, so we'll choose `No`.*
    </Info>

    ```
    
      Add another network?
       Yes /  No
    
    ```
  </Step>

  <Step title="Choose a contract name">
    The contract name will be used to produce generated subgraph code files. This should be a human-readable name that describes the contract you're indexing and must begin with a letter and contain ony letters, numbers, hypens, underscores, and spaces.

    e.g., `NOUNS`

    <Info>
      *The contract name does not need to be all caps, this is just a convention used in this example.*
    </Info>

    ```
    
      Contract name
      NOUNS
    
    ```

    <Tip>
      *see [related argument documentation](#contract-name)*
    </Tip>
  </Step>

  <Step title="Add another contract?">
    In some cases, you may want to index multiple contracts in the same subgraph. If this is the case, you can choose `Yes` and add another contract here to repeat all past steps since previously entering a contract for a new contract. If you only want to index this one contract, you can choose `No` and move on to the next step.

    <Info>
      *In this case, we only want to index this one contract, so we'll choose `No`.*
    </Info>

    ```
    
      Add another contract?
       Yes /  No
    
    ```
  </Step>

  <Step title="Add a description">
    The subgraph description is only for your own reference and will not be used in the generated subgraph code. This can be any text you like, or left empty if no description is desired. The wizard will start with a generic default description.

    e.g., `Goldsky Instant Subgraph for NOUNS`

    <Info>
      *In this case, we'll accept the generic default description.*
    </Info>

    ```
    
      Subgraph description
      Goldsky Instant Subgraph for NOUNS
    
    ```

    <Tip>
      *see [related argument documentation](#description)*
    </Tip>
  </Step>

  <Step title="Enable call handlers?">
    By enabling call hanlders, the subgraph will index all contract calls in addition to events. This will increase the amount of data indexed and may result in a longer indexing time. Choose `Yes` to include calls, otherwise if you only want to index contract events you can choose `No` and move on to the next step.

    <Info>
      *In this case, we will include call handlers, so we'll choose `Yes`.*
    </Info>

    ```
    
      Enable subgraph call handlers?
       Yes /  No
    
    ```

    <Tip>
      *see [related argument documentation](#call-handlers)*
    </Tip>
  </Step>

  <Step title="Proceed with subgraph initialization?">
    We've finished collecting all the necessary information to initialize your subgraph. A brief summary of all your choices as well as a note on whether build and/or deploy is enabled by default is displayed (you will still have an option to cancel before building or deploying). If you're ready to proceed, choose `Yes` to generate the no-code subgraph configuration file. If anything doesn't look quite right you can choose `No` to abort the wizard and start over.

    <Info>
      *In this case, we're happy with all our choices and will choose `Yes` to proceed.*
    </Info>

    ```
    
      Subgraph configuration summary
    
      Build and deploy will be performed
    
      Name: nouns-demo
      Description: Goldsky Instant Subgraph for NOUNS
      Version: 1.0.0-demo+docs
      TargetPath: /Users/someone/my-subgraphs/nouns-demo/1.0.0-demo+docs
      CallHandlers: enabled
      AbiSources:
        - /Users/someone/my-subgraphs/nouns-demo/1.0.0-demo+docs/abis
      Contracts:
        - Address: 0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03
          Name: NOUNS
          Networks:
            - Network: mainnet
              StartBlock: 12985438
    
    
    
      Proceed with subgraph initialization?
       Yes /  No
    
    ```

    <Tip>
      This step is where we fetch any missing ABI's from remote sources.
    </Tip>
  </Step>

  <Step title="Proceed with subgraph build?">
    Once all no-code subgraph configuration files have been written to the target path, the wizard will ask if you would like to proceed with the build stage. This will compile the generated subgraph(s) into a deployable artifact. If you choose `Yes`, the wizard will run the build stage. If you choose `No`, the wizard will exit and all configuration files will remain in the target path.

    <Info>
      *In this case, we will choose `Yes` to proceed with the build stage.*
    </Info>

    <Tip>
      *If you haven't yet logged in with `goldsky login`, the build step will abort with guidance to login first.*
    </Tip>

    ```
    
      Subgraph configuration complete!

      Initializing subgraph nouns-demo/1.0.0-demo+docs
    
      Writing subgraph files to '/Users/someone/my-subgraphs/nouns-demo/1.0.0-demo+docs': All subgraph configuration files written!
    
      Proceed with subgraph build?
       Yes /  No
    
    ```
  </Step>

  <Step title="Proceed with subgraph deploy?">
    Once the build stage has completed, the wizard will ask if you would like to proceed with the deploy stage. This will deploy the built subgraph(s) to Goldsky for the networks configured (1 subgraph per network). If you choose `Yes`, the wizard will run the deploy stage. If you choose `No`, the wizard will exit and all configuration files will remain in the target path.

    <Info>
      *In this case, we will choose `Yes` to proceed with the deploy stage.*
    </Info>

    ```
    
      Building subgraphs: 1 subgraph built!
    
      Proceed with subgraph deploy?
       Yes /  No
    
    ```
  </Step>

  <Step title="Subgraph initialization complete!">
    Our subgraph has now been successfully deployed to Goldsky. The wizard provides a summary of the files written locally, the builds and deploys that were performed, and links to the subgraph dashboard and the GraphiQL web interface to query the subgraph data.

    ```
    
      Deploying 1 subgraphs
    
      nouns-demo-mainnet/1.0.0-demo+docs deployed
    
      Subgraph initialization summary
    
      Configuration files:
    
       /nouns-demo/1.0.0-demo+docs/abis/nouns.json
       /nouns-demo/1.0.0-demo+docs/nouns-demo-mainnet-subgraph.json
    
      Build:
    
        BUILT mainnet
    
      Deploy:
    
        DEPLOYED nouns-demo-mainnet/1.0.0-demo+docs
    
    
    
      Deployed subgraph summary
    
      nouns-demo-mainnet/1.0.0-demo+docs
    
       Dashboard: https://app.goldsky.com//dashboard/subgraphs/nouns-demo-mainnet/1.0.0-demo+docs
       Queries  : https://api.goldsky.com/api/public//subgraphs/nouns-demo-mainnet/1.0.0-demo+docs/gn
    
    
    
      Subgraph initialization complete!
    ```

    <Tip>
      *Most terminals will allow you to `Cmd+click` or `Ctrl+click` on the links to open them in your default browser.*
    </Tip>
  </Step>

  <Step title="Visit the subgraph dashboard">
    With our subgraph deployed we can now monitor its indexing progress and stats using the Goldsky Subgraph *Dashboard* link provided by the wizard. Over the next few minutes our subgraph will reach the edge of mainnet and our queryable data will be fully up to date.

    ![Instant Subgraph Indexing](https://mintlify.s3.us-west-1.amazonaws.com/goldsky-38/images/subgraphs/guides/instant-subgraph-indexing.png)

    <Info>
      *It could take up to a few hours for this subgraph to fully index.*
    </Info>
  </Step>

  <Step title="Query the subgraph data">
    We can now use the GraphiQL *Queries* web interface link provided by the wizard to query the subgraph data. The GraphiQL web interface allows us to test out queries and inspect the indexed data for our the subgraph. The GraphiQL link is also available from the Goldsky Subgraph dashboard. We can use the following query to monitor the latest (`5`) Nouns minted as the subgraph data is indexed.

    ```graphql

    query LatestNouns($count: Int = 5) {
      nounCreateds(first: $count, orderBy: tokenId, orderDirection: desc) {
        id
        block_number
        transactionHash_
        timestamp_
        tokenId
        seed_background
        seed_body
        seed_accessory
        seed_head
        seed_glasses
      }
    }
    ```

    <Tip>
      *We can query the data as it is being indexed, however until our indexing reaches the edge of the chain we won't be able to see the most recent on-chain data.*
    </Tip>
  </Step>
</Steps>

## Wizard CLI arguments

The wizard CLI has many optional arguments that you can use to reduce the amount of manual input required. If sufficient arguments are provided, the wizard will run in non-interactive mode and automatically generate the no-code subgraph configuration file without any prompting. If some arguments are provided but not enough for non-interactive mode, the wizard will run in interactive mode and prompt you for any missing information but automatically prepare the default response with any arguments provided so that you may hit enter to use your supplied argument value.

<Tip>
  All arguments are optional, if none are supplied then all information will be collected interactively.
</Tip>

### `nameAndVersion` positional argument

This is the only positional argument in the format `name`/`version`. It can be omitted completely, provided as only a `name`, or provided as the full `name` and `version` pair. If only the `name` is provided then the `/` should be omitted. It is not possible to only provide a `version` without a `name`.

* The `name` must start with a letter and contain only letters, numbers, underscores, and hyphens for the name portion.
* The `version` must start with a letter or number and contain only letters, numbers, underscores, hyphens, pluses, and periods

#### Examples

* `my-subgraph_2024/1.0.0`
* `my-subgraph_2024`

### `--target-path`

The target path can be an absolute or relative path to a local directory. If the directory does not yet exist then it will be created, if it does exist then the `--force` [argument](#force) must be provided to overwrite existing files.

#### Examples

All of these examples should result in the same target path (for a user named `someone`).

* `~/my-subgraphs`
* `$HOME/my-subgraphs`
* `/Users/someone/my-subgraphs`
* `$(pwd)/my-subgraphs`

### `--force`

This switch prevents the wizard from prompting you to confirm overwriting existing files, or aborting in non-interactive mode.

#### Examples

* `--force` or `--force true` to overwrite
* `--no-force` or `--force false` avoid overwriting

### `--from-config`

If you already have an existing no-code configuration file, you can provide the path to that file here. The wizard will use this file as a template and prompt you for any missing information as well as attempt to fetch any remote files that are not present. Both JSON and yaml formats are supported, and the file must conform to the [version 1 schema](#version-1).

#### Examples

* `~/my-subgraphs/my-subgraph_2024/1.0/subgraph_config.json`
* `~/my-subgraphs/my-subgraph_2024/1.0/subgraph_config.yaml`

### `--abi`

This argument provides the ABI sources, multiple sources can be provided by joining with a comma. Currently only local sources are supported. Known remote sources for ABI's on various supported networks will be automatically used if no local sources can provide an ABI.

#### Examples

* `~/my-subgraphs/abis`
* `~/my-subgraphs/abis,~/my-abis`

### `--contract`

This argument provides the contract address or addresses to index, multiple addresses can be provided by joining with a comma. Each address must begin with a `0x` and be exactly `42` characters long. When supplying multiple contract addresses, interactive mode will provide defaults for each supplied contract successively and default to adding more contracts until until all supplied contracts have been configured.

#### Examples

* `0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03`
* `0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03,0xA178b166bea52449d56895231Bb1194f20c2f102`

### `--contract-events`

This argument provides the contract events to index, multiple events can be provided by joining with a comma. Only valid event names for the contract ABI will be applied, any discrepancy will present the interactive event selector. When supplying no events the interactive event selector always appear.

#### Examples

* `NounCreated`
* `NounCreated,NounBurned`

### `--contract-calls`

This argument provides the contract calls to index, multiple calls can be provided by joining with a comma. Only valid calls names for the contract ABI will be applied, any discrepancy will present the interactive call selector. When supplying no calls the interactive call selector always appear.

#### Examples

* `approve`
* `approve,burn`

### `--network`

This argument provides the network to index the contract on. The network must be one of the supported networks, refer to our [supported networks](/chains/supported-networks) for the full list of available options. Multiple networks can be provided by joining with a comma. When supplying multiple networks, interactive mode will provide defaults for each supplied network successively and default to adding more networks until all supplied networks have been configured. Note that multiple networks will be applied to each contract supplied, so multiple networks and multiple contracts result in the cartesian product of networks and contracts.

#### Examples

* `mainnet`
* `mainnet,xdai` *(for a single contract, means 2 networks for the same contract are indexed)*
* `mainnet,xdai` *(for two contracts, means 2 contracts for each network, 4 contracts total indexed, 2 per network)*

### `--start-block`

This argument provides the start block to index from, multiple start blocks can be provided by joining with a comma. When supplying multiple start blocks, interactive mode will provide defaults for each supplied start block successively and default to adding more start blocks until all supplied start blocks have been configured. Because a start block is required for each contract and network combination, multiple contracts and multiple networks result in the cartesian product of start blocks. In cases where the start block is not known ahead of time for some contract and network pairs, it can be left empty with successive commas to allow the wizard to attempt to determine the start block from a remote source.

#### Examples

* `12985438`
* `12985438,20922867`
* `12985438,,20922867` *(for 2 contracts and 2 networks, where we know the start blocks for both contracts on the 1st network but not the 2nd network)*

### `--contract-name`

This argument provides the contract name to use in the generated subgraph code, multiple contract names can be provided by joining with a comma. If any contract names contain spaces, the whole argument must be wrapped in quotes. Each contract name must start with a letter and contain only letters, numbers, hypens, underscores, and spaces. When supplying multiple contract names, interactive mode will provide defaults for each supplied contract successively and default to adding more contracts until all supplied contracts have been configured.

#### Examples

* `My-Subgraph_Data`
* `"My Subgraph Data"`
* `"My Subgraph Data,My Other Subgraph Data"`
* `subgraph1,subgraph2`

### `--description`

This argument provides the description for the whole no-code subgraph deployment. If multiple networks are supplied the same description will be used for each subgraph deployuments on each network.

### `--call-handlers`

This switch enables call handlers for the subgraph. By default, call handlers are disabled and only events are indexed. Enabling call handlers will increase the amount of data indexed and may result in a longer indexing time but will provide more contract interaction data.

#### Examples

* `--call-handlers` or `--call-handlers true` to enable
* `--no-call-handlers` or `--call-handlers false` to disable

### `--build`

This switch enables the build stage after the wizard has completed writing the configuration files. By default, the build stage is enabled in interactive mode and disabled in non-interactive mode. Enabling the build stage will compile the generated subgraph(s) into a deployable artifact. Explicitly disabling the build stage will also prevent the deploy stage from running, `--no-build` is all that is required to stop after the write files stage.

#### Examples

* `--build` or `--build true` to enable
* `--no-build` or `--build false` to disable

### `--deploy`

This switch enables the deploy stage after the wizard has completed building the subgraph(s). By default, the deploy stage is enabled in interactive mode and disabled in non-interactive mode. Enabling the deploy stage will deploy the built subgraph(s) to the specified network(s). Enabling the deploy stage will implicitly enable the build stage, `--deploy` is all that is required to run both build and deploy stages.

#### Examples

* `--deploy` or `--deploy true` to enable
* `--no-deploy` or `--deploy false` to disable

## Non-interactive mode

If you're looking to automate the process of deploying a subgraph, you can use the wizard in non-interactive mode by passing all the necessary arguments as flags. This can be useful if you're looking to deploy a subgraph as part of a CI/CD pipeline or other automated process. The command will still write all the necessary files to your target path, but it won't prompt you for any input. If the wizard cannot determine a required input value, the command will abort.

It is recommended to use `--force` and `--build` or `--deploy` flags when running the wizard in non-interactive mode. This will ensure that existing files are overwritten and that the subgraph is built and/or deployed after initialization.

### Examples

1. Deploy the **NOUNS** subgraph on `mainnet`

```
goldsky subgraph init nouns-demo/1.0.0 \
  --contract 0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03 \
  --network mainnet \
  --start-block 12985438 \
  --contract-name NOUNS \
  --call-handlers \
  --deploy
```

2. Deploy the **NOUNS** subgraph on `mainnet` with the interactive event and call selectors

```
goldsky subgraph init nouns-demo/1.0.0 \
  --contract 0x9C8fF314C9Bc7F6e59A9d9225Fb22946427eDC03 \
  --contract-events \
  --contract-calls \
  --network mainnet \
  --start-block 12985438 \
  --contract-name NOUNS \
  --call-handlers \
  --deploy
```

3. Deploy the **Uniswap v3** subgraph on `mainnet`

```
goldsky subgraph init uniswap-v3/1.0.0 \
  --contract 0x1f9840a85d5aF5bf1D1762F925BDADdC4201F984 \
  --network mainnet \
  --start-block 10861674 \
  --contract-name UniswapV3 \
  --call-handlers \
  --deploy
```

## Configuration schemas

See the [Instant subgraph configuration reference](/reference/config-file/instant-subgraph) for more information on the configuration schema.


# Introduction
Source: https://docs.goldsky.com/subgraphs/introduction



# Index Onchain Data with Subgraphs

Goldsky provides a completely backwards-compatible subgraph indexing solution. The core of the indexing uses exactly the same WASM processing layer, but in addition, Goldsky offers:

* a rewritten RPC layer, autoscaling query layer, and storage optimizations to improve reliability (99.9%+ uptime) and performance (up to 6x faster)
* webhooks support out-the-box to enable notifications, messaging, and other push-based use cases
* support for custom EVM chains so you can index your own rollup or private blockchain seamlessly

<CardGroup cols={2}>
  <Card title="Deploy a subgraph" icon="upload" href="/subgraphs/deploying-subgraphs" iconType="duotone">
    Deploy a subgraph to Goldsky's shared indexing infrastructure in a number of different ways.
  </Card>

  <Card title="Migrate a subgraph" icon="transporter" href="/subgraphs/migrate-from-the-graph" iconType="duotone">
    Migrate a subgraph from The Graph's hosted service or any other subgraph host.
  </Card>

  <Card title="Manage subgraph endpoints" icon="tags" href="/subgraphs/tags" iconType="duotone">
    Use Tags to manage your subgraph endpoints and swap them in/out seamlessly.
  </Card>

  <Card title="Explore dedicated indexing" icon="microchip" href="/subgraphs/serverless-vs-dedicated" iconType="duotone">
    Learn the pros and cons of Goldsky's dedicated indexing infrastructure.
  </Card>
</CardGroup>

<Snippet file="getting-help.mdx" />


# Migrate from The Graph or another host
Source: https://docs.goldsky.com/subgraphs/migrate-from-the-graph



Goldsky provides a one-step migration for your subgraphs on The Graph's hosted service / decentralized network, or other subgraph host (including your own graph-node). This is a **drop-in replacement** with the following benefits:

* The same subgraph API that your apps already use, allowing for seamless, zero-downtime migration
* A load-balanced network of third-party and on-prem RPC nodes to improve performance and reliability
* Tagging and versioning to hotswap subgraphs, allowing for seamless updates on your frontend
* Alerts and auto-recovery in case of subgraph data consistency issues due to corruption from re-orgs or other issues
* A world-class team who monitors your subgraphs 24/7, with on-call engineering support to help troubleshoot any issues

## Migrate subgraphs to Goldsky

<Accordion title="Install Goldsky's CLI and log in">
  <Snippet file="install-and-login.mdx" />
</Accordion>

If you have subgraphs deployed to The Graphs hosted service, the following command seamlessly migrates your subgraph to Goldsky:

```bash
goldsky subgraph deploy your-subgraph-name/your-version --from-url <your-subgraph-query-url>
```

If you have subgraphs deployed to The Graph's decentralized network, use the IPFS hash instead (visible on The Graph's Explorer page for the specified subgraph):

```bash
goldsky subgraph deploy your-subgraph-name/your-version --from-ipfs-hash <your-subgraph-ipfs-hash>
```

You can get this IPFS deployment hash by querying any subgraph GraphQL endpoint with the following query:

```GraphQL
query {
  _meta {
    deployment
  }
}
```

## Monitor indexing progress

Once you started the migration with the above command, you can monitor your subgraph's indexing status with:

```bash
goldsky subgraph list
```

Alternatively, navigate to [app.goldsky.com](https://app.goldsky.com) to see your subgraphs, their indexing progress, and more.


# Choosing shared vs. dedicated
Source: https://docs.goldsky.com/subgraphs/serverless-vs-dedicated



## Serverless subgraphs

When you make a new subgraph on Goldsky, by default it's hosted on our highly resilient **Serverless Subgraph Platform**.

The platform is fully autoscaling, with a re-engineered RPC and storage layer, and is tuned for fast indexing across the majority of use-cases. It's also completely backwards compatible and runs the same WASM engine as the vanilla open-source graph-node engine.

* Optimized RPC multi-provider layer with a global cache that uses a combination of dedicated and commercial RPC APIs for uptime
* I/O optimized database with under 1ms average commit times

## Dedicated subgraph indexers

When you need improved customizability and performance, Goldsky offers dedicated subgraph indexing nodes. Dedicated machines are provisioned for your project, allowing for customization and optimization at both the indexing and querying layers.

### Indexing enhancements

* support for any EVM-compatible private chain or app chain
* custom RPC layer optimizations methods based on subgraph needs to improve indexing speed

### Querying enhancements

* enable caching with custom rules
* custom database optimizations to speed up specific query patterns, bringing expensive queries down from seconds to milliseconds

To launch a dedicated indexer, please contact us via email at [sales@goldsky.com](mailto:sales@goldsky.com) to get started within one business day.

### Limitations

By default, dedicated indexers are disconnected from Goldsky's [Mirror](mirror/sources/subgraphs) functionality; if you'd like to index and mirror a custom EVM chain, [contact us](mailto:sales@goldsky.com).


# Example Subgraphs Repo
Source: https://docs.goldsky.com/subgraphs/subgraphs-github





# Manage API endpoints with tags
Source: https://docs.goldsky.com/subgraphs/tags



Tags are used to maintain a consistent GraphQL endpoint. You can treat them like pointers or aliases to specific versions, allowing you to swap in new subgraphs in your app without changing your front-end code.

By default, subgraph API endpoints are named after the subgraph name and version, so if you update your subgraph to a new version, you'll need to update your front end to point to the new endpoint.

Using tags, you can manage your versions and seamlessly upgrade your subgraph version without having the URL change.

In this example, we'll assume you have already deployed a subgraph with the name and version `poap-subgraph/1.0.0`. We'll show you how to create a tag and how to move it to another subgraph version.

First, create a tag using the Goldsky CLI and associate it with your subgraph.

```shell
goldsky subgraph tag create subgraph/1.0.0 --tag prod
```

We've now created a new tag called `prod`. Now our GraphQL endpoint will use the word `prod` instead of the version number. You should see the new GraphQL endpoint listed in your terminal after running the command.

Let's say you've upgraded your `poap-subgraph` to verison `2.0.0` and want to start querying it with your `prod` GraphQL endpoint. It's as simple as creating the tag again on the new version.

```shell
goldsky subgraph tag create subgraph/2.0.0 --tag prod
```

Like before, you should see the GraphQL endpoint after running this command, and it should be the same as before. Now your queries will be routed to the `2.0.0` version of the subgraph seamlessly


# Subgraph Webhooks
Source: https://docs.goldsky.com/subgraphs/webhooks

Create webhooks that trigger on every subgraph entity change

When you need to execute code or update a backend based on webhooks, you can use subgraph webhooks to send a payload to an HTTP server for every subgraph entity change.

See the [webhook quick start](/subgraphs/guides/send-subgraph-driven-webhooks) for more a step by step guide in using this feature.

If you're using this feature to push and sync data to a database, consider using [mirror](/subgraphs/guides/create-a-multi-chain-subgraph) to sync subgraph data to your backend with guaranteed data delivery.

## How it works

When a subgraph handler does something like `entity.save()`, an update is written to an intermediate db which powers the subgraph API. This update is interpreted by a real-time watcher and set to your webhook handler, with an `UPDATE`, `INSERT`, or `DELETE` operation.

### Interpreting entity updates

If you're tracking an immutable entity (as in one that does not get updated), then this section is not applicable.

Subgraphs store all versions of entities, each with a `block_range` column which shows it's valid for each block range. This allows you to distinguish between an entity changing vs a change being rolled-back due to blockchain reorgs.

### Entity updates and removals

Updates (when an existing entity's `.save()` is called) in a subgraph entity system is denoted as a new version row being created, with a corresponding update on the last version's row.

There is an entity with the `id: 1` created at block 1. A webhook will fire:

```json
{
    op: "INSERT"
    data: {
        new: {
            id: 1,
            value: 1,
            vid: 2,
            block_range: "[1,)"
        },
        old: null
    }
}
```

In the following block number 2, the entity is updated again.

Two webhooks are then fired. One to track the new version being created,

```json
{
    op: "INSERT"
    data: {
        new: {
            id: 1,
            value: 2,
            vid: 2,
            block_range: "[2,)"
        },
        old: null
    }
}
```

Another to track the previous version being updated,

```json
{
    op: "UPDATE"
    data: {
        new: {
            id: 1,
            value: 1,
            vid: 1,
            block_range: "[1,2)"
        },
        old: {
            id: 1,
            value: 1,
            vid: 1,
            block_range: "[1,)"
        }
    }
}
```

Similar to updates, entity removal in a subgraph mapping handler simply involves updating the block range associated with the entity. There is no actual row deletion outside of blockchain reorganizations.

Entities with a "closed" block range (e.g., \[123, 1234)) can be removed if they aren't needed for historical state.

It is recommended to maintain a "deleted\_at" and "updated\_at" timestamp in the local representation of the entity and keep them updated accordingly.

### Tracking the latest state

If your goal is to track the latest state of an entity for the most recent block, when you see any `CREATE` or `UPDATE` webhook, you can do an `upsert` in your database for the `id`. The `id` always tracks a unique entity. The `vid` in the payload denotes the version of the `id`, where the highest `vid` is the latest version.

### Handling Updates and Race Conditions

It is important to note that there is no guarantee of ordering between the insert and update operation webhooks, as they are part of the same atomic operation when a subgraph mapping handler runs.

An effective strategy involves utilizing the "deleted\_at" and "updated\_at" flags in the local representation to manage any potential race conditions.

## Reference

### Create a new webhook

To create a new webhook for a subgraph entity:

```shell
goldsky subgraph webhook create my-subgraph/1.0.0 --name "" --url "" --entity ""
```

Optionally, you can also add `--secret "some-secret"` to have control over the secret you can use to identify valid traffic from goldsky.

### List webhooks

To see a list of already configured webhooks:

```shell
goldsky subgraph webhook list
```

## Delete a webhook

If you no longer need a webhook, you can delete it with the following command:

```shell
goldsky subgraph webhook delete <name>
```

### Webhook Payload

The webhook payload is a JSON object with the following fields:

```json
 {
  "op": "INSERT", // Can be either INSERT, UPDATE, or DELETE
  "data_source": "x2y2/v1", // The subgraph or indexer that is being tracked
  "data": {
    "old": null, // Entity Data, null if op is INSERT
    "new": { // Entity data, null if op is DELETE
      // This is an example from a subgraph tracking x2y2
      "amount": "1",
      "log_index": 268,
      "price_eth": "0.017",
      "strategy": "STANDARD_SALE",
      "collection": "0x7bdb0a896efacdd130e764f426e555d1ebb52f54",
      "seller": "0xd582a0530a1e5aee63052a68aa745657a8471504",
      "transaction_hash": "0x996d3c9cda22fa47e9bb16e4837a28fccbd5643c952ed687a80fd97ceafb69c6",
      "id": "0x996d3c9cda22fa47e9bb16e4837a28fccbd5643c952ed687a80fd97ceafb69c6-268",
      "block_number": "16322627",
      "vid": "1677156",
      "timestamp": "1672705139",
      "is_bundle": false,
      "buyer": "0x539ea5d6ec0093ff6401dbcd14d049c37a77151b",
      "block_range": "[16322627,)",
      "token_id": "383"
    }
  },
  "webhook_name": "x2y2-webhook", // Name of your webhook
  "webhook_id": "webhook_clcfdc9gb00i50hyd43qeeidu" // Uniquely generated ID for the webhook
  "id": "36a1a4a6-1411-4a13-939c-9dd6422b5674",  // Unique ID for the event
  "delivery_info": {
    "max_retries": 10,
    "current_retry": 0
  },
  "entity": "trade" // The subgraph entity being tracked
}
```


# Teams and projects
Source: https://docs.goldsky.com/teams-and-projects

Teams and projects help you collaborate with your team to build realtime data pipelines.

## Overview

Projects are the primary structure around which work on Goldsky is organized. Every project consists of a group of subgraphs and pipelines, as well as a list of team members who have access to that project.

To manage team members for a given project, select the project and navigate to the [Settings](https://app.goldsky.com/dashboard/settings) menu.

Goldsky supports [RBAC](/rbac) to help restrict who can do what.

<AccordionGroup>
  <Accordion title="Adding team members to a project">
    From the settings menu, click `Invite User` and enter your team members' email address (invitees needs to have a Goldsky account).
  </Accordion>

  <Accordion title="Removing team members from a project">
    From the settings menu, click the tree dots next to the team members' name and click "Remove Team Member".
  </Accordion>

  <Accordion title="Removing yourself from a project">
    <Note>Leaving a project is only available if you're not the only member of the project and it's not your only project.</Note>

    You can remove yourself from a project by clicking the three dots next to your account under Personal and clicking Leave Project.
  </Accordion>
</AccordionGroup>

## Using the Command Line to Manage Teams and Projects

Project and team management is also supported through the command line interface - you can find a description of all project and team-related commands in the [CLI Reference](/reference/cli#project).

* `goldsky login` Login using the API key that you generated in Settings at [https://app.goldsky.com](https://app.goldsky.com) for the given project.
* `goldsky project list` lists all projects youre a member of
* `goldsky project create --name "<projectName>"` creates a new project. Note: this will not log you into that project: you need to go to [https://app.goldsky.com/dashboard/settings](https://app.goldsky.com/dashboard/settings) and generate the API key for that project, then use `goldsky login` with that key.
* `goldsky project update --name "<newProjectName>"` will update the name of the currently active project.
* `goldsky project leave --projectId "<projectId>"` will remove yourself from the project specified. Note: you cannot leave the project you're currently authenticated with.
* `goldsky project users list` will list all the team members of the currently active project
* `goldsky project users invite --emails "<user1email>" "<user2email>" (passing as many emails as you want) --role <role>` will add new users who already have Goldsky accounts to the active project. Note that if you enter email addresses of people who dont already have Goldsky accounts, youll see an error message and none of the users will be invited, even if some people in the list already have Goldsky accounts. Use the `--role` flag to determine what permissions these new user(s) will have in your project.
* `goldsky project users remove --email "<userToRemoveEmail>"` will remove a team member from the active project (youll see an error message if theyre not a team member of the project).
* `goldsky project users update --email "<userToUpdateEmail>" --role <role>` will update the role of a user in the active project (youll see an error message if theyre not a team member of the project)


