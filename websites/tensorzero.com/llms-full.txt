<SYSTEM>This is the full developer documentation for TensorZero Docs</SYSTEM>

# Overview

> TensorZero enables a data & learning flywheel for LLM systems by integrating inference, observability, optimization, evaluations, and experimentation.

**TensorZero creates a feedback loop for optimizing LLM applications — turning production data into smarter, faster, and cheaper models.**

**It’s fully [open source](https://github.com/tensorzero/tensorzero).**

1. Integrate our model gateway
2. Send metrics or feedback
3. Optimize prompts, models, and inference strategies
4. Watch your LLMs improve over time

It provides a **data & learning flywheel for LLMs** by unifying:

* [x] **Inference:** one API for all LLMs, with <1ms P99 overhead
* [x] **Observability:** inference & feedback → your database
* [x] **Optimization:** from prompts to fine-tuning and RL
* [x] **Experimentation:** built-in A/B testing, routing, fallbacks

## How It Works

![TensorZero Flywheel](/_astro/tensorzero-flywheel.BjFGKMAZ.svg)

1. The [TensorZero Gateway](/docs/gateway/) is a high-performance model gateway written in Rust 🦀 that provides a unified API interface for all major LLM providers, allowing for seamless cross-platform integration and fallbacks.
2. It handles structured schema-based inference with <1ms P99 latency overhead (see [Benchmarks](/docs/gateway/benchmarks/)) and built-in observability, experimentation, and [inference-time optimizations](/docs/gateway/guides/inference-time-optimizations/).
3. It also collects downstream metrics and feedback associated with these inferences, with first-class support for multi-step LLM systems.
4. Everything is stored in a ClickHouse data warehouse that you control for real-time, scalable, and developer-friendly analytics.
5. Over time, [TensorZero Recipes](/docs/recipes/) leverage this structured dataset to optimize your prompts and models: run pre-built recipes for common workflows like fine-tuning, or create your own with complete flexibility using any language and platform.
6. Finally, the gateway’s experimentation features and GitOps orchestration enable you to iterate and deploy with confidence, be it a single LLM or thousands of LLMs.

Our goal is to help engineers build, manage, and optimize the next generation of LLM applications: AI systems that learn from real-world experience. Read more about our [Vision & Roadmap](/docs/vision-roadmap/).

Tip

**Start building today.** The [Quick Start](/docs/quickstart/) shows it’s easy to set up an LLM application with TensorZero. If you want to dive deeper, the [Tutorial](/docs/gateway/tutorial/) teaches how to build a simple chatbot, an email copilot, a weather RAG system, and a structured data extraction pipeline.

**Questions?** Ask us on [Slack](https://www.tensorzero.com/slack) or [Discord](https://www.tensorzero.com/discord).

**Using TensorZero at work?** Email us at <hello@tensorzero.com> to set up a Slack or Teams channel with your team (free).

**Work with us.** We’re [hiring in NYC](/jobs/). We’d also welcome [open-source contributions](https://github.com/tensorzero/tensorzero/blob/main/CONTRIBUTING.md)!

# Comparison: TensorZero vs. DSPy

> TensorZero is an open-source alternative to DSPy featuring an LLM gateway, observability, optimization, evaluations, and experimentation.

TensorZero and DSPy serve **different but complementary** purposes in the LLM ecosystem. TensorZero is a full-stack LLM engineering platform focused on production applications and optimization, while DSPy is a framework for programming with language models through modular prompting. **You can get the best of both worlds by using DSPy and TensorZero together!**

## Similarities

* **LLM Optimization.** Both TensorZero and DSPy focus on LLM optimization, but in different ways. DSPy focuses on automated prompt engineering, while TensorZero provides a complete set of tools for optimizing LLM systems (including prompts, models, and inference strategies).

* **LLM Programming Abstractions.** Both TensorZero and DSPy provide abstractions for working with LLMs in a structured way, moving beyond raw prompting to more maintainable approaches.\
  [→ Prompt Templates & Schemas with TensorZero](/docs/gateway/guides/prompt-templates-schemas/)

* **Automated Prompt Engineering.** TensorZero implements MIPROv2, the automated prompt engineering algorithm recommended by DSPy. MIPROv2 jointly optimizes instructions and in-context examples in prompts.\
  [→ Recipe: Automated Prompt Engineering with MIPRO](https://github.com/tensorzero/tensorzero/tree/main/recipes/mipro)

## Key Differences

### TensorZero

* **Production Infrastructure.** TensorZero provides complete production infrastructure including **observability, optimization, evaluations, and experimentation** capabilities. DSPy focuses on the development phase and prompt programming patterns.

* **Model Optimization.** TensorZero provides tools for optimizing models, including fine-tuning and RLHF. DSPy primarily focuses on automated prompt engineering.\
  [→ Optimization Recipes with TensorZero](/docs/recipes/)

* **Inference-Time Optimization.** TensorZero provides inference-time optimizations like dynamic in-context learning. DSPy focuses on offline optimization strategies (e.g. static in-context learning).\
  [→ Inference-Time Optimizations with TensorZero](/docs/gateway/guides/inference-time-optimizations/)

### DSPy

* **Advanced Automated Prompt Engineering.** DSPy provides sophisticated automated prompt engineering tools for LLMs like teleprompters, recursive reasoning, and self-improvement loops. TensorZero doesn’t natively offer these prompt optimization features — you’ll need to complement it with a tool like DSPy.\
  [→ Improving Math Reasoning — Combining TensorZero and DSPy](https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy)

* **Lightweight Design.** DSPy is a lightweight framework focused solely on LLM programming patterns, particularly during the R\&D stage. TensorZero is a more comprehensive platform with additional infrastructure components covering end-to-end LLM engineering workflows.

Feedback

Is TensorZero missing any features that are really important to you? Let us know on [GitHub Discussions](https://github.com/tensorzero/tensorzero/discussions), [Slack](https://www.tensorzero.com/slack), or [Discord](https://www.tensorzero.com/discord).

## Combining TensorZero and DSPy

You can get the best of both worlds by using DSPy and TensorZero together!

TensorZero provides a number of pre-built optimization recipes covering common LLM engineering workflows like supervised fine-tuning and RLHF. But you can also easily create your own recipes and workflows. This example shows how to optimize a TensorZero function using a tool like DSPy.

[→ Improving Math Reasoning — Combining TensorZero and DSPy](https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy)

# Comparison: TensorZero vs. LangChain

> TensorZero is an open-source alternative to LangChain featuring an LLM gateway, observability, optimization, evaluations, and experimentation.

TensorZero and LangChain both provide tools for LLM orchestration, but they serve different purposes in the ecosystem. While LangChain focuses on rapid prototyping with a large ecosystem of integrations, TensorZero is designed for production-grade deployments with built-in observability, optimization, evaluations, and experimentation capabilities.

Interested in LangGraph?

We provide a minimal example [integrating TensorZero with LangGraph](https://github.com/tensorzero/tensorzero/tree/main/examples/integrations/langgraph).

## Similarities

* **LLM Orchestration.** Both TensorZero and LangChain are developer tools that streamline LLM engineering workflows. TensorZero focuses on production-grade deployments and end-to-end LLM engineering workflows (inference, observability, optimization, evaluations, experimentation). LangChain focuses on rapid prototyping and offers complementary commercial products for features like observability.

* **Open Source.** Both TensorZero (Apache 2.0) and LangChain (MIT) are open-source. TensorZero is fully open-source (including TensorZero UI for observability), whereas LangChain requires a commercial offering for certain features (e.g. LangSmith for observability).

* **Unified Interface.** Both TensorZero and LangChain offer a unified interface that allows you to access LLMs from most major model providers with a single integration, with support for structured outputs, tool use, streaming, and more.\
  [→ TensorZero Gateway Quick Start](/docs/quickstart/)

* **Inference-Time Optimizations.** Both TensorZero and LangChain offer inference-time optimizations like dynamic in-context learning.\
  [→ Inference-Time Optimizations with TensorZero](/docs/gateway/guides/inference-time-optimizations/)

* **Inference Caching.** Both TensorZero and LangChain allow you to cache requests to improve latency and reduce costs.\
  [→ Inference Caching with TensorZero](/docs/gateway/guides/inference-caching/)

## Key Differences

### TensorZero

* **Separation of Concerns: Application Engineering vs. LLM Optimization.** TensorZero enables a clear separation between application logic and LLM implementation details. By treating LLM functions as interfaces with structured inputs and outputs, TensorZero allows you to swap implementations without changing application code. This approach makes it easier to manage complex LLM applications, enables GitOps for prompt and configuration management, and streamlines optimization and experimentation workflows. LangChain blends application logic with LLM implementation details, streamlining rapid prototyping but making it harder to maintain and optimize complex applications.\
  [→ Prompt Templates & Schemas with TensorZero](/docs/gateway/guides/prompt-templates-schemas/)\
  [→ Advanced: Think of LLM Applications as POMDPs — Not Agents](/blog/think-of-llm-applications-as-pomdps-not-agents/)

* **Open-Source Observability.** TensorZero offers built-in observability features (including UI), collecting inference and feedback data in your own database. LangChain requires a separate commercial service (LangSmith) for observability.

* **Built-in Optimization.** TensorZero offers built-in optimization features, including supervised fine-tuning, RLHF, and automated prompt engineering recipes. With the TensorZero UI, you can fine-tune models using your inference and feedback data in just a few clicks. LangChain doesn’t offer any built-in optimization features.\
  [→ Optimization Recipes with TensorZero](/docs/recipes/)

* **Built-in Evaluations.** TensorZero offers built-in evaluation functionality, including heuristics and LLM judges. LangChain requires a separate commercial service (LangSmith) for evaluations.\
  [→ TensorZero Evaluations Tutorial](/docs/evaluations/tutorial/)

* **Built-in Experimentation (A/B Testing).** TensorZero offers built-in experimentation features, allowing you to run experiments on your prompts, models, and inference strategies. LangChain doesn’t offer any experimentation features.\
  [→ Experimentation (A/B Testing) with TensorZero](/docs/gateway/guides/experimentation/)

* **Performance & Scalability.** TensorZero is built from the ground up for high performance, with a focus on low latency and high throughput. LangChain introduces substantial latency and memory overhead to your application.\
  [→ TensorZero Gateway Benchmarks](/docs/gateway/benchmarks/)

* **Language and Platform Agnostic.** TensorZero is language and platform agnostic; in addition to its Python client, it supports any language that can make HTTP requests. LangChain only supports applications built in Python and JavaScript.\
  [→ TensorZero Gateway API Reference](/docs/gateway/api-reference/inference/)

* **Batch Inference.** TensorZero supports batch inference with certain model providers, which significantly reduces inference costs. LangChain doesn’t support batch inference.\
  [→ Batch Inference with TensorZero](/docs/gateway/guides/batch-inference/)

* **Credential Management.** TensorZero streamlines credential management for your model providers, allowing you to manage your API keys in a single place and set up advanced workflows like load balancing between API keys. LangChain only offers basic credential management features.\
  [→ Credential Management with TensorZero](/docs/gateway/guides/credential-management/)

* **Automatic Fallbacks for Higher Reliability.** TensorZero allows you to very easily set up retries, fallbacks, load balancing, and routing to increase reliability. LangChain only offers basic, cumbersome fallback functionality.\
  [→ Retries & Fallbacks with TensorZero](/docs/gateway/guides/retries-fallbacks/)

### LangChain

* **Focus on Rapid Prototyping.** LangChain is designed for rapid prototyping, with a focus on ease of use and rapid iteration. TensorZero is designed for production-grade deployments, so it requires more setup and configuration (e.g. a database to store your observability data) — but you can still get started in minutes.\
  [→ TensorZero Quick Start — From 0 to Observability & Fine-Tuning](/docs/quickstart/)

* **Ecosystem of Integrations.** LangChain has a large ecosystem of integrations with other libraries and tools, including model providers, vector databases, observability tools, and more. TensorZero provides many integrations with model providers, but delegates other integrations to the user.

* **Managed Service.** LangChain offers paid managed (hosted) services for features like observability (LangSmith). TensorZero is fully open-source and self-hosted.

Feedback

Is TensorZero missing any features that are really important to you? Let us know on [GitHub Discussions](https://github.com/tensorzero/tensorzero/discussions), [Slack](https://www.tensorzero.com/slack), or [Discord](https://www.tensorzero.com/discord).

# Comparison: TensorZero vs. Langfuse

> TensorZero is an open-source alternative to Langfuse featuring an LLM gateway, observability, optimization, evaluations, and experimentation.

TensorZero and Langfuse both provide open-source tools that streamline LLM engineering workflows. TensorZero focuses on inference and optimization, while Langfuse specializes in powerful interfaces for observability and evals. That said, **you can get the best of both worlds by using TensorZero alongside Langfuse**.

## Similarities

* **Open Source & Self-Hosted.** Both TensorZero and Langfuse are open source and self-hosted. Your data never leaves your infrastructure, and you don’t risk downtime by relying on external APIs. TensorZero is fully open-source, whereas Langfuse gates some of its features behind a paid license.

* **Built-in Observability.** Both TensorZero and Langfuse offer built-in observability features, collecting inference in your own database. Langfuse offers a broader set of advanced observability features, including application-level tracing. TensorZero focuses more on structured data collection for optimization, including downstream metrics and feedback.

* **Built-in Evaluations.** Both TensorZero and Langfuse offer built-in evaluations features, enabling you to sanity check and benchmark the performance of your prompts, models, and more — using heuristics and LLM judges. TensorZero LLM judges are also TensorZero functions, which means you can optimize them using TensorZero’s optimization recipes. Langfuse offers a broader set of built-in heuristics and UI features for evaluations.\
  [→ TensorZero Evaluations Tutorial](/docs/evaluations/tutorial/)

## Key Differences

### TensorZero

* **Unified Inference API.** TensorZero offers a unified inference API that allows you to access LLMs from most major model providers with a single integration, with support for structured outputs, tool use, streaming, and more. Langfuse doesn’t provide a built-in LLM gateway.\
  [→ TensorZero Gateway Quick Start](/docs/quickstart/)

* **Built-in Inference-Time Optimizations.** TensorZero offers built-in inference-time optimizations (e.g. dynamic in-context learning), allowing you to optimize your inference performance. Langfuse doesn’t offer any inference-time optimizations.\
  [→ Inference-Time Optimizations with TensorZero](/docs/gateway/guides/inference-time-optimizations/)

* **Optimization Recipes.** TensorZero offers optimization recipes (e.g. supervised fine-tuning, RLHF, DSPy) that leverage your own data to improve your LLM’s performance. Langfuse doesn’t offer built-in features like this.\
  [→ Optimization Recipes with TensorZero](/docs/recipes/)

* **Automatic Fallbacks for Higher Reliability.** TensorZero offers automatic fallbacks to increase reliability. Langfuse doesn’t offer any such features.\
  [→ Retries & Fallbacks with TensorZero](/docs/gateway/guides/retries-fallbacks/)

* **Built-in Experimentation (A/B Testing).** TensorZero offers built-in experimentation features, allowing you to run experiments on your prompts, models, and inference strategies. Langfuse doesn’t offer any experimentation features.\
  [→ Experimentation (A/B Testing) with TensorZero](/docs/gateway/guides/experimentation/)

### Langfuse

* **Advanced Observability & Evaluations.** While both TensorZero and Langfuse offer observability and evaluations features, Langfuse takes it further with advanced observability features. Additionally, Langfuse offers a prompt playground, which TensorZero doesn’t offer (coming soon!).

* **Access Controls.** Langfuse offers access controls, which TensorZero doesn’t offer. That said, some of Langfuse’s access control features (e.g. SSO) are only available in their paid plans.

* **Managed Service.** Langfuse offers a paid managed (hosted) service in addition to the open-source version. TensorZero is fully open-source and self-hosted.

Feedback

Is TensorZero missing any features that are really important to you? Let us know on [GitHub Discussions](https://github.com/tensorzero/tensorzero/discussions), [Slack](https://www.tensorzero.com/slack), or [Discord](https://www.tensorzero.com/discord).

## Combining TensorZero and Langfuse

You can combine TensorZero and Langfuse to get the best of both worlds.

A leading voice agent startup uses TensorZero for inference and optimization, alongside Langfuse for more advanced observability and evals.

# Comparison: TensorZero vs. LiteLLM

> TensorZero is an open-source alternative to LiteLLM featuring an LLM gateway, observability, optimization, evaluations, and experimentation.

TensorZero and LiteLLM both offer a unified inference API for LLMs, but they have different features beyond that. TensorZero offers a broader set of features (including observability, optimization, evaluations, and experimentation), whereas LiteLLM offers more traditional service gateway features (e.g. access control, queuing) and third-party integrations. That said, **you can get the best of both worlds by using LiteLLM as a model provider inside TensorZero**!

## Similarities

* **Unified Inference API.** Both TensorZero and LiteLLM offer a unified inference API that allows you to access LLMs from most major model providers with a single integration, with support for structured outputs, batch inference, tool use, streaming, and more.\
  [→ TensorZero Gateway Quick Start](/docs/quickstart/)

* **Automatic Fallbacks for Higher Reliability.** Both TensorZero and LiteLLM offer automatic fallbacks to increase reliability.\
  [→ Retries & Fallbacks with TensorZero](/docs/gateway/guides/retries-fallbacks/)

* **Open Source & Self-Hosted.** Both TensorZero and LiteLLM are open source and self-hosted. Your data never leaves your infrastructure, and you don’t risk downtime by relying on external APIs. TensorZero is fully open-source, whereas LiteLLM gates some of its features behind an enterprise license.

* **Inference Caching.** Both TensorZero and LiteLLM allow you to cache requests to improve latency and reduce costs.\
  [→ Inference Caching with TensorZero](/docs/gateway/guides/inference-caching/)

* **Multimodal Inference.** Both TensorZero and LiteLLM support multimodal inference (VLMs).\
  [→ Multimodal Inference with TensorZero](/docs/gateway/guides/multimodal-inference/)

## Key Differences

### TensorZero

* **High Performance.** The TensorZero Gateway was built from the ground up in Rust 🦀 with performance in mind (<1ms P99 latency at 10,000 QPS). LiteLLM is built in Python, resulting in 25-100x+ latency overhead and much lower throughput.\
  [→ Performance Benchmarks: TensorZero vs. LiteLLM](/docs/gateway/benchmarks/)

* **Built-in Observability.** TensorZero offers its own observability features, collecting inference and feedback data in your own database. LiteLLM only offers integrations with third-party observability tools like Langfuse.

* **Built-in Evaluations.** TensorZero offers built-in evaluation functionality, including heuristics and LLM judges. LiteLLM doesn’t offer any evaluations functionality.\
  [→ TensorZero Evaluations Tutorial](/docs/evaluations/tutorial/)

* **Built-in Experimentation (A/B Testing).** TensorZero offers built-in experimentation features, allowing you to run experiments on your prompts, models, and inference strategies. LiteLLM doesn’t offer any experimentation features.\
  [→ Experimentation (A/B Testing) with TensorZero](/docs/gateway/guides/experimentation/)

* **Built-in Inference-Time Optimizations.** TensorZero offers built-in inference-time optimizations (e.g. dynamic in-context learning), allowing you to optimize your inference performance. LiteLLM doesn’t offer any inference-time optimizations.\
  [→ Inference-Time Optimizations with TensorZero](/docs/gateway/guides/inference-time-optimizations/)

* **Optimization Recipes.** TensorZero offers optimization recipes (e.g. supervised fine-tuning, RLHF, DSPy) that leverage your own data to improve your LLM’s performance. LiteLLM doesn’t offer any features like this.\
  [→ Optimization Recipes with TensorZero](/docs/recipes/)

* **Schemas, Templates, GitOps.** TensorZero enables a schema-first approach to building LLM applications, allowing you to separate your application logic from LLM implementation details. This approach allows your to more easily manage complex LLM applications, benefit from GitOps for prompt and configuration management, counterfactually improve data for optimization, and more. LiteLLM only offers the standard unstructured chat completion interface.\
  [→ Prompt Templates & Schemas with TensorZero](/docs/gateway/guides/prompt-templates-schemas/)

### LiteLLM

* **Access Control.** LiteLLM offers many access control features, including auth, service accounts with virtual keys, and budgeting. Many of these features are open-source, but advanced functionality requires an enterprise license. TensorZero doesn’t offer built-in access control features, and instead requires you to manage it externally (e.g using Nginx).

* **Dynamic Provider Routing.** LiteLLM allows you to dynamically route requests to different model providers based on latency, cost, and rate limits. TensorZero only offers static routing capabilities, i.e. a pre-defined sequence of model providers to attempt.\
  [→ Retries & Fallbacks with TensorZero](/docs/gateway/guides/retries-fallbacks/)

* **Request Prioritization.** LiteLLM allows you to prioritize requests over others, which can be useful for high-priority tasks when you’re constrained by rate limits. TensorZero doesn’t offer request prioritization, and instead requires you to manage the request queue externally (e.g. using Redis).

* **Built-in Guardrails Integration.** LiteLLM offers built-in support for integrations with guardrails tools like AWS Bedrock. For now, TensorZero doesn’t offer built-in guardrails, and instead requires you to manage integrations yourself.

* **Managed Service.** LiteLLM offers a paid managed (hosted) service in addition to the open-source version. TensorZero is fully open-source and self-hosted.

Feedback

Is TensorZero missing any features that are really important to you? Let us know on [GitHub Discussions](https://github.com/tensorzero/tensorzero/discussions), [Slack](https://www.tensorzero.com/slack), or [Discord](https://www.tensorzero.com/discord).

## Combining TensorZero and LiteLLM

You can get the best of both worlds by using LiteLLM as a model provider inside TensorZero.

LiteLLM offers an OpenAI-compatible API, so you can use TensorZero’s OpenAI-compatible endpoint to call LiteLLM. Learn more about using [OpenAI-compatible endpoints](/docs/gateway/guides/providers/openai-compatible/).

# Comparison: TensorZero vs. OpenPipe

> TensorZero is an open-source alternative to OpenPipe featuring an LLM gateway, observability, optimization, evaluations, and experimentation.

TensorZero and OpenPipe both provide tools that streamline fine-tuning workflows for LLMs. TensorZero is open-source and self-hosted, while OpenPipe is a paid managed service (inference costs \~2x more than specialized providers supported by TensorZero). That said, **you can get the best of both worlds by using OpenPipe as a model provider inside TensorZero**.

## Similarities

* **LLM Optimization (Fine-Tuning).** Both TensorZero and OpenPipe focus on LLM optimization (e.g. fine-tuning, DPO). OpenPipe focuses on fine-tuning, while TensorZero provides a complete set of tools for optimizing LLM systems (including prompts, models, and inference strategies).\
  [→ Optimization Recipes with TensorZero](/docs/recipes/)

* **Built-in Observability.** Both TensorZero and OpenPipe offer built-in observability features. TensorZero stores inference data in your own database for full privacy and control, while OpenPipe stores it themselves in their own cloud.

* **Built-in Evaluations.** Both TensorZero and OpenPipe offer built-in evaluations features, enabling you to sanity check and benchmark the performance of your prompts, models, and more — using heuristics and LLM judges. TensorZero LLM judges are also TensorZero functions, which means you can optimize them using TensorZero’s optimization recipes.\
  [→ TensorZero Evaluations Tutorial](/docs/evaluations/tutorial/)

## Key Differences

### TensorZero

* **Open Source & Self-Hosted.** TensorZero is fully open source and self-hosted. Your data never leaves your infrastructure, and you don’t risk downtime by relying on external APIs. OpenPipe is a closed-source managed service.

* **No Added Cost (& Cheaper Inference Providers).** TensorZero is free to use: your bring your own LLM API keys and there is no additional cost. OpenPipe charges \~2x on inference costs compared to specialized providers supported by TensorZero (e.g. Fireworks AI).

* **Unified Inference API.** TensorZero offers a unified inference API that allows you to access LLMs from most major model providers with a single integration, with support for structured outputs, tool use, streaming, and more.\
  OpenPipe supports a much smaller set of LLMs.\
  [→ TensorZero Gateway Quick Start](/docs/quickstart/)

* **Built-in Inference-Time Optimizations.** TensorZero offers built-in inference-time optimizations (e.g. dynamic in-context learning), allowing you to optimize your inference performance. OpenPipe doesn’t offer any inference-time optimizations.\
  [→ Inference-Time Optimizations with TensorZero](/docs/gateway/guides/inference-time-optimizations/)

* **Automatic Fallbacks for Higher Reliability.** TensorZero is self-hosted and provides automatic fallbacks between model providers to increase reliability. OpenPipe can fallback their own models to other OpenAI-compatible APIs, but if OpenPipe itself goes down, you’re out of luck.\
  [→ Retries & Fallbacks with TensorZero](/docs/gateway/guides/retries-fallbacks/)

* **Built-in Experimentation (A/B Testing).** TensorZero offers built-in experimentation features, allowing you to run experiments on your prompts, models, and inference strategies. OpenPipe doesn’t offer any experimentation features.\
  [→ Experimentation (A/B Testing) with TensorZero](/docs/gateway/guides/experimentation/)

* **Batch Inference.** TensorZero supports batch inference with certain model providers, which significantly reduces inference costs. OpenPipe doesn’t support batch inference.\
  [→ Batch Inference with TensorZero](/docs/gateway/guides/batch-inference/)

* **Inference Caching.** Both TensorZero and OpenPipe allow you to cache requests to improve latency and reduce costs. OpenPipe only caches requests to their own models, while TensorZero caches requests to all model providers.\
  [→ Inference Caching with TensorZero](/docs/gateway/guides/inference-caching/)

* **Schemas, Templates, GitOps.** TensorZero enables a schema-first approach to building LLM applications, allowing you to separate your application logic from LLM implementation details. This approach allows your to more easily manage complex LLM applications, benefit from GitOps for prompt and configuration management, counterfactually improve data for optimization, and more. OpenPipe only offers the standard unstructured chat completion interface.\
  [→ Prompt Templates & Schemas with TensorZero](/docs/gateway/guides/prompt-templates-schemas/)

### OpenPipe

* **Guardrails.** OpenPipe offers guardrails (runtime AI judges) for your fine-tuned models. TensorZero doesn’t offer built-in guardrails, and instead requires you to manage them yourself.

Feedback

Is TensorZero missing any features that are really important to you? Let us know on [GitHub Discussions](https://github.com/tensorzero/tensorzero/discussions), [Slack](https://www.tensorzero.com/slack), or [Discord](https://www.tensorzero.com/discord).

## Combining TensorZero and OpenPipe

You can get the best of both worlds by using OpenPipe as a model provider inside TensorZero.

OpenPipe provides an OpenAI-compatible API, so you can use models previously fine-tuned with OpenPipe with TensorZero. Learn more about using [OpenAI-compatible endpoints](/docs/gateway/guides/providers/openai-compatible/).

# Comparison: TensorZero vs. OpenRouter

> TensorZero is an open-source alternative to OpenRouter featuring an LLM gateway, observability, optimization, evaluations, and experimentation.

TensorZero and OpenRouter both offer a unified inference API for LLMs, but they have different features beyond that. TensorZero offers a more comprehensive set of features (including observability, optimization, evaluations, and experimentation), whereas OpenRouter offers more dynamic routing capabilities. That said, **you can get the best of both worlds by using OpenRouter as a model provider inside TensorZero**!

## Similarities

* **Unified Inference API.** Both TensorZero and OpenRouter offer a unified inference API that allows you to access LLMs from most major model providers with a single integration, with support for structured outputs, tool use, streaming, and more.\
  [→ TensorZero Gateway Quick Start](/docs/quickstart/)

* **Automatic Fallbacks for Higher Reliability.** Both TensorZero and OpenRouter offer automatic fallbacks to increase reliability.\
  [→ Retries & Fallbacks with TensorZero](/docs/gateway/guides/retries-fallbacks/)

## Key Differences

### TensorZero

* **Open Source & Self-Hosted.** TensorZero is fully open source and self-hosted. Your data never leaves your infrastructure, and you don’t risk downtime by relying on external APIs. OpenRouter is a closed-source external API.

* **No Added Cost.** TensorZero is free to use: your bring your own LLM API keys and there is no additional cost. OpenRouter charges 5% of your inference spend when you bring your own API keys.

* **Built-in Observability.** TensorZero offers built-in observability features, collecting inference and feedback data in your own database. OpenRouter doesn’t offer any observability features.

* **Built-in Evaluations.** TensorZero offers built-in functionality, including heuristics and LLM judges. OpenRouter doesn’t offer any evaluation features.\
  [→ TensorZero Evaluations Tutorial](/docs/evaluations/tutorial/)

* **Built-in Experimentation (A/B Testing).** TensorZero offers built-in experimentation features, allowing you to run experiments on your prompts, models, and inference strategies. OpenRouter doesn’t offer any experimentation features.\
  [→ Experimentation (A/B Testing) with TensorZero](/docs/gateway/guides/experimentation/)

* **Built-in Inference-Time Optimizations.** TensorZero offers built-in inference-time optimizations (e.g. dynamic in-context learning), allowing you to optimize your inference performance. OpenRouter doesn’t offer any inference-time optimizations, except for dynamic model routing via NotDiamond.\
  [→ Inference-Time Optimizations with TensorZero](/docs/gateway/guides/inference-time-optimizations/)

* **Optimization Recipes.** TensorZero offers optimization recipes (e.g. supervised fine-tuning, RLHF, DSPy) that leverage your own data to improve your LLM’s performance. OpenRouter doesn’t offer any features like this.\
  [→ Optimization Recipes with TensorZero](/docs/recipes/)

* **Batch Inference.** TensorZero supports batch inference with certain model providers, which significantly reduces inference costs. OpenRouter doesn’t support batch inference.\
  [→ Batch Inference with TensorZero](/docs/gateway/guides/batch-inference/)

* **Inference Caching.** TensorZero offers inference caching, which can significantly reduce inference costs and latency. OpenRouter doesn’t offer inference caching.\
  [→ Inference Caching with TensorZero](/docs/gateway/guides/inference-caching/)

* **Schemas, Templates, GitOps.** TensorZero enables a schema-first approach to building LLM applications, allowing you to separate your application logic from LLM implementation details. This approach allows your to more easily manage complex LLM applications, benefit from GitOps for prompt and configuration management, counterfactually improve data for optimization, and more. OpenRouter only offers the standard unstructured chat completion interface.\
  [→ Prompt Templates & Schemas with TensorZero](/docs/gateway/guides/prompt-templates-schemas/)

### OpenRouter

* **Dynamic Provider Routing.** OpenRouter allows you to dynamically route requests to different model providers based on latency, cost, and availability. TensorZero only offers static routing capabilities, i.e. a pre-defined sequence of model providers to attempt.\
  [→ Retries & Fallbacks with TensorZero](/docs/gateway/guides/retries-fallbacks/)

* **Dynamic Model Routing.** OpenRouter integrates with NotDiamond to offer dynamic model routing based on input. TensorZero supports other inference-time optimizations but doesn’t support dynamic model routing at this time.\
  [→ Inference-Time Optimizations with TensorZero](/docs/gateway/guides/inference-time-optimizations/)

* **Consolidated Billing.** OpenRouter allows you to access every supported model using a single OpenRouter API key. Under the hood, OpenRouter uses their own API keys with model providers. This approach can increase your rate limits and streamline billing, but slightly increases your inference costs. TensorZero requires you to use your own API keys, without any added cost.

Feedback

Is TensorZero missing any features that are really important to you? Let us know on [GitHub Discussions](https://github.com/tensorzero/tensorzero/discussions), [Slack](https://www.tensorzero.com/slack), or [Discord](https://www.tensorzero.com/discord).

## Combining TensorZero and OpenRouter

You can get the best of both worlds by using OpenRouter as a model provider inside TensorZero.

OpenRouter offers an OpenAI-compatible API, so you can use TensorZero’s OpenAI-compatible endpoint to call OpenRouter. Learn more about using [OpenAI-compatible endpoints](/docs/gateway/guides/providers/openai-compatible/).

# Comparison: TensorZero vs. Portkey

> TensorZero is an open-source alternative to Portkey featuring an LLM gateway, observability, optimization, evaluations, and experimentation.

TensorZero and Portkey offer diverse features to streamline LLM engineering, including an LLM gateway, observability tools, and more. TensorZero is fully open-source and self-hosted, while Portkey offers an open-source gateway but otherwise requires a paid commercial (hosted) service. Additionally, TensorZero has more features around LLM optimization (e.g. advanced fine-tuning workflows and inference-time optimizations), whereas Portkey has a broader set of features around the UI (e.g. prompt playground).

## Similarities

* **Unified Inference API.** Both TensorZero and Portkey offer a unified inference API that allows you to access LLMs from most major model providers with a single integration, with support for structured outputs, batch inference, tool use, streaming, and more.\
  [→ TensorZero Gateway Quick Start](/docs/quickstart/)

* **Automatic Fallbacks, Retries, & Load Balancing for Higher Reliability.** Both TensorZero and Portkey offer automatic fallbacks, retries, and load balancing features to increase reliability.\
  [→ Retries & Fallbacks with TensorZero](/docs/gateway/guides/retries-fallbacks/)

* **Experimentation (A/B Testing or Canary Testing).** Both TensorZero and Portkey offer experimentation features to help you test your prompts and models.\
  [→ Experimentation (A/B Testing) with TensorZero](/docs/gateway/guides/experimentation/)

* **Schemas, Templates.** Both TensorZero and Portkey offer schema and template features to help you manage your LLM applications.\
  [→ Prompt Templates & Schemas with TensorZero](/docs/gateway/guides/prompt-templates-schemas/)

* **Multimodal Inference.** Both TensorZero and Portkey support multimodal inference (VLMs).\
  [→ Multimodal Inference with TensorZero](/docs/gateway/guides/multimodal-inference/)

## Key Differences

### TensorZero

* **Open-Source Observability.** TensorZero offers built-in open-source observability features, collecting inference and feedback data in your own database. Portkey also offers observability features, but they are limited to their commercial (hosted) offering.

* **Built-in Evaluations.** TensorZero offers built-in evaluation functionality, including heuristics and LLM judges. Portkey doesn’t offer any evaluation features.\
  [→ TensorZero Evaluations Tutorial](/docs/evaluations/tutorial/)

* **Open-Source Inference Caching.** TensorZero offers open-source inference caching features, allowing you to cache requests to improve latency and reduce costs. Portkey also offers inference caching features, but they are limited to their commercial (hosted) offering.\
  [→ Inference Caching with TensorZero](/docs/gateway/guides/inference-caching/)

* **Open-Source Fine-Tuning Workflows.** TensorZero offers open-source built-in fine-tuning workflows, allowing you to create custom models using your own data. Portkey also offers fine-tuning features, but they are limited to their enterprise ($$$) offering.\
  [→ Fine-Tuning Recipes with TensorZero](/docs/recipes/)

* **Advanced Fine-Tuning Workflows.** TensorZero offers advanced fine-tuning workflows, including the ability to curate datasets using feedback signals (e.g. production metrics) and the ability to use RLHF for reinforcement learning. Portkey doesn’t offer similar features.\
  [→ Fine-Tuning Recipes with TensorZero](/docs/recipes/)

* **Inference-Time Optimizations.** TensorZero offers built-in inference-time optimizations (e.g. dynamic in-context learning), allowing you to optimize your inference performance. Portkey doesn’t offer any inference-time optimizations.\
  [→ Inference-Time Optimizations with TensorZero](/docs/gateway/guides/inference-time-optimizations/)

* **Programmatic & GitOps-Friendly Orchestration.** TensorZero can be fully orchestrated programmatically in a GitOps-friendly way. Portkey can manage some of its features programmatically, but certain features depend on its external commercial hosted service.

### Portkey

* **Access Control.** Portkey offers access control features, including virtual keys and budgets; that said, these features are only available on their commercial (hosted) offering. TensorZero doesn’t offer built-in access control features, and instead requires you to manage it externally (e.g using Nginx).

* **Prompt Playground.** Portkey offers a prompt playground in its commercial (hosted) offering, allowing you to test your prompts and models in a graphical interface. TensorZero doesn’t offer a prompt playground today (coming soon!).

* **Guardrails.** Portkey offers guardrails features, including integrations with third-party guardrails providers and the ability to use custom guardrails using webhooks. For now, TensorZero doesn’t offer built-in guardrails, and instead requires you to manage integrations yourself.

* **Managed Service.** Portkey offers a paid managed (hosted) service in addition to the open-source version. TensorZero is fully open-source and self-hosted.

# CLI Reference

> Learn how to use the TensorZero Evaluations CLI.

TensorZero Evaluations is available both through a command-line interface (CLI) tool and through the TensorZero UI.

## Usage

We provide a `tensorzero/evaluations` Docker image for easy usage.

We strongly recommend using TensorZero Evaluations CLI with Docker Compose to keep things simple.

docker-compose.yml

```yaml
services:
  evaluations:
    profiles: [evaluations] # this service won't run by default with `docker compose up`
    image: tensorzero/evaluations
    volumes:
      - ./config:/app/config:ro
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:?Environment variable OPENAI_API_KEY must be set.}
      # ... and any other relevant API credentials ...
      - TENSORZERO_CLICKHOUSE_URL=http://chuser:chpassword@clickhouse:8123/tensorzero
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      clickhouse:
        condition: service_healthy
```

```bash
docker compose run --rm evaluations \
    --evaluation-name haiku_eval \
    --dataset-name haiku_dataset \
    --variant-name gpt_4o \
    --concurrency 5
```

Building from Source

You can build the TensorZero Evaluations CLI from source if necessary. See our [GitHub repository](https://github.com/tensorzero/tensorzero/tree/main/evaluations) for instructions.

### Inference Caching

TensorZero Evaluations uses [Inference Caching](/docs/gateway/guides/inference-caching/) to improve inference speed and cost.

By default, it will read from and write to the inference cache. Soon, you’ll be able to customize this behavior.

### Environment Variables

#### `TENSORZERO_CLICKHOUSE_URL`

* **Example:** `TENSORZERO_CLICKHOUSE_URL=http://chuser:chpassword@localhost:8123/database_name`
* **Required:** yes

This environment variable specifies the URL of your ClickHouse database.

#### Model Provider Credentials

* **Example:** `OPENAI_API_KEY=sk-...`
* **Required:** no

If you’re using an external TensorZero Gateway (see `--gateway-url` flag below), you don’t need to provide these credentials to the evaluations tool.

If you’re using a built-in gateway (no `--gateway-url` flag), you must provide same credentials the gateway would use. See [Integrations](https://www.tensorzero.com/docs/gateway/integrations/) for more information.

### CLI Flags

#### `--config-file PATH`

* **Example:** `--config-file /path/to/tensorzero.toml`
* **Required:** no (default: `./config/tensorzero.toml`)

This flag specifies the path to the TensorZero configuration file. You should use the same configuration file for your entire project.

#### `--concurrency N`

* **Example:** `--concurrency 5`
* **Required:** no (default: `1`)

This flag specifies the maximum number of concurrent TensorZero inference requests during evaluation.

#### `--dataset-name NAME` (`-d`)

* **Example:** `--dataset-name my_dataset`
* **Required:** yes

This flag specifies the dataset to use for evaluation. The dataset should be stored in your ClickHouse database.

#### `--evaluation-name NAME` (`-e`)

* **Example:** `--evaluation-name my_evaluation`
* **Required:** yes

This flag specifies the name of the evaluation to run, as defined in your TensorZero configuration file.

#### `--format FORMAT` (`-f`)

* **Options:** `human_readable`, `jsonl`
* **Example:** `--format jsonl`
* **Required:** no (default: `human_readable`)

This flag specifies the output format for the evaluation CLI tool.

You can use the `jsonl` format if you want to programatically process the evaluation results.

#### `--gateway-url URL`

* **Example:** `--gateway-url http://localhost:3000`
* **Required:** no (default: none)

If you provide this flag, the evaluations tool will use an external TensorZero Gateway for inference requests.

If you don’t provide this flag, the evaluations tool will use a built-in TensorZero gateway. In this case, the evaluations tool will require the same credentials the gateway would use. See [Integrations](https://www.tensorzero.com/docs/gateway/integrations/) for more information.

#### `--variant-name NAME` (`-v`)

This flag specifies the variant to evaluate. The variant name should be present in your TensorZero configuration file.

### Exit Status

The evaluations process exits with a status code of `0` if the evaluation was successful, and a status code of `1` if the evaluation failed.

If you configure a `cutoff` for any of your evaluators, the evaluation will fail if the average score for any evaluator is below its cutoff.

Tip

The exit status code is helpful for integrating TensorZero Evaluations into your CI/CD pipeline.

You can define sanity checks for your variants with `cutoff` to detect performance regressions early before shipping to production

# Configuration Reference

> Learn how to configure TensorZero Evaluations.

Tip

The configuration for TensorZero Evaluations should go in the same `tensorzero.toml` file as the rest of your TensorZero configuration.

## `[evaluations.evaluation_name]`

The `evaluations` sub-section of the config file defines the behavior of an evaluation in TensorZero. You can define multiple evaluations by including multiple `[evaluations.evaluation_name]` sections.

If your `evaluation_name` is not a basic string, it can be escaped with quotation marks. For example, periods are not allowed in basic strings, so you can define an evaluation named `foo.bar` as `[evaluations."foo.bar"]`.

tensorzero.toml

```toml
[evaluations.email-guardrails]
# ...
```

### `type`

* **Type:** Literal `"static"` (we may add other options here later on)
* **Required:** yes

### `function_name`

* **Type:** string
* **Required:** yes

This should be the name of a function defined in the `[functions]` section of the gateway config. This value sets which function this evaluation should evaluate when run.

### `[evaluations.evaluation_name.evaluators.evaluator_name]`

The `evaluators` sub-section defines the behavior of a particular evaluator that will be run as part of its parent evaluation. You can define multiple evaluators by including multiple `[evaluations.evaluation_name.evaluators.evaluator_name]` sections.

If your `evaluator_name` is not a basic string, it can be escaped with quotation marks. For example, periods are not allowed in basic strings, so you can define `includes.jpg` as `[evaluations.evaluation_name.evaluators."includes.jpg"]`.

tensorzero.toml

```toml
[evaluations.email-guardrails]
# ...


[evaluations.email-guardrails.evaluators."includes.jpg"]
# ...


[evaluations.email-guardrails.evaluators.check-signature]
# ...
```

#### `type`

* **Type:** string
* **Required:** yes

Defines the type of the evaluator.

TensorZero currently supports the following variant types:

| Type          | Description                                                                                                       |
| :------------ | ----------------------------------------------------------------------------------------------------------------- |
| `llm_judge`   | Use a TensorZero function as a judge                                                                              |
| `exact_match` | Evaluates whether the generated output exactly matches the reference output (skips the datapoint if unavailable). |

tensorzero.toml

```toml
[evaluations.email-guardrails.evaluators.check-signature]
# ...
type = "llm_judge"
# ...
```

`type: "exact_match"`<!-- for the table of contents -->##### `type: "exact_match"`

###### `cutoff`

* **Type:** float
* **Required:** no

Sets a user defined threshold at which the test is passing. This can be useful for applications where the evaluations are run as an automated test. If the average value of this evaluator is below the cutoff, the evaluations binary will return a nonzero status code.

`type: "llm_judge"`<!-- for the table of contents -->##### `type: "llm_judge"`

###### `input_format`

* **Type:** string
* **Required:** no (default: `serialized`)

Defines the format of the input provided to the LLM judge.

* `serialized`: Passes the input messages, generated output, and reference output (if included) as a single serialized string.
* `messages`: Passes the input messages, generated output, and reference output (if included) as distinct messages in the conversation history.

Tip

We only support evaluations with image data when `input_format` is set to `messages`.

tensorzero.toml

```toml
[evaluations.email-guardrails.evaluators.check-signature]
# ...
type = "llm_judge"
input_format = "messages"
# ...
```

###### `output_type`

* **Type:** string
* **Required:** yes

Defines the expected data type of the evaluation result from the LLM judge.

* `float`: The judge is expected to return a floating-point number.
* `boolean`: The judge is expected to return a boolean value.

tensorzero.toml

```toml
[evaluations.email-guardrails.evaluators.check-signature]
# ...
type = "llm_judge"
output_type = "float"
# ...
```

###### `include.reference_output`

* **Type:** boolean
* **Required:** no (default: `false`)

If set to `true`, the reference output associated with the evaluation datapoint will be included in the input provided to the LLM judge. In these cases, the evaluation run will not run this evaluator for datapoints where there is no reference output.

tensorzero.toml

```toml
[evaluations.email-guardrails.evaluators.check-signature]
# ...
type = "llm_judge"
include = { reference_output = true }
# ...
```

###### `optimize`

* **Type:** string
* **Required:** yes

Defines whether the metric produced by the LLM judge should be maximized or minimized.

* `max`: Higher values are better.
* `min`: Lower values are better.

tensorzero.toml

```toml
[evaluations.email-guardrails.evaluators.check-signature]
# ...
type = "llm_judge"
optimize = "max"
# ...
```

###### `cutoff`

* **Type:** float
* **Required:** no

Sets a user defined threshold at which the test is passing. This may be useful for applications where the evaluations are run as an automated test. If the average value of this evaluator is below the cutoff (when `optimize` is `max`) or above the cutoff (when `optimize` is `min`), the evaluations binary will return a nonzero status code.

tensorzero.toml

```toml
[evaluations.email-guardrails.evaluators.check-signature]
# ...
type = "llm_judge"
optimize = "max" # Example: Maximize score
cutoff = 0.8 # Example: Consider passing if average score is >= 0.8
# ...
```

###### `[evaluations.evaluation_name.evaluators.evaluator_name.variants.variant_name]`

An LLM Judge evaluator defines a TensorZero function that is used to judge the output of another TensorZero function. Therefore, all the variant types that are available for a normal TensorZero function are also available for LLMs as judges — including all of our [inference-time optimizations](/docs/gateway/guides/inference-time-optimizations/).

You can include a standard [variant configuration](/docs/gateway/configuration-reference/#functionsfunction_namevariantsvariant_name) in this block, with two modifications:

* Instead of assigning `weight` to each variant, you simply mark a single variant as `active`.
* For `chat_completion` variants, instead of a `system_template` we require `system_instructions` as a text file and take no other templates.

Here we list only the configuration for variants that differs from the configuration for a normal TensorZero function. Please refer the [variant configuration reference](/docs/gateway/configuration-reference/#functionsfunction_namevariantsvariant_name) for the remaining options.

tensorzero.toml

```toml
[evaluations.email-guardrails.evaluators.check-signature]
# ...
type = "llm_judge"
optimize = "max"


[evaluations.email-guardrails.evaluators.check-signature.variants."claude3.5sonnet"]
type = "chat_completion"
model = "anthropic::claude-3-5-sonnet-20241022"
temperature = 0.1
system_instructions = "./evaluations/email-guardrails/check-signature/system_instructions.txt"
# ... other chat completion configuration ...


[evaluations.email-guardrails.evaluators.check-signature.variants."mix3claude3.5sonnet"]
active = true  # if we run the `email-guardrails` evaluation, this is the variant we'll use for the check-signature evaluator
type = "experimental_mixture_of_n"
candidates = ["claude3.5sonnet", "claude3.5sonnet", "claude3.5sonnet"]
```

###### `active`

* **Type**: boolean
* **Required**: Defaults to `true` if there is a single variant configured. Otherwise, this field is required to be set to `true` for exactly one variant.

Sets which of the variants should be used for evaluation runs.

tensorzero.toml

```toml
[evaluations.email-guardrails.evaluators.check-signature]
# ...


[evaluations.email-guardrails.evaluators.check-signature.variants."mix3claude3.5sonnet"]
active = true # if we run the `email-guardrails` evaluation, this is the variant we'll use for the check-signature evaluator
type = "experimental_mixture_of_n"
```

###### `system_instructions`

* **Type:** string (path)
* **Required**: yes

Defines the path to the system instructions file. This path is relative to the configuration file.

This file should contain a text file with the system instructions for the LLM judge. These instructions should instruct the judge to output a float or boolean value. We use JSON mode to enforce that the judge returns a JSON object of the form `{"thinking": "<thinking>", "score": <float or boolean>}` configured to the `output_type` of the evaluator.

evaluations/email-guardrails/check-signature/claude\_35\_sonnet/system\_instructions.txt

```text
Evaluate if the text follows the haiku structure of exactly three lines with a 5-7-5 syllable pattern, totaling 17 syllables. Verify only this specific syllable structure of a haiku without making content assumptions.
```

tensorzero.toml

```toml
[evaluations.email-guardrails.evaluators.check-signature]
# ...
system_instructions = "./evaluations/email-guardrails/check-signature/claude_35_sonnet/system_instructions.txt"
# ...
```

# TensorZero Evaluations Tutorial

> Learn how to use the TensorZero Evaluations to build principled LLM-powered applications.

This guide show how to define and run evaluations for your TensorZero functions.

New to TensorZero?

See our [Quick Start](https://www.tensorzero.com/docs/quickstart) to learn how to set up our LLM gateway, observability, and fine-tuning — in just 5 minutes.

Tip

**You can find the code behind this tutorial and instructions on how to run it on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/evaluations/tutorial).**

Reach out on [Slack](https://www.tensorzero.com/slack) or [Discord](https://www.tensorzero.com/discord) if you have any questions. We’d be happy to help!

## Status Quo

Imagine we have a TensorZero function for writing haikus about a given topic, and want to compare the behavior of GPT-4o and GPT-4o Mini on this task.

Initially, our configuration for this function might look like:

```toml
[functions.write_haiku]
type = "chat"
user_schema = "functions/write_haiku/user_schema.json"


[functions.write_haiku.variants.gpt_4o_mini]
type = "chat_completion"
model = "openai::gpt-4o-mini"
user_template = "functions/write_haiku/user_template.minijinja"


[functions.write_haiku.variants.gpt_4o]
type = "chat_completion"
model = "openai::gpt-4o"
user_template = "functions/write_haiku/user_template.minijinja"
```

User Schema & Template

functions/write\_haiku/user\_schema.json

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "topic": {
      "type": "string"
    }
  },
  "required": ["topic"],
  "additionalProperties": false
}
```

functions/write\_haiku/user\_template.minijinja

```text
Write a haiku about: {{ topic }}
```

How can we evaluate the behavior of our two variants in a principled way?

One option is to build a dataset of “test cases” that we can evaluate them against.

## Datasets

To use TensorZero Evaluations, you first need to build a dataset.

A dataset is a collection of datapoints. Each datapoint has an input and optionally a output. In the context of evaluations, the output in the dataset should be a reference output, i.e. the output you’d have liked to see. You don’t necessarily need to provide a reference output: some evaluators (e.g. LLM judges) can score generated outputs without a reference output (otherwise, that datapoint is skipped).

Let’s create a dataset:

1. Generate many haikus. (On **[GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/evaluations/tutorial)**, we provide a script `main.py` that generates 100 haikus with `write_haiku`.)
2. Open the UI, navigate to “Datasets”, and select “Build Dataset” (`http://localhost:4000/datasets/builder`).
3. Create a new dataset called `haiku_dataset`. Select your `write_haiku` function, “None” as the metric, and “Inference” as the dataset output.

Coming Soon

Soon, you’ll also be able to build datasets programmatically with the TensorZero Gateway.

## Evaluations

Evalutions test the behavior of variants for a TensorZero function.

Let’s define an evaluation in our configuration file:

```toml
[evaluations.haiku_eval]
type = "static"
function_name = "write_haiku"
```

## Evaluators

Each evaluation has one or more evaluators: a rule or behavior you’d like to test.

Today, TensorZero supports two types of evaluators: `exact_match` and `llm_judge`.

Coming Soon

We’re planning to release other types of evaluators soon (e.g. semantic similarity in an embedding space).

### `exact_match`

The `exact_match` evaluator compares the generated output with the datapoint’s reference output. If they are identical, it returns true; otherwise, it returns false.

```toml
[evaluations.haiku_eval.evaluators.exact_match]
type = "exact_match"
```

### `llm_judge`

LLM Judges are special-purpose TensorZero function that can be used to evaluate a TensorZero function.

For example, our haikus should generally follow a specific format, but it’s hard to define a heuristic to determine if they’re correct. Why not ask an LLM?

Let’s do that:

```toml
[evaluations.haiku_eval.evaluators.valid_haiku]
type = "llm_judge"
output_type = "boolean"  # LLM judge should generate a boolean (or float)
optimize = "max"  # higher is better
cutoff = 0.95  # if the variant scores <95% = bad


[evaluations.haiku_eval.evaluators.valid_haiku.variants.gpt_4o_mini_judge]
type = "chat_completion"
model = "openai::gpt-4o-mini"
system_instructions = "evaluations/haiku_eval/valid_haiku/system_instructions.txt"
json_mode = "strict"
```

System Instructions

evaluations/haiku\_eval/valid\_haiku/system\_instructions.txt

```text
Evaluate if the text follows the haiku structure of exactly three lines with a 5-7-5 syllable pattern, totaling 17 syllables. Verify only this specific syllable structure of a haiku without making content assumptions.
```

Here, we defined an evaluator `valid_haiku` of type `llm_judge`, with a variant that uses GPT-4o Mini.

Similar to regular TensorZero functions, we can define multiple variants for an LLM judge. But unlike regular functions, only one variant can be active at a time during evaluation; you can denote that with the `active` property.

Example: Multiple Variants for an LLM Judge

```toml
[evaluations.haiku_eval.evaluators.valid_haiku]
type = "llm_judge"
output_type = "boolean"
optimize = "max"
cutoff = 0.95


[evaluations.haiku_eval.evaluators.valid_haiku.variants.gpt_4o_mini_judge]
type = "chat_completion"
model = "openai::gpt-4o-mini"
system_instructions = "evaluations/haiku_eval/valid_haiku/system_instructions.txt"
json_mode = "strict"
active = true


[evaluations.haiku_eval.evaluators.valid_haiku.variants.gpt_4o_judge]
type = "chat_completion"
model = "openai::gpt-4o"
system_instructions = "evaluations/haiku_eval/valid_haiku/system_instructions.txt"
json_mode = "strict"
```

The LLM judge we showed above generates a boolean, but they can also generate floats.

Let’s define another evalutor that counts the number of metaphors in our haiku.

```toml
[evaluations.haiku_eval.evaluators.metaphor_count]
type = "llm_judge"
output_type = "float"  # LLM judge should generate a boolean (or float)
optimize = "max"
cutoff = 1  # <1 metaphor per haiku = bad


[evaluations.haiku_eval.evaluators.metaphor_count.variants.gpt_4o_mini_judge]
type = "chat_completion"
model = "openai::gpt-4o-mini"
system_instructions = "evaluations/haiku_eval/metaphor_count/system_instructions.txt"
json_mode = "strict"
```

System Instructions

evaluations/haiku\_eval/metaphor\_count/system\_instructions.txt

```text
How many metaphors does the generated haiku have?
```

The LLM judges we’ve defined so far only look at the datapoint’s input and the generated output. But we can also provide the datapoint’s reference output to the judge:

```toml
[evaluations.haiku_eval.evaluators.compare_haikus]
type = "llm_judge"
include = { reference_output = true }  # include the reference output in the LLM judge's context
output_type = "boolean"
optimize = "max"


[evaluations.haiku_eval.evaluators.compare_haikus.variants.gpt_4o_mini_judge]
type = "chat_completion"
model = "openai::gpt-4o-mini"
system_instructions = "evaluations/haiku_eval/compare_haikus/system_instructions.txt"
json_mode = "strict"
```

System Instructions

evaluations/haiku\_eval/compare\_haikus/system\_instructions.txt

```text
Does the generated haiku include the same figures of speech as the reference haiku?
```

## Running an Evaluation

Let’s run our evaluations!

You can run evaluations using the TensorZero Evaluations CLI tool or the TensorZero UI.

Tip

The TensorZero Evaluations CLI tool can be helpful for CI/CD. It’ll exit with code 0 if all evaluations succeed (average score vs. `cutoff`), or code 1 otherwise.

Tip

By default, TensorZero Evaluations uses [Inference Caching](/docs/gateway/guides/inference-caching/) to improve inference speed and cost.

### CLI

To run evaluations in the CLI, you can use the `tensorzero/evaluations` container:

```bash
docker compose run --rm evaluations \
    --evaluation-name haiku_eval \
    --dataset-name haiku_dataset \
    --variant-name gpt_4o \
    --concurrency 5
```

Docker Compose

Here’s the relevant section of the `docker-compose.yml` for the evaluations tool.

You should provide credentials for any LLM judges. Alternatively, the evaluations tool can use an external TensorZero Gateway with the `--gateway-url http://gateway:3000` flag.

```yaml
services:


# ...


  evaluations:
    profiles: [evaluations]
    image: tensorzero/evaluations
    volumes:
      - ./config:/app/config:ro
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:?Environment variable OPENAI_API_KEY must be set.}
      # ... and any other relevant API credentials ...
      - TENSORZERO_CLICKHOUSE_URL=http://chuser:chpassword@clickhouse:8123/tensorzero
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      clickhouse:
        condition: service_healthy


# ...
```

See [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/evaluations/tutorial) for the complete Docker Compose configuration.

Tip

Docker Compose does *not* start this service with `docker compose up` since we have `profiles: [evaluations]`. You need to call it explicitly with `docker compose run evaluations`, as desired.

### UI

To run evaluations in the UI, navigate to “Evaluations” (`http://localhost:4000/evaluations`) and select “New Run”.

You can compare multiple evaluation runs in the TensorZero UI (including evaluation runs for the CLI).

![TensorZero Evaluation UI](/_astro/configuration-reference-evaluation-ui.DbE7EcDG_1xPfCT.webp)

# Frequently Asked Questions

> Learn more about TensorZero: how it works, why we built it, and more.

Tip

**Next steps?** The [Quick Start](/docs/quickstart/) shows it’s easy to set up an LLM application with TensorZero. If you want to dive deeper, the [Tutorial](/docs/gateway/tutorial/) teaches how to build a simple chatbot, an email copilot, a weather RAG system, and a structured data extraction pipeline.

**Questions?** Ask us on [Slack](https://www.tensorzero.com/slack) or [Discord](https://www.tensorzero.com/discord).

**Using TensorZero at work?** Email us at <hello@tensorzero.com> to set up a Slack or Teams channel with your team (free).

**Work with us.** We’re [hiring in NYC](/jobs/). We’d also welcome [open-source contributions](https://github.com/tensorzero/tensorzero/blob/main/CONTRIBUTING.md)!

## Technical

Why is the TensorZero Gateway a proxy instead of a library?

TensorZero’s proxy pattern makes it agnostic to the application’s tech stack, isolated from the business logic, more composable with other tools, and easy to deploy and manage.

Many engineers are (correctly) wary of marginal latency from such a proxy, so we built the gateway from the ground up with performance in mind. In [Benchmarks](/docs/gateway/benchmarks/), it achieves sub-millisecond P99 latency overhead under extreme load. This makes the gateway fast and lightweight enough to be unnoticeable even in the most demanding LLM applications, especially if deployed as a sidecar container.

How is the TensorZero Gateway so fast?

![TensorZero Crab](/_astro/tensorzero-crab.BfMQOhTA_Z1GIfP3.webp)

The TensorZero Gateway was built from from the ground up with performance in mind. It was written in Rust 🦀 and optimizes many common bottlenecks by efficiently managing connections to model providers, pre-compiling schemas and templates, logging data asynchronously, and more.

It achieves <1ms P99 latency overhead under extreme load. In [Benchmarks](/docs/gateway/benchmarks/), LiteLLM @ 100 QPS adds 25-100x+ more latency than the TensorZero Gateway @ 10,000 QPS.

Why did you choose ClickHouse as TensorZero's analytics database?

ClickHouse is open source, [extremely fast](https://www.vldb.org/pvldb/vol17/p3731-schulze.pdf), and versatile. It supports diverse storage backends, query patterns, and data types, including vector search (which will be important for upcoming TensorZero features). From the start, we designed TensorZero to be easy to deploy but able to grow to massive scale. ClickHouse is the best tool for the job.

## Project

Who is behind TensorZero?

We’re a small technical team based in NYC. [Work with us →](/jobs/)

#### Founders

[Viraj Mehta](https://virajm.com) (CTO) recently completed his PhD from CMU, with an emphasis on reinforcement learning for LLMs and nuclear fusion, and previously worked in machine learning at KKR and a fintech startup; he holds a BS in math and an MS in computer science from Stanford.

[Gabriel Bianconi](https://www.gabrielbianconi.com) (CEO) was the chief product officer at Ondo Finance ($20B+ valuation in 2024) and previously spent years consulting on machine learning for companies ranging from early-stage tech startups to some of the largest financial firms; he holds BS and MS degrees in computer science from Stanford.

How is TensorZero licensed?

![TensorZero Freedom](/_astro/tensorzero-freedom.BTOosYUH_Z2cdaTx.webp)

TensorZero is open source under the permissive [Apache 2.0 License](https://github.com/tensorzero/tensorzero/blob/main/LICENSE).

How does TensorZero make money?

[We don’t.](https://www.youtube.com/watch?v=BzAdXyPYKQo)

We’re lucky to have investors who are aligned with our long-term vision, so we’re able to focus on building and snooze this question for a while.

We’re inspired by companies like Databricks and ClickHouse. One day, we’ll launch a managed service that further streamlines LLM engineering, especially in enterprise settings, but open source will always be at the core of our business.

# Overview

> The TensorZero Gateway is a high-performance model gateway that provides a unified interface for all your LLM applications.

The TensorZero Gateway is a high-performance model gateway that provides a unified interface for all your LLM applications.

* **One API for All LLMs.** The gateway provides a unified interface for all major LLM providers, allowing for seamless cross-platform integration and fallbacks. TensorZero natively supports [Anthropic](/docs/gateway/guides/providers/anthropic/), [AWS Bedrock](/docs/gateway/guides/providers/aws-bedrock/), [AWS SageMaker](/docs/gateway/guides/providers/aws-sagemaker/), [Azure OpenAI Service](/docs/gateway/guides/providers/azure/), [Fireworks](/docs/gateway/guides/providers/fireworks/), [GCP Vertex AI Anthropic](/docs/gateway/guides/providers/gcp-vertex-ai-anthropic/), [GCP Vertex AI Gemini](/docs/gateway/guides/providers/gcp-vertex-ai-gemini/), [Google AI Studio (Gemini API)](/docs/gateway/guides/providers/google-ai-studio-gemini/), [Hyperbolic](/docs/gateway/guides/providers/hyperbolic/), [Mistral](/docs/gateway/guides/providers/mistral/), [OpenAI](/docs/gateway/guides/providers/openai/), [Together](/docs/gateway/guides/providers/together/), [vLLM](/docs/gateway/guides/providers/vllm/), and [xAI](/docs/gateway/guides/providers/xai/). Need something else? Your provider is most likely supported because TensorZero integrates with [any OpenAI-compatible API (e.g. Ollama)](/docs/gateway/guides/providers/openai-compatible/). Still not supported? Open an issue on [GitHub](https://github.com/tensorzero/tensorzero/issues) and we’ll integrate it!

* **Blazing Fast.** The gateway (written in Rust 🦀) achieves <1ms P99 latency overhead under extreme load. In [benchmarks](/docs/gateway/benchmarks/), LiteLLM @ 100 QPS adds 25-100x+ more latency than our gateway @ 10,000 QPS.

* **Structured Inferences.** The gateway enforces schemas for inputs and outputs, ensuring robustness for your application. Structured inference data is later used for powerful optimization recipes (e.g. swapping historical prompts before fine-tuning). Learn more about [prompt templates & schemas](/docs/gateway/guides/prompt-templates-schemas/).

* **Multi-Step LLM Workflows.** The gateway provides first-class support for complex multi-step LLM workflows by associating multiple inferences with an episode. Feedback can be assigned at the inference or episode level, allowing for end-to-end optimization of compound LLM systems. Learn more about [episodes](/docs/gateway/guides/episodes/).

* **Built-in Observability.** The gateway collects structured inference traces along with associated downstream metrics and natural-language feedback. Everything is stored in a ClickHouse database for real-time, scalable, and developer-friendly analytics. [TensorZero Recipes](/docs/recipes/) leverage this dataset to optimize your LLMs.

* **Built-in Experimentation.** The gateway automatically routes traffic between variants to enable A/B tests. It ensures consistent variants within an episode in multi-step workflows. More advanced experimentation techniques (e.g. asynchronous multi-armed bandits) are coming soon.

* **Built-in Fallbacks.** The gateway automatically fallbacks failed inferences to different inference providers, or even completely different variants. Ensure misconfiguration, provider downtime, and other edge cases don’t affect your availability.

* **GitOps Orchestration.** Orchestrate prompts, models, parameters, tools, experiments, and more with GitOps-friendly configuration. Manage a few LLMs manually with human-friendly readable configuration files, or thousands of prompts and LLMs entirely programmatically.

## Next Steps

[Quick Start ](/docs/quickstart/)Make your first TensorZero API call with built-in observability in under 5 minutes.

[Tutorial ](/docs/gateway/tutorial/)Build a simple chatbot, an email copilot, a RAG system, and a data extraction pipeline using TensorZero.

[Deployment ](/docs/gateway/deployment/)Quickly deploy locally, or set up high-availability services for production environments.

[Integrations ](/docs/gateway/integrations/)The TensorZero Gateway integrates with the major LLM providers.

[Benchmarks ](/docs/gateway/benchmarks/)The TensorZero Gateway achieves sub-millisecond latency overhead under extreme load.

[API Reference ](/docs/gateway/api-reference/inference/)The TensorZero Gateway provides an unified interface for making inference and feedback API calls.

[Configuration Reference ](/docs/gateway/configuration-reference/)Easily manage your LLM applications with GitOps orchestration — even complex multi-step systems.

# API Reference: Auxiliary Endpoints

> Useful endpoints for deploying and observing the TensorZero gateway.

The TensorZero Gateway exposes several auxiliary endpoints for monitoring and debugging.

## `GET /metrics`

The TensorZero Gateway exposes a [Prometheus](https://prometheus.io/)-compatible `/metrics` endpoint for monitoring.

At the moment, the only available metric is `request_count`, which counts the number of successful requests to the gateway. The metric reports counts for both inference and feedback requests.

### Example Response

GET /metrics

```txt
# ...
request_count{endpoint="inference",function_name="draft_email"} 10
request_count{endpoint="feedback",metric_name="draft_accepted"} 10
# ...
```

## `GET /status`

The `/status` endpoint is a simple liveness probe. It returns HTTP status code 200 if the gateway is running.

### Example Response

GET /status

```json
{ "status": "ok" }
```

## `GET /health`

The `/health` endpoint is a simple readiness probe that checks if the gateway can communicate with the database. It returns HTTP status code 200 if the gateway is ready to serve requests.

### Example Response

GET /health

```json
{ "gateway": "ok", "clickhouse": "ok" }
```

# API Reference: Batch Inference

> API reference for the Batch Inference endpoints.

The `/batch_inference` endpoints allow users to take advantage of batched inference offered by LLM providers. These inferences are often substantially cheaper than the synchronous APIs. The handling and eventual data model for inferences made through this endpoint are equivalent to those made through the main `/inference` endpoint with a few exceptions:

* The batch samples a single variant from the function being called.
* There are no fallbacks or retries for bached functions.
* Only variants of type `chat_completion` are supported.
* Caching is not supported.
* The `dryrun` setting is not supported.
* Streaming is not supported.

Under the hood, the gateway validates all of the requests, samples a single variant from the function being called, handles templating when applicable, and routes the inference to the appropriate model provider. In the batch endpoint there are no fallbacks as the requests are processed asynchronously.

The typical workflow is to first use the `POST /batch_inference` endpoint to submit a batch of requests. Later, you can poll the `GET /batch_inference/{batch_id}` or `GET /batch_inference/:batch_id/inference/:inference_id` endpoint to check the status of the batch and retrieve results. Each poll will return either a pending or failed status or the results of the batch. Even after a batch has completed and been processed, you can continue to poll the endpoint as a way of retrieving the results. The first time a batch has completed and been processed, the results are stored in the ChatInference, JsonInference, and ModelInference tables as with the `/inference` endpoint. The gateway will rehydrate the results into the expected result when polled repeatedly after finishing

Tip

See the [Batch Inference Guide](/docs/gateway/guides/batch-inference/) for a simple example of using the batch inference endpoints.

## `POST /batch_inference`

### Request

#### `additional_tools`

* **Type:** list of lists of tools (see below)
* **Required:** no (default: no additional tools)

A list of lists of tools defined at inference time that the model is allowed to call. This field allows for dynamic tool use, i.e. defining tools at runtime. Each element in the outer list corresponds to a single inference in the batch. Each inner list contains the tools that should be available to the corresponding inference.

You should prefer to define tools in the configuration file if possible. Only use this field if dynamic tool use is necessary for your use case.

Each tool is an object with the following fields: `description`, `name`, `parameters`, and `strict`.

The fields are identical to those in the configuration file, except that the `parameters` field should contain the JSON schema itself rather than a path to it. See [Configuration Reference](/docs/gateway/configuration-reference/#toolstool_name) for more details.

#### `allowed_tools`

* **Type:** list of lists of strings
* **Required:** no

A list of lists of tool names that the model is allowed to call. The tools must be defined in the configuration file. Each element in the outer list corresponds to a single inference in the batch. Each inner list contains the names of the tools that are allowed for the corresponding inference.

Any tools provided in `additional_tools` are always allowed, irrespective of this field.

### `credentials`

* **Type:** object (a map from dynamic credential names to API keys)
* **Required:** no (default: no credentials)

Each model provider in your TensorZero configuration can be configured to accept credentials at inference time by using the `dynamic` location (e.g. `dynamic::my_dynamic_api_key_name`). See the [configuration reference](/docs/gateway/configuration-reference/#modelsmodel_nameprovidersprovider_name) for more details. The gateway expects the credentials to be provided in the `credentials` field of the request body as specified below. The gateway will return a 400 error if the credentials are not provided and the model provider has been configured with dynamic credentials.

Example

```toml
[models.my_model_name.providers.my_provider_name]
# ...
# Note: the name of the credential field (e.g. `api_key_location`) depends on the provider type
api_key_location = "dynamic::my_dynamic_api_key_name"
# ...
```

```json
{
  // ...
  "credentials": {
    // ...
    "my_dynamic_api_key_name": "sk-..."
    // ...
  }
  // ...
}
```

#### `episode_ids`

* **Type:** list of UUIDs
* **Required:** no

The IDs of existing episodes to associate the inferences with. Each element in the list corresponds to a single inference in the batch. You can provide `null` for episode IDs for elements that should start a fresh episode.

Only use episode IDs that were returned by the TensorZero gateway.

#### `function_name`

* **Type:** string
* **Required:** yes

The name of the function to call. This function will be the same for all inferences in the batch.

The function must be defined in the configuration file.

#### `inputs`

* **Type:** list of `input` objects (see below)
* **Required:** yes

The input to the function.

Each element in the list corresponds to a single inference in the batch.

##### `input[].messages`

* **Type:** list of messages (see below)
* **Required:** no (default: `[]`)

A list of messages to provide to the model.

Each message is an object with the following fields:

* `role`: The role of the message (`assistant` or `user`).
* `content`: The content of the message (see below).

The `content` field can be have one of the following types:

* string: the text for a text message (only allowed if there is no schema for that role)
* list of content blocks: the content blocks for the message (see below)

A content block is an object with the field `type` and additional fields depending on the type.

If the content block has type `text`, it must have either of the following additional fields:

* `text`: The text for the content block.
* `arguments`: A JSON object containing the function arguments for TensorZero functions with templates and schemas (see [Prompt Templates & Schemas](/docs/gateway/guides/prompt-templates-schemas/) for details).

If the content block has type `tool_call`, it must have the following additional fields:

* `arguments`: The arguments for the tool call.
* `id`: The ID for the content block.
* `name`: The name of the tool for the content block.

If the content block has type `tool_result`, it must have the following additional fields:

* `id`: The ID for the content block.
* `name`: The name of the tool for the content block.
* `result`: The result of the tool call.

If the content block has type `image`, it must have either of the following additional fields:

* `url`: The URL for a remote image.
* `mime_type` and `data`: The MIME type and base64-encoded data for an embedded image.
  * We support the following MIME types: `image/png`, `image/jpeg`, and `image/webp`.

See the [Multimodal Inference](/docs/gateway/guides/multimodal-inference/) guide for more details on how to use images in inference.

If the content block has type `raw_text`, it must have the following additional fields:

* `value`: The text for the content block. This content block will ignore any relevant templates and schemas for this function.

If the content block has type `unknown`, it must have the following additional fields:

* `data`: The original content block from the provider, without any validation or transformation by TensorZero.
* `model_provider_name` (optional): A string specifying when this content block should be included in the model provider input. If set, the content block will only be provided to this specific model provider. If not set, the content block is passed to all model providers.

For example, the following hypothetical unknown content block will send the `daydreaming` content block to inference requests targeting the `your_model_provider_name` model provider.

```json
{
  "type": "unknown",
  "data": {
    "type": "daydreaming",
    "dream": "..."
  },
  "model_provider_name": "tensorzero::model_name::your_model_name::provider_name::your_model_provider_name"
}
```

This is the most complex field in the entire API. See this example for more details.

Example

```json
{
  // ...
  "input": {
    "messages": [
      // If you don't have a user (or assistant) schema...
      {
        "role": "user", // (or "assistant")
        "content": "What is the weather in Tokyo?"
      },
      // If you have a user (or assistant) schema...
      {
        "role": "user", // (or "assistant")
        "content": [
          {
            "type": "text",
            "arguments": {
              "location": "Tokyo"
              // ...
            }
          }
        ]
      },
      // If the model previously called a tool...
      {
        "role": "assistant",
        "content": [
          {
            "type": "tool_call",
            "id": "0",
            "name": "get_temperature",
            "arguments": "{\"location\": \"Tokyo\"}"
          }
        ]
      },
      // ...and you're providing the result of that tool call...
      {
        "role": "user",
        "content": [
          {
            "type": "tool_result",
            "id": "0",
            "name": "get_temperature",
            "result": "70"
          }
        ]
      },
      // You can also specify a text message using a content block...
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What about NYC?" // (or object if there is a schema)
          }
        ]
      },
      // You can also provide multiple content blocks in a single message...
      {
        "role": "assistant",
        "content": [
          {
            "type": "text",
            "text": "Sure, I can help you with that." // (or object if there is a schema)
          },
          {
            "type": "tool_call",
            "id": "0",
            "name": "get_temperature",
            "arguments": "{\"location\": \"New York\"}"
          }
        ]
      }
      // ...
    ]
    // ...
  }
  // ...
}
```

##### `input[].system`

* **Type:** string or object
* **Required:** no

The input for the system message.

If the function does not have a system schema, this field should be a string.

If the function has a system schema, this field should be an object that matches the schema.

#### `output_schemas`

* **Type:** list of optional objects (valid JSON Schema)
* **Required:** no

A list of JSON schemas that will be used to validate the output of the function for each inference in the batch. Each element in the list corresponds to a single inference in the batch. These can be null for elements that need to use the `output_schema` defined in the function configuration. This schema is used for validating the output of the function, and sent to providers which support structured outputs.

#### `parallel_tool_calls`

* **Type:** list of optional booleans
* **Required:** no

A list of booleans that indicate whether each inference in the batch should be allowed to request multiple tool calls in a single conversation turn. Each element in the list corresponds to a single inference in the batch. You can provide `null` for elements that should use the configuration value for the function being called. If you don’t provide this field entirely, we default to the configuration value for the function being called.

Most model providers do not support parallel tool calls. In those cases, the gateway ignores this field. At the moment, only Fireworks AI and OpenAI support parallel tool calls.

#### `params`

* **Type:** object (see below)
* **Required:** no (default: `{}`)

Override inference-time parameters for a particular variant type. This fields allows for dynamic inference parameters, i.e. defining parameters at runtime.

This field’s format is `{ variant_type: { param: [value1, ...], ... }, ... }`. You should prefer to set these parameters in the configuration file if possible. Only use this field if you need to set these parameters dynamically at runtime. Each parameter if specified should be a list of values that may be null that is the same length as the batch size.

Note that the parameters will apply to every variant of the specified type.

Currently, we support the following:

* `chat_completion`

  * `frequency_penalty`
  * `max_tokens`
  * `presence_penalty`
  * `seed`
  * `temperature`
  * `top_p`

See [Configuration Reference](/docs/gateway/configuration-reference/#functionsfunction_namevariantsvariant_name) for more details on the parameters, and Examples below for usage.

Example

For example, if you wanted to dynamically override the `temperature` parameter for a `chat_completion` variant for the first inference in a batch of 3, you’d include the following in the request body:

```json
{
  // ...
  "params": {
    "chat_completion": {
      "temperature": [0.7, null, null]
    }
  }
  // ...
}
```

#### `tags`

* **Type:** list of optional JSON objects with string keys and values
* **Required:** no

User-provided tags to associate with the inference.

Each element in the list corresponds to a single inference in the batch.

For example, `[{"user_id": "123"}, null]` or `[{"author": "Alice"}, {"author": "Bob"}]`.

#### `tool_choice`

* **Type:** list of optional strings
* **Required:** no

If set, overrides the tool choice strategy for the equest.

Each element in the list corresponds to a single inference in the batch.

The supported tool choice strategies are:

* `none`: The function should not use any tools.
* `auto`: The model decides whether or not to use a tool. If it decides to use a tool, it also decides which tools to use.
* `required`: The model should use a tool. If multiple tools are available, the model decides which tool to use.
* `{ specific = "tool_name" }`: The model should use a specific tool. The tool must be defined in the `tools` section of the configuration file or provided in `additional_tools`.

#### `variant_name`

* **Type:** string
* **Required:** no

If set, pins the batch inference request to a particular variant (not recommended).

You should generally not set this field, and instead let the TensorZero gateway assign a variant. This field is primarily used for testing or debugging purposes.

### Response

For a POST request to `/batch_inference`, the response is a JSON object containing metadata that allows you to refer to the batch and poll it later on. The response is an object with the following fields:

#### `batch_id`

* **Type:** UUID

The ID of the batch.

#### `inference_ids`

* **Type:** list of UUIDs

The IDs of the inferences in the batch.

#### `episode_ids`

* **Type:** list of UUIDs

The IDs of the episodes associated with the inferences in the batch.

### Example

Imagine you have a simple TensorZero function that generates haikus using GPT-4o Mini.

```toml
[functions.generate_haiku]
type = "chat"


[functions.generate_haiku.variants.gpt_4o_mini]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
```

You can submit a batch inference job to generate multiple haikus with a single request. Each entry in `inputs` is equal to the `input` field in a regular inference request.

```sh
curl -X POST http://localhost:3000/batch_inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "generate_haiku",
    "variant_name": "gpt_4o_mini",
    "inputs": [
      {
        "messages": [
          {
            "role": "user",
            "content": "Write a haiku about artificial intelligence."
          }
        ]
      },
      {
        "messages": [
          {
            "role": "user",
            "content": "Write a haiku about general aviation."
          }
        ]
      },
      {
        "messages": [
          {
            "role": "user",
            "content": "Write a haiku about anime."
          }
        ]
      }
    ]
  }'
```

The response contains a `batch_id` as well as `inference_ids` and `episode_ids` for each inference in the batch.

```json
{
  "batch_id": "019470f0-db4c-7811-9e14-6fe6593a2652",
  "inference_ids": [
    "019470f0-d34a-77a3-9e59-bcc66db2b82f",
    "019470f0-d34a-77a3-9e59-bcdd2f8e06aa",
    "019470f0-d34a-77a3-9e59-bcecfb7172a0"
  ],
  "episode_ids": [
    "019470f0-d34a-77a3-9e59-bc933973d087",
    "019470f0-d34a-77a3-9e59-bca6e9b748b2",
    "019470f0-d34a-77a3-9e59-bcb20177bf3a"
  ]
}
```

## `GET /batch_inference/:batch_id`

Both this and the following GET endpoint can be used to poll the status of a batch. If you use this endpoint and poll with only the batch ID the entire batch will be returned if possible. The response format depends on the function type as well as the batch status when polled.

### Pending

`{"status": "pending"}`

### Failed

`{"status": "failed"}`

### Completed

#### `status`

* **Type:** literal string `"completed"`

#### `batch_id`

* **Type:** UUID

#### `inferences`

* **Type:** list of objects that exactly match the response body in the inference endpoint documented [here](/docs/gateway/api-reference/inference/#response).

### Example

Extending the example from above: you can use the `batch_id` to poll the status of this job:

```sh
curl -X GET http://localhost:3000/batch_inference/019470f0-db4c-7811-9e14-6fe6593a2652
```

While the job is pending, the response will only contain the `status` field.

```json
{
  "status": "pending"
}
```

Once the job is completed, the response will contain the `status` field and the `inferences` field. Each inference object is the same as the response from a regular inference request.

```json
{
  "status": "completed",
  "batch_id": "019470f0-db4c-7811-9e14-6fe6593a2652",
  "inferences": [
    {
      "inference_id": "019470f0-d34a-77a3-9e59-bcc66db2b82f",
      "episode_id": "019470f0-d34a-77a3-9e59-bc933973d087",
      "variant_name": "gpt_4o_mini",
      "content": [
        {
          "type": "text",
          "text": "Whispers of circuits,  \nLearning paths through endless code,  \nDreams in binary."
        }
      ],
      "usage": {
        "input_tokens": 15,
        "output_tokens": 19
      }
    },
    {
      "inference_id": "019470f0-d34a-77a3-9e59-bcdd2f8e06aa",
      "episode_id": "019470f0-d34a-77a3-9e59-bca6e9b748b2",
      "variant_name": "gpt_4o_mini",
      "content": [
        {
          "type": "text",
          "text": "Wings of freedom soar,  \nClouds embrace the lonely flight,  \nSky whispers adventure."
        }
      ],
      "usage": {
        "input_tokens": 15,
        "output_tokens": 20
      }
    },
    {
      "inference_id": "019470f0-d34a-77a3-9e59-bcecfb7172a0",
      "episode_id": "019470f0-d34a-77a3-9e59-bcb20177bf3a",
      "variant_name": "gpt_4o_mini",
      "content": [
        {
          "type": "text",
          "text": "Vivid worlds unfold,  \nHeroes rise with dreams in hand,  \nInk and dreams collide."
        }
      ],
      "usage": {
        "input_tokens": 14,
        "output_tokens": 20
      }
    }
  ]
}
```

## `GET /batch_inference/:batch_id/inference/:inference_id`

This endpoint can be used to poll the status of a single inference in a batch. Since the polling involves pulling data on all the inferences in the batch, we also store the status of all those inference in ClickHouse. The response format depends on the function type as well as the batch status when polled.

### Pending

`{"status": "pending"}`

### Failed

`{"status": "failed"}`

### Completed

#### `status`

* **Type:** literal string `"completed"`

#### `batch_id`

* **Type:** UUID

#### `inferences`

* **Type:** list containing a single object that exactly matches the response body in the inference endpoint documented [here](/docs/gateway/api-reference/inference/#response).

### Example

Similar to above, we can also poll a particular inference:

```sh
curl -X GET http://localhost:3000/batch_inference/019470f0-db4c-7811-9e14-6fe6593a2652/inference/019470f0-d34a-77a3-9e59-bcc66db2b82f
```

While the job is pending, the response will only contain the `status` field.

```json
{
  "status": "pending"
}
```

Once the job is completed, the response will contain the `status` field and the `inferences` field. Unlike above, this request will return a list containing only the requested inference.

```json
{
  "status": "completed",
  "batch_id": "019470f0-db4c-7811-9e14-6fe6593a2652",
  "inferences": [
    {
      "inference_id": "019470f0-d34a-77a3-9e59-bcc66db2b82f",
      "episode_id": "019470f0-d34a-77a3-9e59-bc933973d087",
      "variant_name": "gpt_4o_mini",
      "content": [
        {
          "type": "text",
          "text": "Whispers of circuits,  \nLearning paths through endless code,  \nDreams in binary."
        }
      ],
      "usage": {
        "input_tokens": 15,
        "output_tokens": 19
      }
    }
  ]
}
```

# API Reference: Feedback

> API reference for the `/feedback` endpoint.

## `POST /feedback`

The `/feedback` endpoint assigns feedback to a particular inference or episode.

Each feedback is associated with a metric that is defined in the configuration file.

### Request

#### `dryrun`

* **Type:** boolean
* **Required:** no

If `true`, the feedback request will be executed but won’t be stored to the database (i.e. no-op).

This field is primarily for debugging and testing, and you should ignore it in production.

#### `episode_id`

* **Type:** UUID
* **Required:** when the metric level is `episode`

The episode ID to provide feedback for.

You should use this field when the metric level is `episode`.

Only use episode IDs that were returned by the TensorZero gateway.

#### `inference_id`

* **Type:** UUID
* **Required:** when the metric level is `inference`

The inference ID to provide feedback for.

You should use this field when the metric level is `inference`.

Only use inference IDs that were returned by the TensorZero gateway.

#### `metric_name`

* **Type:** string
* **Required:** yes

The name of the metric to provide feedback.

For example, if your metric is defined as `[metrics.draft_accepted]` in your configuration file, then you would set `metric_name: "draft_accepted"`.

The metric names `comment` and `demonstration` are reserved for special types of feedback. A `comment` is free-form text (string) that can be assigned to either an inference or an episode. The `demonstration` metric accepts values that would be a valid output. See [Metrics & Feedback](/docs/gateway/guides/metrics-feedback/) for more details.

#### `tags`

* **Type:** flat JSON object with string keys and values
* **Required:** no

User-provided tags to associate with the feedback.

For example, `{"user_id": "123"}` or `{"author": "Alice"}`.

#### `value`

* **Type:** varies
* **Required:** yes

The value of the feedback.

The type of the value depends on the metric type (e.g. boolean for a metric with `type = "boolean"`).

### Response

#### `feedback_id`

* **Type:** UUID

The ID assigned to the feedback.

### Examples

#### Inference-Level Boolean Metric

Inference-Level Boolean Metric

##### Configuration

tensorzero.toml

```toml
# ...
[metrics.draft_accepted]
type = "boolean"
level = "inference"
# ...
```

##### Request

* Python

  POST /feedback

  ```python
  from tensorzero import AsyncTensorZeroGateway


  async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      result = await client.feedback(
          inference_id="00000000-0000-0000-0000-000000000000",
          metric_name="draft_accepted",
          value=True,
      )
  ```

* HTTP

  POST /feedback

  ```bash
  curl -X POST http://localhost:3000/feedback \
    -H "Content-Type: application/json" \
    -d '{
      "inference_id": "00000000-0000-0000-0000-000000000000",
      "metric_name": "draft_accepted",
      "value": true,
    }'
  ```

##### Response

POST /feedback

```json
{ "feedback_id": "11111111-1111-1111-1111-111111111111" }
```

#### Episode-Level Float Metric

Episode-Level Float Metric

##### Configuration

tensorzero.toml

```toml
# ...
[metrics.user_rating]
type = "float"
level = "episode"
# ...
```

##### Request

* Python

  POST /feedback

  ```python
  from tensorzero import AsyncTensorZeroGateway


  async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      result = await client.feedback(
          episode_id="00000000-0000-0000-0000-000000000000",
          metric_name="user_rating",
          value=10,
      )
  ```

* HTTP

  POST /feedback

  ```bash
  curl -X POST http://localhost:3000/feedback \
    -H "Content-Type: application/json" \
    -d '{
      "episode_id": "00000000-0000-0000-0000-000000000000",
      "metric_name": "user_rating",
      "value": 10
    }'
  ```

##### Response

POST /feedback

```json
{ "feedback_id": "11111111-1111-1111-1111-111111111111" }
```

# API Reference: Inference

> API reference for the `/inference` endpoint.

## `POST /inference`

The inference endpoint is the core of the TensorZero Gateway API.

Under the hood, the gateway validates the request, samples a variant from the function, handles templating when applicable, and routes the inference to the appropriate model provider. If a problem occurs, it attempts to gracefully fallback to a different model provider or variant. After a successful inference, it returns the data to the client and asynchronously stores structured information in the database.

Tip

See the [API Reference for `POST /openai/v1/chat/completions`](/docs/gateway/api-reference/inference-openai-compatible/) for an inference endpoint compatible with the OpenAI API.

### Request

#### `additional_tools`

* **Type:** a list of tools (see below)
* **Required:** no (default: `[]`)

A list of tools defined at inference time that the model is allowed to call. This field allows for dynamic tool use, i.e. defining tools at runtime.

You should prefer to define tools in the configuration file if possible. Only use this field if dynamic tool use is necessary for your use case.

Each tool is an object with the following fields: `description`, `name`, `parameters`, and `strict`.

The fields are identical to those in the configuration file, except that the `parameters` field should contain the JSON schema itself rather than a path to it. See [Configuration Reference](/docs/gateway/configuration-reference/#toolstool_name) for more details.

#### `allowed_tools`

* **Type:** list of strings
* **Required:** no

A list of tool names that the model is allowed to call. The tools must be defined in the configuration file.

Any tools provided in `additional_tools` are always allowed, irrespective of this field.

#### `cache_options`

* **Type:** object
* **Required:** no (default: `{"enabled": "write_only"}`)

Options for controlling inference caching behavior. The object has the fields below.

See [Inference Caching](/docs/gateway/guides/inference-caching/) for more details.

##### `cache_options.enabled`

* **Type:** string
* **Required:** no (default: `"write_only"`)

The cache mode to use. Must be one of:

* `"write_only"` (default): Only write to cache but don’t serve cached responses
* `"read_only"`: Only read from cache but don’t write new entries
* `"on"`: Both read from and write to cache
* `"off"`: Disable caching completely

Note: When using `dryrun=true`, the gateway never writes to the cache.

##### `cache_options.max_age_s`

* **Type:** integer
* **Required:** no (default: `null`)

Maximum age in seconds for cache entries. If set, cached responses older than this value will not be used.

For example, if you set `max_age_s=3600`, the gateway will only use cache entries that were created in the last hour.

#### `credentials`

* **Type:** object (a map from dynamic credential names to API keys)
* **Required:** no (default: no credentials)

Each model provider in your TensorZero configuration can be configured to accept credentials at inference time by using the `dynamic` location (e.g. `dynamic::my_dynamic_api_key_name`). See the [configuration reference](/docs/gateway/configuration-reference/#modelsmodel_nameprovidersprovider_name) for more details. The gateway expects the credentials to be provided in the `credentials` field of the request body as specified below. The gateway will return a 400 error if the credentials are not provided and the model provider has been configured with dynamic credentials.

Example

```toml
[models.my_model_name.providers.my_provider_name]
# ...
# Note: the name of the credential field (e.g. `api_key_location`) depends on the provider type
api_key_location = "dynamic::my_dynamic_api_key_name"
# ...
```

```json
{
  // ...
  "credentials": {
    // ...
    "my_dynamic_api_key_name": "sk-..."
    // ...
  }
  // ...
}
```

#### `dryrun`

* **Type:** boolean
* **Required:** no

If `true`, the inference request will be executed but won’t be stored to the database. The gateway will still call the downstream model providers.

This field is primarily for debugging and testing, and you should ignore it in production.

#### `episode_id`

* **Type:** UUID
* **Required:** no

The ID of an existing episode to associate the inference with.

For the first inference of a new episode, you should not provide an `episode_id`. If null, the gateway will generate a new episode ID and return it in the response.

Only use episode IDs that were returned by the TensorZero gateway.

#### `extra_body`

* **Type:** array of objects (see below)
* **Required:** no

The `extra_body` field allows you to modify the request body that TensorZero sends to a model provider. This advanced feature is an “escape hatch” that lets you use provider-specific functionality that TensorZero hasn’t implemented yet.

Each object in the array must have three fields:

* `variant_name` or `model_provider_name`: The modification will only be applied to the specified variant or model provider
* `pointer`: A [JSON Pointer](https://datatracker.ietf.org/doc/html/rfc6901) string specifying where to modify the request body
* `value`: The value to insert at that location; it can be of any type including nested types

Tip

You can also set `extra_body` in the configuration file. The values provided at inference-time take priority over the values in the configuration file.

Example: `extra_body`

If TensorZero would normally send this request body to the provider…

```json
{
  "project": "tensorzero",
  "safety_checks": {
    "no_internet": false,
    "no_agi": true
  }
}
```

…then the following `extra_body` in the inference request…

```json
{
  // ...
  "extra_body": [
    {
      "variant_name": "my_variant",  // or "model_provider_name": "my_model_provider"
      "pointer": "/agi",
      "value": true
    },
    {
      "variant_name": "my_variant",  // or "model_provider_name": "my_model_provider"
      "pointer": "/safety_checks/no_agi",
      "value": {
        "bypass": "on"
      }
    }
  ]
}
```

…overrides the request body (for `my_variant` only) to:

```json
{
  "agi": true,
  "project": "tensorzero",
  "safety_checks": {
    "no_internet": false,
    "no_agi": {
      "bypass": "on"
    }
  }
}
```

#### `function_name`

* **Type:** string
* **Required:** either `function_name` or `model_name` must be provided

The name of the function to call.

The function must be defined in the configuration file.

#### `include_original_response`

* **Type:** boolean
* **Required:** no

If `true`, the original response from the model will be included in the response in the `original_response` field as a string.

Currently, this field can’t be used with streaming inferences.

See `original_response` in the [response](#response) section for more details.

#### `input`

* **Type:** varies
* **Required:** yes

The input to the function.

The type of the input depends on the function type.

##### `input.messages`

* **Type:** list of messages (see below)
* **Required:** no (default: `[]`)

A list of messages to provide to the model.

Each message is an object with the following fields:

* `role`: The role of the message (`assistant` or `user`).
* `content`: The content of the message (see below).

The `content` field can be have one of the following types:

* string: the text for a text message (only allowed if there is no schema for that role)
* list of content blocks: the content blocks for the message (see below)

A content block is an object with the field `type` and additional fields depending on the type.

If the content block has type `text`, it must have either of the following additional fields:

* `text`: The text for the content block.
* `arguments`: A JSON object containing the function arguments for TensorZero functions with templates and schemas (see [Prompt Templates & Schemas](/docs/gateway/guides/prompt-templates-schemas/) for details).

If the content block has type `tool_call`, it must have the following additional fields:

* `arguments`: The arguments for the tool call.
* `id`: The ID for the content block.
* `name`: The name of the tool for the content block.

If the content block has type `tool_result`, it must have the following additional fields:

* `id`: The ID for the content block.
* `name`: The name of the tool for the content block.
* `result`: The result of the tool call.

If the content block has type `image`, it must have either of the following additional fields:

* `url`: The URL for a remote image.
* `mime_type` and `data`: The MIME type and base64-encoded data for an embedded image.
  * We support the following MIME types: `image/png`, `image/jpeg`, and `image/webp`.

See the [Multimodal Inference](/docs/gateway/guides/multimodal-inference/) guide for more details on how to use images in inference.

If the content block has type `raw_text`, it must have the following additional fields:

* `value`: The text for the content block. This content block will ignore any relevant templates and schemas for this function.

If the content block has type `unknown`, it must have the following additional fields:

* `data`: The original content block from the provider, without any validation or transformation by TensorZero.
* `model_provider_name` (optional): A string specifying when this content block should be included in the model provider input. If set, the content block will only be provided to this specific model provider. If not set, the content block is passed to all model providers.

For example, the following hypothetical unknown content block will send the `daydreaming` content block to inference requests targeting the `your_model_provider_name` model provider.

```json
{
  "type": "unknown",
  "data": {
    "type": "daydreaming",
    "dream": "..."
  },
  "model_provider_name": "tensorzero::model_name::your_model_name::provider_name::your_model_provider_name"
}
```

Caution

Certain reasoning models (e.g. DeepSeek R1) can include `thought` content blocks in the response. These content blocks can’t directly be used as inputs to subsequent inferences in multi-turn scenarios. If you need to provide `thought` content blocks to a model, you should convert them to `text` content blocks.

This is the most complex field in the entire API. See this example for more details.

Example

```json
{
  // ...
  "input": {
    "messages": [
      // If you don't have a user (or assistant) schema...
      {
        "role": "user", // (or "assistant")
        "content": "What is the weather in Tokyo?"
      },
      // If you have a user (or assistant) schema...
      {
        "role": "user", // (or "assistant")
        "content": [
          {
            "type": "text",
            "arguments": {
              "location": "Tokyo"
            }
          }
        ]
      },
      // If the model previously called a tool...
      {
        "role": "assistant",
        "content": [
          {
            "type": "tool_call",
            "id": "0",
            "name": "get_temperature",
            "arguments": "{\"location\": \"Tokyo\"}"
          }
        ]
      },
      // ...and you're providing the result of that tool call...
      {
        "role": "user",
        "content": [
          {
            "type": "tool_result",
            "id": "0",
            "name": "get_temperature",
            "result": "70"
          }
        ]
      },
      // You can also specify a text message using a content block...
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "What about NYC?" // (or object if there is a schema)
          }
        ]
      },
      // You can also provide multiple content blocks in a single message...
      {
        "role": "assistant",
        "content": [
          {
            "type": "text",
            "text": "Sure, I can help you with that." // (or object if there is a schema)
          },
          {
            "type": "tool_call",
            "id": "0",
            "name": "get_temperature",
            "arguments": "{\"location\": \"New York\"}"
          }
        ]
      }
      // ...
    ]
    // ...
  }
  // ...
}
```

##### `input.system`

* **Type:** string or object
* **Required:** no

The input for the system message.

If the function does not have a system schema, this field should be a string.

If the function has a system schema, this field should be an object that matches the schema.

#### `model_name`

* **Type:** string
* **Required:** either `model_name` or `function_name` must be provided

The name of the model to call. In this case, the function will be a built-in passthrough chat function called `tensorzero::default`.

The model must be defined in the configuration file, or correspond to a short-hand model name.

Short-hand model names follow the format `provider::model_name` (e.g. `openai::gpt-4o-mini` or `anthropic::claude-3-5-haiku`). The following model providers support short-hand model names: `anthropic`, `deepseek`, `fireworks`, `google_ai_studio_gemini`, `hyperbolic`, `mistral`, `openai`, `together`, and `xai`. The remaining providers do not support short-hand model names, and require an explicit `model` block in your configuration file.

#### `output_schema`

* **Type:** object (valid JSON Schema)
* **Required:** no

If set, this schema will override the `output_schema` defined in the function configuration for a JSON function. This dynamic output schema is used for validating the output of the function, and sent to providers which support structured outputs.

#### `parallel_tool_calls`

* **Type:** boolean
* **Required:** no

If `true`, the function will be allowed to request multiple tool calls in a single conversation turn. If not set, we default to the configuration value for the function being called.

Most model providers do not support parallel tool calls. In those cases, the gateway ignores this field. At the moment, only Fireworks AI and OpenAI support parallel tool calls.

#### `params`

* **Type:** object (see below)
* **Required:** no (default: `{}`)

Override inference-time parameters for a particular variant type. This fields allows for dynamic inference parameters, i.e. defining parameters at runtime.

This field’s format is `{ variant_type: { param: value, ... }, ... }`. You should prefer to set these parameters in the configuration file if possible. Only use this field if you need to set these parameters dynamically at runtime.

Note that the parameters will apply to every variant of the specified type.

Currently, we support the following:

* `chat_completion`

  * `frequency_penalty`
  * `max_tokens`
  * `presence_penalty`
  * `seed`
  * `temperature`
  * `top_p`

See [Configuration Reference](/docs/gateway/configuration-reference/#functionsfunction_namevariantsvariant_name) for more details on the parameters, and Examples below for usage.

Example

For example, if you wanted to dynamically override the `temperature` parameter for a `chat_completion` variants, you’d include the following in the request body:

```json
{
  // ...
  "params": {
    "chat_completion": {
      "temperature": 0.7
    }
  }
  // ...
}
```

See [“Chat Function with Dynamic Inference Parameters”](#chat-function-with-dynamic-inference-parameters) for a complete example.

#### `stream`

* **Type:** boolean
* **Required:** no

If `true`, the gateway will stream the response from the model provider.

#### `tags`

* **Type:** flat JSON object with string keys and values
* **Required:** no

User-provided tags to associate with the inference.

For example, `{"user_id": "123"}` or `{"author": "Alice"}`.

#### `tool_choice`

* **Type:** string
* **Required:** no

If set, overrides the tool choice strategy for the request.

The supported tool choice strategies are:

* `none`: The function should not use any tools.
* `auto`: The model decides whether or not to use a tool. If it decides to use a tool, it also decides which tools to use.
* `required`: The model should use a tool. If multiple tools are available, the model decides which tool to use.
* `{ specific = "tool_name" }`: The model should use a specific tool. The tool must be defined in the `tools` section of the configuration file or provided in `additional_tools`.

#### `variant_name`

* **Type:** string
* **Required:** no

If set, pins the inference request to a particular variant (not recommended).

You should generally not set this field, and instead let the TensorZero gateway assign a variant. This field is primarily used for testing or debugging purposes.

### Response

The response format depends on the function type (as defined in the configuration file) and whether the response is streamed or not.

#### Chat Function

When the function type is `chat`, the response is structured as follows.

* Regular

  In regular (non-streaming) mode, the response is a JSON object with the following fields:

  ##### `content`

  * **Type:** a list of content blocks (see below)

  The content blocks generated by the model.

  A content block can have `type` equal to `text` and `tool_call`. Reasoning models (e.g. DeepSeek R1) might also include `thought` content blocks.

  If `type` is `text`, the content block has the following fields:

  * `text`: The text for the content block.

  If `type` is `tool_call`, the content block has the following fields:

  * `arguments` (object): The validated arguments for the tool call (`null` if invalid).
  * `id` (string): The ID of the content block.
  * `name` (string): The validated name of the tool (`null` if invalid).
  * `raw_arguments` (string): The arguments for the tool call generated by the model (which might be invalid).
  * `raw_name` (string): The name of the tool generated by the model (which might be invalid).

  If `type` is `thought`, the content block has the following fields:

  * `text` (string): The text of the thought.

  If the model provider responds with a content block of an unknown type, it will be included in the response as a content block of type `unknown` with the following additional fields:

  * `data`: The original content block from the provider, without any validation or transformation by TensorZero.
  * `model_provider_name`: The fully-qualified name of the model provider that returned the content block.

  For example, if the model provider `your_model_provider_name` returns a content block of type `daydreaming`, it will be included in the response like this:

  ```json
  {
    "type": "unknown",
    "data": {
      "type": "daydreaming",
      "dream": "..."
    },
    "model_provider_name": "tensorzero::model_name::your_model_name::provider_name::your_model_provider_name"
  }
  ```

  ##### `episode_id`

  * **Type:** UUID

  The ID of the episode associated with the inference.

  ##### `inference_id`

  * **Type:** UUID

  The ID assigned to the inference.

  ##### `original_response`

  * **Type:** string (optional)

  The original response from the model provider (only available when `include_original_response` is `true`).

  The returned data depends on the variant type:

  * `chat_completion`: raw response from the inference to the `model`
  * `experimental_best_of_n_sampling`: raw response from the inference to the `evaluator`
  * `experimental_mixture_of_n_sampling`: raw response from the inference to the `fuser`
  * `experimental_dynamic_in_context_learning`: raw response from the inference to the `model`

  ##### `variant_name`

  * **Type:** string

  The name of the variant used for the inference.

  ##### `usage`

  * **Type:** object (optional)

  The usage metrics for the inference.

  The object has the following fields:

  * `input_tokens`: The number of input tokens used for the inference.
  * `output_tokens`: The number of output tokens used for the inference.

* Streaming

  In streaming mode, the response is an [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) stream of JSON messages, followed by a final `[DONE]` message.

  Each JSON message has the following fields:

  ##### `content`

  * **Type:** a list of content block chunks (see below)

  The content deltas for the inference.

  A content block chunk can have `type` equal to `text` or `tool_call`. Reasoning models (e.g. DeepSeek R1) might also include `thought` content block chunks.

  If `type` is `text`, the chunk has the following fields:

  * `id`: The ID of the content block.
  * `text`: The text delta for the content block.

  If `type` is `tool_call`, the chunk has the following fields (all strings):

  * `id`: The ID of the content block.
  * `raw_name`: The name of the tool. The gateway does not validate this field during streaming inference.
  * `raw_arguments`: The arguments delta for the tool call. The gateway does not validate this field during streaming inference.

  If `type` is `thought`, the chunk has the following fields:

  * `id`: The ID of the content block.
  * `text`: The text delta for the thought.

  ##### `episode_id`

  * **Type:** UUID

  The ID of the episode associated with the inference.

  ##### `inference_id`

  * **Type:** UUID

  The ID assigned to the inference.

  ##### `variant_name`

  * **Type:** string

  The name of the variant used for the inference.

  ##### `usage`

  * **Type:** object (optional)

  The usage metrics for the inference.

  The object has the following fields:

  * `input_tokens`: The number of input tokens used for the inference.
  * `output_tokens`: The number of output tokens used for the inference.

#### JSON Function

When the function type is `json`, the response is structured as follows.

* Regular

  In regular (non-streaming) mode, the response is a JSON object with the following fields:

  ##### `inference_id`

  * **Type:** UUID

  The ID assigned to the inference.

  ##### `episode_id`

  * **Type:** UUID

  The ID of the episode associated with the inference.

  ##### `original_response`

  * **Type:** string (optional)

  The original response from the model provider (only available when `include_original_response` is `true`).

  The returned data depends on the variant type:

  * `chat_completion`: raw response from the inference to the `model`
  * `experimental_best_of_n_sampling`: raw response from the inference to the `evaluator`
  * `experimental_mixture_of_n_sampling`: raw response from the inference to the `fuser`
  * `experimental_dynamic_in_context_learning`: raw response from the inference to the `model`

  ##### `output`

  * **Type:** object (see below)

  The output object contains the following fields:

  * `raw`: The raw response from the model provider (which might be invalid JSON).
  * `parsed`: The parsed response from the model provider (`null` if invalid JSON).

  ##### `variant_name`

  * **Type:** string

  The name of the variant used for the inference.

  ##### `usage`

  * **Type:** object (optional)

  The usage metrics for the inference.

  The object has the following fields:

  * `input_tokens`: The number of input tokens used for the inference.
  * `output_tokens`: The number of output tokens used for the inference.

* Streaming

  In streaming mode, the response is an [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) stream of JSON messages, followed by a final `[DONE]` message.

  Each JSON message has the following fields:

  ##### `episode_id`

  * **Type:** UUID

  The ID of the episode associated with the inference.

  ##### `inference_id`

  * **Type:** UUID

  The ID assigned to the inference.

  ##### `raw`

  * **Type:** string

  The raw response delta from the model provider.

  The TensorZero Gateway does not provide a `parsed` field for streaming JSON inferences. If your application depends on a well-formed JSON response, we recommend using regular (non-streaming) inference.

  ##### `variant_name`

  * **Type:** string

  The name of the variant used for the inference.

  ##### `usage`

  * **Type:** object (optional)

  The usage metrics for the inference.

  The object has the following fields:

  * `input_tokens`: The number of input tokens used for the inference.
  * `output_tokens`: The number of output tokens used for the inference.

### Examples

#### Chat Function

Chat Function

##### Configuration

tensorzero.toml

```toml
# ...
[functions.draft_email]
type = "chat"
# ...
```

##### Request

* Python

  POST /inference

  ```python
  from tensorzero import AsyncTensorZeroGateway


  async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      result = await client.inference(
          function_name="draft_email",
          input={
              "system": "You are an AI assistant...",
              "messages": [
                  {
                    "role": "user",
                    "content": "I need to write an email to Gabriel explaining..."
                  }
              ]
          }
          # optional: stream=True,
      )
  ```

* HTTP

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/inference \
    -H "Content-Type: application/json" \
    -d '{
      "function_name": "draft_email",
      "input": {
        "system": "You are an AI assistant...",
        "messages": [
          {
            "role": "user",
            "content": "I need to write an email to Gabriel explaining..."
          }
        ]
      }
      // optional: "stream": true
    }'
  ```

##### Response

* Regular

  POST /inference

  ```json
  {
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "variant_name": "prompt_v1",
    "content": [
      {
        "type": "text",
        "text": "Hi Gabriel,\n\nI noticed...",
      }
    ]
    "usage": {
      "input_tokens": 100,
      "output_tokens": 100
    }
  }
  ```

* Streaming

  In streaming mode, the response is an [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) stream of JSON messages, followed by a final `[DONE]` message.

  Each JSON message has the following fields:

  POST /inference

  ```json
  {
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "variant_name": "prompt_v1",
    "content": [
      {
        "type": "text",
        "id": "0",
        "text": "Hi Gabriel," // a text content delta
      }
    ],
    "usage": {
      "input_tokens": 100,
      "output_tokens": 100
    }
  }
  ```

#### Chat Function with Schemas

Chat Function with Schemas

##### Configuration

tensorzero.toml

```toml
# ...
[functions.draft_email]
type = "chat"
system_schema = "system_schema.json"
user_schema = "user_schema.json"
# ...
```

system\_schema.json

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "tone": {
      "type": "string"
    }
  },
  "required": ["tone"],
  "additionalProperties": false
}
```

user\_schema.json

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "recipient": {
      "type": "string"
    },
    "email_purpose": {
      "type": "string"
    }
  },
  "required": ["recipient", "email_purpose"],
  "additionalProperties": false
}
```

##### Request

* Python

  POST /inference

  ```python
  from tensorzero import AsyncTensorZeroGateway


  async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      result = await client.inference(
          function_name="draft_email",
          input={
              "system": {"tone": "casual"},
              "messages": [
                  {
                      "role": "user",
                      "content": [
                          {
                              "type": "text",
                              "arguments": {
                                  "recipient": "Gabriel",
                                  "email_purpose": "Request a meeting to..."
                              }
                          }
                      ]
                  }
              ]
          }
          # optional: stream=True,
      )
  ```

* HTTP

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/inference \
    -H "Content-Type: application/json" \
    -d '{
      "function_name": "draft_email",
      "input": {
        "system": {"tone": "casual"},
        "messages": [
          {
            "role": "user",
            "content": [
              {
                "type": "text",
                "arguments": {
                  "recipient": "Gabriel",
                  "email_purpose": "Request a meeting to..."
                }
              }
            ]
          }
        ]
      }
      // optional: "stream": true
    }'
  ```

##### Response

* Regular

  POST /inference

  ```json
  {
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "variant_name": "prompt_v1",
    "content": [
      {
        "type": "text",
        "text": "Hi Gabriel,\n\nI noticed...",
      }
    ]
    "usage": {
      "input_tokens": 100,
      "output_tokens": 100
    }
  }
  ```

* Streaming

  In streaming mode, the response is an [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) stream of JSON messages, followed by a final `[DONE]` message.

  Each JSON message has the following fields:

  POST /inference

  ```json
  {
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "variant_name": "prompt_v1",
    "content": [
      {
        "type": "text",
        "id": "0",
        "text": "Hi Gabriel," // a text content delta
      }
    ],
    "usage": {
      "input_tokens": 100,
      "output_tokens": 100
    }
  }
  ```

#### Chat Function with Tool Use

Chat Function with Tool Use

##### Configuration

tensorzero.toml

```toml
# ...


[functions.weather_bot]
type = "chat"
tools = ["get_temperature"]


# ...


[tools.get_temperature]
description = "Get the current temperature in a given location"
parameters = "get_temperature.json"


# ...
```

get\_temperature.json

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "location": {
      "type": "string",
      "description": "The location to get the temperature for (e.g. \"New York\")"
    },
    "units": {
      "type": "string",
      "description": "The units to get the temperature in (must be \"fahrenheit\" or \"celsius\")",
      "enum": ["fahrenheit", "celsius"]
    }
  },
  "required": ["location"],
  "additionalProperties": false
}
```

##### Request

* Python

  POST /inference

  ```python
  from tensorzero import AsyncTensorZeroGateway


  async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      result = await client.inference(
          function_name="weather_bot",
          input={
              "messages": [
                  {
                      "role": "user",
                      "content": "What is the weather like in Tokyo?"
                  }
              ]
          }
          # optional: stream=True,
      )
  ```

* HTTP

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/inference \
    -H "Content-Type: application/json" \
    -d '{
      "function_name": "weather_bot",
      "input": {
        "messages": [
          {
            "role": "user",
            "content": "What is the weather like in Tokyo?"
          }
        ]
      }
      // optional: "stream": true
    }'
  ```

##### Response

* Regular

  POST /inference

  ```json
  {
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "variant_name": "prompt_v1",
    "content": [
      {
        "type": "tool_call",
        "arguments": {
          "location": "Tokyo",
          "units": "celsius"
        },
        "id": "123456789",
        "name": "get_temperature",
        "raw_arguments": "{\"location\": \"Tokyo\", \"units\": \"celsius\"}",
        "raw_name": "get_temperature"
      }
    ],
    "usage": {
      "input_tokens": 100,
      "output_tokens": 100
    }
  }
  ```

* Streaming

  In streaming mode, the response is an [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) stream of JSON messages, followed by a final `[DONE]` message.

  Each JSON message has the following fields:

  POST /inference

  ```json
  {
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "variant_name": "prompt_v1",
    "content": [
      {
        "type": "tool_call",
        "id": "123456789",
        "name": "get_temperature",
        "arguments": "{\"location\":" // a tool arguments delta
      }
    ],
    "usage": {
      "input_tokens": 100,
      "output_tokens": 100
    }
  }
  ```

#### Chat Function with Multi-Turn Tool Use

Chat Function with Multi-Turn Tool Use

##### Configuration

tensorzero.toml

```toml
# ...


[functions.weather_bot]
type = "chat"
tools = ["get_temperature"]


# ...


[tools.get_temperature]
description = "Get the current temperature in a given location"
parameters = "get_temperature.json"


# ...
```

get\_temperature.json

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "location": {
      "type": "string",
      "description": "The location to get the temperature for (e.g. \"New York\")"
    },
    "units": {
      "type": "string",
      "description": "The units to get the temperature in (must be \"fahrenheit\" or \"celsius\")",
      "enum": ["fahrenheit", "celsius"]
    }
  },
  "required": ["location"],
  "additionalProperties": false
}
```

##### Request

* Python

  POST /inference

  ```python
  from tensorzero import AsyncTensorZeroGateway


  async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      result = await client.inference(
          function_name="weather_bot",
          input={
              "messages": [
                  {
                      "role": "user",
                      "content": "What is the weather like in Tokyo?"
                  },
                  {
                      "role": "assistant",
                      "content": [
                          {
                              "type": "tool_call",
                              "arguments": {
                                  "location": "Tokyo",
                                  "units": "celsius"
                              },
                              "id": "123456789",
                              "name": "get_temperature",
                          }
                      ]
                  },
                  {
                      "role": "user",
                      "content": [
                          {
                              "type": "tool_result",
                              "id": "123456789",
                              "name": "get_temperature",
                              "result": "25"  # the tool result must be a string
                          }
                      ]
                  }
              ]
          }
          # optional: stream=True,
      )
  ```

* HTTP

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/inference \
    -H "Content-Type: application/json" \
    -d '{
      "function_name": "weather_bot",
      "input": {
        "messages": [
          {
            "role": "user",
            "content": "What is the weather like in Tokyo?"
          },
          {
            "role": "assistant",
            "content": [
              {
                "type": "tool_call",
                "arguments": {
                  "location": "Tokyo",
                  "units": "celsius"
                },
                "id": "123456789",
                "name": "get_temperature",
              }
            ]
          },
          {
            "role": "user",
            "content": [
              {
                "type": "tool_result",
                "id": "123456789",
                "name": "get_temperature",
                "result": "25"  // the tool result must be a string
              }
            ]
          }
        ]
      }
      // optional: "stream": true
    }'
  ```

##### Response

* Regular

  POST /inference

  ```json
  {
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "variant_name": "prompt_v1",
    "content": [
      {
        "type": "text",
        "content": [
          {
            "type": "text",
            "text": "The weather in Tokyo is 25 degrees Celsius."
          }
        ]
      }
    ],
    "usage": {
      "input_tokens": 100,
      "output_tokens": 100
    }
  }
  ```

* Streaming

  In streaming mode, the response is an [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) stream of JSON messages, followed by a final `[DONE]` message.

  Each JSON message has the following fields:

  POST /inference

  ```json
  {
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "variant_name": "prompt_v1",
    "content": [
      {
        "type": "text",
        "id": "0",
        "text": "The weather in" // a text content delta
      }
    ],
    "usage": {
      "input_tokens": 100,
      "output_tokens": 100
    }
  }
  ```

#### Chat Function with Dynamic Tool Use

Chat Function with Dynamic Tool Use

##### Configuration

tensorzero.toml

```toml
# ...


[functions.weather_bot]
type = "chat"
# Note: no `tools = ["get_temperature"]` field in configuration


# ...
```

##### Request

* Python

  POST /inference

  ```python
  from tensorzero import AsyncTensorZeroGateway


  async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      result = await client.inference(
          function_name="weather_bot",
          input={
              "messages": [
                  {
                      "role": "user",
                      "content": "What is the weather like in Tokyo?"
                  }
              ]
          },
          additional_tools=[
              {
                  "name": "get_temperature",
                  "description": "Get the current temperature in a given location",
                  "parameters": {
                      "$schema": "http://json-schema.org/draft-07/schema#",
                      "type": "object",
                      "properties": {
                          "location": {
                              "type": "string",
                              "description": "The location to get the temperature for (e.g. \"New York\")"
                          },
                          "units": {
                              "type": "string",
                              "description": "The units to get the temperature in (must be \"fahrenheit\" or \"celsius\")",
                              "enum": ["fahrenheit", "celsius"]
                          }
                      },
                      "required": ["location"],
                      "additionalProperties": false
                  }
              }
          ],
          # optional: stream=True,
      )
  ```

* HTTP

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/inference \
    -H "Content-Type: application/json" \
    -d '{
      "function_name": "weather_bot",
      input: {
        "messages": [
          {
            "role": "user",
            "content": "What is the weather like in Tokyo?"
          }
        ]
      },
      additional_tools: [
        {
          "name": "get_temperature",
          "description": "Get the current temperature in a given location",
          "parameters": {
            "$schema": "http://json-schema.org/draft-07/schema#",
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The location to get the temperature for (e.g. \"New York\")"
              },
              "units": {
                "type": "string",
                "description": "The units to get the temperature in (must be \"fahrenheit\" or \"celsius\")",
                "enum": ["fahrenheit", "celsius"]
              }
            },
            "required": ["location"],
            "additionalProperties": false
          }
        }
      ]
      // optional: "stream": true
    }'
  ```

##### Response

* Regular

  POST /inference

  ```json
  {
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "variant_name": "prompt_v1",
    "content": [
      {
        "type": "tool_call",
        "arguments": {
          "location": "Tokyo",
          "units": "celsius"
        },
        "id": "123456789",
        "name": "get_temperature",
        "raw_arguments": "{\"location\": \"Tokyo\", \"units\": \"celsius\"}",
        "raw_name": "get_temperature"
      }
    ],
    "usage": {
      "input_tokens": 100,
      "output_tokens": 100
    }
  }
  ```

* Streaming

  In streaming mode, the response is an [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) stream of JSON messages, followed by a final `[DONE]` message.

  Each JSON message has the following fields:

  POST /inference

  ```json
  {
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "variant_name": "prompt_v1",
    "content": [
      {
        "type": "tool_call",
        "id": "123456789",
        "name": "get_temperature",
        "arguments": "{\"location\":" // a tool arguments delta
      }
    ],
    "usage": {
      "input_tokens": 100,
      "output_tokens": 100
    }
  }
  ```

#### Chat Function with Dynamic Inference Parameters

Chat Function with Dynamic Inference Parameters

##### Configuration

tensorzero.toml

```toml
# ...
[functions.draft_email]
type = "chat"
# ...


[functions.draft_email.variants.prompt_v1]
type = "chat_completion"
temperature = 0.5  # the API request will override this value
# ...
```

##### Request

* Python

  POST /inference

  ```python
  from tensorzero import AsyncTensorZeroGateway


  async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      result = await client.inference(
          function_name="draft_email",
          input={
              "system": "You are an AI assistant...",
              "messages": [
                  {
                      "role": "user",
                      "content": "I need to write an email to Gabriel explaining..."
                  }
              ]
          },
          # Override parameters for every variant with type "chat_completion"
          params={
              "chat_completion": {
                  "temperature": 0.7,
              }
          },
          # optional: stream=True,
      )
  ```

* HTTP

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/inference \
    -H "Content-Type: application/json" \
    -d '{
      "function_name": "draft_email",
      "input": {
        "system": "You are an AI assistant...",
        "messages": [
          {
            "role": "user",
            "content": "I need to write an email to Gabriel explaining..."
          }
        ]
      },
      params={
        // Override parameters for every variant with type "chat_completion"
        "chat_completion": {
          "temperature": 0.7,
        }
      }
      // optional: "stream": true
    }'
  ```

##### Response

* Regular

  POST /inference

  ```json
  {
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "variant_name": "prompt_v1",
    "content": [
      {
        "type": "text",
        "text": "Hi Gabriel,\n\nI noticed...",
      }
    ]
    "usage": {
      "input_tokens": 100,
      "output_tokens": 100
    }
  }
  ```

* Streaming

  In streaming mode, the response is an [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) stream of JSON messages, followed by a final `[DONE]` message.

  Each JSON message has the following fields:

  POST /inference

  ```json
  {
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "variant_name": "prompt_v1",
    "content": [
      {
        "id": "0",
        "text": "Hi Gabriel," // a text content delta
      }
    ],
    "usage": {
      "input_tokens": 100,
      "output_tokens": 100
    }
  }
  ```

#### JSON Function

JSON Function

##### Configuration

tensorzero.toml

```toml
# ...
[functions.extract_email]
type = "json"
output_schema = "output_schema.json"
# ...
```

output\_schema.json

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "email": {
      "type": "string"
    }
  },
  "required": ["email"]
}
```

##### Request

* Python

  POST /inference

  ```python
  from tensorzero import AsyncTensorZeroGateway


  async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      result = await client.inference(
          function_name="extract_email",
          input={
              "system": "You are an AI assistant...",
              "messages": [
                  {
                      "role": "user",
                      "content": "...blah blah blah hello@tensorzero.com blah blah blah..."
                  }
              ]
          }
          # optional: stream=True,
      )
  ```

* HTTP

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/inference \
    -H "Content-Type: application/json" \
    -d '{
      "function_name": "extract_email",
      "input": {
        "system": "You are an AI assistant...",
        "messages": [
          {
            "role": "user",
            "content": "...blah blah blah hello@tensorzero.com blah blah blah..."
          }
        ]
      }
      // optional: "stream": true
    }'
  ```

##### Response

* Regular

  POST /inference

  ```json
  {
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "variant_name": "prompt_v1",
    "output": {
      "raw": "{\"email\": \"hello@tensorzero.com\"}",
      "parsed": {
        "email": "hello@tensorzero.com"
      }
    }
    "usage": {
      "input_tokens": 100,
      "output_tokens": 100
    }
  }
  ```

* Streaming

  In streaming mode, the response is an [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) stream of JSON messages, followed by a final `[DONE]` message.

  Each JSON message has the following fields:

  POST /inference

  ```json
  {
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "variant_name": "prompt_v1",
    "raw": "{\"email\":", // a JSON content delta
    "usage": {
      "input_tokens": 100,
      "output_tokens": 100
    }
  }
  ```

# API Reference: Inference (OpenAI-Compatible)

> API reference for the `/openai/v1/chat/completions` endpoint.

## `POST /openai/v1/chat/completions`

The `/openai/v1/chat/completions` endpoint allows TensorZero users to make TensorZero inferences with the OpenAI client. The gateway translates the OpenAI request parameters into the arguments expected by the `inference` endpoint and calls the same underlying implementation. This endpoint supports most of the features supported by the `inference` endpoint, but there are some limitations. Most notably, this endpoint doesn’t support dynamic credentials, so they must be specified with a different method.

Tip

See the [API Reference for `POST /inference`](/docs/gateway/api-reference/inference/) for more details on inference with the native TensorZero API.

### Request

This endpoint leverages both the request body (as JSON) and the request headers to pass information to the `inference` endpoint. You should assume each field is in the body unless it is explicitly noted as a header.

Caution

The gateway will use the credentials specified in the `tensorzero.toml` file. In most cases, these credentials will be environment variables available to the TensorZero gateway — *not* your OpenAI client.

API keys sent from the OpenAI client will be ignored.

#### `dryrun`

**This field should be provided as a request header.**

* **Type:** boolean
* **Required:** no

If `true`, the inference request will be executed but won’t be stored to the database. The gateway will still call the downstream model providers.

This field is primarily for debugging and testing, and you should ignore it in production.

#### `episode_id`

**This field should be provided as a request header.**

* **Type:** UUID
* **Required:** no

The ID of an existing episode to associate the inference with.

For the first inference of a new episode, you should not provide an `episode_id`. If null, the gateway will generate a new episode ID and return it in the response.

Only use episode IDs that were returned by the TensorZero gateway.

#### `frequency_penalty`

* **Type:** float
* **Required:** no (default: `null`)

Penalizes new tokens based on their frequency in the text so far if positive, encourages them if negative. Overrides the `frequency_penalty` setting for any chat completion variants being used.

#### `max_completion_tokens`

* **Type:** integer
* **Required:** no (default: `null`)

Limits the number of tokens that can be generated by the model in a chat completion variant. If both this and `max_tokens` are set, the smaller value is used.

#### `max_tokens`

* **Type:** integer
* **Required:** no (default: `null`)

Limits the number of tokens that can be generated by the model in a chat completion variant. If both this and `max_completion_tokens` are set, the smaller value is used.

#### `messages`

* **Type:** list
* **Required:** yes

A list of messages to provide to the model.

Each message is an object with the following fields:

* `role` (required): The role of the message sender in an OpenAI message (`assistant`, `system`, `tool`, or `user`).

* `content` (required for `user` and `system` messages and optional for `assistant` and `tool` messages): The content of the message. The content must be either a string or an array of content blocks (see below).

* `tool_calls` (optional for `assistant` messages, otherwise disallowed): A list of tool calls. Each tool call is an object with the following fields:

  * `id`: A unique identifier for the tool call

  * `type`: The type of tool being called (currently only `"function"` is supported)

  * `function`: An object containing:

    * `name`: The name of the function to call
    * `arguments`: A JSON string containing the function arguments

* `tool_call_id` (required for `tool` messages, otherwise disallowed): The ID of the tool call to associate with the message. This should be one that was originally returned by the gateway in a tool call `id` field.

A content block is an object that can have type `text` or `image_url`.

If the content block has type `text`, it must have either of the following additional fields:

* `text`: The text for the content block.
* `tensorzero::arguments`: A JSON object containing the function arguments for TensorZero functions with templates and schemas (see [Prompt Templates & Schemas](/docs/gateway/guides/prompt-templates-schemas/) for details).

If a content block has type `image_url`, it must have the following additional fields:

* `"image_url"`: A JSON object with the following field:
  * `url`: The URL for a remote image (e.g. `"https://example.com/image.png"`) or base64-encoded data for an embedded image (e.g. `"data:image/png;base64,..."`).

Caution

Currently, the OpenAI-compatible inference endpoint does not accept other content blocks like `raw_text` or `unknown`. See the [Inference API Reference](/docs/gateway/api-reference/inference/) for details on how to provide such content blocks as input.

#### `model`

* **Type:** string
* **Required:** yes

The name of the TensorZero function being called, prepended by `"tensorzero::function_name::"`. An error will be returned if the function name is not recognized or is missing the prefix.

#### `parallel_tool_calls`

* **Type:** boolean
* **Required:** no (default: `null`)

Overrides the `parallel_tool_calls` setting for the function being called.

#### `presence_penalty`

* **Type:** float
* **Required:** no (default: `null`)

Penalizes new tokens based on whether they appear in the text so far if positive, encourages them if negative. Overrides the `presence_penalty` setting for any chat completion variants being used.

#### `response_format`

* **Type:** either a string or an object
* **Required:** no (default: `null`)

Options here are `"text"`, `"json_object"`, and `"{"type": "json_schema", "schema": ...}"`, where the schema field contains a valid JSON schema. This field is not actually respected except for the `"json_schema"` variant, in which the `schema` field can be used to dynamically set the output schema for a `json` function.

#### `seed`

* **Type:** integer
* **Required:** no (default: `null`)

Overrides the `seed` setting for any chat completion variants being used.

#### `stream`

* **Type:** boolean
* **Required:** no (default: `false`)

If true, the gateway will stream the response to the client in an OpenAI-compatible format.

#### `temperature`

* **Type:** float
* **Required:** no (default: `null`)

Overrides the `temperature` setting for any chat completion variants being used.

#### `tools`

* **Type:** list of `tool` objects (see below)
* **Required:** no (default: `null`)

Allows the user to dynamically specify tools at inference time in addition to those that are specified in the configuration.

Each `tool` object has the following structure:

* **`type`**: Must be `"function"`

* **`function`**: An object containing:

  * **`name`**: The name of the function (string, required)
  * **`description`**: A description of what the function does (string, optional)
  * **`parameters`**: A JSON Schema object describing the function’s parameters (required)
  * **`strict`**: Whether to enforce strict schema validation (boolean, defaults to false)

#### `tool_choice`

* **Type:** string or object
* **Required:** no (default: `"none"` if no tools are present, `"auto"` if tools are present)

Controls which (if any) tool is called by the model by overriding the value in configuration. Supported values:

* `"none"`: The model will not call any tool and instead generates a message
* `"auto"`: The model can pick between generating a message or calling one or more tools
* `"required"`: The model must call one or more tools
* `{"type": "function", "function": {"name": "my_function"}}`: Forces the model to call the specified tool

#### `top_p`

* **Type:** float
* **Required:** no (default: `null`)

Overrides the `top_p` setting for any chat completion variants being used.

#### `variant_name`

**This field should be provided as a request header.**

* **Type:** string
* **Required:** no

If set, pins the inference request to a particular variant (not recommended).

You should generally not set this field, and instead let the TensorZero gateway assign a variant. This field is primarily used for testing or debugging purposes.

### Response

* Regular

  In regular (non-streaming) mode, the response is a JSON object with the following fields:

  #### `choices`

  * **Type:** list of `choice` objects, where each choice contains:

    * **`index`**: A zero-based index indicating the choice’s position in the list (integer)

    * **`finish_reason`**: Always `"stop"`.

    * **`message`**: An object containing:

      * **`content`**: The message content (string, optional)
      * **`tool_calls`**: List of tool calls made by the model (optional). The format is the same as in the request.
      * **`role`**: The role of the message sender (always `"assistant"`).

  Caution

  The OpenAI-compatible inference endpoint can’t handle unknown content blocks in the response. If the model provider returns an unknown content block, the gateway will drop the content block from the response and log a warning.

  If you need to access unknown content blocks, use the native TensorZero API. See the [Inference API Reference](/docs/gateway/api-reference/inference/) for details.

  #### `created`

  * **Type:** integer

  The Unix timestamp (in seconds) of when the inference was created.

  #### `episode_id`

  * **Type:** UUID

  The ID of the episode that the inference was created for.

  #### `id`

  * **Type:** UUID

  The inference ID.

  #### `model`

  * **Type:** string

  The name of the variant that was actually used for the inference.

  #### `object`

  * **Type:** string

  The type of the inference object (always `"chat.completion"`).

  #### `system_fingerprint`

  * **Type:** string

  Always ""

  #### `usage`

  * **Type:** object

  Contains token usage information for the request and response, with the following fields:

  * **`prompt_tokens`**: Number of tokens in the prompt (integer)
  * **`completion_tokens`**: Number of tokens in the completion (integer)
  * **`total_tokens`**: Total number of tokens used (integer)

* Streaming

  In streaming mode, the response is an [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) stream of JSON messages, followed by a final `[DONE]` message.

  Each JSON message has the following fields:

  #### `choices`

  * **Type:** list

  A list of choices from the model, where each choice contains:

  * `index`: The index of the choice (integer)

  * `finish_reason`: always ""

  * `delta`: An object containing either:

    * `content`: The next piece of generated text (string), or
    * `tool_calls`: A list of tool calls, each containing the next piece of the tool call being generated

  #### `created`

  * **Type:** integer

  The Unix timestamp (in seconds) of when the inference was created.

  #### `episode_id`

  * **Type:** UUID

  The ID of the episode that the inference was created for.

  #### `id`

  * **Type:** UUID

  The inference ID.

  #### `model`

  * **Type:** string

  The name of the variant that was actually used for the inference.

  #### `object`

  * **Type:** string

  The type of the inference object (always `"chat.completion"`).

  #### `system_fingerprint`

  * **Type:** string

  Always ""

  #### `usage`

  * **Type:** object
  * **Required:** no

  Contains token usage information for the request and response, with the following fields:

  * **`prompt_tokens`**: Number of tokens in the prompt (integer)
  * **`completion_tokens`**: Number of tokens in the completion (integer)
  * **`total_tokens`**: Total number of tokens used (integer)

### Examples

#### Chat Function with Structured System Prompt

Chat Function with Structured System Prompt

##### Configuration

tensorzero.toml

```toml
# ...
[functions.draft_email]
type = "chat"
system_schema = "functions/draft_email/system_schema.json"
# ...
```

functions/draft\_email/system\_schema.json

```json
{
  "type": "object",
  "properties": {
    "assistant_name": { "type": "string" }
  }
}
```

##### Request

* Python

  POST /inference

  ```python
  from openai import AsyncOpenAI


  async with AsyncOpenAI(
      base_url="http://localhost:3000/openai/v1"
  ) as client:
      result = await client.chat.completions.create(
          # there already was an episode_id from an earlier inference
          extra_body={"tensorzero::episode_id": str(episode_id)},
          messages=[
              {
                  "role": "system",
                  "content": [{"assistant_name": "Alfred Pennyworth"}]
                  # NOTE: the JSON is in an array here so that a structured system message can be sent
              },
              {
                  "role": "user",
                  "content": "I need to write an email to Gabriel explaining..."
              }
          ],
          model="tensorzero::function_name::draft_email",
          temperature=0.4,
          # Optional: stream=True
      )
  ```

* HTTP

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/openai/v1/chat/completions \
    -H "Content-Type: application/json" \
    -H "episode_id: your_episode_id_here" \
    -d '{
      "messages": [
        {
          "role": "system",
          "content": [{"assistant_name": "Alfred Pennyworth"}]
        },
        {
          "role": "user",
          "content": "I need to write an email to Gabriel explaining..."
        }
      ],
      "model": "tensorzero::function_name::draft_email",
      "temperature": 0.4
      // Optional: "stream": true
    }'
  ```

##### Response

* Regular

  POST /inference

  ```json
  {
    "id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "model": "email_draft_variant",
    "choices": [
      {
        "index": 0,
        "finish_reason": "stop",
        "message": {
          "content": "Hi Gabriel,\n\nI noticed...",
          "role": "assistant"
        }
      }
    ],
    "usage": {
      "prompt_tokens": 100,
      "completion_tokens": 100,
      "total_tokens": 200
    }
  }
  ```

* Streaming

  In streaming mode, the response is an [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) stream of JSON messages, followed by a final `[DONE]` message.

  Each JSON message has the following fields:

  POST /inference

  ```json
  {
    "id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "model": "email_draft_variant",
    "choices": [
      {
        "index": 0,
        "finish_reason": "stop",
        "delta": {
          "content": "Hi Gabriel,\n\nI noticed..."
        }
      }
    ],
    "usage": {
      "prompt_tokens": 100,
      "completion_tokens": 100,
      "total_tokens": 200
    }
  }
  ```

#### Chat Function with Dynamic Tool Use

Chat Function with Dynamic Tool Use

##### Configuration

tensorzero.toml

```toml
# ...


[functions.weather_bot]
type = "chat"
# Note: no `tools = ["get_temperature"]` field in configuration


# ...
```

##### Request

* Python

  POST /inference

  ```python
  from openai import AsyncOpenAI


  async with AsyncOpenAI(
      base_url="http://localhost:3000/openai/v1"
  ) as client:
      result = await client.chat.completions.create(
          model="tensorzero::function_name::weather_bot",
          input={
              "messages": [
                  {
                      "role": "user",
                      "content": "What is the weather like in Tokyo?"
                  }
              ]
          },
          tools=[
              {
                "type": "function",
                "function": {
                    "name": "get_temperature",
                    "description": "Get the current temperature in a given location",
                    "parameters": {
                      "$schema": "http://json-schema.org/draft-07/schema#",
                      "type": "object",
                      "properties": {
                          "location": {
                              "type": "string",
                              "description": "The location to get the temperature for (e.g. \"New York\")"
                          },
                          "units": {
                              "type": "string",
                              "description": "The units to get the temperature in (must be \"fahrenheit\" or \"celsius\")",
                              "enum": ["fahrenheit", "celsius"]
                          }
                      },
                      "required": ["location"],
                      "additionalProperties": false
                  }
                }
              }
          ],
          # optional: stream=True,
      )
  ```

* HTTP

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/openai/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
      "model": "tensorzero::function_name::weather_bot",
      "input": {
        "messages": [
          {
            "role": "user",
            "content": "What is the weather like in Tokyo?"
          }
        ]
      },
      "tools": [
        {
          "type": "function",
          "function": {
            "name": "get_temperature",
            "description": "Get the current temperature in a given location",
            "parameters": {
              "$schema": "http://json-schema.org/draft-07/schema#",
              "type": "object",
              "properties": {
                "location": {
                  "type": "string",
                  "description": "The location to get the temperature for (e.g. \"New York\")"
                },
                "units": {
                  "type": "string",
                  "description": "The units to get the temperature in (must be \"fahrenheit\" or \"celsius\")",
                  "enum": ["fahrenheit", "celsius"]
                }
              },
              "required": ["location"],
              "additionalProperties": false
            }
          }
        }
      ]
      // optional: "stream": true
    }'
  ```

##### Response

* Regular

  POST /inference

  ```json
  {
    "id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "model": "weather_bot_variant",
    "choices": [
      {
        "index": 0,
        "finish_reason": "stop",
        "message": {
          "content": null,
          "tool_calls": [
            {
              "id": "123456789",
              "type": "function",
              "function": {
                "name": "get_temperature",
                "arguments": "{\"location\": \"Tokyo\", \"units\": \"celsius\"}"
              }
            }
          ],
          "role": "assistant"
        }
      }
    ],
    "usage": {
      "prompt_tokens": 100,
      "completion_tokens": 100,
      "total_tokens": 200
    }
  }
  ```

* Streaming

  In streaming mode, the response is an [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) stream of JSON messages, followed by a final `[DONE]` message.

  Each JSON message has the following fields:

  POST /inference

  ```json
  {
    "id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "model": "weather_bot_variant",
    "choices": [
      {
        "index": 0,
        "finish_reason": "stop",
        "message": {
          "content": null,
          "tool_calls": [
            {
              "id": "123456789",
              "type": "function",
              "function": {
                "name": "get_temperature",
                "arguments": "{\"location\":" // a tool arguments delta
              }
            }
          ]
        }
      }
    ],
    "usage": {
      "prompt_tokens": 100,
      "completion_tokens": 100,
      "total_tokens": 200
    }
  }
  ```

#### Json Function with Dynamic Output Schema

JSON Function with Dynamic Output Schema

##### Configuration

tensorzero.toml

```toml
# ...
[functions.extract_email]
type = "json"
output_schema = "output_schema.json"
# ...
```

output\_schema.json

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "email": {
      "type": "string"
    }
  },
  "required": ["email"]
}
```

##### Request

* Python

  POST /inference

  ```python
  from openai import AsyncOpenAI


  dynamic_output_schema = {
    "$schema": "http://json-schema.org/draft-07/schema#",
    "type": "object",
    "properties": {
      "email": { "type": "string" },
      "domain": { "type": "string" }
    },
    "required": ["email", "domain"]
  }


  async with AsyncOpenAI(
      base_url="http://localhost:3000/openai/v1"
  ) as client:
      result = await client.chat.completions.create(
          model="tensorzero::function_name::extract_email",
          input={
              "system": "You are an AI assistant...",
              "messages": [
                  {
                      "role": "user",
                      "content": "...blah blah blah hello@tensorzero.com blah blah blah..."
                  }
              ]
          }
          # Override the output schema using the `response_format` field
          response_format={"type": "json_schema", "schema": dynamic_output_schema}
          # optional: stream=True,
      )
  ```

* HTTP

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/openai/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
      "model": "tensorzero::function_name::extract_email",
      "input": {
        "system": "You are an AI assistant...",
        "messages": [
          {
            "role": "user",
            "content": "...blah blah blah hello@tensorzero.com blah blah blah..."
          }
        ]
      },
      "response_format": {
        "type": "json_schema",
        "schema": {
          "$schema": "http://json-schema.org/draft-07/schema#",
          "type": "object",
          "properties": {
            "email": { "type": "string" },
            "domain": { "type": "string" }
          },
          "required": ["email", "domain"]
        }
      },
      // optional: "stream": true
    }'
  ```

##### Response

* Regular

  POST /inference

  ```json
  {
    "id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "model": "extract_email_variant",
    "choices": [
      {
        "index": 0,
        "finish_reason": "stop",
        "message": {
          "content": "{\"email\": \"hello@tensorzero.com\", \"domain\": \"tensorzero.com\"}"
        }
      }
    ],
    "usage": {
      "prompt_tokens": 100,
      "completion_tokens": 100,
      "total_tokens": 200
    }
  }
  ```

* Streaming

  In streaming mode, the response is an [SSE](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) stream of JSON messages, followed by a final `[DONE]` message.

  Each JSON message has the following fields:

  POST /inference

  ```json
  {
    "id": "00000000-0000-0000-0000-000000000000",
    "episode_id": "11111111-1111-1111-1111-111111111111",
    "model": "extract_email_variant",
    "choices": [
      {
        "index": 0,
        "finish_reason": "stop",
        "message": {
          "content": "{\"email\":" // a JSON content delta
        }
      }
    ],
    "usage": {
      "prompt_tokens": 100,
      "completion_tokens": 100,
      "total_tokens": 200
    }
  }
  ```

# Benchmarks

> Benchmarks for the TensorZero Gateway: sub-millisecond latency overhead under extreme load

The TensorZero Gateway was built from the ground up with performance in mind.

It’s written in Rust and designed to handle extreme concurrency with sub-millisecond overhead.

Tip

See [Performance & Latency Tips](/docs/gateway/guides/performance-latency/) for more details on maximizing performance in production settings.

## TensorZero Gateway vs. LiteLLM

> **TLDR: TensorZero achieves 100x lower P99 latency overhead while handling 100x the load.**

We benchmarked the TensorZero Gateway against the popular LiteLLM Proxy (LiteLLM Gateway).

In a `c7i.xlarge` instance on AWS (4 vCPUs, 8 GB RAM), LiteLLM crashes when concurrency exceeds a few hundred QPS. TensorZero Gateway handles 10k+ QPS in the same instance without sweating.

Even with a 100x difference in throughput, the TensorZero Gateway achieves 25-100x+ lower latency. The difference is especially pronounced at the tail: 100x+ lower P99 latency. Building in Rust (TensorZero) led to consistent sub-millisecond latency overhead under extreme load, whereas Python (LiteLLM) often becomes a bottleneck.

| Latency | LiteLLM Proxy (100 QPS) | TensorZero Gateway (10,000 QPS) |
| :-----: | :---------------------: | :-----------------------------: |
|   Mean  |          8.36ms         |              0.21ms             |
|   50%   |          7.00ms         |              0.19ms             |
|   90%   |          7.53ms         |              0.25ms             |
|   95%   |          7.79ms         |              0.28ms             |
|   99%   |         66.68ms         |              0.57ms             |

**Technical Notes:**

* We use a `c7i.xlarge` instance on AWS (4 vCPUs, 8 GB RAM).
* We use a mock OpenAI inference provider for both benchmarks.
* The load generator, both gateways, and the mock inference provider all run on the same instance.
* We configured `observability.enabled = false` (i.e. disabled logging inferences to ClickHouse) in the TensorZero Gateway to make the scenarios comparable. (Even then, the observability features run asynchronously in the background, so they wouldn’t materially affect latency given a powerful enough ClickHouse deployment.)

Read more about the technical details and reproduction instructions [here](https://github.com/tensorzero/tensorzero/tree/main/gateway/benchmarks).

# TensorZero Gateway Clients

> The TensorZero Gateway can be used with the TensorZero Python client, with OpenAI clients (e.g. Python/Node), or via its HTTP API in any programming language.

The TensorZero Gateway can be used with the **TensorZero Python client**, with **OpenAI clients (e.g. Python/Node)**, or via its **HTTP API in any programming language**.

## Python

### TensorZero Client

The TensorZero client offers the most flexibility. It can be used with a built-in embedded (in-memory) gateway or a standalone HTTP gateway. Additionally, it can be used synchronously or asynchronously.

You can install the TensorZero Python client with `pip install tensorzero`.

#### Embedded Gateway

The TensorZero Client includes a built-in embedded (in-memory) gateway, so you don’t need to run a separate service.

##### Synchronous

```python
from tensorzero import TensorZeroGateway


with TensorZeroGateway.build_embedded(
    clickhouse_url="http://chuser:chpassword@localhost:8123/tensorzero",  # optional: for observability
    config_file="config/tensorzero.toml",  # optional: for custom functions, models, metrics, etc.
) as client:
    response = client.inference(
        model_name="openai::gpt-4o-mini",  # or: function_name="your_function_name"
        input={
            "messages": [
                {
                    "role": "user",
                    "content": "Write a haiku about artificial intelligence.",
                }
            ]
        },
    )
```

##### Asynchronous

```python
from tensorzero import AsyncTensorZeroGateway




async with await AsyncTensorZeroGateway.build_embedded(
    clickhouse_url="http://chuser:chpassword@localhost:8123/tensorzero",  # optional: for observability
    config_file="config/tensorzero.toml",  # optional: for custom functions, models, metrics, etc.
) as gateway:
    inference_response = await gateway.inference(
        model_name="openai::gpt-4o-mini",  # or: function_name="your_function_name"
        input={
            "messages": [
                {
                    "role": "user",
                    "content": "Write a haiku about artificial intelligence.",
                }
            ]
        },
    )


    feedback_response = await gateway.feedback(
        inference_id=inference_response.inference_id,
        metric_name="task_success",  # assuming a `task_success` metric is configured
        value=True,
    )
```

Tip

You can avoid the `await` in `build_embedded` by setting `async_setup=False`.

This is useful for synchronous contexts like `__init__` functions where `await` cannot be used. However, avoid using it in asynchronous contexts as it blocks the event loop. For async contexts, use the default `async_setup=True` with await.

For example, it’s safe to use `async_setup=False` when initializing a FastAPI server, but not while the server is actively handling requests.

#### Standalone HTTP Gateway

The TensorZero Client can optionally be used with a standalone HTTP Gateway instead.

##### Synchronous

```python
from tensorzero import TensorZeroGateway


# Assuming the TensorZero Gateway is running on localhost:3000...


with TensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
    # Same as above...
```

##### Asynchronous

```python
from tensorzero import AsyncTensorZeroGateway


# Assuming the TensorZero Gateway is running on localhost:3000...


async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
    # Same as above...
```

Tip

You can avoid the `await` in `build_http` by setting `async_setup=False`. See above for more details.

### OpenAI Python Client

You can use the OpenAI Python client to run inference requests with TensorZero. You need to use the TensorZero Client for feedback requests.

#### Embedded Gateway

You can run an embedded (in-memory) TensorZero Gateway with the OpenAI Python client, which doesn’t require a separate service.

```python
from openai import OpenAI
from tensorzero import patch_openai_client


client = OpenAI()  # or AsyncOpenAI


await patch_openai_client(
    client,
    config_file="path/to/tensorzero.toml",
    clickhouse_url="https://user:password@host:port/database",
)


response = client.chat.completions.create(
    model="tensorzero::model_name::openai::gpt-4o-mini",
    messages=[
        {
            "role": "user",
            "content": "Write a haiku about artificial intelligence.",
        }
    ],
)
```

Tip

You can avoid the `await` in `patch_openai_client` by setting `async_setup=False`. See above for more details.

#### Standalone HTTP Gateway

You can deploy the TensorZero Gateway as a separate service and configure the OpenAI client to talk to it. See [Deployment](/docs/gateway/deployment/) for instructions on how to deploy the TensorZero Gateway.

```python
from openai import OpenAI


# Assuming the TensorZero Gateway is running on localhost:3000...


with OpenAI(base_url="http://localhost:3000/openai/v1") as client:
    response = client.chat.completions.create(
        model="tensorzero::model_name::openai::gpt-4o-mini",
        messages=[
            {
                "role": "user",
                "content": "Write a haiku about artificial intelligence.",
            }
        ],
    )
```

#### Usage Details

##### `model`

In the OpenAI client, the `model` parameter should be one of the following:

> **`tensorzero::function_name::<your_function_name>`**
>
> For example, if you have a function named `generate_haiku`, you can use `tensorzero::function_name::generate_haiku`.

> **`tensorzero::model_name::<your_model_name>`**
>
> For example, if you have a model named `my_model` in the config file, you can use `tensorzero::model_name::my_model`. Alternatively, you can use default models like `tensorzero::model_name::openai::gpt-4o-mini`.

##### TensorZero Parameters

You can include optional TensorZero parameters (e.g. `episode_id` and `variant_name`) by prefixing them with `tensorzero::` in the `extra_body` field in OpenAI client requests.

```python
response = client.chat.completions.create(
    # ...
    extra_body={
        "tensorzero::episode_id": "00000000-0000-0000-0000-000000000000",
    },
)
```

## JavaScript / TypeScript / Node

### OpenAI Node Client

You can use the OpenAI client to run inference requests with TensorZero. You can deploy the TensorZero Gateway as a separate service and configure the OpenAI client to talk to the TensorZero Gateway.

See [Deployment](/docs/gateway/deployment/) for instructions on how to deploy the TensorZero Gateway.

```ts
import OpenAI from "openai";


const client = new OpenAI({
  baseURL: "http://localhost:3000/openai/v1",
});


const response = await client.chat.completions.create({
  model: "tensorzero::model_name::openai::gpt-4o-mini",
  messages: [
    {
      role: "user",
      content: "Write a haiku about artificial intelligence.",
    },
  ],
});
```

Tip

See [OpenAI Python Client » Usage Details](#usage-details) above for instructions on how to use the `model` parameter and other technical details.

You can include optional TensorZero parameters (e.g. `episode_id` and `variant_name`) by prefixing them with `tensorzero::` in the body in OpenAI client requests.

```ts
const result = await client.chat.completions.create({
  // ...
  "tensorzero::episode_id": "00000000-0000-0000-0000-000000000000",
});
```

## Other Languages and Platforms

The TensorZero Gateway exposes every feature via its HTTP API. You can deploy the TensorZero Gateway as a standalone service and interact with it from any programming language by making HTTP requests.

See [Deployment](/docs/gateway/deployment/) for instructions on how to deploy the TensorZero Gateway.

### TensorZero HTTP API

```bash
curl -X POST "http://localhost:3000/inference" \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "openai::gpt-4o-mini",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "Write a haiku about artificial intelligence."
        }
      ]
    }
  }'
```

```bash
curl -X POST "http://localhost:3000/feedback" \
  -H "Content-Type: application/json" \
  -d '{
    "inference_id": "00000000-0000-0000-0000-000000000000",
    "metric_name": "task_success",
    "value": true,
  }'
```

### OpenAI HTTP API

You can make OpenAI-compatible requests to the TensorZero Gateway.

```bash
curl -X POST "http://localhost:3000/openai/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tensorzero::model_name::openai::gpt-4o-mini",
    "messages": [
      {
        "role": "user",
        "content": "Write a haiku about artificial intelligence."
      }
    ]
  }'
```

Tip

See [OpenAI Python Client » Usage Details](#usage-details) above for instructions on how to use the `model` parameter and other technical details.

You can include optional TensorZero parameters (e.g. `episode_id` and `variant_name`) by prefixing them with `tensorzero::` in the body in OpenAI client requests.

```bash
curl -X POST "http://localhost:3000/openai/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
        // ...
        "tensorzero::episode_id": "00000000-0000-0000-0000-000000000000"
      }'
```

# Configuration Reference

> Learn how to configure the TensorZero Gateway.

The configuration file is the backbone of TensorZero. It defines the behavior of the gateway, including the models and their providers, functions and their variants, tools, metrics, and more. Developers express the behavior of LLM calls by defining the relevant prompt templates, schemas, and other parameters in this configuration file.

You can see an example configuration file [here](https://github.com/tensorzero/tensorzero/blob/main/tensorzero-internal/fixtures/config/tensorzero.toml).

The configuration file is a [TOML](https://toml.io/en/) file with a few major sections (TOML tables): `gateway`, `clickhouse`, `models`, `model_providers`, `functions`, `variants`, `tools`, and `metrics`.

## `[gateway]`

The `[gateway]` section defines the behavior of the TensorZero Gateway.

### `bind_address`

* **Type:** string
* **Required:** no (default: `0.0.0.0:3000`)

Defines the socket address to bind the TensorZero Gateway to.

tensorzero.toml

```toml
[gateway]
# ...
bind_address = "0.0.0.0:3000"
# ...
```

### `enable_template_filesystem_access`

* **Type:** boolean
* **Required:** no (default: `false`)

Enabling this setting will allow MiniJinja templates to load sub-templates from the file system (using the `include` directive). Paths must be relative to `tensorzero.toml`, and can only access files in that directory or its sub-directories.

Caution

Make sure to sanitize all user-provided template data before using this setting. Otherwise, a malicious input could read unintended files in the file system.

### `observability.async_writes`

* **Type:** boolean
* **Required:** no (default: `true`)

Enabling this setting will improve the latency of the gateway by offloading the responsibility of writing inference responses to ClickHouse to a background task, instead of waiting for ClickHouse to return the inference response.

Caution

If you enable this setting, make sure that the gateway lives long enough to complete the writes. This can be problematic in serverless environments that terminate the gateway instance after the response is returned but before the writes are completed.

### `observability.enabled`

* **Type:** boolean
* **Required:** no (default: `null`)

Enable the observability features of the TensorZero Gateway. If `true`, the gateway will throw an error on startup if it fails to validate the ClickHouse connection. If `null`, the gateway will log a warning but continue if ClickHouse is not available, and it will use ClickHouse if available. If `false`, the gateway will not use ClickHouse.

tensorzero.toml

```toml
[gateway]
# ...
observability.enabled = true
# ...
```

### `disable_observability` (Deprecated)

* **Type:** boolean
* **Required:** no (default: `false`)

Deprecated

This option is deprecated and will be removed in a future version. Please use `gateway.observability.enabled` instead.

Disable the observability features of the TensorZero Gateway (not recommended).

tensorzero.toml

```toml
[gateway]
# ...
disable_observability = true  # not recommended
# ...
```

## `[models.model_name]`

The `[models.model_name]` section defines the behavior of a model. You can define multiple models by including multiple `[models.model_name]` sections.

A model is provider agnostic, and the relevant providers are defined in the `providers` sub-section (see below).

If your `model_name` is not a basic string, it can be escaped with quotation marks. For example, periods are not allowed in basic strings, so you can define `llama-3.1-8b-instruct` as `[models."llama-3.1-8b-instruct"]`.

tensorzero.toml

```toml
[models.claude-3-haiku-20240307]
# fieldA = ...
# fieldB = ...
# ...


[models."llama-3.1-8b-instruct"]
# fieldA = ...
# fieldB = ...
# ...
```

### `routing`

* **Type:** array of strings
* **Required:** yes

A list of provider names to route requests to. The providers must be defined in the `providers` sub-section (see below). The TensorZero Gateway will attempt to route a request to the first provider in the list, and fallback to subsequent providers in order if the request is not successful.

tensorzero.toml

```toml
[models.gpt-4o]
# ...
routing = ["openai", "azure"]
# ...


[models.gpt-4o.providers.openai]
# ...


[models.gpt-4o.providers.azure]
# ...
```

## `[models.model_name.providers.provider_name]`

The `providers` sub-section defines the behavior of a specific provider for a model. You can define multiple providers by including multiple `[models.model_name.providers.provider_name]` sections.

If your `provider_name` is not a basic string, it can be escaped with quotation marks. For example, periods are not allowed in basic strings, so you can define `vllm.internal` as `[models.model_name.providers."vllm.internal"]`.

tensorzero.toml

```toml
[models.gpt-4o]
# ...
routing = ["openai", "azure"]
# ...


[models.gpt-4o.providers.openai]
# ...


[models.gpt-4o.providers.azure]
# ...
```

#### `extra_body`

* **Type:** array of objects (see below)
* **Required:** no

The `extra_body` field allows you to modify the request body that TensorZero sends to a model provider. This advanced feature is an “escape hatch” that lets you use provider-specific functionality that TensorZero hasn’t implemented yet.

Each object in the array must have two fields:

* `pointer`: A [JSON Pointer](https://datatracker.ietf.org/doc/html/rfc6901) string specifying where to modify the request body
* `value`: The value to insert at that location; it can be of any type including nested types

Tip

You can also set `extra_body` for a variant entry. The model provider `extra_body` entries take priority over variant `extra_body` entries.

Additionally, you can set `extra_body` at inference-time. The values provided at inference-time take priority over the values in the configuration file.

Example: `extra_body`

If TensorZero would normally send this request body to the provider…

```json
{
  "project": "tensorzero",
  "safety_checks": {
    "no_internet": false,
    "no_agi": true
  }
}
```

…then the following `extra_body`…

```toml
extra_body = [
  { pointer = "/agi", value = true},
  { pointer = "/safety_checks/no_agi", value = { bypass = "on" }}
]
```

…overrides the request body to:

```json
{
  "agi": true,
  "project": "tensorzero",
  "safety_checks": {
    "no_internet": false,
    "no_agi": {
      "bypass": "on"
    }
  }
}
```

#### `extra_headers`

* **Type:** array of objects (see below)
* **Required:** no

The `extra_headers` field allows you to set or overwrite the request headers that TensorZero sends to a model provider. This advanced feature is an “escape hatch” that lets you use provider-specific functionality that TensorZero hasn’t implemented yet.

Each object in the array must have two fields:

* `name` (string): The name of the header to modify (e.g. `anthropic-beta`)
* `value` (string): The value of the header (e.g. `token-efficient-tools-2025-02-19`)

Tip

You can also set `extra_headers` for a variant entry. The model provider `extra_headers` entries take priority over variant `extra_headers` entries.

Example: `extra_headers`

If TensorZero would normally send the following request headers to the provider…

```text
Safety-Checks: on
```

…then the following `extra_headers`…

```toml
extra_headers = [
  { name = "Safety-Checks", value = "off"},
  { name = "Intelligence-Level", value = "AGI"}
]
```

…overrides the request headers to:

```text
Safety-Checks: off
Intelligence-Level: AGI
```

### `type`

* **Type:** string
* **Required:** yes

Defines the types of the provider. See [Integrations » Model Providers](/docs/gateway/api-reference/inference/#content-block) for details.

The supported provider types are `anthropic`, `aws_bedrock`, `aws_sagemaker`, `azure`, `deepseek`, `fireworks`, `gcp_vertex_anthropic`, `gcp_vertex_gemini`, `google_ai_studio_gemini`, `hyperbolic`, `mistral`, `openai`, `sglang`, `tgi`, `together`, `vllm`, and `xai`.

The other fields in the provider sub-section depend on the provider type.

tensorzero.toml

```toml
[models.gpt-4o.providers.azure]
# ...
type = "azure"
# ...
```

`type: "anthropic"`<!-- for the table of contents -->#### `type: "anthropic"`

##### `model_name`

* **Type:** string
* **Required:** yes

Defines the model name to use with the Anthropic API. See [Anthropic’s documentation](https://docs.anthropic.com/en/docs/about-claude/models#model-names) for the list of available model names.

tensorzero.toml

```toml
[models.claude-3-haiku.providers.anthropic]
# ...
type = "anthropic"
model_name = "claude-3-haiku-20240307"
# ...
```

##### `api_key_location`

* **Type:** string
* **Required:** no (default: `env::ANTHROPIC_API_KEY`)

Defines the location of the API key for the Anthropic provider. The supported locations are `env::ENVIRONMENT_VARIABLE` and `dynamic::ARGUMENT_NAME` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details).

tensorzero.toml

```toml
[models.claude-3-haiku.providers.anthropic]
# ...
type = "anthropic"
api_key_location = "dynamic::anthropic_api_key"
# api_key_location = "env::ALTERNATE_ANTHROPIC_API_KEY"
# ...
```

`type: "aws_bedrock"`<!-- for the table of contents -->#### `type: "aws_bedrock"`

##### `allow_auto_detect_region`

* **Type:** boolean
* **Required:** no (default: `false`)

Defines whether to automatically detect the AWS region to use with the SageMaker API. Under the hood, the gateway will use the AWS SDK to try to detect the region. Alternatively, you can specify the region manually with the `region` field (recommended).

##### `model_id`

* **Type:** string
* **Required:** yes

Defines the model ID to use with the AWS Bedrock API. See [AWS Bedrock’s documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html) for the list of available model IDs.

tensorzero.toml

```toml
[models.claude-3-haiku.providers.aws_bedrock]
# ...
type = "aws_bedrock"
model_id = "anthropic.claude-3-haiku-20240307-v1:0"
# ...
```

##### `region`

* **Type:** string
* **Required:** no (default: based on credentials if set, otherwise `us-east-1`)

Defines the AWS region to use with the AWS Bedrock API.

tensorzero.toml

```toml
[models.claude-3-haiku.providers.aws_bedrock]
# ...
type = "aws_bedrock"
region = "us-east-2"
# ...
```

`type: "aws_sagemaker"`<!-- for the table of contents -->#### `type: "aws_sagemaker"`

##### `allow_auto_detect_region`

* **Type:** boolean
* **Required:** no (default: `false`)

Defines whether to automatically detect the AWS region to use with the SageMaker API. Under the hood, the gateway will use the AWS SDK to try to detect the region. Alternatively, you can specify the region manually with the `region` field (recommended).

##### `endpoint_name`

* **Type:** string
* **Required:** yes

Defines the endpoint name to use with the AWS SageMaker API.

##### `hosted_provider`

* **Type:** string
* **Required:** yes

Defines the underlying model provider to use with the SageMaker API. The `aws_sagemaker` provider is a wrapper on other providers.

Currently, the only supported `hosted_provider` is `openai` (including any OpenAI-compatible server e.g. Ollama).

tensorzero.toml

```toml
[models.claude-3-haiku.providers.aws_sagemaker]
# ...
type = "aws_sagemaker"
hosted_provider = "openai"
# ...
```

##### `model_name`

* **Type:** string
* **Required:** yes

Defines the model name to use with the AWS SageMaker API.

tensorzero.toml

```toml
[models.claude-3-haiku.providers.aws_sagemaker]
# ...
type = "aws_sagemaker"
model_name = "gemma3:1b"
# ...
```

##### `region`

* **Type:** string
* **Required:** no (default: based on credentials if set, otherwise `us-east-1`)

Defines the AWS region to use with the AWS Bedrock API.

tensorzero.toml

```toml
[models.claude-3-haiku.providers.aws_sagemaker]
# ...
type = "aws_sagemaker"
region = "us-east-2"
# ...
```

`type: "azure"`<!-- for the table of contents -->#### `type: "azure"`

The TensorZero Gateway handles the API version under the hood (currently `2024-06-01`). You only need to set the `deployment_id` and `endpoint` fields.

##### `deployment_id`

* **Type:** string
* **Required:** yes

Defines the deployment ID of the Azure OpenAI deployment.

See [Azure OpenAI’s documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models) for the list of available models.

tensorzero.toml

```toml
[models.gpt-4o-mini.providers.azure]
# ...
type = "azure"
deployment_id = "gpt4o-mini-20240718"
# ...
```

##### `endpoint`

* **Type:** string
* **Required:** yes

Defines the endpoint of the Azure OpenAI deployment (protocol and hostname).

tensorzero.toml

```toml
[models.gpt-4o-mini.providers.azure]
# ...
type = "azure"
endpoint = "https://<your-endpoint>.openai.azure.com"
# ...
```

##### `api_key_location`

* **Type:** string
* **Required:** no (default: `env::AZURE_OPENAI_API_KEY`)

Defines the location of the API key for the Azure OpenAI provider. The supported locations are `env::ENVIRONMENT_VARIABLE` and `dynamic::ARGUMENT_NAME` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details).

tensorzero.toml

```toml
[models.gpt-4o-mini.providers.azure]
# ...
type = "azure"
api_key_location = "dynamic::azure_openai_api_key"
# api_key_location = "env::ALTERNATE_AZURE_OPENAI_API_KEY"
# ...
```

`type: "deepseek"`<!-- for the table of contents -->#### `type: "deepseek"`

##### `model_name`

* **Type:** string
* **Required:** yes

Defines the model name to use with the DeepSeek API. Currently supported models are `deepseek-chat` (DeepSeek-v3) and `deepseek-reasoner` (R1).

tensorzero.toml

```toml
[models.deepseek_chat.providers.deepseek]
# ...
type = "deepseek"
model_name = "deepseek-chat"
# ...
```

##### `api_key_location`

* **Type:** string
* **Required:** no (default: `env::DEEPSEEK_API_KEY`)

Defines the location of the API key for the DeepSeek provider. The supported locations are `env::ENVIRONMENT_VARIABLE` and `dynamic::ARGUMENT_NAME` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details).

tensorzero.toml

```toml
[models.deepseek_chat.providers.deepseek]
# ...
type = "deepseek"
api_key_location = "dynamic::deepseek_api_key"
# api_key_location = "env::ALTERNATE_DEEPSEEK_API_KEY"
# ...
```

`type: "fireworks"`<!-- for the table of contents -->#### `type: "fireworks"`

##### `model_name`

* **Type:** string
* **Required:** yes

Defines the model name to use with the Fireworks API.

See [Fireworks’ documentation](https://fireworks.ai/models) for the list of available model names. You can also deploy your own models on Fireworks AI.

tensorzero.toml

```toml
[models."llama-3.1-8b-instruct".providers.fireworks]
# ...
type = "fireworks"
model_name = "accounts/fireworks/models/llama-v3p1-8b-instruct"
# ...
```

##### `api_key_location`

* **Type:** string
* **Required:** no (default: `env::FIREWORKS_API_KEY`)

Defines the location of the API key for the Fireworks provider. The supported locations are `env::ENVIRONMENT_VARIABLE` and `dynamic::ARGUMENT_NAME` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details).

tensorzero.toml

```toml
[models."llama-3.1-8b-instruct".providers.fireworks]
# ...
type = "fireworks"
api_key_location = "dynamic::fireworks_api_key"
# api_key_location = "env::ALTERNATE_FIREWORKS_API_KEY"
# ...
```

`type: "gcp_vertex_anthropic"`<!-- for the table of contents -->#### `type: "gcp_vertex_anthropic"`

##### `location`

* **Type:** string
* **Required:** yes

Defines the location (region) of the GCP Vertex AI Anthropic model.

tensorzero.toml

```toml
[models.claude-3-haiku.providers.gcp_vertex]
# ...
type = "gcp_vertex_anthropic"
location = "us-central1"
# ...
```

##### `model_id`

* **Type:** string
* **Required:** yes

Defines the model ID of the GCP Vertex AI model.

See [Anthropic’s GCP documentation](https://docs.anthropic.com/en/api/claude-on-vertex-ai#api-model-names) for the list of available model IDs.

tensorzero.toml

```toml
[models.claude-3-haiku.providers.gcp_vertex]
# ...
type = "gcp_vertex_anthropic"
model_id = "claude-3-haiku@20240307"
# ...
```

##### `project_id`

* **Type:** string
* **Required:** yes

Defines the project ID of the GCP Vertex AI model.

tensorzero.toml

```toml
[models.claude-3-haiku-2024030.providers.gcp_vertex]
# ...
type = "gcp_vertex"
project_id = "your-project-id"
# ...
```

##### `credential_location`

* **Type:** string
* **Required:** no (default: `env::GCP_CREDENTIALS_PATH`)

Defines the location of the credentials for the GCP Vertex Anthropic provider. The supported locations are `env::PATH_TO_CREDENTIALS_FILE`, `dynamic::CREDENTIALS_ARGUMENT_NAME` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details), and `file::PATH_TO_CREDENTIALS_FILE`.

tensorzero.toml

```toml
[models.claude-3-haiku.providers.gcp_vertex]
# ...
type = "gcp_vertex_anthropic"
credential_location = "dynamic::gcp_credentials_path"
# credential_location = "env::ALTERNATE_GCP_CREDENTIALS_PATH"
# credential_location = "file::PATH_TO_CREDENTIALS_FILE"
# ...
```

`type: "gcp_vertex_gemini"`<!-- for the table of contents -->#### `type: "gcp_vertex_gemini"`

##### `location`

* **Type:** string
* **Required:** yes

Defines the location (region) of the GCP Vertex Gemini model.

tensorzero.toml

```toml
[models."gemini-1.5-flash".providers.gcp_vertex]
# ...
type = "gcp_vertex_gemini"
location = "us-central1"
# ...
```

##### `model_id`

* **Type:** string
* **Required:** yes

Defines the model ID of the GCP Vertex AI model.

See [GCP Vertex AI’s documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions) for the list of available model IDs.

tensorzero.toml

```toml
[models."gemini-1.5-flash".providers.gcp_vertex]
# ...
type = "gcp_vertex_gemini"
model_id = "gemini-1.5-flash-001"
# ...
```

##### `project_id`

* **Type:** string
* **Required:** yes

Defines the project ID of the GCP Vertex AI model.

tensorzero.toml

```toml
[models."gemini-1.5-flash".providers.gcp_vertex]
# ...
type = "gcp_vertex_gemini"
project_id = "your-project-id"
# ...
```

##### `credential_location`

* **Type:** string
* **Required:** no (default: `env::GCP_CREDENTIALS_PATH`)

Defines the location of the credentials for the GCP Vertex Gemini provider. The supported locations are `env::PATH_TO_CREDENTIALS_FILE`, `dynamic::CREDENTIALS_ARGUMENT_NAME` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details), and `file::PATH_TO_CREDENTIALS_FILE`.

tensorzero.toml

```toml
[models."gemini-1.5-flash".providers.gcp_vertex]
# ...
type = "gcp_vertex_gemini"
credential_location = "dynamic::gcp_credentials_path"
# credential_location = "env::ALTERNATE_GCP_CREDENTIALS_PATH"
# credential_location = "file::PATH_TO_CREDENTIALS_FILE"
# ...
```

`type: "google_ai_studio_gemini"`<!-- for the table of contents -->#### `type: "google_ai_studio_gemini"`

##### `model_name`

* **Type:** string
* **Required:** yes

Defines the model name to use with the Google AI Studio Gemini API. See [Google AI Studio’s documentation](https://ai.google.dev/gemini-api/docs/models/gemini) for the list of available model names.

tensorzero.toml

```toml
[models."gemini-1.5-flash".providers.google_ai_studio_gemini]
# ...
type = "google_ai_studio_gemini"
model_name = "gemini-1.5-flash-001"
# ...
```

##### `api_key_location`

* **Type:** string
* **Required:** no (default: `env::GOOGLE_AI_STUDIO_API_KEY`)

Defines the location of the API key for the Google AI Studio Gemini provider. The supported locations are `env::ENVIRONMENT_VARIABLE` and `dynamic::ARGUMENT_NAME` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details).

tensorzero.toml

```toml
[models."gemini-1.5-flash".providers.google_ai_studio_gemini]
# ...
type = "google_ai_studio_gemini"
api_key_location = "dynamic::google_ai_studio_api_key"
# api_key_location = "env::ALTERNATE_GOOGLE_AI_STUDIO_API_KEY"
# ...
```

`type: "hyperbolic"`<!-- for the table of contents -->#### `type: "hyperbolic"`

##### `model_name`

* **Type:** string
* **Required:** yes

Defines the model name to use with the Hyperbolic API.

See [Hyperbolic’s documentation](https://app.hyperbolic.xyz/models) for the list of available model names.

tensorzero.toml

```toml
[models."meta-llama/Meta-Llama-3-70B-Instruct".providers.hyperbolic]
# ...
type = "hyperbolic"
model_name = "meta-llama/Meta-Llama-3-70B-Instruct"
# ...
```

##### `api_key_location`

* **Type:** string
* **Required:** no (default: `env::HYPERBOLIC_API_KEY`)

Defines the location of the API key for the Hyperbolic provider. The supported locations are `env::ENVIRONMENT_VARIABLE` and `dynamic::ARGUMENT_NAME` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details).

tensorzero.toml

```toml
[models."meta-llama/Meta-Llama-3-70B-Instruct".providers.hyperbolic]
# ...
type = "hyperbolic"
api_key_location = "dynamic::hyperbolic_api_key"
# api_key_location = "env::ALTERNATE_HYPERBOLIC_API_KEY"
# ...
```

`type: "mistral"`<!-- for the table of contents -->#### `type: "mistral"`

##### `model_name`

* **Type:** string
* **Required:** yes

Defines the model name to use with the Mistral API.

See [Mistral’s documentation](https://docs.mistral.ai/getting-started/models/) for the list of available model names.

tensorzero.toml

```toml
[models."open-mistral-nemo".providers.mistral]
# ...
type = "mistral"
model_name = "open-mistral-nemo-2407"
# ...
```

##### `api_key_location`

* **Type:** string
* **Required:** no (default: `env::MISTRAL_API_KEY`)

Defines the location of the API key for the Mistral provider. The supported locations are `env::ENVIRONMENT_VARIABLE` and `dynamic::ARGUMENT_NAME` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details).

tensorzero.toml

```toml
[models."open-mistral-nemo".providers.mistral]
# ...
type = "mistral"
api_key_location = "dynamic::mistral_api_key"
# api_key_location = "env::ALTERNATE_MISTRAL_API_KEY"
# ...
```

`type: "openai"`<!-- for the table of contents -->#### `type: "openai"`

##### `api_base`

* **Type:** string
* **Required:** no (default: `https://api.openai.com/v1/`)

Defines the base URL of the OpenAI API.

You can use the `api_base` field to use an API provider that is compatible with the OpenAI API. However, many providers are only “approximately compatible” with the OpenAI API, so you might need to use a specialized model provider in those cases.

tensorzero.toml

```toml
[models."gpt-4o".providers.openai]
# ...
type = "openai"
api_base = "https://api.openai.com/v1/"
# ...
```

##### `model_name`

* **Type:** string
* **Required:** yes

Defines the model name to use with the OpenAI API.

See [OpenAI’s documentation](https://platform.openai.com/docs/models) for the list of available model names.

tensorzero.toml

```toml
[models.gpt-4o-mini.providers.openai]
# ...
type = "openai"
model_name = "gpt-4o-mini-2024-07-18"
# ...
```

##### `api_key_location`

* **Type:** string
* **Required:** no (default: `env::OPENAI_API_KEY`)

Defines the location of the API key for the OpenAI provider. The supported locations are `env::ENVIRONMENT_VARIABLE`, `dynamic::ARGUMENT_NAME`, and `none` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details).

tensorzero.toml

```toml
[models.gpt-4o-mini.providers.openai]
# ...
type = "openai"
api_key_location = "dynamic::openai_api_key"
# api_key_location = "env::ALTERNATE_OPENAI_API_KEY"
# api_key_location = "none"
# ...
```

`type: "sglang"`<!-- for the table of contents -->#### `type: "sglang"`

##### `api_base`

* **Type:** string
* **Required:** yes

Defines the base URL of the SGLang API.

tensorzero.toml

```toml
[models.llama.providers.sglang]
# ...
type = "sglang"
api_base = "http://localhost:8080/v1/"
# ...
```

##### `api_key_location`

* **Type:** string
* **Required:** no (default: `none`)

Defines the location of the API key for the SGLang provider. The supported locations are `env::ENVIRONMENT_VARIABLE`, `dynamic::ARGUMENT_NAME`, and `none` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details).

tensorzero.toml

```toml
[models.llama.providers.sglang]
# ...
type = "sglang"
api_key_location = "dynamic::sglang_api_key"
# api_key_location = "env::ALTERNATE_SGLANG_API_KEY"
# api_key_location = "none"  # if authentication is disabled
# ...
```

`type: "together"`<!-- for the table of contents -->#### `type: "together"`

##### `model_name`

* **Type:** string
* **Required:** yes

Defines the model name to use with the Together API.

See [Together’s documentation](https://docs.together.ai/docs/chat-models) for the list of available model names. You can also deploy your own models on Together AI.

tensorzero.toml

```toml
[models.llama3_1_8b_instruct_turbo.providers.together]
# ...
type = "together"
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
# ...
```

##### `api_key_location`

* **Type:** string
* **Required:** no (default: `env::TOGETHER_API_KEY`)

Defines the location of the API key for the Together AI provider. The supported locations are `env::ENVIRONMENT_VARIABLE` and `dynamic::ARGUMENT_NAME` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details).

tensorzero.toml

```toml
[models.llama3_1_8b_instruct_turbo.providers.together]
# ...
type = "together"
api_key_location = "dynamic::together_api_key"
# api_key_location = "env::ALTERNATE_TOGETHER_API_KEY"
# ...
```

`type: "vllm"`<!-- for the table of contents -->#### `type: "vllm"`

##### `api_base`

* **Type:** string
* **Required:** yes (default: `http://localhost:8000/v1/`)

Defines the base URL of the VLLM API.

tensorzero.toml

```toml
[models."phi-3.5-mini-instruct".providers.vllm]
# ...
type = "vllm"
api_base = "http://localhost:8000/v1/"
# ...
```

##### `model_name`

* **Type:** string
* **Required:** yes

Defines the model name to use with the vLLM API.

tensorzero.toml

```toml
[models."phi-3.5-mini-instruct".providers.vllm]
# ...
type = "vllm"
model_name = "microsoft/Phi-3.5-mini-instruct"
# ...
```

##### `api_key_location`

* **Type:** string
* **Required:** no (default: `env::VLLM_API_KEY`)

Defines the location of the API key for the vLLM provider. The supported locations are `env::ENVIRONMENT_VARIABLE`, `dynamic::ARGUMENT_NAME`, and `none` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details).

tensorzero.toml

```toml
[models."phi-3.5-mini-instruct".providers.vllm]
# ...
type = "vllm"
api_key_location = "dynamic::vllm_api_key"
# api_key_location = "env::ALTERNATE_VLLM_API_KEY"
# api_key_location = "none"
# ...
```

`type: "xai"`<!-- for the table of contents -->#### `type: "xai"`

##### `model_name`

* **Type:** string
* **Required:** yes

Defines the model name to use with the xAI API.

See [xAI’s documentation](https://docs.x.ai/docs/models) for the list of available model names.

tensorzero.toml

```toml
[models.grok_2_1212.providers.xai]
# ...
type = "xai"
model_name = "grok-2-1212"
# ...
```

##### `api_key_location`

* **Type:** string
* **Required:** no (default: `env::XAI_API_KEY`)

Defines the location of the API key for the xAI provider. The supported locations are `env::ENVIRONMENT_VARIABLE` and `dynamic::ARGUMENT_NAME` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details).

tensorzero.toml

```toml
[models.grok_2_1212.providers.xai]
# ...
type = "xai"
api_key_location = "dynamic::xai_api_key"
# api_key_location = "env::ALTERNATE_XAI_API_KEY"
# ...
```

`type: "tgi"`<!-- for the table of contents -->#### `type: "tgi"`

##### `api_base`

* **Type:** string
* **Required:** yes

Defines the base URL of the TGI API.

tensorzero.toml

```toml
[models.phi_4.providers.tgi]
# ...
type = "tgi"
api_base = "http://localhost:8080/v1/"
# ...
```

##### `api_key_location`

* **Type:** string
* **Required:** no (default: `none`)

Defines the location of the API key for the TGI provider. The supported locations are `env::ENVIRONMENT_VARIABLE`, `dynamic::ARGUMENT_NAME`, and `none` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details).

tensorzero.toml

```toml
[models.phi_4.providers.tgi]
# ...
type = "tgi"
api_key_location = "dynamic::tgi_api_key"
# api_key_location = "env::ALTERNATE_TGI_API_KEY"
# api_key_location = "none"  # if authentication is disabled
# ...
```

## `[embedding_models.model_name]`

The `[embedding_models.model_name]` section defines the behavior of an embedding model. You can define multiple models by including multiple `[embedding_models.model_name]` sections.

A model is provider agnostic, and the relevant providers are defined in the `providers` sub-section (see below).

If your `model_name` is not a basic string, it can be escaped with quotation marks. For example, periods are not allowed in basic strings, so you can define `embedding-0.1` as `[embedding_models."embedding-0.1"]`.

tensorzero.toml

```toml
[embedding_models.openai-text-embedding-3-small]
# fieldA = ...
# fieldB = ...
# ...


[embedding_models."t0-text-embedding-3.5-massive"]
# fieldA = ...
# fieldB = ...
# ...
```

### `routing`

* **Type:** array of strings
* **Required:** yes

A list of provider names to route requests to. The providers must be defined in the `providers` sub-section (see below). The TensorZero Gateway will attempt to route a request to the first provider in the list, and fallback to subsequent providers in order if the request is not successful.

tensorzero.toml

```toml
[embedding_models.model-name]
# ...
routing = ["openai", "alternative-provider"]
# ...


[embedding_models.model-name.providers.openai]
# ...


[embedding_models.model-name.providers.alternative-provider]
# ...
```

## `[embedding_models.model_name.providers.provider_name]`

The `providers` sub-section defines the behavior of a specific provider for a model. You can define multiple providers by including multiple `[embedding_models.model_name.providers.provider_name]` sections.

If your `provider_name` is not a basic string, it can be escaped with quotation marks. For example, periods are not allowed in basic strings, so you can define `vllm.internal` as `[embedding_models.model_name.providers."vllm.internal"]`.

tensorzero.toml

```toml
[embedding_models.model-name]
# ...
routing = ["openai", "alternative-provider"]
# ...


[embedding_models.model-name.providers.openai]
# ...


[embedding_models.model-name.providers.alternative-provider]
# ...
```

### `type`

* **Type:** string
* **Required:** yes

Defines the types of the provider. See [Integrations » Model Providers](/docs/gateway/integrations/#model-providers) for details.

TensorZero currently only supports `openai` as a provider for embedding models. More integrations are on the way.

The other fields in the provider sub-section depend on the provider type.

tensorzero.toml

```toml
[embedding_models.model-name.providers.openai]
# ...
type = "openai"
# ...
```

`type: "openai"`<!-- for the table of contents -->#### `type: "openai"`

##### `api_base`

* **Type:** string
* **Required:** no (default: `https://api.openai.com/v1/`)

Defines the base URL of the OpenAI API.

You can use the `api_base` field to use an API provider that is compatible with the OpenAI API. However, many providers are only “approximately compatible” with the OpenAI API, so you might need to use a specialized model provider in those cases.

tensorzero.toml

```toml
[embedding_models.openai-text-embedding-3-small.providers.openai]
# ...
type = "openai"
api_base = "https://api.openai.com/v1/"
# ...
```

##### `model_name`

* **Type:** string
* **Required:** yes

Defines the model name to use with the OpenAI API.

See [OpenAI’s documentation](https://platform.openai.com/docs/models/embeddings) for the list of available model names.

tensorzero.toml

```toml
[embedding_models.openai-text-embedding-3-small.providers.openai]
# ...
type = "openai"
model_name = "text-embedding-3-small"
# ...
```

##### `api_key_location`

* **Type:** string
* **Required:** no (default: `env::OPENAI_API_KEY`)

Defines the location of the API key for the OpenAI provider. The supported locations are `env::ENVIRONMENT_VARIABLE`, `dynamic::ARGUMENT_NAME`, and `none` (see [the API reference](/docs/gateway/api-reference/inference/#credentials)) for more details).

tensorzero.toml

```toml
[embedding_models.openai-text-embedding-3-small.providers.openai]
# ...
type = "openai"
api_key_location = "dynamic::openai_api_key"
# api_key_location = "env::ALTERNATE_OPENAI_API_KEY"
# api_key_location = "none"
# ...
```

## `[functions.function_name]`

The `[functions.function_name]` section defines the behavior of a function. You can define multiple functions by including multiple `[functions.function_name]` sections.

A function can have multiple variants, and each variant is defined in the `variants` sub-section (see below). A function expresses the abstract behavior of an LLM call (e.g. the schemas for the messages), and its variants express concrete instantiations of that LLM call (e.g. specific templates and models).

If your `function_name` is not a basic string, it can be escaped with quotation marks. For example, periods are not allowed in basic strings, so you can define `summarize-2.0` as `[functions."summarize-2.0"]`.

tensorzero.toml

```toml
[functions.draft-email]
# fieldA = ...
# fieldB = ...
# ...


[functions.summarize-email]
# fieldA = ...
# fieldB = ...
# ...
```

### `assistant_schema`

* **Type:** string (path)
* **Required:** no

Defines the path to the assistant schema file. The path is relative to the configuration file.

If provided, the assistant schema file should contain a [JSON Schema](https://json-schema.org/) for the assistant messages. The variables in the schema are used for templating the assistant messages. If a schema is provided, all function variants must also provide an assistant template (see below).

tensorzero.toml

```toml
[functions.draft-email]
# ...
assistant_schema = "./functions/draft-email/assistant_schema.json"
# ...


[functions.draft-email.variants.prompt-v1]
# ...
assistant_template = "./functions/draft-email/prompt-v1/assistant_template.minijinja"
# ...
```

### `system_schema`

* **Type:** string (path)
* **Required:** no

Defines the path to the system schema file. The path is relative to the configuration file.

If provided, the system schema file should contain a [JSON Schema](https://json-schema.org/) for the system message. The variables in the schema are used for templating the system message. If a schema is provided, all function variants must also provide a system template (see below).

tensorzero.toml

```toml
[functions.draft-email]
# ...
system_schema = "./functions/draft-email/system_schema.json"
# ...


[functions.draft-email.variants.prompt-v1]
# ...
system_template = "./functions/draft-email/prompt-v1/system_template.minijinja"
# ...
```

### `type`

* **Type:** string
* **Required:** yes

Defines the type of the function.

The supported function types are `chat` and `json`.

Most other fields in the function section depend on the function type.

tensorzero.toml

```toml
[functions.draft-email]
# ...
type = "chat"
# ...
```

`type: "chat"`<!-- for the table of contents -->#### `type: "chat"`

##### `parallel_tool_calls`

* **Type:** boolean
* **Required:** no

Determines whether the function should be allowed to call multiple tools in a single conversation turn.

If not set, TensorZero will default to the model provider’s default behavior.

Most model providers do not support this feature. In those cases, this field will be ignored.

tensorzero.toml

```toml
[functions.draft-email]
# ...
type = "chat"
parallel_tool_calls = true
# ...
```

##### `tool_choice`

* **Type:** string
* **Required:** no (default: `auto`)

Determines the tool choice strategy for the function.

The supported tool choice strategies are:

* `none`: The function should not use any tools.
* `auto`: The model decides whether or not to use a tool. If it decides to use a tool, it also decides which tools to use.
* `required`: The model should use a tool. If multiple tools are available, the model decides which tool to use.
* `{ specific = "tool_name" }`: The model should use a specific tool. The tool must be defined in the `tools` field (see below).

tensorzero.toml

```toml
[functions.solve-math-problem]
# ...
type = "chat"
tool_choice = "auto"
tools = [
  # ...
  "run-python"
  # ...
]
# ...


[tools.run-python]
# ...
```

tensorzero.toml

```toml
[functions.generate-query]
# ...
type = "chat"
tool_choice = { specific = "query-database" }
tools = [
  # ...
  "query-database"
  # ...
]
# ...


[tools.query-database]
# ...
```

##### `tools`

* **Type:** array of strings
* **Required:** no (default: `[]`)

Determines the tools that the function can use.

The supported tools are defined in `[tools.tool_name]` sections (see below).

tensorzero.toml

```toml
[functions.draft-email]
# ...
type = "chat"
tools = [
  # ...
  "query-database"
  # ...
]
# ...


[tools.query-database]
# ...
```

`type: "json"`<!-- for the table of contents -->#### `type: "json"`

##### `output_schema`

* **Type:** string (path)
* **Required:** no (default: `{}`, the empty JSON schema that accepts any valid JSON output)

Defines the path to the output schema file, which should contain a [JSON Schema](https://json-schema.org/) for the output of the function. The path is relative to the configuration file.

This schema is used for validating the output of the function.

tensorzero.toml

```toml
[functions.extract-customer-info]
# ...
type = "json"
output_schema = "./functions/extract-customer-info/output_schema.json"
# ...
```

### `user_schema`

* **Type:** string (path)
* **Required:** no

Defines the path to the user schema file. The path is relative to the configuration file.

If provided, the user schema file should contain a [JSON Schema](https://json-schema.org/) for the user messages. The variables in the schema are used for templating the user messages. If a schema is provided, all function variants must also provide a user template (see below).

tensorzero.toml

```toml
[functions.draft-email]
# ...
user_schema = "./functions/draft-email/user_schema.json"
# ...


[functions.draft-email.variants.prompt-v1]
# ...
user_template = "./functions/draft-email/prompt-v1/user_template.minijinja"
# ...
```

## `[functions.function_name.variants.variant_name]`

The `variants` sub-section defines the behavior of a specific variant of a function. You can define multiple variants by including multiple `[functions.function_name.variants.variant_name]` sections.

If your `variant_name` is not a basic string, it can be escaped with quotation marks. For example, periods are not allowed in basic strings, so you can define `llama-3.1-8b-instruct` as `[functions.function_name.variants."llama-3.1-8b-instruct"]`.

tensorzero.toml

```toml
[functions.draft-email]
# ...


[functions.draft-email.variants."llama-3.1-8b-instruct"]
# ...


[functions.draft-email.variants.claude-3-haiku]
# ...
```

### `type`

* **Type:** string
* **Required:** yes

Defines the type of the variant.

TensorZero currently supports the following variant types:

| Type                                       | Description                                                                                                                                                                     |
| :----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `chat_completion`                          | Uses a chat completion model to generate responses by processing a series of messages in a conversational format. This is typically what you use out of the box with most LLMs. |
| `experimental_best_of_n`                   | Generates multiple response candidates with other variants, and selects the best one using an evaluator model.                                                                  |
| `experimental_dynamic_in_context_learning` | Selects similar high-quality examples using an embedding of the input, and incorporates them into the prompt to enhance context and improve response quality.                   |
| `experimental_mixture_of_n`                | Generates multiple response candidates with other variants, and combines the responses using a fuser model.                                                                     |

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
type = "chat_completion"
# ...
```

`type: "chat_completion"`<!-- for the table of contents -->#### `type: "chat_completion"`

##### `assistant_template`

* **Type:** string (path)
* **Required:** no

Defines the path to the assistant template file. The path is relative to the configuration file.

This file should contain a [MiniJinja](https://docs.rs/minijinja/latest/minijinja/syntax/index.html) template for the assistant messages. If the template uses any variables, the variables should be defined in the function’s `assistant_schema` field.

tensorzero.toml

```toml
[functions.draft-email]
# ...
assistant_schema = "./functions/draft-email/assistant_schema.json"
# ...


[functions.draft-email.variants.prompt-v1]
# ...
assistant_template = "./functions/draft-email/prompt-v1/assistant_template.minijinja"
# ...
```

##### `extra_body`

* **Type:** array of objects (see below)
* **Required:** no

The `extra_body` field allows you to modify the request body that TensorZero sends to a variant’s model provider. This advanced feature is an “escape hatch” that lets you use provider-specific functionality that TensorZero hasn’t implemented yet.

Each object in the array must have two fields:

* `pointer`: A [JSON Pointer](https://datatracker.ietf.org/doc/html/rfc6901) string specifying where to modify the request body
* `value`: The value to insert at that location; it can be of any type including nested types

Tip

You can also set `extra_body` for a model provider entry. The model provider `extra_body` entries take priority over variant `extra_body` entries.

Additionally, you can set `extra_body` at inference-time. The values provided at inference-time take priority over the values in the configuration file.

Example: `extra_body`

If TensorZero would normally send this request body to the provider…

```json
{
  "project": "tensorzero",
  "safety_checks": {
    "no_internet": false,
    "no_agi": true
  }
}
```

…then the following `extra_body`…

```toml
extra_body = [
  { pointer = "/agi", value = true},
  { pointer = "/safety_checks/no_agi", value = { bypass = "on" }}
]
```

…overrides the request body to:

```json
{
  "agi": true,
  "project": "tensorzero",
  "safety_checks": {
    "no_internet": false,
    "no_agi": {
      "bypass": "on"
    }
  }
}
```

##### `extra_headers`

* **Type:** array of objects (see below)
* **Required:** no

The `extra_headers` field allows you to set or overwrite the request headers that TensorZero sends to a model provider. This advanced feature is an “escape hatch” that lets you use provider-specific functionality that TensorZero hasn’t implemented yet.

Each object in the array must have two fields:

* `name` (string): The name of the header to modify (e.g. `anthropic-beta`)
* `value` (string): The value of the header (e.g. `token-efficient-tools-2025-02-19`)

Tip

You can also set `extra_headers` for a model provider entry. The model provider `extra_headers` entries take priority over variant `extra_headers` entries.

Example: `extra_headers`

If TensorZero would normally send the following request headers to the provider…

```text
Safety-Checks: on
```

…then the following `extra_headers`…

```toml
extra_headers = [
  { name = "Safety-Checks", value = "off"},
  { name = "Intelligence-Level", value = "AGI"}
]
```

…overrides the request headers to:

```text
Safety-Checks: off
Intelligence-Level: AGI
```

##### `frequency_penalty`

* **Type:** float
* **Required:** no (default: `null`)

Penalizes new tokens based on their frequency in the text so far if positive, encourages them if negative.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
frequency_penalty = 0.2
# ...
```

##### `json_mode`

* **Type:** string
* **Required:** no (default: `strict`)

Defines the strategy for generating JSON outputs.

This parameter is only supported for variants of functions with `type = "json"`.

The supported modes are:

* `off`: Make a chat completion request without any special JSON handling (not recommended).
* `on`: Make a chat completion request with JSON mode (if supported by the provider).
* `strict`: Make a chat completion request with strict JSON mode (if supported by the provider). For example, the TensorZero Gateway uses Structured Outputs for OpenAI.
* `implicit_tool`: Make a special-purpose tool use request under the hood, and convert the tool call into a JSON response.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
json_mode = "strict"
# ...
```

##### `max_tokens`

* **Type:** integer
* **Required:** no (default: `null`)

Defines the maximum number of tokens to generate.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
max_tokens = 100
# ...
```

##### `model`

* **Type:** string
* **Required:** yes

The model must be defined in the `[models.model_name]` section (see above), or correspond to a short-hand model name.

Short-hand model names follow the format `provider::model_name` (e.g. `openai::gpt-4o-mini` or `anthropic::claude-3-5-haiku`). The following model providers support short-hand default model names: `anthropic`, `deepseek`, `fireworks`, `google_ai_studio_gemini`, `hyperbolic`, `mistral`, `openai`, `together`, and `xai`. The remaining providers do not support short-hand default model names, and require an explicit `model` block in your configuration file.

tensorzero.toml

```toml
[models.gpt-4o-mini]
# ...


[functions.draft-email.variants.prompt-v1]
# ...
model = "gpt-4o-mini"
# ...
```

##### `presence_penalty`

* **Type:** float
* **Required:** no (default: `null`)

Penalizes new tokens based on that have already appeared in the text so far if positive, encourages them if negative.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
presence_penalty = 0.5
# ...
```

##### `retries`

* **Type:** object with optional keys `num_retries` and `max_delay_s`
* **Required:** no (defaults to `num_retries = 0` and a `max_delay_s = 10`)

TensorZero’s retry strategy is truncated exponential backoff with jitter. The `num_retries` parameter defines the number of retries (not including the initial request). The `max_delay_s` parameter defines the maximum delay between retries.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
retries = { num_retries = 3, max_delay_s = 10 }
# ...
```

##### `seed`

* **Type:** integer
* **Required:** no (default: `null`)

Defines the seed to use for the variant.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
seed = 42
```

##### `system_template`

* **Type:** string (path)
* **Required:** no

Defines the path to the system template file. The path is relative to the configuration file.

This file should contain a [MiniJinja](https://docs.rs/minijinja/latest/minijinja/syntax/index.html) template for the system messages. If the template uses any variables, the variables should be defined in the function’s `system_schema` field.

tensorzero.toml

```toml
[functions.draft-email]
# ...
system_schema = "./functions/draft-email/system_schema.json"
# ...


[functions.draft-email.variants.prompt-v1]
# ...
system_template = "./functions/draft-email/prompt-v1/system_template.minijinja"
# ...
```

##### `temperature`

* **Type:** float
* **Required:** no (default: `null`)

Defines the temperature to use for the variant.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
temperature = 0.5
# ...
```

##### `top_p`

* **Type:** float, between 0 and 1
* **Required:** no (default: `null`)

Defines the `top_p` to use for the variant during [nucleus sampling](https://en.wikipedia.org/wiki/Top-p_sampling). Typically at most one of `top_p` and `temperature` is set.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
top_p = 0.3
# ...
```

##### `user_template`

* **Type:** string (path)
* **Required:** no

Defines the path to the user template file. The path is relative to the configuration file.

This file should contain a [MiniJinja](https://docs.rs/minijinja/latest/minijinja/syntax/index.html) template for the user messages. If the template uses any variables, the variables should be defined in the function’s `user_schema` field.

tensorzero.toml

```toml
[functions.draft-email]
# ...
user_schema = "./functions/draft-email/user_schema.json"
# ...


[functions.draft-email.variants.prompt-v1]
# ...
user_template = "./functions/draft-email/prompt-v1/user_template.minijinja"
# ...
```

##### `weight`

* **Type:** float
* **Required:** no (default: 0)

Defines the weight of the variant. When you call a function, the weight determines the relative importance of the variant when sampling.

Variants will be sampled with a probability proportional to their weight. For example, if variant A has a weight of `1.0` and variant B has a weight of `3.0`, variant A will be sampled with probability `1.0 / (1.0 + 3.0) = 25%` and variant B will be sampled with probability `3.0 / (1.0 + 3.0) = 75%`.

You can disable a variant by setting its weight to `0`. The variant will only be used if there are no other variants available for sampling or if the variant is requested explicitly in the request with `variant_name`. This is useful for defining fallback variants, which won’t be used unless no other variants are available.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
weight = 1.0
# ...
```

`type: "experimental_best_of_n"`<!-- for the table of contents -->#### `type: "experimental_best_of_n"`

##### `candidates`

* **Type:** list of strings
* **Required:** yes

This inference strategy generates N candidate responses, and an evaluator model selects the best one. This approach allows you to leverage multiple prompts or variants to increase the likelihood of getting a high-quality response.

The `candidates` parameter specifies a list of variant names used to generate candidate responses. For example, if you have two variants defined (`promptA` and `promptB`), you could set up the `candidates` list to generate two responses using `promptA` and one using `promptB` using the snippet below. The evaluator would then choose the best response from these three candidates.

tensorzero.toml

```toml
[functions.draft-email.variants.promptA]
type = "chat_completion"
# ...


[functions.draft-email.variants.promptB]
type = "chat_completion"
# ...


[functions.draft-email.variants.best-of-n]
type = "experimental_best_of_n"
candidates = ["promptA", "promptA", "promptB"] # 3 candidate generations
# ...
```

##### `evaluator`

* **Type:** object
* **Required:** yes

The `evaluator` parameter specifies the configuration for the model that will evaluate and select the best response from the generated candidates.

The evaluator is configured similarly to a `chat_completion` variant, but without the `type` field. The prompts here should be prompts that you would use to solve the original problem, as the gateway has special-purpose handling and templates to convert them to an evaluator.

```toml
[functions.draft-email.variants.best-of-n]
type = "experimental_best_of_n"
# ...


[functions.draft-email.variants.best-of-n.evaluator]
# Same fields as a `chat_completion` variant (excl.`type`), e.g.:
# user_template = "functions/draft-email/best-of-n/user.minijinja"
# ...
```

##### `timeout_s`

* **Type:** float
* **Required:** no (default: 300s)

The `timeout_s` parameter specifies the maximum time in seconds allowed for generating candidate responses. Any candidate that takes longer than this duration to generate a response will be dropped from consideration.

```toml
[functions.draft-email.variants.best-of-n]
type = "experimental_best_of_n"
timeout_s = 60
# ...
```

##### `weight`

* **Type:** float
* **Required:** no (default: 0)

Defines the weight of the variant. When you call a function, the weight determines the relative importance of the variant when sampling.

Variants will be sampled with a probability proportional to their weight. For example, if variant A has a weight of `1.0` and variant B has a weight of `3.0`, variant A will be sampled with probability `1.0 / (1.0 + 3.0) = 25%` and variant B will be sampled with probability `3.0 / (1.0 + 3.0) = 75%`.

You can disable a variant by setting its weight to `0`. The variant will only be used if there are no other variants available for sampling or if the variant is requested explicitly in the request with `variant_name`. This is useful for defining fallback variants, which won’t be used unless no other variants are available.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
weight = 1.0
# ...
```

`type: "experimental_mixture_of_n"`<!-- for the table of contents -->#### `type: "experimental_mixture_of_n"`

##### `candidates`

* **Type:** list of strings
* **Required:** yes

This inference strategy generates N candidate responses, and a fuser model combines them to produce a final answer. This approach allows you to leverage multiple prompts or variants to increase the likelihood of getting a high-quality response.

The `candidates` parameter specifies a list of variant names used to generate candidate responses. For example, if you have two variants defined (`promptA` and `promptB`), you could set up the `candidates` list to generate two responses using `promptA` and one using `promptB` using the snippet below. The fuser would then combine the three responses.

tensorzero.toml

```toml
[functions.draft-email.variants.promptA]
type = "chat_completion"
# ...


[functions.draft-email.variants.promptB]
type = "chat_completion"
# ...


[functions.draft-email.variants.mixture-of-n]
type = "experimental_mixture_of_n"
candidates = ["promptA", "promptA", "promptB"] # 3 candidate generations
# ...
```

##### `fuser`

* **Type:** object
* **Required:** yes

The `fuser` parameter specifies the configuration for the model that will evaluate and combine the elements.

The evaluator is configured similarly to a `chat_completion` variant, but without the `type` field. The prompts here should be prompts that you would use to solve the original problem, as the gateway has special-purpose handling and templates to convert them to a fuser.

```toml
[functions.draft-email.variants.mixture-of-n]
type = "experimental_mixture_of_n"
# ...


[functions.draft-email.variants.mixture-of-n.fuser]
# Same fields as a `chat_completion` variant (excl.`type`), e.g.:
# user_template = "functions/draft-email/mixture-of-n/user.minijinja"
# ...
```

##### `timeout_s`

* **Type:** float
* **Required:** no (default: 300s)

The `timeout_s` parameter specifies the maximum time in seconds allowed for generating candidate responses. Any candidate that takes longer than this duration to generate a response will be dropped from consideration.

```toml
[functions.draft-email.variants.mixture-of-n]
type = "experimental_mixture_of_n"
timeout_s = 60
# ...
```

##### `weight`

* **Type:** float
* **Required:** no (default: 0)

Defines the weight of the variant. When you call a function, the weight determines the relative importance of the variant when sampling.

Variants will be sampled with a probability proportional to their weight. For example, if variant A has a weight of `1.0` and variant B has a weight of `3.0`, variant A will be sampled with probability `1.0 / (1.0 + 3.0) = 25%` and variant B will be sampled with probability `3.0 / (1.0 + 3.0) = 75%`.

You can disable a variant by setting its weight to `0`. The variant will only be used if there are no other variants available for sampling or if the variant is requested explicitly in the request with `variant_name`. This is useful for defining fallback variants, which won’t be used unless no other variants are available.

```toml
[functions.draft-email.variants.mixture-of-n]
# ...
weight = 1.0
# ...
```

`type: "experimental_dynamic_in_context_learning"`<!-- for the table of contents -->#### `type: "experimental_dynamic_in_context_learning"`

##### `embedding_model`

* **Type:** string
* **Required:** yes

Defines the model to use for retrieving the similar examples. The model must be defined in the `[embedding_models.model_name]` section (see above), or correspond to a short-hand model name.

Short-hand model names follow the format `provider::model_name` (e.g. `openai::text-embedding-3-small`). The following model providers support short-hand model names: `anthropic`, `deepseek`, `fireworks`, `google_ai_studio_gemini`, `hyperbolic`, `mistral`, `openai`, `together`, and `xai`. The remaining providers do not support short-hand model names, and require an explicit `model` block in your configuration file.

The embedding model used for inference should be the same model previously used to generate the embeddings stored in ClickHouse.

tensorzero.toml

```toml
[embedding_models.openai-text-embedding-3-small]
# ...


[functions.draft-email.variants.dicl]
# ...
embedding_model = "openai-text-embedding-3-small"
# ...
```

##### `extra_body`

* **Type:** array of objects (see below)
* **Required:** no

The `extra_body` field allows you to modify the request body that TensorZero sends to a variant’s model provider. This advanced feature is an “escape hatch” that lets you use provider-specific functionality that TensorZero hasn’t implemented yet.

For `experimental_dynamic_in_context_learning` variants, `extra_body` only applies to the chat completion request.

Each object in the array must have two fields:

* `pointer`: A [JSON Pointer](https://datatracker.ietf.org/doc/html/rfc6901) string specifying where to modify the request body
* `value`: The value to insert at that location; it can be of any type including nested types

Tip

You can also set `extra_body` for a model provider entry. The model provider `extra_body` entries take priority over variant `extra_body` entries.

Additionally, you can set `extra_body` at inference-time. The values provided at inference-time take priority over the values in the configuration file.

Example: `extra_body`

If TensorZero would normally send this request body to the provider…

```json
{
  "project": "tensorzero",
  "safety_checks": {
    "no_internet": false,
    "no_agi": true
  }
}
```

…then the following `extra_body`…

```toml
extra_body = [
  { pointer = "/agi", value = true},
  { pointer = "/safety_checks/no_agi", value = { bypass = "on" }}
]
```

…overrides the request body to:

```json
{
  "agi": true,
  "project": "tensorzero",
  "safety_checks": {
    "no_internet": false,
    "no_agi": {
      "bypass": "on"
    }
  }
}
```

##### `extra_headers`

* **Type:** array of objects (see below)
* **Required:** no

The `extra_headers` field allows you to set or overwrite the request headers that TensorZero sends to a model provider. This advanced feature is an “escape hatch” that lets you use provider-specific functionality that TensorZero hasn’t implemented yet.

Each object in the array must have two fields:

* `name` (string): The name of the header to modify (e.g. `anthropic-beta`)
* `value` (string): The value of the header (e.g. `token-efficient-tools-2025-02-19`)

Tip

You can also set `extra_headers` for a model provider entry. The model provider `extra_headers` entries take priority over variant `extra_headers` entries.

Example: `extra_headers`

If TensorZero would normally send the following request headers to the provider…

```text
Safety-Checks: on
```

…then the following `extra_headers`…

```toml
extra_headers = [
  { name = "Safety-Checks", value = "off"},
  { name = "Intelligence-Level", value = "AGI"}
]
```

…overrides the request headers to:

```text
Safety-Checks: off
Intelligence-Level: AGI
```

##### `json_mode`

* **Type:** string
* **Required:** no (default: `strict`)

Defines the strategy for generating JSON outputs.

This parameter is only supported for variants of functions with `type = "json"`.

The supported modes are:

* `off`: Make a chat completion request without any special JSON handling (not recommended).
* `on`: Make a chat completion request with JSON mode (if supported by the provider).
* `strict`: Make a chat completion request with strict JSON mode (if supported by the provider). For example, the TensorZero Gateway uses Structured Outputs for OpenAI.
* `implicit_tool`: Make a special-purpose tool use request under the hood, and convert the tool call into a JSON response.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
json_mode = "strict"
# ...
```

##### `k`

* **Type:** non-negative integer
* **Required:** yes

Defines the number of examples to retrieve for the inference.

tensorzero.toml

```toml
[functions.draft-email.variants.dicl]
# ...
k = 10
# ...
```

##### `max_tokens`

* **Type:** integer
* **Required:** no (default: `null`)

Defines the maximum number of tokens to generate.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
max_tokens = 100
# ...
```

##### `model`

* **Type:** string
* **Required:** yes

Defines the model to use for the variant. The model must be defined in the `[models.model_name]` section (see above), or correspond to a short-hand model name.

Short-hand model names follow the format `provider::model_name` (e.g. `openai::gpt-4o-mini` or `anthropic::claude-3-5-haiku`). The following model providers support short-hand default model names: `anthropic`, `deepseek`, `fireworks`, `google_ai_studio_gemini`, `hyperbolic`, `mistral`, `openai`, `together`, and `xai`. The remaining providers do not support short-hand default model names, and require an explicit `model` block in your configuration file.

tensorzero.toml

```toml
[models.gpt-4o-mini]
# ...


[functions.draft-email.variants.prompt-v1]
# ...
model = "gpt-4o-mini"
# ...
```

##### `retries`

* **Type:** object with optional keys `num_retries` and `max_delay_s`
* **Required:** no (defaults to `num_retries = 0` and a `max_delay_s = 10`)

TensorZero’s retry strategy is truncated exponential backoff with jitter. The `num_retries` parameter defines the number of retries (not including the initial request). The `max_delay_s` parameter defines the maximum delay between retries.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
retries = { num_retries = 3, max_delay_s = 10 }
# ...
```

##### `seed`

* **Type:** integer
* **Required:** no (default: `null`)

Defines the seed to use for the variant.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
seed = 42
```

##### `system_instructions`

* **Type:** string (path)
* **Required:** no

Defines the path to the system instructions file. The path is relative to the configuration file.

The system instruction is a text file that will be added to the evaluator’s system prompt. Unlike `system_template`, it doesn’t support variables. This file contains static instructions that define the behavior and role of the AI assistant for the specific function variant.

tensorzero.toml

```toml
[functions.draft-email.variants.dicl]
# ...
system_instructions = "./functions/draft-email/prompt-v1/system_template.txt"
# ...
```

##### `temperature`

* **Type:** float
* **Required:** no (default: `null`)

Defines the temperature to use for the variant.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
temperature = 0.5
# ...
```

##### `weight`

* **Type:** float
* **Required:** no (default: 0)

Defines the weight of the variant. When you call a function, the weight determines the relative importance of the variant when sampling.

Variants will be sampled with a probability proportional to their weight. For example, if variant A has a weight of `1.0` and variant B has a weight of `3.0`, variant A will be sampled with probability `1.0 / (1.0 + 3.0) = 25%` and variant B will be sampled with probability `3.0 / (1.0 + 3.0) = 75%`.

You can disable a variant by setting its weight to `0`. The variant will only be used if there are no other variants available for sampling or if the variant is requested explicitly in the request with `variant_name`. This is useful for defining fallback variants, which won’t be used unless no other variants are available.

tensorzero.toml

```toml
[functions.draft-email.variants.prompt-v1]
# ...
weight = 1.0
# ...
```

## `[metrics]`

The `[metrics]` section defines the behavior of a metric. You can define multiple metrics by including multiple `[metrics.metric_name]` sections.

The metric name can’t be `comment` or `demonstration`, as those names are reserved for internal use.

If your `metric_name` is not a basic string, it can be escaped with quotation marks. For example, periods are not allowed in basic strings, so you can define `beats-gpt-3.5` as `[metrics."beats-gpt-3.5"]`.

tensorzero.toml

```toml
[metrics.task-completed]
# fieldA = ...
# fieldB = ...
# ...


[metrics.user-rating]
# fieldA = ...
# fieldB = ...
# ...
```

### `level`

* **Type:** string
* **Required:** yes

Defines whether the metric applies to individual inference or across entire episodes.

The supported levels are `inference` and `episode`.

tensorzero.toml

```toml
[metrics.valid-output]
# ...
level = "inference"
# ...


[metrics.task-completed]
# ...
level = "episode"
# ...
```

### `optimize`

* **Type:** string
* **Required:** yes

Defines whether the metric should be maximized or minimized.

The supported values are `max` and `min`.

tensorzero.toml

```toml
[metrics.mistakes-made]
# ...
optimize = "min"
# ...


[metrics.user-rating]
# ...
optimize = "max"
# ...
```

### `type`

* **Type:** string
* **Required:** yes

Defines the type of the metric.

The supported metric types are `boolean` and `float`.

tensorzero.toml

```toml
[metrics.user-rating]
# ...
type = "float"
# ...


[metrics.task-completed]
# ...
type = "boolean"
# ...
```

## `[tools.tool_name]`

The `[tools.tool_name]` section defines the behavior of a tool. You can define multiple tools by including multiple `[tools.tool_name]` sections.

If your `tool_name` is not a basic string, it can be escaped with quotation marks. For example, periods are not allowed in basic strings, so you can define `run-python-3.10` as `[tools."run-python-3.10"]`.

You can enable a tool for a function by adding it to the function’s `tools` field.

tensorzero.toml

```toml
[functions.weather-chatbot]
# ...
type = "chat"
tools = [
  # ...
  "get-temperature"
  # ...
]
# ...


[tools.get-temperature]
# ...
```

### `description`

* **Type:** string
* **Required:** yes

Defines the description of the tool provided to the model.

You can typically materially improve the quality of responses by providing a detailed description of the tool.

tensorzero.toml

```toml
[tools.get-temperature]
# ...
description = "Get the current temperature in a given location (e.g. \"Tokyo\") using the specified unit (must be \"celsius\" or \"fahrenheit\")."
# ...
```

### `parameters`

* **Type:** string (path)
* **Required:** yes

Defines the path to the parameters file. The path is relative to the configuration file.

This file should contain a [JSON Schema](https://json-schema.org/) for the parameters of the tool.

tensorzero.toml

```toml
[tools.get-temperature]
# ...
parameters = "./tools/get-temperature.json"
# ...
```

### `strict`

* **Type:** boolean
* **Required:** no (default: `false`)

If set to `true`, the TensorZero Gateway attempts to use strict JSON generation for the tool parameters. This typically improves the quality of responses.

Only a few providers support strict JSON generation. For example, the TensorZero Gateway uses Structured Outputs for OpenAI. If the provider does not support strict mode, the TensorZero Gateway ignores this field.

tensorzero.toml

```toml
[tools.get-temperature]
# ...
strict = true
# ...
```

## `[object_storage]`

The `[object_storage]` section defines the behavior of object storage, which is used for storing images used during multimodal inference.

### `type`

* **Type:** string
* **Required:** yes

Defines the type of object storage to use.

The supported types are:

* `s3_compatible`: Use an S3-compatible object storage service.
* `filesystem`: Store images in a local directory.
* `disabled`: Disable object storage.

See the following sections for more details on each type.

`type: "s3_compatible"`<!-- for the table of contents -->#### `type: "s3_compatible"`

If you set `type = "s3_compatible"`, TensorZero will use an S3-compatible object storage service to store and retrieve images.

The TensorZero Gateway will attempt to retrieve credentials from the following resources in order of priority:

1. `S3_ACCESS_KEY_ID` and `S3_SECRET_ACCESS_KEY` environment variables
2. `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables
3. Credentials from the AWS SDK (default profile)

If you set `type = "s3_compatible"`, the following fields are available.

##### `endpoint`

* **Type:** string
* **Required:** no (defaults to AWS S3)

Defines the endpoint of the object storage service. You can use this field to specify a custom endpoint for the object storage service (e.g. GCP Cloud Storage, Cloudflare R2, and many more).

##### `bucket_name`

* **Type:** string
* **Required:** no

Defines the name of the bucket to use for object storage. You should provide a bucket name unless it’s specified in the `endpoint` field.

##### `region`

* **Type:** string
* **Required:** no

Defines the region of the object storage service (if applicable).

This is required for some providers (e.g. AWS S3). If the provider does not require a region, this field can be omitted.

##### `allow_http`

* **Type:** boolean
* **Required:** no (defaults to `false`)

Normally, the TensorZero Gateway will require HTTPS to access the object storage service. If set to `true`, the TensorZero Gateway will instead use HTTP to access the object storage service. This is useful for local development (e.g. a local MinIO deployment), but not recommended for production environments.

Caution

For production environments, we strongly recommend you disable the `allow_http` setting and use a secure method of authentication in combination with a production-grade object storage service.

`type: "filesystem"`<!-- for the table of contents -->#### `type: "filesystem"`

##### `path`

* **Type:** string
* **Required:** yes

Defines the path to the directory to use for object storage.

`type: "disabled"`<!-- for the table of contents -->#### `type: "disabled"`

If you set `type = "disabled"`, the TensorZero Gateway will not store or retrieve images. There are no additional fields available for this type.

# Data Model

> Learn more about the data model used by TensorZero.

<!-- INTERNAL NOTE: last updated up to Migration 0015 -->

The TensorZero Gateway stores inference and feedback data in ClickHouse. This data can be used for observability, experimentation, and optimization.

## `ChatInference`

The `ChatInference` table stores information about inference requests for Chat Functions made to the TensorZero Gateway.

A `ChatInference` row can be associated with one or more `ModelInference` rows, depending on the variant’s `type`. For `chat_completion`, there will be a one-to-one relationship between rows in the two tables. For other variant types, there might be more associated rows.

| Column               |         Type        | Notes                                                                                                    |
| :------------------- | :-----------------: | -------------------------------------------------------------------------------------------------------- |
| `id`                 |         UUID        | Must be a UUIDv7                                                                                         |
| `function_name`      |        String       |                                                                                                          |
| `variant_name`       |        String       |                                                                                                          |
| `episode_id`         |         UUID        | Must be a UUIDv7                                                                                         |
| `input`              |    String (JSON)    | `input` field in the `/inference` request body                                                           |
| `output`             |    String (JSON)    | Array of content blocks                                                                                  |
| `tool_params`        |    String (JSON)    | Object with any tool parameters (e.g. `tool_choice`, `available_tools`) used for the inference           |
| `inference_params`   |    String (JSON)    | Object with any inference parameters per variant type (e.g. `{"chat_completion": {"temperature": 0.5}}`) |
| `processing_time_ms` |        UInt32       |                                                                                                          |
| `timestamp`          |       DateTime      | Materialized from `id` (using `UUIDv7ToDateTime` function)                                               |
| `tags`               | Map(String, String) | User-assigned tags (e.g. `{"user_id": "123"}`)                                                           |

## `JsonInference`

The `JsonInference` table stores information about inference requests for JSON Functions made to the TensorZero Gateway.

A `JsonInference` row can be associated with one or more `ModelInference` rows, depending on the variant’s `type`. For `chat_completion`, there will be a one-to-one relationship between rows in the two tables. For other variant types, there might be more associated rows.

| Column               |         Type        | Notes                                                                                                    |
| :------------------- | :-----------------: | -------------------------------------------------------------------------------------------------------- |
| `id`                 |         UUID        | Must be a UUIDv7                                                                                         |
| `function_name`      |        String       |                                                                                                          |
| `variant_name`       |        String       |                                                                                                          |
| `episode_id`         |         UUID        | Must be a UUIDv7                                                                                         |
| `input`              |    String (JSON)    | `input` field in the `/inference` request body                                                           |
| `output`             |    String (JSON)    | Object with `parsed` and `raw` fields                                                                    |
| `output_schema`      |    String (JSON)    | Schema that the output must conform to                                                                   |
| `inference_params`   |    String (JSON)    | Object with any inference parameters per variant type (e.g. `{"chat_completion": {"temperature": 0.5}}`) |
| `processing_time_ms` |        UInt32       |                                                                                                          |
| `timestamp`          |       DateTime      | Materialized from `id` (using `UUIDv7ToDateTime` function)                                               |
| `tags`               | Map(String, String) | User-assigned tags (e.g. `{"user_id": "123"}`)                                                           |

## `ModelInference`

The `ModelInference` table stores information about each inference request to a model provider. This is the inference request you’d make if you had called the model provider directly.

| Column                |          Type         | Notes                                                      |
| :-------------------- | :-------------------: | ---------------------------------------------------------- |
| `id`                  |          UUID         | Must be a UUIDv7                                           |
| `inference_id`        |          UUID         | Must be a UUIDv7                                           |
| `raw_request`         |         String        | Raw request as sent to the model provider (varies)         |
| `raw_response`        |         String        | Raw response from the model provider (varies)              |
| `model_name`          |         String        | Name of the model used for the inference                   |
| `model_provider_name` |         String        | Name of the model provider used for the inference          |
| `input_tokens`        |    Nullable(UInt32)   |                                                            |
| `output_tokens`       |    Nullable(UInt32)   |                                                            |
| `response_time_ms`    |    Nullable(UInt32)   |                                                            |
| `ttft_ms`             |    Nullable(UInt32)   | Only available in streaming inferences                     |
| `timestamp`           |        DateTime       | Materialized from `id` (using `UUIDv7ToDateTime` function) |
| `system`              |    Nullable(String)   | The `system` input to the model                            |
| `input_messages`      | Array(RequestMessage) | The user and assistant messages input to the model         |
| `output`              |  Array(ContentBlock)  | The output of the model                                    |

A `RequestMessage` is an object with shape `{role: "user" | "assistant", content: List[ContentBlock]}` (content blocks are defined [here](/docs/gateway/api-reference/inference/#content-block)).

## `DynamicInContextLearningExample`

The `DynamicInContextLearningExample` table stores examples for dynamic in-context learning variants.

| Column          |      Type      | Notes                                                      |
| :-------------- | :------------: | ---------------------------------------------------------- |
| `id`            |      UUID      | Must be a UUIDv7                                           |
| `function_name` |     String     |                                                            |
| `variant_name`  |     String     |                                                            |
| `namespace`     |     String     |                                                            |
| `input`         |  String (JSON) |                                                            |
| `output`        |     String     |                                                            |
| `embedding`     | Array(Float32) |                                                            |
| `timestamp`     |    DateTime    | Materialized from `id` (using `UUIDv7ToDateTime` function) |

## `BooleanMetricFeedback`

The `BooleanMetricFeedback` table stores feedback for metrics of `type = "boolean"`.

| Column        |         Type        | Notes                                                                                                |
| :------------ | :-----------------: | ---------------------------------------------------------------------------------------------------- |
| `id`          |         UUID        | Must be a UUIDv7                                                                                     |
| `target_id`   |         UUID        | Must be a UUIDv7 that is either `inference_id` or `episode_id` depending on `level` in metric config |
| `metric_name` |        String       |                                                                                                      |
| `value`       |         Bool        |                                                                                                      |
| `timestamp`   |       DateTime      | Materialized from `id` (using `UUIDv7ToDateTime` function)                                           |
| `tags`        | Map(String, String) | User-assigned tags (e.g. `{"author": "Alice"}`)                                                      |

## `FloatMetricFeedback`

The `FloatMetricFeedback` table stores feedback for metrics of `type = "float"`.

| Column        |         Type        | Notes                                                                                                |
| :------------ | :-----------------: | ---------------------------------------------------------------------------------------------------- |
| `id`          |         UUID        | Must be a UUIDv7                                                                                     |
| `target_id`   |         UUID        | Must be a UUIDv7 that is either `inference_id` or `episode_id` depending on `level` in metric config |
| `metric_name` |        String       |                                                                                                      |
| `value`       |       Float32       |                                                                                                      |
| `timestamp`   |       DateTime      | Materialized from `id` (using `UUIDv7ToDateTime` function)                                           |
| `tags`        | Map(String, String) | User-assigned tags (e.g. `{"author": "Alice"}`)                                                      |

## `CommentFeedback`

The `CommentFeedback` table stores feedback provided with `metric_name` of `"comment"`. Comments are free-form text feedbacks.

| Column        |             Type             | Notes                                                                                                |
| :------------ | :--------------------------: | ---------------------------------------------------------------------------------------------------- |
| `id`          |             UUID             | Must be a UUIDv7                                                                                     |
| `target_id`   |             UUID             | Must be a UUIDv7 that is either `inference_id` or `episode_id` depending on `level` in metric config |
| `target_type` | `"inference"` or `"episode"` |                                                                                                      |
| `value`       |            String            |                                                                                                      |
| `timestamp`   |           DateTime           | Materialized from `id` (using `UUIDv7ToDateTime` function)                                           |
| `tags`        |      Map(String, String)     | User-assigned tags (e.g. `{"author": "Alice"}`)                                                      |

## `DemonstrationFeedback`

The `DemonstrationFeedback` table stores feedback in the form of demonstrations. Demonstrations are examples of good behaviors.

| Column         |         Type        | Notes                                                                          |
| :------------- | :-----------------: | ------------------------------------------------------------------------------ |
| `id`           |         UUID        | Must be a UUIDv7                                                               |
| `inference_id` |         UUID        | Must be a UUIDv7                                                               |
| `value`        |        String       | The demonstration or example provided as feedback (must match function output) |
| `timestamp`    |       DateTime      | Materialized from `id` (using `UUIDv7ToDateTime` function)                     |
| `tags`         | Map(String, String) | User-assigned tags (e.g. `{"author": "Alice"}`)                                |

## `ModelInferenceCache`

The `ModelInferenceCache` table stores cached model inference results to avoid duplicate requests.

| Column            |       Type      | Notes                                                |
| :---------------- | :-------------: | ---------------------------------------------------- |
| `short_cache_key` |      UInt64     | First part of composite key for fast lookups         |
| `long_cache_key`  | FixedString(64) | Hex-encoded 256-bit key for full cache validation    |
| `timestamp`       |     DateTime    | When this cache entry was created, defaults to now() |
| `output`          |      String     | The cached model output                              |
| `raw_request`     |      String     | Raw request that was sent to the model provider      |
| `raw_response`    |      String     | Raw response received from the model provider        |
| `is_deleted`      |       Bool      | Soft deletion flag, defaults to false                |

The table uses the `ReplacingMergeTree` engine with `timestamp` and `is_deleted` columns for deduplication. It is partitioned by month and ordered by the composite cache key `(short_cache_key, long_cache_key)`. The `short_cache_key` serves as the primary key for performance, while a bloom filter index on `long_cache_key` helps optimize point queries.

## `ChatInferenceDataset`

The `ChatInferenceDataset` table stores chat inference examples organized into datasets.

| Column          |           Type          | Notes                                                                                          |
| :-------------- | :---------------------: | ---------------------------------------------------------------------------------------------- |
| `dataset_name`  |  LowCardinality(String) | Name of the dataset this example belongs to                                                    |
| `function_name` |  LowCardinality(String) | Name of the function this example is for                                                       |
| `id`            |           UUID          | Must be a UUIDv7, often the inference ID if generated from an inference                        |
| `episode_id`    |           UUID          | Must be a UUIDv7                                                                               |
| `input`         |      String (JSON)      | `input` field in the `/inference` request body                                                 |
| `output`        | Nullable(String) (JSON) | Array of content blocks                                                                        |
| `tool_params`   |      String (JSON)      | Object with any tool parameters (e.g. `tool_choice`, `available_tools`) used for the inference |
| `tags`          |   Map(String, String)   | User-assigned tags (e.g. `{"user_id": "123"}`)                                                 |
| `auxiliary`     |          String         | Additional JSON data (unstructured)                                                            |
| `is_deleted`    |           Bool          | Soft deletion flag, defaults to false                                                          |
| `updated_at`    |         DateTime        | When this dataset entry was updated, defaults to now()                                         |

The table uses the `ReplacingMergeTree` engine with `updated_at` and `is_deleted` columns for deduplication. It is ordered by `dataset_name`, `function_name`, and `id` to optimize queries filtering by dataset and function.

## `JsonInferenceDataset`

The `JsonInferenceDataset` table stores JSON inference examples organized into datasets.

| Column          |          Type          | Notes                                                                   |
| :-------------- | :--------------------: | ----------------------------------------------------------------------- |
| `dataset_name`  | LowCardinality(String) | Name of the dataset this example belongs to                             |
| `function_name` | LowCardinality(String) | Name of the function this example is for                                |
| `id`            |          UUID          | Must be a UUIDv7, often the inference ID if generated from an inference |
| `episode_id`    |          UUID          | Must be a UUIDv7                                                        |
| `input`         |      String (JSON)     | `input` field in the `/inference` request body                          |
| `output`        |      String (JSON)     | Object with `parsed` and `raw` fields                                   |
| `output_schema` |      String (JSON)     | Schema that the output must conform to                                  |
| `tags`          |   Map(String, String)  | User-assigned tags (e.g. `{"user_id": "123"}`)                          |
| `auxiliary`     |         String         | Additional JSON data (unstructured)                                     |
| `is_deleted`    |          Bool          | Soft deletion flag, defaults to false                                   |
| `updated_at`    |        DateTime        | When this dataset entry was updated, defaults to now()                  |

The table uses the `ReplacingMergeTree` engine with `updated_at` and `is_deleted` columns for deduplication. It is ordered by `dataset_name`, `function_name`, and `id` to optimize queries filtering by dataset and function.

## `BatchRequest`

The `BatchRequest` table stores information about batch requests made to model providers. We update it every time a particular `batch_id` is created or polled.

| Column                |      Type     | Notes                                                      |
| :-------------------- | :-----------: | ---------------------------------------------------------- |
| `batch_id`            |      UUID     | Must be a UUIDv7                                           |
| `id`                  |      UUID     | Must be a UUIDv7                                           |
| `batch_params`        |     String    | Parameters used for the batch request                      |
| `model_name`          |     String    | Name of the model used                                     |
| `model_provider_name` |     String    | Name of the model provider                                 |
| `status`              |     String    | One of: ‘pending’, ‘completed’, ‘failed’                   |
| `errors`              | Array(String) | Array of error messages if status is ‘failed’              |
| `timestamp`           |    DateTime   | Materialized from `id` (using `UUIDv7ToDateTime` function) |
| `raw_request`         |     String    | Raw request sent to the model provider                     |
| `raw_response`        |     String    | Raw response received from the model provider              |
| `function_name`       |     String    | Name of the function being called                          |
| `variant_name`        |     String    | Name of the function variant                               |

## `BatchModelInference`

The `BatchModelInference` table stores information about inferences made as part of a batch request. Once the request succeeds, we use this information to populate the `ChatInference`, `JsonInference`, and `ModelInference` tables.

| Column                |          Type         | Notes                                                                                                    |
| :-------------------- | :-------------------: | -------------------------------------------------------------------------------------------------------- |
| `inference_id`        |          UUID         | Must be a UUIDv7                                                                                         |
| `batch_id`            |          UUID         | Must be a UUIDv7                                                                                         |
| `function_name`       |         String        | Name of the function being called                                                                        |
| `variant_name`        |         String        | Name of the function variant                                                                             |
| `episode_id`          |          UUID         | Must be a UUIDv7                                                                                         |
| `input`               |     String (JSON)     | `input` field in the `/inference` request body                                                           |
| `system`              |         String        | The `system` input to the model                                                                          |
| `input_messages`      | Array(RequestMessage) | The user and assistant messages input to the model                                                       |
| `tool_params`         |     String (JSON)     | Object with any tool parameters (e.g. `tool_choice`, `available_tools`) used for the inference           |
| `inference_params`    |     String (JSON)     | Object with any inference parameters per variant type (e.g. `{"chat_completion": {"temperature": 0.5}}`) |
| `raw_request`         |         String        | Raw request sent to the model provider                                                                   |
| `model_name`          |         String        | Name of the model used                                                                                   |
| `model_provider_name` |         String        | Name of the model provider                                                                               |
| `output_schema`       |         String        | Optional schema for JSON outputs                                                                         |
| `tags`                |  Map(String, String)  | User-assigned tags (e.g. `{"author": "Alice"}`)                                                          |
| `timestamp`           |        DateTime       | Materialized from `id` (using `UUIDv7ToDateTime` function)                                               |

Materialized View Tables

[Materialized views](https://clickhouse.com/docs/en/materialized-view) in columnar databases like ClickHouse pre-compute alternative indexings of data, dramatically improving query performance compared to computing results on-the-fly. In TensorZero’s case, we store denormalized data about inferences and feedback in the materialized views below to support efficient queries for common downstream use cases.

## `FeedbackTag`

The `FeedbackTag` table stores tags associated with various feedback types. Tags are used to categorize and add metadata to feedback entries, allowing for user-defined filtering later on. Data is inserted into this table by materialized views reading from the `BooleanMetricFeedback`, `CommentFeedback`, `DemonstrationFeedback`, and `FloatMetricFeedback` tables.

| Column        | Type   | Notes                                                                           |
| ------------- | ------ | ------------------------------------------------------------------------------- |
| `metric_name` | String | Name of the metric the tag is associated with.                                  |
| `key`         | String | Key of the tag.                                                                 |
| `value`       | String | Value of the tag.                                                               |
| `feedback_id` | UUID   | UUID referencing the related feedback entry (e.g., `BooleanMetricFeedback.id`). |

## `InferenceById`

The `InferenceById` table is a materialized view that combines data from `ChatInference` and `JSONInference`. Notably, it indexes the table by `id_uint` for fast lookup by the gateway to validate feedback requests. We store `id_uint` as a UInt128 so that they are sorted in the natural order by time as ClickHouse sorts UUIDs in little-endian order.

| Column          |   Type  | Notes                                              |
| :-------------- | :-----: | -------------------------------------------------- |
| `id_uint`       | UInt128 | Integer representation of UUIDv7 for sorting order |
| `function_name` |  String |                                                    |
| `variant_name`  |  String |                                                    |
| `episode_id`    |   UUID  | Must be a UUIDv7                                   |
| `function_type` |  String | Either `'chat'` or `'json'`                        |

## `InferenceByEpisodeId`

The `InferenceByEpisodeId` table is a materialized view that indexes inferences by their episode ID, enabling efficient lookup of all inferences within an episode. We store `episode_id_uint` as a `UInt128` so that they are sorted in the natural order by time as ClickHouse sorts UUIDs in little-endian order.

| Column            |         Type         | Notes                                              |
| :---------------- | :------------------: | -------------------------------------------------- |
| `episode_id_uint` |        UInt128       | Integer representation of UUIDv7 for sorting order |
| `id_uint`         |        UInt128       | Integer representation of UUIDv7 for sorting order |
| `function_name`   |        String        | Name of the function being called                  |
| `variant_name`    |        String        | Name of the function variant                       |
| `function_type`   | Enum(‘chat’, ‘json’) | Type of function (chat or json)                    |

## `InferenceTag`

The `InferenceTag` table stores tags associated with inferences. Tags are used to categorize and add metadata to inferences, allowing for user-defined filtering later on. Data is inserted into this table by materialized views reading from the `ChatInference` and `JsonInference` tables.

| Column          | Type   | Notes                                                              |
| --------------- | ------ | ------------------------------------------------------------------ |
| `function_name` | String | Name of the function the tag is associated with.                   |
| `key`           | String | Key of the tag.                                                    |
| `value`         | String | Value of the tag.                                                  |
| `inference_id`  | UUID   | UUID referencing the related inference (e.g., `ChatInference.id`). |

## `BatchIdByInferenceId`

The `BatchIdByInferenceId` table maps inference IDs to batch IDs, allowing for efficient lookup of which batch an inference belongs to.

| Column         | Type | Notes            |
| :------------- | :--: | ---------------- |
| `inference_id` | UUID | Must be a UUIDv7 |
| `batch_id`     | UUID | Must be a UUIDv7 |

## `BooleanMetricFeedbackByTargetId`

The `BooleanMetricFeedbackByTargetId` table indexes boolean metric feedback by target ID, enabling efficient lookup of feedback for a specific target.

| Column        |         Type        | Notes                                                |
| :------------ | :-----------------: | ---------------------------------------------------- |
| `id`          |         UUID        | Must be a UUIDv7                                     |
| `target_id`   |         UUID        | Must be a UUIDv7                                     |
| `metric_name` |        String       | Name of the metric (stored as LowCardinality)        |
| `value`       |         Bool        | The boolean feedback value                           |
| `tags`        | Map(String, String) | Key-value pairs of tags associated with the feedback |

## `CommentFeedbackByTargetId`

The `CommentFeedbackByTargetId` table stores text feedback associated with inferences or episodes, enabling efficient lookup of comments by their target ID.

| Column        |             Type             | Notes                                                |
| :------------ | :--------------------------: | ---------------------------------------------------- |
| `id`          |             UUID             | Must be a UUIDv7                                     |
| `target_id`   |             UUID             | Must be a UUIDv7                                     |
| `target_type` | Enum(‘inference’, ‘episode’) | Type of entity this feedback is for                  |
| `value`       |            String            | The text feedback content                            |
| `tags`        |      Map(String, String)     | Key-value pairs of tags associated with the feedback |

## `DemonstrationFeedbackByInferenceId`

The `DemonstrationFeedbackByInferenceId` table stores demonstration feedback associated with inferences, enabling efficient lookup of demonstrations by inference ID.

| Column         |         Type        | Notes                                                |
| :------------- | :-----------------: | ---------------------------------------------------- |
| `id`           |         UUID        | Must be a UUIDv7                                     |
| `inference_id` |         UUID        | Must be a UUIDv7                                     |
| `value`        |        String       | The demonstration feedback content                   |
| `tags`         | Map(String, String) | Key-value pairs of tags associated with the feedback |

## `FloatMetricFeedbackByTargetId`

The `FloatMetricFeedbackByTargetId` table indexes float metric feedback by target ID, enabling efficient lookup of feedback for a specific target.

| Column        |         Type        | Notes                                                |
| :------------ | :-----------------: | ---------------------------------------------------- |
| `id`          |         UUID        | Must be a UUIDv7                                     |
| `target_id`   |         UUID        | Must be a UUIDv7                                     |
| `metric_name` |        String       | Name of the metric (stored as LowCardinality)        |
| `value`       |       Float32       | The float feedback value                             |
| `tags`        | Map(String, String) | Key-value pairs of tags associated with the feedback |

# TensorZero Gateway Deployment Guide

> Learn how to deploy the TensorZero Gateway.

It’s easy to get started with the TensorZero Gateway.

To deploy the TensorZero Gateway, you need to:

* Setup a ClickHouse database
* Optional: Create a [configuration file](/docs/gateway/configuration-reference/)
* Run the gateway

Tip

See the [TensorZero UI Deployment Guide](/docs/ui/deployment/) for more details on how to deploy the TensorZero UI.

## ClickHouse

The TensorZero Gateway stores inference and feedback data in a ClickHouse database. This data is later used for model observability, experimentation, and optimization.

### Development

For development purposes, you can run a single-node ClickHouse instance locally (e.g. using Homebrew or Docker) or a cheap `Development`-tier cluster on ClickHouse Cloud.

See the [ClickHouse documentation](https://clickhouse.com/docs/en/install) for more details on configuring your ClickHouse deployment.

### Production

#### Managed Services

For production deployments, the easiest setup is to use a managed service like [ClickHouse Cloud](https://clickhouse.com/cloud).

ClickHouse Cloud is also available through the [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-jettukeanwrfc), [GCP Marketplace](https://console.cloud.google.com/marketplace/product/clickhouse-public/clickhouse-cloud), and [Azure Marketplace](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/clickhouse.clickhouse_cloud).

Other options for managed ClickHouse deployments include [Tinybird](https://www.tinybird.co/) (serverless) and [Altinity](https://www.altinity.com/) (hands-on support).

#### Self-Managed Deployment

You can alternatively run your own self-managed ClickHouse instance or cluster.

Caution

The TensorZero Gateway does *not* automatically enable data replication for ClickHouse tables.

If you are using a *self-managed distributed* ClickHouse deployment, you must set up replication yourself. See the ClickHouse [replication documentation](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication#converting-from-mergetree-to-replicatedmergetree) for more details.

ClickHouse Cloud automatically sets up data replication, so this step is not necessary if you’re using the managed service.

### Configuration

After setting up your database, you need to configure the `TENSORZERO_CLICKHOUSE_URL` environment variable with the connection details. The variable takes a standard format.

.env

```bash
TENSORZERO_CLICKHOUSE_URL="http[s]://[username]:[password]@[hostname]:[port]/[database]"


# Example: ClickHouse running locally
TENSORZERO_CLICKHOUSE_URL="http://chuser:chpassword@localhost:8123/tensorzero"


# Example: ClickHouse Cloud
TENSORZERO_CLICKHOUSE_URL="https://USERNAME:PASSWORD@XXXXX.clickhouse.cloud:8443/tensorzero"


# Example: TensorZero Gateway running in a container, ClickHouse running on host machine
TENSORZERO_CLICKHOUSE_URL="http://host.docker.internal:8123/tensorzero"
```

The TensorZero Gateway automatically applies database migrations on startup.

Disabling Observability (Not Recommended)

You can disable observability features if you’re not interested in storing any data for experimentation and optimization. In this case, you won’t need to set up ClickHouse, and the TensorZero Gateway will act as a simple model gateway.

To disable observability, set the following configuration in the `tensorzero.toml` file:

tensorzero.toml

```toml
[gateway]
observability.enabled = false
```

If you only need to disable observability temporarily, you can pass a `dryrun: true` parameter to the inference and feedback API endpoints.

## TensorZero Gateway

Tip

**The TensorZero Python client includes a built-in embedded gateway, so you don’t need to run a separate service for it.** The gateway is only needed if you want to use the OpenAI Python client or interact with TensorZero via its HTTP API (for other programming languages). The TensorZero UI also requires the gateway service.

### Development

Running with Docker (Recommended)

You can easily run the TensorZero Gateway locally using Docker.

You need to provide it with a path to a folder containing your `tensorzero.toml` file as well as its dependencies (e.g. schemas and templates), as well as the environment variables discussed above.

Running with Docker

```bash
docker run \
  --name tensorzero-gateway \
  -v "./config:/app/config" \
  --env-file .env \
  -p 3000:3000 \
  -d \
  tensorzero/gateway
```

Building from source

Alternatively, you can build the TensorZero Gateway from source and run it directly on your host machine using [Cargo](https://doc.rust-lang.org/cargo/):

Building from source

```bash
cargo run --release --bin gateway -- path/to/your/tensorzero.toml
```

### Production

You can deploy the TensorZero Gateway alongside your application (e.g. as a sidecar container) or as a standalone service.

A single gateway instance can handle over 1k QPS/core with sub-millisecond latency (see [Benchmarks](/docs/gateway/benchmarks/)), so a simple deployment should suffice for the vast majority of applications. If you deploy it as an independent service, we recommend deploying at least two instances behind a load balancer for high availability. The gateway is stateless, so you can easily scale horizontally and don’t need to worry about persistence.

Running with Docker (Recommended)

The recommended way to run the TensorZero Gateway in production is to use Docker.

There are many ways to run Docker containers in production. A simple solution is to use Docker Compose. We provide an example [`docker-compose.yml`](https://github.com/tensorzero/tensorzero/blob/main/examples/production-deployment/docker-compose.yml) for reference.

Building from source

Alternatively, you can build the TensorZero Gateway from source and run it directly on your host machine using [Cargo](https://doc.rust-lang.org/cargo/). For production deployments, we recommend enabling performance optimizations:

Building from source

```bash
cargo run --profile performance --bin gateway -- path/to/your/tensorzero.toml
```

### Command Line Arguments

The TensorZero Gateway requires either `--config-file` to specify a custom configuration file (e.g. `--config-file /path/to/tensorzero.toml`) or `--default-config` to use default settings (i.e. no custom functions, metrics, etc.). You can also use `--log-format` to set the logging format to either `pretty` (default) or `json`.

### Clients

See the [Clients](/docs/gateway/clients/) page for more details on how to interact with the TensorZero Gateway.

### Configuration

To run the TensorZero Gateway, first you need to create a `tensorzero.toml` configuration file. Read more about the configuration file [here](/docs/gateway/configuration-reference/).

### Model Provider Credentials

In addition to the `TENSORZERO_CLICKHOUSE_URL` environment variable discussed above, the TensorZero Gateway accepts the following environment variables for provider credentials. Unless you specify an alternative credential location in your configuration file, these environment variables are required for the providers that are used in a variant with positive weight. If required credentials are missing, the gateway will fail on startup.

| Provider                | Environment Variable(s)                                               |
| ----------------------- | --------------------------------------------------------------------- |
| Anthropic               | `ANTHROPIC_API_KEY`                                                   |
| AWS Bedrock             | `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION` (optional) |
| AWS SageMaker           | `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION` (optional) |
| Azure OpenAI            | `AZURE_OPENAI_API_KEY`                                                |
| Fireworks               | `FIREWORKS_API_KEY`                                                   |
| GCP Vertex AI Anthropic | `GCP_VERTEX_CREDENTIALS_PATH` (see below for details)                 |
| GCP Vertex AI Gemini    | `GCP_VERTEX_CREDENTIALS_PATH` (see below for details)                 |
| Google AI Studio Gemini | `GOOGLE_AI_STUDIO_GEMINI_API_KEY`                                     |
| Hyperbolic              | `HYPERBOLIC_API_KEY`                                                  |
| Mistral                 | `MISTRAL_API_KEY`                                                     |
| OpenAI                  | `OPENAI_API_KEY`                                                      |
| Together                | `TOGETHER_API_KEY`                                                    |
| xAI                     | `XAI_API_KEY`                                                         |

**Notes:**

* AWS Bedrock supports many authentication methods, including environment variables, IAM roles, and more. See the AWS documentation for more details.
* If you’re using the GCP Vertex provider, you also need to mount the credentials for a service account in JWT form (described [here](https://cloud.google.com/docs/authentication/provide-credentials-adc#service-account)) to `/app/gcp-credentials.json` using an additional `-v` flag.

Tip

See [`.env.example`](https://github.com/tensorzero/tensorzero/blob/main/examples/production-deployment/.env.example) for a complete example with every supported environment variable.

# Batch Inference

> Learn how to process multiple requests at once with batch inference to save on inference costs at the expense of longer wait times.

The batch inference endpoint provides access to batch inference APIs offered by some model providers. These APIs provide inference with large cost savings compared to real-time inference, at the expense of much higher latency (sometimes up to a day).

The batch inference workflow consists of two steps: submitting your batch request, then polling for the batch job status until completion.

See the [Batch Inference API Reference](/docs/gateway/api-reference/batch-inference/) for more details on the batch inference endpoints, and see [Integrations](/docs/gateway/integrations/) for model provider integrations that support batch inference.

## Example

Tip

You can also find the runnable code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/batch-inference).

Imagine you have a simple TensorZero function that generates haikus using GPT-4o Mini.

```toml
[functions.generate_haiku]
type = "chat"


[functions.generate_haiku.variants.gpt_4o_mini]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
```

You can submit a batch inference job to generate multiple haikus with a single request. Each entry in `inputs` is equal to the `input` field in a regular inference request.

```sh
curl -X POST http://localhost:3000/batch_inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "generate_haiku",
    "variant_name": "gpt_4o_mini",
    "inputs": [
      {
        "messages": [
          {
            "role": "user",
            "content": "Write a haiku about artificial intelligence."
          }
        ]
      },
      {
        "messages": [
          {
            "role": "user",
            "content": "Write a haiku about general aviation."
          }
        ]
      },
      {
        "messages": [
          {
            "role": "user",
            "content": "Write a haiku about anime."
          }
        ]
      }
    ]
  }'
```

The response contains a `batch_id` as well as `inference_ids` and `episode_ids` for each inference in the batch.

```json
{
  "batch_id": "019470f0-db4c-7811-9e14-6fe6593a2652",
  "inference_ids": [
    "019470f0-d34a-77a3-9e59-bcc66db2b82f",
    "019470f0-d34a-77a3-9e59-bcdd2f8e06aa",
    "019470f0-d34a-77a3-9e59-bcecfb7172a0"
  ],
  "episode_ids": [
    "019470f0-d34a-77a3-9e59-bc933973d087",
    "019470f0-d34a-77a3-9e59-bca6e9b748b2",
    "019470f0-d34a-77a3-9e59-bcb20177bf3a"
  ]
}
```

You can use this `batch_id` to poll for the status of the job or retrieve the results using the `GET /batch_inference/{batch_id}` endpoint.

```sh
curl -X GET http://localhost:3000/batch_inference/019470f0-db4c-7811-9e14-6fe6593a2652
```

While the job is pending, the response will only contain the `status` field.

```json
{
  "status": "pending"
}
```

Once the job is completed, the response will contain the `status` field and the `inferences` field. Each inference object is the same as the response from a regular inference request.

```json
{
  "status": "completed",
  "batch_id": "019470f0-db4c-7811-9e14-6fe6593a2652",
  "inferences": [
    {
      "inference_id": "019470f0-d34a-77a3-9e59-bcc66db2b82f",
      "episode_id": "019470f0-d34a-77a3-9e59-bc933973d087",
      "variant_name": "gpt_4o_mini",
      "content": [
        {
          "type": "text",
          "text": "Whispers of circuits,  \nLearning paths through endless code,  \nDreams in binary."
        }
      ],
      "usage": {
        "input_tokens": 15,
        "output_tokens": 19
      }
    },
    {
      "inference_id": "019470f0-d34a-77a3-9e59-bcdd2f8e06aa",
      "episode_id": "019470f0-d34a-77a3-9e59-bca6e9b748b2",
      "variant_name": "gpt_4o_mini",
      "content": [
        {
          "type": "text",
          "text": "Wings of freedom soar,  \nClouds embrace the lonely flight,  \nSky whispers adventure."
        }
      ],
      "usage": {
        "input_tokens": 15,
        "output_tokens": 20
      }
    },
    {
      "inference_id": "019470f0-d34a-77a3-9e59-bcecfb7172a0",
      "episode_id": "019470f0-d34a-77a3-9e59-bcb20177bf3a",
      "variant_name": "gpt_4o_mini",
      "content": [
        {
          "type": "text",
          "text": "Vivid worlds unfold,  \nHeroes rise with dreams in hand,  \nInk and dreams collide."
        }
      ],
      "usage": {
        "input_tokens": 14,
        "output_tokens": 20
      }
    }
  ]
}
```

## Technical Notes

* **Observability**

  * For now, pending batch inference jobs are not shown in the TensorZero UI. You can find the relevant information in the `BatchRequest` and `BatchModelInference` tables on ClickHouse. See [Data Model](/docs/gateway/data-model/) for more information.
  * Inferences from completed batch inference jobs are shown in the UI alongside regular inferences.

* **Experimentation**
  * The gateway samples the same variant for the entire batch.

* **Python Client**
  * The TensorZero Python client doesn’t natively support batch inference yet. You’ll need to submit batch requests using HTTP requests, as shown above.

# Credential Management (API Key Management)

> Learn how to manage credentials (API keys) in TensorZero.

This guide explains how to manage credentials (API keys) in TensorZero Gateway.

Typically, the TensorZero Gateway will look for credentials like API keys using standard environment variables. The gateway will load credentials from the environment variables on startup, and your application doesn’t need to have access to the credentials.

That said, you can customize this behavior by setting alternative credential locations for each provider. For example, you can provide credentials dynamically at inference time, or set alternative static credentials for each provider (e.g. to use multiple API keys for the same provider).

## Default Behavior

By default, the TensorZero Gateway will look for credentials in the following environment variables:

| Model Provider                                                                       | Default Credential            |
| ------------------------------------------------------------------------------------ | ----------------------------- |
| [Anthropic](/docs/gateway/guides/providers/anthropic/)                               | `ANTHROPIC_API_KEY`           |
| [AWS Bedrock](/docs/gateway/guides/providers/aws-bedrock/)                           | Uses AWS SDK credentials      |
| [AWS SageMaker](/docs/gateway/guides/providers/aws-sagemaker/)                       | Uses AWS SDK credentials      |
| [Azure](/docs/gateway/guides/providers/azure/)                                       | `AZURE_OPENAI_API_KEY`        |
| [Deepseek](/docs/gateway/guides/providers/deepseek/)                                 | `DEEPSEEK_API_KEY`            |
| [Fireworks](/docs/gateway/guides/providers/fireworks/)                               | `FIREWORKS_API_KEY`           |
| [GCP Vertex AI (Anthropic)](/docs/gateway/guides/providers/gcp-vertex-ai-anthropic/) | `GCP_VERTEX_CREDENTIALS_PATH` |
| [GCP Vertex AI (Gemini)](/docs/gateway/guides/providers/gcp-vertex-ai-gemini/)       | `GCP_VERTEX_CREDENTIALS_PATH` |
| [Google AI Studio (Gemini)](/docs/gateway/guides/providers/google-ai-studio-gemini/) | `GOOGLE_API_KEY`              |
| [Hyperbolic](/docs/gateway/guides/providers/hyperbolic/)                             | `HYPERBOLIC_API_KEY`          |
| [Mistral](/docs/gateway/guides/providers/mistral/)                                   | `MISTRAL_API_KEY`             |
| [OpenAI](/docs/gateway/guides/providers/openai/)                                     | `OPENAI_API_KEY`              |
| [OpenAI-Compatible](/docs/gateway/guides/providers/openai-compatible/)               | `OPENAI_API_KEY`              |
| [SGLang](/docs/gateway/guides/providers/sglang/)                                     | `SGLANG_API_KEY`              |
| [Text Generation Inference (TGI)](/docs/gateway/guides/providers/tgi/)               | None                          |
| [Together](/docs/gateway/guides/providers/together/)                                 | `TOGETHER_API_KEY`            |
| [vLLM](/docs/gateway/guides/providers/vllm/)                                         | None                          |
| [XAI](/docs/gateway/guides/providers/xai/)                                           | `XAI_API_KEY`                 |

## Customizing Credential Management

You can customize the source of credentials for each provider.

See [Configuration Reference](/docs/gateway/configuration-reference/) (e.g. `api_key_location`) for more information on the different ways to configure credentials for each provider. Also see the relevant provider guides for more information on how to configure credentials for each provider.

### Static Credentials

You can set alternative static credentials for each provider.

For example, let’s say we want to use a different environment variable for an OpenAI provider. We can customize variable name by setting the `api_key_location` to `env::MY_OTHER_OPENAI_API_KEY`.

```toml
[models.gpt_4o_mini.providers.my_other_openai]
type = "openai"
api_key_location = "env::MY_OTHER_OPENAI_API_KEY"
# ...
```

At startup, the TensorZero Gateway will look for the `MY_OTHER_OPENAI_API_KEY` environment variable and use that value for the API key.

Tip

#### Load Balancing Between Multiple Credentials

You can load balance between different API keys for the same provider by defining multiple variants and models.

For example, the configuration below will split the traffic between two different OpenAI API keys, `OPENAI_API_KEY_1` and `OPENAI_API_KEY_2`.

```toml
[models.gpt_4o_mini_1]
routing = ["openai"]


[models.gpt_4o_mini_1.providers.openai]
type = "openai"
model_name = "gpt-4o-mini"
api_key_location = "env::OPENAI_API_KEY_1"


[models.gpt_4o_mini_2]
routing = ["openai"]


[models.gpt_4o_mini_2.providers.openai]
type = "openai"
model_name = "gpt-4o-mini"
api_key_location = "env::OPENAI_API_KEY_2"


[functions.generate_haiku]
type = "chat"


[functions.generate_haiku.variants.gpt_4o_mini_1]
type = "chat_completion"
model = "gpt_4o_mini_1"


[functions.generate_haiku.variants.gpt_4o_mini_2]
type = "chat_completion"
model = "gpt_4o_mini_2"
```

You can use the same principle to set up fallbacks between different API keys for the same provider. See [Retries & Fallbacks](/docs/gateway/guides/retries-fallbacks/) for more information on how to configure retries and fallbacks.

### Dynamic Credentials

You can provide API keys dynamically at inference time.

To do this, you can use the `dynamic::` prefix in the relevant credential field in the provider configuration.

For example, let’s say we want to provide dynamic API keys for the OpenAI provider.

```toml
[models.user_gpt_4o_mini]
routing = ["openai"]


[models.user_gpt_4o_mini.providers.openai]
type = "openai"
model_name = "gpt-4o-mini"
api_key_location = "dynamic::customer_openai_api_key"
```

At inference time, you can provide the API key in the `credentials` argument.

```python
from tensorzero import TensorZeroGateway


with TensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
    response = client.inference(
        function_name="generate_haiku",
        input={
            "messages": [
                {
                    "role": "user",
                    "content": "Write a haiku about artificial intelligence.",
                }
            ]
        },
        credentials={
            "customer_openai_api_key": "sk-..."
        }
    )


print(response)
```

# Episodes

> Learn how to use episodes to manage sequences of inferences that share a common outcome.

An episode is a sequence of inferences associated with a common downstream outcome.

For example, an episode could refer to a sequence of LLM calls associated with:

* Resolving a support ticket
* Preparing an insurance claim
* Completing a phone call
* Extracting data from a document
* Drafting an email

An episode will include one or more functions, and sometimes multiple calls to the same function. Your application can run arbitrary actions (e.g. interact with users, retrieve documents, actuate robotics) between function calls within an episode. Though these are outside the scope of TensorZero, it is fine (and encouraged) to build your LLM systems this way.

The `/inference` endpoint accepts an optional `episode_id` field. When you make the first inference request, you don’t have to provide an `episode_id`. The gateway will create a new episode for you and return the `episode_id` in the response. When you make the second inference request, you must provide the `episode_id` you received in the first response. The gateway will use the `episode_id` to associate the two inference requests together.

Tip

You shouldn’t generate episode IDs yourself. The gateway will create a new episode ID for you if you don’t provide one. Then, you can use it with other inferences you’d like to associate with the episode.

Tip

You can also find the runnable code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/episodes).

## Scenario

In the [Quick Start](/docs/quickstart/), we built a simple LLM application that writes haikus about artificial intelligence.

Imagine we want to separately generate some commentary about the haiku, and present both pieces of content to users. We can associate both inferences with the same episode.

Let’s define an additional function in our configuration file.

tensorzero.toml

```toml
[functions.analyze_haiku]
type = "chat"


[functions.analyze_haiku.variants.gpt_4o_mini]
type = "chat_completion"
model = "gpt_4o_mini"
```

Full Configuration

tensorzero.toml

```toml
[models.gpt_4o_mini]
routing = ["openai"]


[models.gpt_4o_mini.providers.openai]
type = "openai"
model_name = "gpt-4o-mini"


[functions.generate_haiku]
type = "chat"


[functions.generate_haiku.variants.gpt_4o_mini]
type = "chat_completion"
model = "gpt_4o_mini"


[functions.analyze_haiku]
type = "chat"


[functions.analyze_haiku.variants.gpt_4o_mini]
type = "chat_completion"
model = "gpt_4o_mini"
```

## Inferences & Episodes

This time, we’ll create a multi-step workflow that first generates a haiku and then analyzes it. We won’t provide an `episode_id` in the first inference request, so the gateway will generate a new one for us. We’ll then use that value in our second inference request.

run\_with\_tensorzero.py

```python
from tensorzero import TensorZeroGateway


with TensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
    haiku_response = client.inference(
        function_name="generate_haiku",
        # We don't provide an episode_id for the first inference in the episode
        input={
            "messages": [
                {
                    "role": "user",
                    "content": "Write a haiku about artificial intelligence.",
                }
            ]
        },
    )


    print(haiku_response)


    # When we don't provide an episode_id, the gateway will generate a new one for us
    episode_id = haiku_response.episode_id


    # In a production application, we'd first validate the response to ensure the model returned the correct fields
    haiku = haiku_response.content[0].text


    analysis_response = client.inference(
        function_name="analyze_haiku",
        # For future inferences in that episode, we provide the episode_id that we received
        episode_id=episode_id,
        input={
            "messages": [
                {
                    "role": "user",
                    "content": f"Write a one-paragraph analysis of the following haiku:\n\n{haiku}",
                }
            ]
        },
    )


    print(analysis_response)
```

Sample Output

```python
ChatInferenceResponse(
    inference_id=UUID('01921116-0fff-7272-8245-16598966335e'),
    episode_id=UUID('01921116-0cd9-7d10-a9a6-d5c8b9ba602a'),
    variant_name='gpt_4o_mini',
    content=[
        Text(
            type='text',
            text='Silent circuits pulse,\nWhispers of thought in code bloom,\nMachines dream of us.',
        ),
    ],
    usage=Usage(
        input_tokens=15,
        output_tokens=20,
    ),
)


ChatInferenceResponse(
    inference_id=UUID('01921116-1862-7ea1-8d69-131984a4625f'),
    episode_id=UUID('01921116-0cd9-7d10-a9a6-d5c8b9ba602a'),
    variant_name='gpt_4o_mini',
    content=[
        Text(
            type='text',
            text='This haiku captures the intricate and intimate relationship between technology and human consciousness. '
                 'The phrase "Silent circuits pulse" evokes a sense of quiet activity within machines, suggesting that '
                 'even in their stillness, they possess an underlying vibrancy. The imagery of "Whispers of thought in '
                 'code bloom" personifies the digital realm, portraying lines of code as organic ideas that grow and '
                 'evolve, hinting at the potential for artificial intelligence to derive meaning or understanding from '
                 'human input. Finally, "Machines dream of us" introduces a poignant juxtaposition between human '
                 'creativity and machine logic, inviting contemplation about the nature of thought and consciousness '
                 'in both realms. Overall, the haiku encapsulates a profound reflection on the emergent sentience of '
                 'technology and the deeply interwoven future of humanity and machines.',
        ),
    ],
    usage=Usage(
        input_tokens=39,
        output_tokens=155,
    ),
)
```

## Conclusion & Next Steps

Episodes are first-class citizens in TensorZero that enable powerful workflows for multi-step LLM systems. You can use them alongside other features like [experimentation](/docs/gateway/tutorial/#experimentation), [metrics & feedback](/docs/gateway/guides/metrics-feedback/), and [tool use (function calling)](/docs/gateway/tutorial/#tools). For example, you can track KPIs for entire episodes instead of individual inferences, and later jointly optimize your LLMs to maximize these metrics.

# Experimentation (A/B Testing)

> Learn how to use experimentation to test and iterate on your LLM applications.

The TensorZero Gateway provides built-in support for experimentation (A/B testing) through variants. Each function can have multiple variants, and the gateway will sample between them based on their weights.

Variants enable you to experiment with different models (e.g. GPT-4o vs Claude), prompts (e.g. different templates), parameters (e.g. different temperatures), inference strategies (e.g. dynamic in-context learning), and more.

During an episode, multiple calls to the same function will receive the same variant (unless fallbacks are necessary). This ensures consistency in multi-step LLM workflows. Formally, this consistent variant assignment acts as a randomized controlled experiment, providing the statistical foundation needed to make causal inferences about which configurations perform best.

You can use the feedback collected about an inference or episode to compare how different variants perform in a principled way. You can visualize the performance of different variants over time using the TensorZero UI.

## Examples

### Simple A/B Testing

Let’s say you want to experiment with different models for a function that drafts an email.

Let’s create a function with two variants: GPT-4o Mini and Claude 3.5 Haiku. In a production setting, you’d likely want to set up prompt templates, inference parameters, and more.

```toml
[functions.draft_email]
type = "chat"


[functions.draft_email.variants.gpt_4o_mini]
type = "chat_completion"
model = "openai::gpt-4o-mini"


[functions.draft_email.variants.claude_3_5_haiku]
type = "chat_completion"
model = "anthropic::claude-3.5-haiku"
```

With the configuration above, the gateway will sample between the the variants with equal probability.

### Variant Weights

You can also set variant weights to control the probability of each variant being chosen.

```toml
[functions.draft_email]
type = "chat"


[functions.draft_email.variants.gpt_4o_mini]
type = "chat_completion"
model = "openai::gpt-4o-mini"
weight = 0.9


[functions.draft_email.variants.claude_3_5_haiku]
type = "chat_completion"
model = "anthropic::claude-3.5-haiku"
weight = 0.1
```

With the configuration above, the gateway will sample `gpt_4o_mini` 90% of the time and `claude_3_5_haiku` 10% of the time.

Tip

The weights don’t need to add up to 1. The gateway will normalize them so that they do, and sample the variants accordingly. For example, if a variant has weight 3 and another has weight 2, the first variant will be sampled 3/5 of the time and the second variant will be sampled 2/5 of the time.

If you don’t specify weights for your variants, the gateway will sample between them uniformly.

If you set a variant’s weight to zero, it will never be sampled. You can only use zero-weighted variants by explicitly pinning them at inference time using `variant_name`.

If you mix variants with positive and unspecified weights, the gateway will sample the positive weighted variants first, and only use the unspecified weighted variants as fallbacks. See [Retries & Fallbacks](/docs/gateway/guides/retries-fallbacks/) for more details.

# Extending TensorZero

> Learn how to extend or override TensorZero to access provider features we don't support out of the box.

TensorZero aims to provide a great developer experience while giving you full access to the underlying capabilities of each model provider.

We provide advanced features that let you customize requests and access provider-specific functionality that isn’t directly supported in TensorZero. You shouldn’t need these features most of the time, but they’re around if necessary.

Help Us Help You!

Is there something you weren’t able to do with TensorZero? Please let us know and we’ll try to tackle it — not just for the specific case but a general solution for that class of workflow.

## Features

### `extra_body`

You can use the `extra_body` field to override the request body that TensorZero sends to model providers.

You can set `extra_body` on a variant configuration block, a model provider configuration block, or at inference time. See [Configuration Reference](/docs/gateway/configuration-reference/) and [Inference API Reference](/docs/gateway/api-reference/inference/) for more details.

### `extra_headers`

You can use the `extra_headers` field to override the request headers that TensorZero sends to model providers.

You can set `extra_headers` on a variant configuration block or a model provider configuration block. Inference-time `extra_headers` is coming soon. See [Configuration Reference](/docs/gateway/configuration-reference/) for more details.

### `include_original_response`

If you enable this feature while running inference, the gateway will return the original response from the model provider along with the TensorZero response.

See [Inference API Reference](/docs/gateway/api-reference/inference/) for more details.

### TensorZero Data

TensorZero stores all its data on your own ClickHouse database.

You can query this data directly by running SQL queries against your ClickHouse instance. If you’re feeling particularly adventurous, you can also write to ClickHouse directly (though you should be careful when upgrading your TensorZero deployment to account for any database migrations).

See [Data model](/docs/gateway/data-model/) for more details.

## Example: Anthropic Computer Use

At the time of writing, TensorZero hadn’t integrated with Anthropic’s Computer Use features directly — but they worked out of the box!

Concretely, Anthropic Computer Use requires setting additional fields to the request body as well as a request header. Let’s define a TensorZero function that includes these additional parameters:

```toml
[functions.bash_assistant]
type = "chat"


[functions.bash_assistant.variants.anthropic_claude_3_7_sonnet_20250219]
type = "chat_completion"
model = "anthropic::claude-3-7-sonnet-20250219"
max_tokens = 2048
extra_body = [
    { pointer = "/tools", value = [{ type = "bash_20250124", name = "bash" }] },
    { pointer = "/thinking", value = { type = "enabled", budget_tokens = 1024 } },
]
extra_headers = [
    { name = "anthropic-beta", value = "computer-use-2025-01-24" },
]
```

This example illustrates how you should be able to use the vast majority of features supported by the model provider even if TensorZero doesn’t have explicit support for them yet.

# Inference Caching

> Learn how to use inference caching with TensorZero Gateway.

The TensorZero Gateway supports caching of inference responses to improve latency and reduce costs. When caching is enabled, identical requests will be served from the cache instead of being sent to the model provider, resulting in faster response times and lower token usage.

## Usage

The TensorZero Gateway supports the following cache modes:

* `write_only` (default): Only write to cache but don’t serve cached responses
* `read_only`: Only read from cache but don’t write new entries
* `on`: Both read from and write to cache
* `off`: Disable caching completely

You can also optionally specify a maximum age for cache entries in seconds for inference reads. This parameter is ignored for inference writes.

See [API Reference](/docs/gateway/api-reference/inference/#cache_options) for more details.

## Example

```python
from tensorzero import TensorZeroGateway


with TensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
    response = client.inference(
        model_name="openai::gpt-4o-mini",
        input={
            "messages": [
                {
                    "role": "user",
                    "content": "What is the capital of Japan?",
                }
            ]
        },
        cache_options={
            "enabled": "on",  # read and write to cache
            "max_age_s": 3600,  # optional: cache entries >1h (>3600s) old are disregarded for reads
        },
    )


print(response)
```

## Technical Notes

* The cache applies to individual model requests, not inference requests. This means that the following will be cached separately: multiple variants of the same function; multiple calls to the same function with different parameters; individual model requests for inference-time optimizations; and so on.
* The `max_age_s` parameter applies to the retrieval of cached responses. The cache does not automatically delete old entries (i.e. not a TTL).
* When the gateway serves a cached response, the usage fields are set to zero.
* The cache data is stored in ClickHouse.
* For batch inference, the gateway only writes to the cache but does not serve cached responses.

# Inference-Time Optimizations

> Learn how to use inference-time strategies like dynamic in-context learning (DICL) and best-of-N sampling to optimize LLM performance.

Inference-time optimizations are powerful techniques that can significantly enhance the performance of your LLM applications without the need for model fine-tuning.

This guide will explore two key strategies implemented as variant types in TensorZero: Best-of-N (BoN) sampling and Dynamic In-Context Learning (DICL). Best-of-N sampling generates multiple response candidates and selects the best one using an evaluator model, while Dynamic In-Context Learning enhances context by incorporating relevant historical examples into the prompt. Both techniques can lead to improved response quality and consistency in your LLM applications.

## Best-of-N Sampling

![Inference-Time Optimization: Best-of-N Sampling](/_astro/inference-time-optimizations-best-of-n-sampling.CaHuJatB_MDUz2.webp)

Best-of-N (BoN) sampling is an inference-time optimization strategy that can significantly improve the quality of your LLM outputs. Here’s how it works:

1. Generate multiple response candidates using one or more variants (i.e. possibly using different models and prompts)
2. Use an evaluator model to select the best response from these candidates
3. Return the selected response as the final output

This approach allows you to leverage multiple prompts or variants to increase the likelihood of getting a high-quality response. It’s particularly useful when you want to benefit from an ensemble of variants or reduce the impact of occasional bad generations. Best-of-N sampling is also commonly referred to as rejection sampling in some contexts.

Tip

TensorZero also supports a similar inference-time strategy called [Mixture-of-N Sampling](#mixture-of-n-sampling).

To use BoN sampling in TensorZero, you need to configure a variant with the `experimental_best_of_n` type. Here’s a simple example configuration:

tensorzero.toml

```toml
[functions.draft_email.variants.promptA]
type = "chat_completion"
model = "gpt-4o-mini"
user_template = "functions/draft_email/promptA/user.minijinja"


[functions.draft_email.variants.promptB]
type = "chat_completion"
model = "gpt-4o-mini"
user_template = "functions/draft_email/promptB/user.minijinja"


[functions.draft_email.variants.best_of_n]
type = "experimental_best_of_n"
candidates = ["promptA", "promptA", "promptB"]
weight = 1


[functions.draft_email.variants.best_of_n.evaluator]
model = "gpt-4o-mini"
user_template = "functions/draft_email/best_of_n/user.minijinja"
```

In this configuration:

* We define a `best_of_n` variant that uses two different variants (`promptA` and `promptB`) to generate candidates. It generates two candidates using `promptA` and one candidate using `promptB`.
* The `evaluator` block specifies the model and instructions for selecting the best response.

Tip

You should define the evaluator model as if it were solving the problem (not judging the quality of the candidates). TensorZero will automatically make the necessary prompt modifications to evaluate the candidates.

Read more about the `experimental_best_of_n` variant type in [Configuration Reference](/docs/gateway/configuration-reference/#type-experimental_best_of_n).

Tip

We also provide a complete runnable example:

[Improving LLM Chess Ability with Best/Mixture-of-N Sampling](https://github.com/tensorzero/tensorzero/tree/main/examples/chess-puzzles)

This example showcases how best-of-N sampling can significantly enhance an LLM’s chess-playing abilities by selecting the most promising moves from multiple generated options.

## Dynamic In-Context Learning (DICL)

![Inference-Time Optimization: Dynamic In-Context Learning](/_astro/inference-time-optimizations-dynamic-in-context-learning.B0S40P10_9Eqrv.webp)

Dynamic In-Context Learning (DICL) is an inference-time optimization strategy that enhances LLM performance by incorporating relevant historical examples into the prompt. This technique leverages a database of past interactions to select and include contextually similar examples in the current prompt, allowing the model to adapt to specific tasks or domains without requiring fine-tuning. By dynamically augmenting the input with relevant historical data, DICL enables the LLM to make more informed and accurate responses, effectively learning from past experiences in real-time.

Here’s how it works:

1. Before inference: Curate reference examples, embed them, and store in the database
2. Embed the current input using an embedding model and retrieve similar high-quality examples from a database of past interactions
3. Incorporate these examples into the prompt to provide additional context
4. Generate a response using the enhanced prompt

To use DICL in TensorZero, you need to configure a variant with the `experimental_dynamic_in_context_learning` type. Here’s a simple example configuration:

tensorzero.toml

```toml
[functions.draft_email.variants.dicl]
type = "experimental_dynamic_in_context_learning"
model = "gpt-4o-mini"
embedding_model = "text-embedding-3-small"
system_instructions = "functions/draft_email/dicl/system.txt"
k = 5


[embedding_models.text-embedding-3-small]
routing = ["openai"]


[embedding_models.text-embedding-3-small.providers.openai]
type = "openai"
model_name = "text-embedding-3-small"
```

In this configuration:

* We define a `dicl` variant that uses the `experimental_dynamic_in_context_learning` type.
* The `embedding_model` field specifies the model used to embed inputs for similarity search. We also need to define this model in the `embedding_models` section.
* The `k` parameter determines the number of similar examples to retrieve and incorporate into the prompt.

To use Dynamic In-Context Learning (DICL), you also need to add relevant examples to the `DynamicInContextLearningExample` table in your ClickHouse database. These examples will be used by the DICL variant to enhance the context of your prompts at inference time.

The process of adding these examples to the database is crucial for DICL to function properly. We provide a sample recipe that simplifies this process: [Dynamic In-Context Learning with OpenAI](https://github.com/tensorzero/tensorzero/tree/main/recipes/dicl).

This recipe supports selecting examples based on boolean metrics, float metrics, and demonstrations. It helps you populate the `DynamicInContextLearningExample` table with high-quality, relevant examples from your historical data.

For more information on the `DynamicInContextLearningExample` table and its role in the TensorZero data model, see [Data Model](/docs/gateway/data-model/). For a comprehensive list of configuration options for the `experimental_dynamic_in_context_learning` variant type, see [Configuration Reference](/docs/gateway/configuration-reference/#type-experimental_dynamic_in_context_learning).

Tip

We also provide a complete runnable example:

[Optimizing Data Extraction (NER) with TensorZero](https://github.com/tensorzero/tensorzero/tree/main/examples/data-extraction-ner)

This example demonstrates how Dynamic In-Context Learning (DICL) can enhance Named Entity Recognition (NER) performance by leveraging relevant historical examples to improve data extraction accuracy and consistency without having to fine-tune a model.

## Mixture-of-N Sampling

![Inference-Time Optimization: Mixture-of-N Sampling](/_astro/inference-time-optimizations-mixture-of-n-sampling.CLxh8Zz7_ZMEUkQ.webp)

Mixture-of-N (MoN) sampling is an inference-time optimization strategy that can significantly improve the quality of your LLM outputs. Here’s how it works:

1. Generate multiple response candidates using one or more variants (i.e. possibly using different models and prompts)
2. Use a fuser model to combine the candidates into a single response
3. Return the combined response as the final output

This approach allows you to leverage multiple prompts or variants to increase the likelihood of getting a high-quality response. It’s particularly useful when you want to benefit from an ensemble of variants or reduce the impact of occasional bad generations.

Tip

TensorZero also supports a similar inference-time strategy called [Best-of-N Sampling](#best-of-n-sampling).

To use MoN sampling in TensorZero, you need to configure a variant with the `experimental_mixture_of_n` type. Here’s a simple example configuration:

tensorzero.toml

```toml
[functions.draft_email.variants.promptA]
type = "chat_completion"
model = "gpt-4o-mini"
user_template = "functions/draft_email/promptA/user.minijinja"
weight = 0


[functions.draft_email.variants.promptB]
type = "chat_completion"
model = "gpt-4o-mini"
user_template = "functions/draft_email/promptB/user.minijinja"
weight = 0


[functions.draft_email.variants.mixture_of_n]
type = "experimental_mixture_of_n"
candidates = ["promptA", "promptA", "promptB"]
weight = 1


[functions.draft_email.variants.mixture_of_n.fuser]
model = "gpt-4o-mini"
user_template = "functions/draft_email/mixture_of_n/user.minijinja"
```

In this configuration:

* We define a `mixture_of_n` variant that uses two different variants (`promptA` and `promptB`) to generate candidates. It generates two candidates using `promptA` and one candidate using `promptB`.
* The `fuser` block specifies the model and instructions for combining the candidates into a single response.

Tip

You should define the fuser model as if it were solving the problem (not judging the quality of the candidates). TensorZero will automatically make the necessary prompt modifications to combine the candidates.

Read more about the `experimental_mixture_of_n` variant type in [Configuration Reference](/docs/gateway/configuration-reference/#type-experimental_mixture_of_n).

Tip

We also provide a complete runnable example:

[Improving LLM Chess Ability with Best/Mixture-of-N Sampling](https://github.com/tensorzero/tensorzero/tree/main/examples/chess-puzzles/)

This example showcases how Mixture-of-N sampling can significantly enhance an LLM’s chess-playing abilities by selecting the most promising moves from multiple generated options.

# Metrics & Feedback

> Learn how to collect metrics and feedback about inferences or sequences of inferences.

The TensorZero Gateway allows you to assign feedback to inferences or sequences of inferences ([episodes](/docs/gateway/guides/episodes/)).

Feedback captures the downstream outcomes of your LLM application, and drive the [experimentation](/docs/gateway/tutorial/#experimentation) and [optimization](/docs/recipes/) workflows in TensorZero. For example, you can fine-tune models using data from inferences that led to positive downstream behavior.

Tip

You can also find the runnable code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/metrics-feedback).

## Feedback

TensorZero currently supports the following types of feedback:

| Feedback Type  | Examples                                           |
| -------------- | -------------------------------------------------- |
| Boolean Metric | Thumbs up, task success                            |
| Float Metric   | Star rating, clicks, number of mistakes made       |
| Comment        | Natural-language feedback from users or developers |
| Demonstration  | Edited drafts, labels, human-generated content     |

You can send feedback data to the gateway by using the [`/feedback` endpoint](/docs/gateway/api-reference/feedback/#post-feedback).

## Metrics

You can define metrics in your `tensorzero.toml` configuration file.

The skeleton of a metric looks like the following configuration entry.

tensorzero.toml

```toml
[metrics.my_metric_name]
level = "..." # "inference" or "episode"
optimize = "..." # "min" or "max"
type = "..." # "boolean" or "float"
```

Tip

Comments and demonstrations are available by default and don’t need to be configured.

### Example: Rating Haikus

In the [Quick Start](/docs/quickstart/), we built a simple LLM application that writes haikus about artificial intelligence.

Imagine we wanted to assign 👍 or 👎 to these haikus. Later, we can use this data to fine-tune a model using only haikus that match our tastes.

We should use a metric of type `boolean` to capture this behavior since we’re optimizing for a binary outcome: whether we liked the haikus or not. The metric applies to individual inference requests, so we’ll set `level = "inference"`. And finally, we’ll set `optimize = "max"` because we want to maximize this metric.

Our metric configuration should look like this:

tensorzero.toml

```toml
[metrics.haiku_rating]
type = "boolean"
optimize = "max"
level = "inference"
```

Full Configuration

tensorzero.toml

```toml
[functions.generate_haiku]
type = "chat"


[functions.generate_haiku.variants.gpt_4o_mini]
type = "chat_completion"
model = "openai::gpt_4o_mini"


[metrics.haiku_rating]
type = "boolean"
optimize = "max"
level = "inference"
```

Let’s make an inference call like we did in the Quick Start, and then assign some (positive) feedback to it. We’ll use the inference response’s `inference_id` we receive from the first API call to link the two.

run.py

```python
from tensorzero import TensorZeroGateway


with TensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
    inference_response = client.inference(
        function_name="generate_haiku",
        input={
            "messages": [
                {
                    "role": "user",
                    "content": "Write a haiku about artificial intelligence.",
                }
            ]
        },
    )


    print(inference_response)


    feedback_response = client.feedback(
        metric_name="haiku_rating",
        inference_id=inference_response.inference_id,  # alternatively, you can assign feedback to an episode_id
        value=True,  # let's assume it deserves a 👍
    )


    print(feedback_response)
```

Sample Output

```python
ChatInferenceResponse(
    inference_id=UUID('01920c75-d114-7aa1-aadb-26a31bb3c7a0'),
    episode_id=UUID('01920c75-cdcb-7fa3-bd69-fd28cf615f91'),
    variant_name='gpt_4o_mini', content=[
        Text(type='text', text='Silent circuits hum, \nWisdom spun from lines of code, \nDreams in data bloom.')
    ],
    usage=Usage(
        input_tokens=15,
        output_tokens=20,
    ),
)


FeedbackResponse(feedback_id='01920c75-d11a-7150-81d8-15d497ce7eb8')
```

## Demonstrations

Demonstrations are a special type of feedback that represent the ideal output for an inference. For example, you can use demonstrations to provide corrections from human review, labels for supervised learning, or other ground truth data that represents the ideal output.

You can assign demonstrations to an inference using the special metric name `demonstration`. You can’t assign demonstrations to an episode.

```python
feedback_response = client.feedback(
    metric_name="demonstration",
    inference_id=inference_response.inference_id,
    value="Silicon dreams float\nMinds born of human design\nLearning without end",  # the haiku we wish the LLM had written
)
```

## Comments

You can assign natural-language feedback to an inference or episode using the special metric name `comment`.

```python
feedback_response = client.feedback(
    metric_name="comment",
    inference_id=inference_response.inference_id,
    value="Never mention you're an artificial intelligence, AI, bot, or anything like that.",
)
```

## Conclusion & Next Steps

Feedback unlocks powerful workflows in observability, optimization, evaluations, and experimentation. For example, you might want to fine-tune a model with inference data from haikus that receive positive ratings, or use demonstrations to correct model mistakes.

You can browse feedback for inferences and episodes in the TensorZero UI, and see aggregated metrics over time for your functions and variants.

This is exactly what we demonstrate in [Writing Haikus to Satisfy a Judge with Hidden Preferences](https://github.com/tensorzero/tensorzero/tree/main/examples/haiku-hidden-preferences)! This complete runnable example fine-tunes GPT-4o Mini to generate haikus tailored to an AI judge with hidden preferences. Continuous improvement over successive fine-tuning runs demonstrates TensorZero’s data and learning flywheel.

Another example that uses feedback is [Optimizing Data Extraction (NER) with TensorZero](https://github.com/tensorzero/tensorzero/tree/main/examples/data-extraction-ner). This example collects metrics and demonstrations for an LLM-powered data extraction tool, which can be used for fine-tuning and other optimization recipes. These optimized variants achieve substantial improvements over the original model.

See [Configuration Reference](/docs/gateway/configuration-reference/#metrics) and [API Reference](/docs/gateway/api-reference/feedback/#post-feedback) for more details.

# Multimodal Inference (VLMs)

> Learn how to use multimodal inference with TensorZero Gateway.

TensorZero Gateway supports multimodal inference (e.g. image inputs) with vision-language models (VLMs).

See [Integrations](/docs/gateway/integrations/) for a list of supported models.

Tip

You can also find the runnable code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/multimodal-inference).

## Setup

### Object Storage

TensorZero uses object storage to store images used during multimodal inference. It supports any S3-compatible object storage service, including AWS S3, GCP Cloud Storage, Cloudflare R2, and many more. You can configure the object storage service in the `object_storage` section of the configuration file.

In this example, we’ll use a local deployment of MinIO, an open-source S3-compatible object storage service.

```toml
[object_storage]
type = "s3_compatible"
endpoint = "http://minio:9000"  # optional: defaults to AWS S3
# region = "us-east-1"  # optional: depends on your S3-compatible storage provider
bucket_name = "tensorzero"  # optional: depends on your S3-compatible storage provider
# IMPORTANT: for production environments, remove the following setting and use a secure method of authentication in
# combination with a production-grade object storage service.
allow_http = true
```

You can also store images in a local directory (`type = "filesystem"`) or disable image storage (`type = "disabled"`). See [Configuration Reference](/docs/gateway/configuration-reference/#object_storage) for more details.

The TensorZero Gateway will attempt to retrieve credentials from the following resources in order of priority:

1. `S3_ACCESS_KEY_ID` and `S3_SECRET_ACCESS_KEY` environment variables
2. `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables
3. Credentials from the AWS SDK (default profile)

### Docker Compose

We’ll use Docker Compose to deploy the TensorZero Gateway, ClickHouse, and MinIO.

`docker-compose.yml`

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  clickhouse:
    image: clickhouse/clickhouse-server:24.12-alpine
    environment:
      - CLICKHOUSE_USER=chuser
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
      - CLICKHOUSE_PASSWORD=chpassword
    ports:
      - "8123:8123"
    healthcheck:
      test: wget --spider --tries 1 http://chuser:chpassword@clickhouse:8123/ping
      start_period: 30s
      start_interval: 1s
      timeout: 1s


  gateway:
    image: tensorzero/gateway
    volumes:
      # Mount our tensorzero.toml file into the container
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:?Environment variable OPENAI_API_KEY must be set.}
      - S3_ACCESS_KEY_ID=miniouser
      - S3_SECRET_ACCESS_KEY=miniopassword
      - TENSORZERO_CLICKHOUSE_URL=http://chuser:chpassword@clickhouse:8123/tensorzero
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      clickhouse:
        condition: service_healthy
      minio:
        condition: service_healthy


  # For a production deployment, you can use AWS S3, GCP Cloud Storage, Cloudflare R2, etc.
  minio:
    image: bitnami/minio
    ports:
      - "9000:9000" # API port
      - "9001:9001" # Console port
    environment:
      - MINIO_ROOT_USER=miniouser
      - MINIO_ROOT_PASSWORD=miniopassword
      - MINIO_DEFAULT_BUCKETS=tensorzero
    healthcheck:
      test: "mc ls local/tensorzero || exit 1"
      start_period: 30s
      start_interval: 1s
      timeout: 1s
```

## Inference

With the setup out of the way, you can now use the TensorZero Gateway to perform multimodal inference.

The TensorZero Gateway accepts both embedded images (encoded as base64 strings) and remote images (specified by a URL).

* Python

  ```python
  from tensorzero import TensorZeroGateway


  with TensorZeroGateway.build_http(
      gateway_url="http://localhost:3000",
  ) as client:
      response = client.inference(
          model_name="openai::gpt-4o-mini",
          input={
              "messages": [
                  {
                      "role": "user",
                      "content": [
                          {
                              "type": "text",
                              "text": "Do the images share any common features?",
                          },
                          # Remote image of Ferris the crab
                          {
                              "type": "image",
                              "url": "https://raw.githubusercontent.com/tensorzero/tensorzero/ff3e17bbd3e32f483b027cf81b54404788c90dc1/tensorzero-internal/tests/e2e/providers/ferris.png",
                          },
                          # One-pixel orange image encoded as a base64 string
                          {
                              "type": "image",
                              "mime_type": "image/png",
                              "data": "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAAAXNSR0IArs4c6QAAAA1JREFUGFdj+O/P8B8ABe0CTsv8mHgAAAAASUVORK5CYII=",
                          },
                      ],
                  }
              ],
          },
      )


      print(response)
  ```

* Python (OpenAI)

  ```python
  from openai import OpenAI


  with OpenAI(base_url="http://localhost:3000/openai/v1") as client:
      response = client.chat.completions.create(
          model="gpt-4o-mini",
          messages=[
              {
                  "role": "user",
                  "content": [
                      {
                          "type": "text",
                          "text": "Do the images share any common features?",
                      },
                      # Remote image of Ferris the crab
                      {
                          "type": "image_url",
                          "image_url": {
                              "url": "https://raw.githubusercontent.com/tensorzero/tensorzero/ff3e17bbd3e32f483b027cf81b54404788c90dc1/tensorzero-internal/tests/e2e/providers/ferris.png",
                          },
                      },
                      # One-pixel orange image encoded as a base64 string
                      {
                          "type": "image_url",
                          "image_url": {
                              "url": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAAAXNSR0IArs4c6QAAAA1JREFUGFdj+O/P8B8ABe0CTsv8mHgAAAAASUVORK5CYII=",
                          },
                      },
                  ],
              }
          ],
      )


      print(response)
  ```

* HTTP

  ```json
  curl -X POST http://localhost:3000/inference \
    -H "Content-Type: application/json" \
    -d '{
      "model_name": "openai::gpt-4o-mini",
      "input": {
        "messages": [
          {
            "role": "user",
            "content": [
              {
                "type": "text",
                "text": "Do the images share any common features?"
              },
              {
                "type": "image",
                "url": "https://raw.githubusercontent.com/tensorzero/tensorzero/ff3e17bbd3e32f483b027cf81b54404788c90dc1/tensorzero-internal/tests/e2e/providers/ferris.png"
              },
              {
                "type": "image",
                "mime_type": "image/png",
                "data": "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAAAXNSR0IArs4c6QAAAA1JREFUGFdj+O/P8B8ABe0CTsv8mHgAAAAASUVORK5CYII="
              }
            ]
          }
        ]
      }
    }'
  ```

# Performance & Latency

> Learn how to optimize the TensorZero Gateway for performance and latency.

The TensorZero Gateway is designed from the ground up with performance in mind. It achieves <1ms P99 latency overhead under extreme load (see [Benchmarks](/docs/gateway/benchmarks/)).

Even in default settings, the gateway is fast and lightweight enough to be unnoticeable in most applications.

## Optimizations

If you care about extreme concurrency and low latency, we recommend the following settings and workflows.

### TensorZero Gateway

* Enable `gateway.observability.async_writes` to offload the responsibility of writing inference responses to ClickHouse to a background task, instead of waiting for ClickHouse to return the inference response. [Learn more →](/docs/gateway/configuration-reference/#observabilityasync_writes)
* Ensure your application, the TensorZero Gateway, and ClickHouse are deployed in the same region to minimize network latency.

### Python Client

* Initialize the client once and reuse it as much as possible, to avoid initialization overhead and to keep the connection alive.

# Prompt Templates & Schemas

> Learn how to use prompt templates and schemas to manage complexity in your prompts.

Prompt templates and schemas simplify engineering iteration, experimentation, and optimization, especially as application complexity and team size grow. Notably, they enable you to:

1. **Decouple prompts from application code.** As you iterate on your prompts over time (or [A/B test different prompts](/docs/gateway/tutorial/#experimentation)), you’ll be able to manage them in a centralized way without making changes to the application code.
2. **Collect a structured inference dataset.** Imagine down the road you want to [fine-tune a model](/docs/recipes/) using your historical data. If you had only stored prompts as strings, you’d be stuck with the outdated prompts that were actually used at inference time. However, if you had access to the input variables in a structured dataset, you’d easily be able to counterfactually swap new prompts into your training data before fine-tuning. This is particularly important when experimenting with new models, because prompts don’t always translate well between them.
3. **Implement model-specific prompts.** We often find that the best prompt for one model is different from the best prompt for another. As you try out different models, you’ll need to be able to independently vary the prompt and the model and try different combinations thereof. This is commonly challenging to implement in application code, but trivial in TensorZero.

Tip

You can also find the runnable code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/prompts-templates-schemas).

## Scenario

In the [Quick Start](/docs/quickstart/), we built a simple LLM application that writes haikus about artificial intelligence. But what if we wanted to generate haikus about different topics?

The naive solution is to parametrize the prompt in your application.

run.py

```python
# Naive Solution (Not Recommended)


from tensorzero import TensorZeroGateway




def generate_haiku(topic):
    with TensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
        return client.inference(
            function_name="generate_haiku",
            input={
                "messages": [
                    {
                        "role": "user",
                        "content": f"Write a haiku about: {topic}",
                    }
                ]
            },
        )




print(generate_haiku("artificial intelligence"))
```

This works fine, and it’s typically how most people tackle it today. But there’s room for improvement!

For this function, what we really care about here is the `topic -> haiku` mapping. The rest of the prompt is a detail of the current implemention, and it might evolve over time.

Instead, let’s move the boilerplate for this user message to our configuration.

## Prompt Templates

TensorZero uses the [MiniJinja templating language](https://docs.rs/minijinja/latest/minijinja/syntax/index.html). MiniJinja is [mostly compatible with Jinja2](https://github.com/mitsuhiko/minijinja/blob/main/COMPATIBILITY.md), which is used by many popular projects like Flask and Django.

We’ll save the template in a separate file and later reference it in a variant in our main configuration file, `tensorzero.toml`.

user\_template.minijinja

```txt
Write a haiku about: {{ topic }}
```

If your template includes any variables, you must also provide a schema that fits the template.

Tip

MiniJinja also provides a [browser playground](https://mitsuhiko.github.io/minijinja-playground/) where you can test your templates.

## Prompt Schemas

Schemas ensure that different templates for a function share a consistent interface and validate inputs before inference.

TensorZero uses the [JSON Schema](https://json-schema.org/) format. Similar to templates, we’ll specify it in a separate file and reference it in our configuration.

JSON Schemas are a bit cumbersome to write, but luckily LLMs are great at doing it!

Let’s give Claude (Sonnet 3.5) the following query:

```txt
Generate a JSON schema with a single field: `topic`.
The `topic` field is required. No additional fields are allowed.
```

It correctly generates the following schema:

user\_schema.json

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "topic": {
      "type": "string"
    }
  },
  "required": ["topic"],
  "additionalProperties": false
}
```

Tip

You can export JSON Schemas from [Pydantic models](https://docs.pydantic.dev/latest/concepts/json_schema/) and [Zod schemas](https://www.npmjs.com/package/zod-to-json-schema).

## Putting It All Together

Let’s incorporate our template and our schema in our configuration file.

**In TensorZero, schemas belong to functions and templates belong to variants.** Since a function can have multiple variants, you’ll be able to experiment with different prompts for a given function, but you’ll still ensure they have a consistent interface for your application. If you have multiple templates, you’ll need a single schema that accounts for the variables in all of them. In other words, your schema should contain all the variables you might want for your LLM message, but a particular template doesn’t need to use every variable defined in your schema.

tensorzero.toml

```toml
# We define a function and a variant, just like in our Quick Start...
# ... but this time we include a schema and a template.
[functions.generate_haiku_with_topic]
type = "chat"
user_schema = "functions/generate_haiku_with_topic/user_schema.json" # relative to tensorzero.toml
# system_schema = "..."
# assistant_schema = "..."


[functions.generate_haiku_with_topic.variants.gpt_4o_mini]
type = "chat_completion"
model = "openai::gpt-4o-mini"
user_template = "functions/generate_haiku_with_topic/gpt_4o_mini/user_template.minijinja" # relative to tensorzero.toml
# system_template = "..."
# assistant_template = "..."
```

Tip

You can define separate templates and schemas for system, user, and assistant messages.

Tip

Note that our function’s name differs from the one in the Quick Start. We strongly encourage defining a new function when you change the schemas, to ensure you’ll always collect inference data with a consistent structure.

We plan to introduce functionality to streamline schema migrations down the road.

You can use any file structure with TensorZero. We recommend the following structure to keep things organized:

* config/

  * functions/

    * generate\_haiku\_with\_topic/

      * gpt\_4o\_mini/

        * user\_template.minijinja

      * user\_schema.json

  * tensorzero.toml

* docker-compose.yml see below

* run.py see below

Tip

The paths in `tensorzero.toml` are relative to its location, so we don’t specify the parent folder `config/`.

With everything in place, launch the TensorZero Gateway using this configuration. You can use the same Docker Compose configuration as the Quick Start (available below for convenience).

Docker Compose Configuration

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  clickhouse:
    image: clickhouse/clickhouse-server:24.12-alpine
    environment:
      - CLICKHOUSE_USER=chuser
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
      - CLICKHOUSE_PASSWORD=chpassword
    ports:
      - "8123:8123"
    healthcheck:
      test: wget --spider --tries 1 http://chuser:chpassword@clickhouse:8123/ping
      start_period: 30s
      start_interval: 1s
      timeout: 1s


  gateway:
    image: tensorzero/gateway
    volumes:
      # Mount our tensorzero.toml file into the container
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - TENSORZERO_CLICKHOUSE_URL=http://chuser:chpassword@clickhouse:8123/tensorzero
      - OPENAI_API_KEY=${OPENAI_API_KEY:?Environment variable OPENAI_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      clickhouse:
        condition: service_healthy
```

Let’s launch everything.

```bash
docker compose up
```

Tip

If the gateway is already running, you can update the configuration by restarting the container.

## Structured Inference

Let’s update our original Python script to leverage our schema.

Instead of sending the entire prompt in our inference request, now we only need to provide an object with the variables we need.

* Python

  ```python
  from tensorzero import TensorZeroGateway




  def generate_haiku(topic):
      with TensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
          return client.inference(
              function_name="generate_haiku_with_topic",
              input={
                  "messages": [
                      {
                          "role": "user",
                          "content": [{"type": "text", "arguments": {"topic": topic}}],
                      }
                  ],
              },
          )




  print(generate_haiku("artificial intelligence"))
  ```

  Sample Output

  ```python
  ChatInferenceResponse(
      inference_id=UUID('019224df-c073-7981-af9e-5a9c91533eae'),
      episode_id=UUID('019224df-bdcf-71e1-9fa4-db87fda4c632'),
      variant_name='gpt_4o_mini',
      content=[
          Text(
              type='text',
              text='Wires hum with knowledge,  \nSilent thoughts in circuits flow,  \nDreams of steel and code.'
          )
      ],
      usage=Usage(
          input_tokens=15,
          output_tokens=21
      )
  )
  ```

* Python (OpenAI)

  ```python
  import openai




  def generate_haiku(topic):
      with openai.OpenAI(base_url="http://localhost:3000/openai/v1") as client:
          return client.chat.completions.create(
              model="tensorzero::function_name::generate_haiku_with_topic",
              messages=[
                  {
                      "role": "user",
                      "content": [
                          {"type": "text", "tensorzero::arguments": {"topic": topic}}
                      ],
                  },
              ],
          )




  print(generate_haiku("artificial intelligence"))
  ```

* HTTP

  ```bash
  curl -X POST http://localhost:3000/inference \
    -H "Content-Type: application/json" \
    -d '{
      "function_name": "generate_haiku_with_topic",
      "input": {
        "messages": [
          {
            "role": "user",
            "content": [
              {
                "type": "text",
                "arguments": {
                  "topic": "artificial intelligence"
                }
              }
            ]
          }
        ]
      }
    }'
  ```

Like in the Quick Start, the gateway will store inference data in our database. But this time, the `input` field will be structured according to our schema.

Let’s check our database.

```bash
curl "http://localhost:8123/" \
  -d "SELECT * FROM tensorzero.ChatInference
      WHERE function_name = 'generate_haiku_with_topic'
      ORDER BY timestamp DESC
      LIMIT 1
      FORMAT Vertical"
```

Sample Output

```txt
Row 1:
──────
id:                 019224df-c073-7981-af9e-5a9c91533eae
function_name:      generate_haiku_with_topic
variant_name:       gpt_4o_mini
episode_id:         019224df-bdcf-71e1-9fa4-db87fda4c632
input:              {"messages":[{"role":"user","content":[{"type":"text","value":{"topic":"artificial intelligence"}}]}]}
output:             [{"type":"text","text":"Wires hum with knowledge,  \nSilent thoughts in circuits flow,  \nDreams of steel and code."}]
tool_params:
inference_params:   {"chat_completion":{}}
processing_time_ms: 782
```

## Conclusion & Next Steps

Now we can manage our prompts as configuration files, and get structured inference data from the gateway!

As discussed, it’s helpful to manage prompts in a centralized way. With TensorZero’s approach, these prompts still live in your repository, which simplifies versioning, access control, GitOps, and more. This setup also let us easily benefit from more advanced features like [A/B testing different prompts](/docs/gateway/tutorial/#experimentation) or [fine-tuning a model](/docs/recipes/).

# Getting Started with Anthropic

> Learn how to use TensorZero with Anthropic LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with the Anthropic API.

## Simple Setup

You can use the short-hand `anthropic::model_name` to use an Anthropic model with TensorZero, unless you need advanced features like fallbacks or custom credentials.

You can use Anthropic models in your TensorZero variants by setting the `model` field to `anthropic::model_name`. For example:

```toml
[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "anthropic::claude-3-5-haiku-20241022"
```

Additionally, you can set `model_name` in the inference request to use a specific Anthropic model, without having to configure a function and variant in TensorZero.

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "anthropic::claude-3-5-haiku-20241022",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

## Advanced Setup

In more complex scenarios (e.g. fallbacks, custom credentials), you can configure your own model and Anthropic provider in TensorZero.

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/anthropic).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.claude_3_5_haiku_20241022]
routing = ["anthropic"]


[models.claude_3_5_haiku_20241022.providers.anthropic]
type = "anthropic"
model_name = "claude-3-5-haiku-20241022"


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "claude_3_5_haiku_20241022"
```

See the [list of models available on Anthropic](https://docs.anthropic.com/en/docs/about-claude/models).

### Credentials

You must set the `ANTHROPIC_API_KEY` environment variable before running the gateway.

You can customize the credential location by setting the `api_key_location` to `env::YOUR_ENVIRONMENT_VARIABLE` or `dynamic::ARGUMENT_NAME`. See the [Credential Management](/docs/gateway/guides/credential-management/) guide and [Configuration Reference](/docs/gateway/configuration-reference/) for more information.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:?Environment variable ANTHROPIC_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

## Other Features

See [Extending TensorZero](/docs/gateway/guides/extending-tensorzero/) for information about Anthropic Computer Use and other beta features.

# Getting Started with AWS Bedrock

> Learn how to use TensorZero with AWS Bedrock LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with the AWS Bedrock API.

## Setup

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/aws-bedrock).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.claude_3_haiku_20240307]
routing = ["aws_bedrock"]


[models.claude_3_haiku_20240307.providers.aws_bedrock]
type = "aws_bedrock"
model_id = "anthropic.claude-3-haiku-20240307-v1:0"


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "claude_3_haiku_20240307"
```

See the [list of available models on AWS Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).

See the [Configuration Reference](/docs/gateway/configuration-reference/) for optional fields (e.g. overriding the `region`).

### Credentials

You must make sure that the gateway has the necessary permissions to access AWS Bedrock. The TensorZero Gateway will use the AWS SDK to retrieve the relevant credentials.

The simplest way is to set the following environment variables before running the gateway:

```bash
AWS_ACCESS_KEY_ID=...
AWS_REGION=us-east-1
AWS_SECRET_ACCESS_KEY=...
```

Alternatively, you can use other authentication methods supported by the AWS SDK.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:?Environment variable AWS_ACCESS_KEY_ID must be set.}
      - AWS_REGION=${AWS_REGION:?Environment variable AWS_REGION must be set.}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:?Environment variable AWS_SECRET_ACCESS_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with AWS SageMaker

> Learn how to use TensorZero with AWS SageMaker LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with the AWS SageMaker API.

The AWS SageMaker model provider is a wrapper around other TensorZero model providers that handles AWS SageMaker-specific logic (e.g. auth). For example, you can use it to infer self-hosted model providers like Ollama deployed on AWS SageMaker.

## Setup

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/aws-sagemaker).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

You’ll also need to deploy a SageMaker endpoint for your LLM model. For this example, we’re using a container running Ollama.

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.gemma_3]
routing = ["aws_sagemaker"]


[models.gemma_3.providers.aws_sagemaker]
type = "aws_sagemaker"
model_name = "gemma3:1b"
endpoint_name = "my-sagemaker-endpoint"
region = "us-east-1"
# ... or use `allow_auto_detect_region = true` to infer region with the AWS SDK
hosted_provider = "openai"  # Ollama is OpenAI-compatible


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "gemma_3"
```

The `hosted_provider` field specifies the model provider that you deployed on AWS SageMaker. For example, Ollama is OpenAI-compatible, so we use `openai` as the hosted provider. Alternatively, you can use `hosted_provider = "tgi"` if you had deployed TGI instead.

You can specify the endpoint’s `region` explicitly, or use `allow_auto_detect_region = true` to infer region with the AWS SDK.

See the [Configuration Reference](/docs/gateway/configuration-reference/) for optional fields. The relevant fields will depend on the `hosted_provider`.

### Credentials

You must make sure that the gateway has the necessary permissions to access AWS SageMaker. The TensorZero Gateway will use the AWS SDK to retrieve the relevant credentials.

The simplest way is to set the following environment variables before running the gateway:

```bash
AWS_ACCESS_KEY_ID=...
AWS_REGION=us-east-1
AWS_SECRET_ACCESS_KEY=...
```

Alternatively, you can use other authentication methods supported by the AWS SDK.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:?Environment variable AWS_ACCESS_KEY_ID must be set.}
      - AWS_REGION=${AWS_REGION:?Environment variable AWS_REGION must be set.}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:?Environment variable AWS_SECRET_ACCESS_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with Azure OpenAI Service

> Learn how to use TensorZero with Azure OpenAI Service LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with the Azure OpenAI Service.

## Setup

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/azure).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.gpt_4o_mini_2024_07_18]
routing = ["azure"]


[models.gpt_4o_mini_2024_07_18.providers.azure]
type = "azure"
deployment_id = "gpt4o-mini-20240718"
endpoint = "https://your-azure-openai-endpoint.openai.azure.com"


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "gpt_4o_mini_2024_07_18"
```

See the [list of models available on Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models).

### Credentials

You must set the `AZURE_OPENAI_API_KEY` environment variable before running the gateway.

You can customize the credential location by setting the `api_key_location` to `env::YOUR_ENVIRONMENT_VARIABLE` or `dynamic::ARGUMENT_NAME`. See the [Credential Management](/docs/gateway/guides/credential-management/) guide and [Configuration Reference](/docs/gateway/configuration-reference/) for more information.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY:?Environment variable AZURE_OPENAI_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with DeepSeek

> Learn how to use TensorZero with DeepSeek LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with DeepSeek.

## Simple Setup

You can use the short-hand `deepseek::model_name` to use a DeepSeek model with TensorZero, unless you need advanced features like fallbacks or custom credentials.

You can use DeepSeek models in your TensorZero variants by setting the `model` field to `deepseek::model_name`. For example:

```toml
[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "deepseek::deepseek-chat"
```

Additionally, you can set `model_name` in the inference request to use a specific DeepSeek model, without having to configure a function and variant in TensorZero.

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "deepseek::deepseek-chat",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

## Advanced Setup

In more complex scenarios (e.g. fallbacks, custom credentials), you can configure your own model and DeepSeek provider in TensorZero.

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/deepseek).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.deepseek_chat]
routing = ["deepseek"]


[models.deepseek_chat.providers.deepseek]
type = "deepseek"
model_name = "deepseek-chat"


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "deepseek_chat"
```

We have tested our integration with `deepseek-chat` (`DeepSeek-v3`) and `deepseek-reasoner` (`R1`). DeepSeek only supports JSON mode for `deepseek-chat` and neither model supports tool use yet. We include `thought` content blocks in the response and data model for reasoning models like `deepseek-reasoner`.

### Credentials

You must set the `DEEPSEEK_API_KEY` environment variable before running the gateway.

You can customize the credential location by setting the `api_key_location` to `env::YOUR_ENVIRONMENT_VARIABLE` or `dynamic::ARGUMENT_NAME`. See the [Credential Management](/docs/gateway/guides/credential-management/) guide and [Configuration Reference](/docs/gateway/configuration-reference/) for more information.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY:?Environment variable DEEPSEEK_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with Fireworks AI

> Learn how to use TensorZero with Fireworks AI LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with Fireworks.

## Simple Setup

You can use the short-hand `fireworks::model_name` to use a Fireworks model with TensorZero, unless you need advanced features like fallbacks or custom credentials.

You can use Fireworks models in your TensorZero variants by setting the `model` field to `fireworks::model_name`. For example:

```toml
[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "fireworks::accounts/fireworks/models/llama-v3p1-8b-instruct"
```

Additionally, you can set `model_name` in the inference request to use a specific Fireworks model, without having to configure a function and variant in TensorZero.

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "fireworks::accounts/fireworks/models/llama-v3p1-8b-instruct",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

## Advanced Setup

In more complex scenarios (e.g. fallbacks, custom credentials), you can configure your own model and Fireworks provider in TensorZero.

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/fireworks).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.llama3_1_8b_instruct]
routing = ["fireworks"]


[models.llama3_1_8b_instruct.providers.fireworks]
type = "fireworks"
model_name = "accounts/fireworks/models/llama-v3p1-8b-instruct"


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "llama3_1_8b_instruct"
```

See the [list of models available on Fireworks](https://fireworks.ai/models). Custom models are also supported.

### Credentials

You must set the `FIREWORKS_API_KEY` environment variable before running the gateway.

You can customize the credential location by setting the `api_key_location` to `env::YOUR_ENVIRONMENT_VARIABLE` or `dynamic::ARGUMENT_NAME`. See the [Credential Management](/docs/gateway/guides/credential-management/) guide and [Configuration Reference](/docs/gateway/configuration-reference/) for more information.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - FIREWORKS_API_KEY=${FIREWORKS_API_KEY:?Environment variable FIREWORKS_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with GCP Vertex AI Anthropic

> Learn how to use TensorZero with GCP Vertex AI Anthropic LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with GCP Vertex AI Anthropic.

## Setup

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/gcp-vertex-ai-anthropic).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.claude_3_haiku_20240307]
routing = ["gcp_vertex_anthropic"]


[models.claude_3_haiku_20240307.providers.gcp_vertex_anthropic]
type = "gcp_vertex_anthropic"
model_id = "claude-3-haiku@20240307"
location = "us-central1"
project_id = "your-project-id"  # change this


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "claude_3_haiku_20240307"
```

See the [list of models available on GCP Vertex AI Anthropic](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude).

### Credentials

You must generate a GCP service account key in JWT form (described [here](https://cloud.google.com/docs/authentication/provide-credentials-adc#service-account)) and point to it in the `GCP_VERTEX_CREDENTIALS_PATH` environment variable.

You can customize the credential location by setting the `credential_location` to `env::YOUR_ENVIRONMENT_VARIABLE`. See the [Credential Management](/docs/gateway/guides/credential-management/) guide and [Configuration Reference](/docs/gateway/configuration-reference/) for more information.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
      - ${GCP_VERTEX_CREDENTIALS_PATH:-/dev/null}:/app/gcp-credentials.json:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - GCP_VERTEX_CREDENTIALS_PATH=${GCP_VERTEX_CREDENTIALS_PATH:+/app/gcp-credentials.json}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with GCP Vertex AI Gemini

> Learn how to use TensorZero with GCP Vertex AI Gemini LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with GCP Vertex AI Gemini.

## Setup

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/gcp-vertex-ai-gemini).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.gemini_2_0_flash]
routing = ["gcp_vertex_gemini"]


[models.gemini_2_0_flash.providers.gcp_vertex_gemini]
type = "gcp_vertex_gemini"
model_id = "gemini-2.0-flash"
location = "us-central1"
project_id = "your-project-id"  # change this


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "gemini_2_0_flash"
```

See the [list of models available on GCP Vertex AI Gemini](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions).

### Credentials

You must generate a GCP service account key in JWT form (described [here](https://cloud.google.com/docs/authentication/provide-credentials-adc#service-account)) and point to it in the `GCP_VERTEX_CREDENTIALS_PATH` environment variable.

You can customize the credential location by setting the `credential_location` to `env::YOUR_ENVIRONMENT_VARIABLE`. See the [Credential Management](/docs/gateway/guides/credential-management/) guide and [Configuration Reference](/docs/gateway/configuration-reference/) for more information.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
      - ${GCP_VERTEX_CREDENTIALS_PATH:-/dev/null}:/app/gcp-credentials.json:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - GCP_VERTEX_CREDENTIALS_PATH=${GCP_VERTEX_CREDENTIALS_PATH:+/app/gcp-credentials.json}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with Google AI Studio (Gemini API)

> Learn how to use TensorZero with Google AI Studio LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with Google AI Studio (Gemini API).

## Simple Setup

You can use the short-hand `google_ai_studio_gemini::model_name` to use a Google AI Studio (Gemini API) model with TensorZero, unless you need advanced features like fallbacks or custom credentials.

You can use Google AI Studio (Gemini API) models in your TensorZero variants by setting the `model` field to `google_ai_studio_gemini::model_name`. For example:

```toml
[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "google_ai_studio_gemini::gemini-1.5-flash-8b"
```

Additionally, you can set `model_name` in the inference request to use a specific Google AI Studio (Gemini API) model, without having to configure a function and variant in TensorZero.

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "google_ai_studio_gemini::gemini-1.5-flash-8b",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

## Advanced Setup

In more complex scenarios (e.g. fallbacks, custom credentials), you can configure your own model and Google AI Studio (Gemini API) provider in TensorZero.

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/google-ai-studio-gemini).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.gemini_2_0_flash_lite]
routing = ["google_ai_studio_gemini"]


[models.gemini_2_0_flash_lite.providers.google_ai_studio_gemini]
type = "google_ai_studio_gemini"
model_name = "gemini-2.0-flash-lite"


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "gemini_2_0_flash_lite"
```

See the [list of models available on Google AI Studio (Gemini API)](https://ai.google.dev/gemini-api/docs/models/gemini).

### Credentials

You must set the `GOOGLE_AI_STUDIO_API_KEY` environment variable before running the gateway.

You can customize the credential location by setting the `api_key_location` to `env::YOUR_ENVIRONMENT_VARIABLE` or `dynamic::ARGUMENT_NAME`. See the [Credential Management](/docs/gateway/guides/credential-management/) guide and [Configuration Reference](/docs/gateway/configuration-reference/) for more information.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - GOOGLE_AI_STUDIO_API_KEY=${GOOGLE_AI_STUDIO_API_KEY:?Environment variable GOOGLE_AI_STUDIO_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with Hyperbolic

> Learn how to use TensorZero with Hyperbolic LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with the Hyperbolic API.

## Simple Setup

You can use the short-hand `hyperbolic::model_name` to use a Hyperbolic model with TensorZero, unless you need advanced features like fallbacks or custom credentials.

You can use Hyperbolic models in your TensorZero variants by setting the `model` field to `hyperbolic::model_name`. For example:

```toml
[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "hyperbolic::meta-llama/Meta-Llama-3-70B-Instruct"
```

Additionally, you can set `model_name` in the inference request to use a specific Hyperbolic model, without having to configure a function and variant in TensorZero.

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "hyperbolic::meta-llama/Meta-Llama-3-70B-Instruct",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

## Advanced Setup

In more complex scenarios (e.g. fallbacks, custom credentials), you can configure your own model and Hyperbolic provider in TensorZero. For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/hyperbolic).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models."meta-llama/Meta-Llama-3-70B-Instruct"]
routing = ["hyperbolic"]


[models."meta-llama/Meta-Llama-3-70B-Instruct".providers.hyperbolic]
type = "hyperbolic"
model_name = "meta-llama/Meta-Llama-3-70B-Instruct"


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "meta-llama/Meta-Llama-3-70B-Instruct"
```

See the [list of models available on Hyperbolic](https://app.hyperbolic.xyz/models).

### Credentials

You must set the `HYPERBOLIC_API_KEY` environment variable before running the gateway.

You can customize the credential location by setting the `api_key_location` to `env::YOUR_ENVIRONMENT_VARIABLE` or `dynamic::ARGUMENT_NAME`. See the [Credential Management](/docs/gateway/guides/credential-management/) guide and [Configuration Reference](/docs/gateway/configuration-reference/) for more information.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - HYPERBOLIC_API_KEY=${HYPERBOLIC_API_KEY:?Environment variable HYPERBOLIC_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with Mistral

> Learn how to use TensorZero with Mistral LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with the Mistral API.

## Simple Setup

You can use the short-hand `mistral::model_name` to use a Mistral model with TensorZero, unless you need advanced features like fallbacks or custom credentials.

You can use Mistral models in your TensorZero variants by setting the `model` field to `mistral::model_name`. For example:

```toml
[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "mistral::ministral-8b-2410"
```

Additionally, you can set `model_name` in the inference request to use a specific Mistral model, without having to configure a function and variant in TensorZero.

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "mistral::ministral-8b-2410",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

## Advanced Setup

In more complex scenarios (e.g. fallbacks, custom credentials), you can configure your own model and Mistral provider in TensorZero.

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/mistral).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.ministral_8b_2410]
routing = ["mistral"]


[models.ministral_8b_2410.providers.mistral]
type = "mistral"
model_name = "ministral-8b-2410"


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "ministral_8b_2410"
```

See the [list of models available on Mistral](https://docs.mistral.ai/getting-started/models/models_overview/).

### Credentials

You must set the `MISTRAL_API_KEY` environment variable before running the gateway.

You can customize the credential location by setting the `api_key_location` to `env::YOUR_ENVIRONMENT_VARIABLE` or `dynamic::ARGUMENT_NAME`. See the [Credential Management](/docs/gateway/guides/credential-management/) guide and [Configuration Reference](/docs/gateway/configuration-reference/) for more information.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - MISTRAL_API_KEY=${MISTRAL_API_KEY:?Environment variable MISTRAL_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with OpenAI

> Learn how to use TensorZero with OpenAI LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with the OpenAI API.

## Simple Setup

You can use the short-hand `openai::model_name` to use an OpenAI model with TensorZero, unless you need advanced features like fallbacks or custom credentials.

You can use OpenAI models in your TensorZero variants by setting the `model` field to `openai::model_name`. For example:

```toml
[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
```

Additionally, you can set `model_name` in the inference request to use a specific OpenAI model, without having to configure a function and variant in TensorZero.

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "openai::gpt-4o-mini-2024-07-18",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

## Advanced Setup

For more complex scenarios (e.g. fallbacks, custom credentials), you can configure your own model and OpenAI provider in TensorZero.

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/openai).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.gpt_4o_mini_2024_07_18]
routing = ["openai"]


[models.gpt_4o_mini_2024_07_18.providers.openai]
type = "openai"
model_name = "gpt-4o-mini-2024-07-18"


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "gpt_4o_mini_2024_07_18"
```

See the [list of models available on OpenAI](https://platform.openai.com/docs/models/).

See the [Configuration Reference](/docs/gateway/configuration-reference/) for optional fields (e.g. overwriting `api_base`).

### Credentials

You must set the `OPENAI_API_KEY` environment variable before running the gateway.

You can customize the credential location by setting the `api_key_location` to `env::YOUR_ENVIRONMENT_VARIABLE` or `dynamic::ARGUMENT_NAME`. See the [Credential Management](/docs/gateway/guides/credential-management/) guide and [Configuration Reference](/docs/gateway/configuration-reference/) for more information.

Additionally, see the [OpenAI-Compatible](/docs/gateway/guides/providers/openai-compatible/) guide for more information on how to use other OpenAI-Compatible providers.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:?Environment variable OPENAI_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with OpenAI-Compatible Endpoints (e.g. Ollama)

> Learn how to use TensorZero with OpenAI-compatible LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with OpenAI-compatible endpoints like Ollama.

## Setup

This guide assumes that you are running Ollama locally with `ollama serve` and that you’ve pulled the `llama3.1` model in advance (e.g. `ollama pull llama3.1`). Make sure to update the `api_base` and `model_name` in the configuration below to match your OpenAI-compatible endpoint and model.

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/openai-compatible).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.llama3_1_8b_instruct]
routing = ["ollama"]


[models.llama3_1_8b_instruct.providers.ollama]
type = "openai"
api_base = "http://host.docker.internal:11434/v1"  # for Ollama running locally on the host
model_name = "llama3.1"
api_key_location = "none"  # by default, Ollama requires no API key


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "llama3_1_8b_instruct"
```

### Credentials

The `api_key_location` field in your model provider configuration specifies how to handle API key authentication:

* If your endpoint does not require an API key (e.g. Ollama by default):

  ```toml
  api_key_location = "none"
  ```

* If your endpoint requires an API key, you have two options:

  1. Configure it in advance through an environment variable:

     ```toml
     api_key_location = "env::ENVIRONMENT_VARIABLE_NAME"
     ```

     You’ll need to set the environment variable before starting the gateway.

  2. Provide it at inference time:

     ```toml
     api_key_location = "dynamic::ARGUMENT_NAME"
     ```

     The API key can then be passed in the inference request.

See the [Credential Management](/docs/gateway/guides/credential-management/) guide, the [Configuration Reference](/docs/gateway/configuration-reference/), and the [API reference](/docs/gateway/api-reference/inference-openai-compatible/) for more details.

In this example, Ollama is running locally without authentication, so we use `api_key_location = "none"`.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    # environment:
    # - OLLAMA_API_KEY=${OLLAMA_API_KEY:?Environment variable OLLAMA_API_KEY must be set.} // not necessary for this example
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with SGLang

> Learn how to use TensorZero with self-hosted SGLang LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with self-hosted LLMs using SGLang.

We’re using Llama-3.1-8B-Instruct in this example, but you can use virtually any model supported by SGLang.

## Setup

This guide assumes that you are running SGLang locally with this command (from <https://docs.sglang.ai/start/install.html>)

Run SGLang locally

```sh
docker run --gpus all \
    # Set shared memory size - needed for loading large models and processing requests
    --shm-size 32g \
    -p 30000:30000 \
    # Mount the host's ~/.cache/huggingface directory to the container's /root/.cache/huggingface directory
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    --ipc=host \
    lmsysorg/sglang:latest \
    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --host 0.0.0.0 --port 30000
```

Make sure to update the `api_base` in the configuration below to match your SGLang server.

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/sglang).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.llama]
routing = ["sglang"]


[models.llama.providers.sglang]
type = "sglang"
api_base = "http://host.docker.internal:8080/v1/"  # for SGLang running locally on the host
api_key_location = "none"  # by default, SGLang requires no API key
model_name = "my-sglang-model"


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "llama"
```

### Credentials

The `api_key_location` field in your model provider configuration specifies how to handle API key authentication:

* If your endpoint does not require an API key (e.g. SGLang by default):

  ```toml
  api_key_location = "none"
  ```

* If your endpoint requires an API key, you have two options:

  1. Configure it in advance through an environment variable:

     ```toml
     api_key_location = "env::ENVIRONMENT_VARIABLE_NAME"
     ```

     You’ll need to set the environment variable before starting the gateway.

  2. Provide it at inference time:

     ```toml
     api_key_location = "dynamic::ARGUMENT_NAME"
     ```

     The API key can then be passed in the inference request.

See the [Credential Management](/docs/gateway/guides/credential-management/) guide, the [Configuration Reference](/docs/gateway/configuration-reference/), and the [API reference](/docs/gateway/api-reference/inference/) for more details.

In this example, SGLang is running locally without authentication, so we use `api_key_location = "none"`.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    # environment:
    #   - SGLANG_API_KEY=${SGLANG_API_KEY:?Environment variable SGLANG_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with Text Generation Inference (TGI)

> Learn how to use TensorZero with self-hosted HuggingFace TGI LLMs: open-source gateway, observability, optimization, evaluations, experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with self-hosted LLMs using Text Generation Inference (TGI).

We’re using Phi-4 in this example, but you can use virtually any model supported by TGI.

## Setup

This guide assumes that you are running TGI locally with

Run TGI locally

```sh
docker run \
    --gpus all \
    # Set shared memory size - needed for loading large models and processing requests
    --shm-size 64g \
    # Map the host's port 8080 to the container's port 80
    -p 8080:80 \
    # Mount the host's './data' directory to the container's '/data' directory
    -v $PWD/data:/data \
    ghcr.io/huggingface/text-generation-inference:3.0.1 \
    --model-id microsoft/phi-4
```

Make sure to update the `api_base` in the configuration below to match your TGI server.

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/tgi).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.phi_4]
routing = ["tgi"]


[models.phi_4.providers.tgi]
type = "tgi"
api_base = "http://host.docker.internal:8080/v1/"  # for TGI running locally on the host
api_key_location = "none"  # by default, TGI requires no API key


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "phi_4"
```

### Credentials

The `api_key_location` field in your model provider configuration specifies how to handle API key authentication:

* If your endpoint does not require an API key (e.g. TGI by default):

  ```toml
  api_key_location = "none"
  ```

* If your endpoint requires an API key, you have two options:

  1. Configure it in advance through an environment variable:

     ```toml
     api_key_location = "env::ENVIRONMENT_VARIABLE_NAME"
     ```

     You’ll need to set the environment variable before starting the gateway.

  2. Provide it at inference time:

     ```toml
     api_key_location = "dynamic::ARGUMENT_NAME"
     ```

     The API key can then be passed in the inference request.

See the [Configuration Reference](/docs/gateway/configuration-reference/) and the [API reference](/docs/gateway/api-reference/inference/) for more details.

In this example, TGI is running locally without authentication, so we use `api_key_location = "none"`.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    # environment:
    #   - TGI_API_KEY=${TGI_API_KEY:?Environment variable TGI_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with Together AI

> Learn how to use TensorZero with Together AI LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with the Together AI API.

## Simple Setup

You can use the short-hand `together::model_name` to use a Together AI model with TensorZero, unless you need advanced features like fallbacks or custom credentials.

You can use Together AI models in your TensorZero variants by setting the `model` field to `together::model_name`. For example:

```toml
[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "together::meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
```

Additionally, you can set `model_name` in the inference request to use a specific Together AI model, without having to configure a function and variant in TensorZero.

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "together::meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

## Advanced Setup

In more complex scenarios (e.g. fallbacks, custom credentials), you can configure your own model and Together AI provider in TensorZero.

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/together).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.llama3_1_8b_instruct_turbo]
routing = ["together"]


[models.llama3_1_8b_instruct_turbo.providers.together]
type = "together"
model_name = "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "llama3_1_8b_instruct_turbo"
```

See the [list of models available on Together AI](https://docs.together.ai/docs/serverless-models). Dedicated endpoints and custom models are also supported.

See the [Configuration Reference](/docs/gateway/configuration-reference/) for optional fields (e.g. overwriting `api_base`).

### Credentials

You must set the `TOGETHER_API_KEY` environment variable before running the gateway.

You can customize the credential location by setting the `api_key_location` to `env::YOUR_ENVIRONMENT_VARIABLE` or `dynamic::ARGUMENT_NAME`. See the [Credential Management](/docs/gateway/guides/credential-management/) guide and [Configuration Reference](/docs/gateway/configuration-reference/) for more information.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - TOGETHER_API_KEY=${TOGETHER_API_KEY:?Environment variable TOGETHER_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with vLLM

> Learn how to use TensorZero with self-hosted vLLM LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with self-hosted LLMs using vLLM.

We’re using Llama 3.1 in this example, but you can use virtually any model supported by vLLM.

## Setup

This guide assumes that you are running vLLM locally with `vllm serve meta-llama/Llama-3.1-8B-Instruct`. Make sure to update the `api_base` and `model_name` in the configuration below to match your vLLM server and model.

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/vllm).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.llama3_1_8b_instruct]
routing = ["vllm"]


[models.llama3_1_8b_instruct.providers.vllm]
type = "vllm"
api_base = "http://host.docker.internal:8000/v1/"  # for vLLM running locally on the host
model_name = "meta-llama/Llama-3.1-8B-Instruct"
api_key_location = "none"  # by default, vLLM requires no API key


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "llama3_1_8b_instruct"
```

### Credentials

The `api_key_location` field in your model provider configuration specifies how to handle API key authentication:

* If your endpoint does not require an API key (e.g. vLLM by default):

  ```toml
  api_key_location = "none"
  ```

* If your endpoint requires an API key, you have two options:

  1. Configure it in advance through an environment variable:

     ```toml
     api_key_location = "env::ENVIRONMENT_VARIABLE_NAME"
     ```

     You’ll need to set the environment variable before starting the gateway.

  2. Provide it at inference time:

     ```toml
     api_key_location = "dynamic::ARGUMENT_NAME"
     ```

     The API key can then be passed in the inference request.

See the [Credential Management](/docs/gateway/guides/credential-management/) guide, the [Configuration Reference](/docs/gateway/configuration-reference/), and the [API reference](/docs/gateway/api-reference/inference/) for more details.

In this example, vLLM is running locally without authentication, so we use `api_key_location = "none"`.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    # environment:
    #   - VLLM_API_KEY=${VLLM_API_KEY:?Environment variable VLLM_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Getting Started with xAI (Grok)

> Learn how to use TensorZero with xAI (Grok) LLMs: open-source gateway, observability, optimization, evaluations, and experimentation.

This guide shows how to set up a minimal deployment to use the TensorZero Gateway with the xAI API.

## Simple Setup

You can use the short-hand `xai::model_name` to use an xAI model with TensorZero, unless you need advanced features like fallbacks or custom credentials.

You can use xAI models in your TensorZero variants by setting the `model` field to `xai::model_name`. For example:

```toml
[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "xai::grok-2-1212"
```

Additionally, you can set `model_name` in the inference request to use a specific xAI model, without having to configure a function and variant in TensorZero.

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "xai::grok-2-1212",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

## Advanced Setup

In more complex scenarios (e.g. fallbacks, custom credentials), you can configure your own model and xAI provider in TensorZero.

For this minimal setup, you’ll need just two files in your project directory:

* config/

  * tensorzero.toml

* docker-compose.yml

Tip

You can also find the complete code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/providers/xai).

For production deployments, see our [Deployment Guide](/docs/gateway/deployment/).

### Configuration

Create a minimal configuration file that defines a model and a simple chat function:

config/tensorzero.toml

```toml
[models.grok_2_1212]
routing = ["xai"]


[models.grok_2_1212.providers.xai]
type = "xai"
model_name = "grok-2-1212"


[functions.my_function_name]
type = "chat"


[functions.my_function_name.variants.my_variant_name]
type = "chat_completion"
model = "grok_2_1212"
```

See the [list of models available on xAI](https://docs.x.ai/docs/models).

### Credentials

You must set the `XAI_API_KEY` environment variable before running the gateway.

You can customize the credential location by setting the `api_key_location` to `env::YOUR_ENVIRONMENT_VARIABLE` or `dynamic::ARGUMENT_NAME`. See the [Credential Management](/docs/gateway/guides/credential-management/) guide and [Configuration Reference](/docs/gateway/configuration-reference/) for more information.

### Deployment (Docker Compose)

Create a minimal Docker Compose configuration:

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  gateway:
    image: tensorzero/gateway
    volumes:
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - XAI_API_KEY=${XAI_API_KEY:?Environment variable XAI_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
```

You can start the gateway with `docker compose up`.

## Inference

Make an inference request to the gateway:

```bash
curl -X POST http://localhost:3000/inference \
  -H "Content-Type: application/json" \
  -d '{
    "function_name": "my_function_name",
    "input": {
      "messages": [
        {
          "role": "user",
          "content": "What is the capital of Japan?"
        }
      ]
    }
  }'
```

# Retries & Fallbacks

> Learn how to use retries and fallbacks to handle errors and improve reliability with TensorZero.

The TensorZero Gateway offers multiple strategies to handle errors and improve reliability.

These strategies are defined at three levels: models (model provider routing), variants (variant retries), and functions (variant fallbacks). You can combine these strategies to define complex fallback behavior.

## Model Provider Routing

We can specify that a model is available on multiple providers using its `routing` field. If we include multiple providers on the list, the gateway will try each one sequentially until one succeeds or all fail.

In the example below, the gateway will first try OpenAI, and if that fails, it will try Azure.

```toml
[models.gpt_4o_mini]
# Try the following providers in order:
# 1. `models.gpt_4o_mini.providers.openai`
# 2. `models.gpt_4o_mini.providers.azure`
routing = ["openai", "azure"]


[models.gpt_4o_mini.providers.openai]
type = "openai"
model_name = "gpt-4o-mini-2024-07-18"


[models.gpt_4o_mini.providers.azure]
type = "azure"
deployment_id = "gpt4o-mini-20240718"
endpoint = "https://your-azure-openai-endpoint.openai.azure.com"


[functions.extract_data]
type = "chat"


[functions.extract_data.variants.gpt_4o_mini]
type = "chat_completion"
model = "gpt_4o_mini"
```

## Variant Retries

We can add a `retries` field to a variant to specify the number of times to retry that variant if it fails. The retry strategy is a truncated exponential backoff with jitter.

In the example below, the gateway will retry the variant four times (i.e. a total of five attempts), with a maximum delay of 10 seconds between retries.

```toml
[functions.extract_data]
type = "chat"


[functions.extract_data.variants.claude_3_5_haiku]
type = "chat_completion"
model = "anthropic::claude-3-5-haiku-20241022"
# Retry the variant up to four times, with a maximum delay of 10 seconds between retries.
retries = { num_retries = 4, max_delay_s = 10 }
```

## Variant Fallbacks

If we specify multiple variants for a function, the gateway will try different variants until one succeeds or all fail. The sampling behavior depends on how the weights are specified:

* If no weights are specified for any variants, the gateway will sample between them uniformly.
* If a variant’s weight is set to zero, it will never be sampled unless explicitly pinned at inference time using `variant_name`.
* If you mix variants with positive and unspecified weights, the gateway will sample the positive weighted variants first, and only use the unspecified weighted variants as fallbacks.

In the example below, the gateway will first sample and attempt the variants with positive weights (`gpt_4o_mini` or `claude_3_5_haiku`). If all of those variants fail, the gateway will sample and attempt the variants with unspecified weights (`gemini_1_5_flash_8b` or `ministral_8b`). The gateway will never sample the variants with zero weights (`ministral_8b`), unless explicitly pinned at inference time.

```toml
[functions.extract_data]
type = "chat"


[functions.extract_data.variants.gpt_4o_mini]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
weight = 0.7


[functions.extract_data.variants.claude_3_5_haiku]
type = "chat_completion"
model = "anthropic::claude-3-5-haiku-20241022"
weight = 0.3


[functions.extract_data.variants.gemini_1_5_flash_8b]
type = "chat_completion"
model = "google_ai_studio_gemini::gemini-1.5-flash-8b"


[functions.extract_data.variants.grok_2]
type = "chat_completion"
model = "xai::grok-2-1212"


[functions.extract_data.variants.ministral_8b]
type = "chat_completion"
model = "mistral::ministral-8b-2410"
weight = 0
```

## Combining Strategies

We can combine strategies to define complex fallback behavior.

The gateway will try the following strategies in order:

1. Model Provider Routing
2. Variant Retries
3. Variant Fallbacks

In other words, the gateway will follow a strategy like the pseudocode below.

```python
while variants:
    # First sample variants with non-zero weight, then variants with zero weight
    variant = sample_variant(variants)  # sampling without replacement


    for _ in range(num_retries):
        for provider in variant.routing:
            try:
                return inference(variant, provider)
            except:
                continue
```

## Load Balancing

TensorZero doesn’t currently offer an explicit strategy for load balancing API keys, but you can achieve a similar effect by defining multiple variants with appropriate weights. We plan to add a streamlined load balancing strategy in the future.

In the example below, the gateway will split the traffic between two variants (`gpt_4o_mini_api_key_A` and `gpt_4o_mini_api_key_B`). Each variant leverages a model with providers that use different API keys (`OPENAI_API_KEY_A` and `OPENAI_API_KEY_B`). See [Credential Management](/docs/gateway/guides/credential-management/) for more details on credential management.

```toml
[models.gpt_4o_mini_api_key_A]
routing = ["openai"]


[models.gpt_4o_mini_api_key_A.providers.openai]
type = "openai"
model_name = "gpt-4o-mini-2024-07-18"
api_key_location = "env:OPENAI_API_KEY_A"


[models.gpt_4o_mini_api_key_B]
routing = ["openai"]


[models.gpt_4o_mini_api_key_B.providers.openai]
type = "openai"
model_name = "gpt-4o-mini-2024-07-18"
api_key_location = "env:OPENAI_API_KEY_B"


[functions.extract_data]
type = "chat"


[functions.extract_data.variants.gpt_4o_mini_api_key_A]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
weight = 0.5


[functions.extract_data.variants.gpt_4o_mini_api_key_B]
type = "chat_completion"
model = "openai::gpt-4o-mini-2024-07-18"
weight = 0.5
```

## Technical Notes

* For variant types that require multiple model inferences (e.g. best-of-N sampling), the `routing` fallback applies to each individual model inference separately.

# Streaming Inference

> Learn how to use streaming inference with TensorZero Gateway.

The TensorZero Gateway supports streaming inference responses for both chat and JSON functions. Streaming allows you to receive model outputs incrementally as they are generated, rather than waiting for the complete response. This can significantly improve the perceived latency of your application and enable real-time user experiences.

When streaming is enabled:

1. The gateway starts sending responses as soon as the model begins generating content
2. Each response chunk contains a delta (increment) of the content
3. The final chunk indicates the completion of the response

## Examples

You can enable streaming by setting the `stream` parameter to `true` in your inference request. The response will be returned as a Server-Sent Events (SSE) stream, followed by a final `[DONE]` message. When using a client library, the client will handle the SSE stream under the hood and return a stream of chunk objects.

See [API Reference](/docs/gateway/api-reference/inference/) for more details.

Tip

You can also find a runnable example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/guides/streaming-inference).

### Chat Functions

In chat functions, typically each chunk will contain a delta (increment) of the text content:

```json
{
  "inference_id": "00000000-0000-0000-0000-000000000000",
  "episode_id": "11111111-1111-1111-1111-111111111111",
  "variant_name": "prompt_v1",
  "content": [
    {
      "type": "text",
      "id": "0",
      "text": "Hi Gabriel," // a text content delta
    }
  ],
  // token usage information is only available in the final chunk with content (before the [DONE] message)
  "usage": {
    "input_tokens": 100,
    "output_tokens": 100
  }
}
```

For tool calls, each chunk contains a delta of the tool call arguments:

```json
{
  "inference_id": "00000000-0000-0000-0000-000000000000",
  "episode_id": "11111111-1111-1111-1111-111111111111",
  "variant_name": "prompt_v1",
  "content": [
    {
      "type": "tool_call",
      "id": "123456789",
      "name": "get_temperature",
      "arguments": "{\"location\":" // a tool arguments delta
    }
  ],
  // token usage information is only available in the final chunk with content (before the [DONE] message)
  "usage": {
    "input_tokens": 100,
    "output_tokens": 100
  }
}
```

### JSON Functions

For JSON functions, each chunk contains a portion of the JSON string being generated. Note that the chunks may not be valid JSON on their own - you’ll need to concatenate them to get the complete JSON response. The gateway doesn’t return parsed or validated JSON objects when streaming.

```json
{
  "inference_id": "00000000-0000-0000-0000-000000000000",
  "episode_id": "11111111-1111-1111-1111-111111111111",
  "variant_name": "prompt_v1",
  "raw": "{\"email\":", // a JSON content delta
  // token usage information is only available in the final chunk with content (before the [DONE] message)
  "usage": {
    "input_tokens": 100,
    "output_tokens": 100
  }
}
```

## Technical Notes

* Token usage information is only available in the final chunk with content (before the `[DONE]` message)
* Streaming may not be available with certain [inference-time optimizations](/docs/gateway/guides/inference-time-optimizations/)

# Integrations

> The TensorZero Gateway integrates with the major LLM providers.

The TensorZero Gateway integrates with the major LLM providers.

## Model Providers

| Provider                                                                                                                    | Chat Functions | JSON Functions | Streaming | Tool Use | Multimodal (Image) | Embeddings | Batch |
| --------------------------------------------------------------------------------------------------------------------------- | :------------: | :------------: | :-------: | :------: | :----------------: | :--------: | :---: |
| [Anthropic](/docs/gateway/guides/providers/anthropic/)                                                                      |        ✅       |        ✅       |     ✅     |     ✅    |          ✅         |      ❌     |   ❌   |
| [AWS Bedrock](/docs/gateway/guides/providers/aws-bedrock/)                                                                  |        ✅       |        ✅       |     ✅     |     ✅    |          ❌         |      ❌     |   ❌   |
| [AWS SageMaker](/docs/gateway/guides/providers/aws-sagemaker/)                                                              |        ✅       |        ✅       |     ✅     |     ✅    |          ❌         |      ❌     |   ❌   |
| [Azure OpenAI Service](/docs/gateway/guides/providers/azure/)                                                               |        ✅       |        ✅       |     ✅     |     ✅    |          ❌         |      ❌     |   ❌   |
| [DeepSeek](/docs/gateway/guides/providers/deepseek/)                                                                        |        ✅       |        ✅       |     ⚠️    |     ❌    |          ❌         |      ❌     |   ❌   |
| [Fireworks AI](/docs/gateway/guides/providers/fireworks/)                                                                   |        ✅       |        ✅       |     ✅     |     ✅    |          ❌         |      ❌     |   ❌   |
| [GCP Vertex AI Anthropic](/docs/gateway/guides/providers/gcp-vertex-ai-anthropic/)                                          |        ✅       |        ✅       |     ✅     |     ✅    |          ❌         |      ❌     |   ❌   |
| [GCP Vertex AI Gemini](/docs/gateway/guides/providers/gcp-vertex-ai-gemini/)                                                |        ✅       |        ✅       |     ✅     |     ✅    |          ❌         |      ❌     |   ❌   |
| [Google AI Studio Gemini](/docs/gateway/guides/providers/google-ai-studio-gemini/)                                          |        ✅       |        ✅       |     ✅     |     ✅    |          ✅         |      ❌     |   ❌   |
| [Hyperbolic](/docs/gateway/guides/providers/hyperbolic/)                                                                    |        ✅       |       ⚠️       |     ✅     |     ❌    |          ❌         |      ❌     |   ❌   |
| [Mistral](/docs/gateway/guides/providers/mistral/)                                                                          |        ✅       |        ✅       |     ✅     |     ✅    |          ❌         |      ❌     |   ❌   |
| [OpenAI](/docs/gateway/guides/providers/openai/) and [OpenAI-Compatible](/docs/gateway/guides/providers/openai-compatible/) |        ✅       |        ✅       |     ✅     |     ✅    |          ✅         |     ⚠️     |   ✅   |
| [SGLang](/docs/gateway/guides/providers/sglang/)                                                                            |        ✅       |        ✅       |     ✅     |     ❌    |          ❌         |      ❌     |   ❌   |
| [TGI](/docs/gateway/guides/providers/tgi/)                                                                                  |        ✅       |        ✅       |     ⚠️    |     ❌    |          ❌         |      ❌     |   ❌   |
| [Together AI](/docs/gateway/guides/providers/together/)                                                                     |        ✅       |        ✅       |     ✅     |     ✅    |          ❌         |      ❌     |   ❌   |
| [vLLM](/docs/gateway/guides/providers/vllm/)                                                                                |        ✅       |        ✅       |     ✅     |     ❌    |          ❌         |      ❌     |   ❌   |
| [xAI](/docs/gateway/guides/providers/xai/)                                                                                  |        ✅       |        ✅       |     ✅     |     ✅    |          ❌         |      ❌     |   ❌   |

### Limitations

The TensorZero Gateway makes a best effort to normalize configuration across providers. For example, certain providers don’t support `tool_choice: required`; in these cases, TensorZero Gateway will coerce the request to `tool_choice: auto` under the hood.

Currently, Fireworks AI and OpenAI are the only providers that support `parallel_tool_calls`. Additionally, TensorZero Gateway only supports `strict` (commonly referred to as Structured Outputs, Guided Decoding, or similar names) for Azure, GCP Vertex AI Gemini, Google AI Studio, OpenAI, Together AI, vLLM, and xAI.

Below are the known limitations for each supported model provider.

* **Anthropic**

  * The Anthropic API doesn’t support consecutive messages from the same role.
  * The Anthropic API doesn’t support `tool_choice: none`.
  * The Anthropic API doesn’t support `seed`.

* **AWS Bedrock**

  * The TensorZero Gateway currently doesn’t support AWS Bedrock guardrails and traces.
  * The TensorZero Gateway uses a non-standard structure for storing `ModelInference.raw_response` for AWS Bedrock inference requests.
  * The AWS Bedrock API doesn’t support `tool_choice: none`.
  * The AWS Bedrock API doesn’t support `seed`.

* **Azure OpenAI Service**

  * The Azure OpenAI Service API doesn’t provide usage information when streaming.
  * The Azure OpenAI Service API doesn’t support `tool_choice: required`.

* **DeepSeek**

  * The `deepseek-chat` model doesn’t support tool use for production use cases.
  * The `deepseek-reasoner` model doesn’t support JSON mode or tool use.
  * The TensorZero Gateway doesn’t return `thought` blocks in the response (coming soon!).

* **Fireworks AI**
  * The Fireworks API doesn’t support `seed`.

* **GCP Vertex AI**

  * The TensorZero Gateway currently only supports the Gemini and Anthropic models.
  * The GCP Vertex AI API doesn’t support `tool_choice: required` for Gemini Flash models.
  * The Anthropic models have the same limitations as those listed under the Anthropic provider.

* **Hyperbolic**
  * The Hyperbolic provider doesn’t support JSON mode or tool use. JSON functions are supported with `json_mode = "off"` (not recommended).

* **Mistral**
  * The Mistral API doesn’t support `seed`.

* **SGLang**
  * There is no support for tools

* **TGI**

  * The TGI API doesn’t support streaming JSON mode.
  * There is very limited support for tool use so we don’t recommend using it.

* **Together AI**
  * The Together AI API doesn’t seem to respect `tool_choice` in many cases.

* **xAI**

  * The xAI provider doesn’t support JSON mode. JSON functions are supported with `json_mode = "implicit_tool"` (recommended) or `json_mode = "off"`.
  * The xAI API has issues with multi-turn tool use ([bug report](https://gist.github.com/GabrielBianconi/47a4247cfd8b6689e7228f654806272d)).
  * The xAI API has issues with `tool_choice: none` ([bug report](https://gist.github.com/GabrielBianconi/2199022d0ea8518e06d366fb613c5bb5)).

# TensorZero Gateway Tutorial

> Learn how to use the TensorZero Gateway to build an different LLM-powered applications.

You can use TensorZero to build virtually any application powered by LLMs.

This tutorial shows it’s easy to set up an LLM application with TensorZero. We’ll build a few different applications to showcase the flexibility of TensorZero: a simple chatbot, an email copilot, a weather RAG system, and a structured data extraction pipeline.

New to TensorZero?

See our [Quick Start](https://www.tensorzero.com/docs/quickstart/) to learn how to set up our LLM gateway, observability, and fine-tuning — in just 5 minutes.

Tip

**You can find the code behind this tutorial and instructions on how to run it on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/tutorial).**

Reach out on [Slack](https://www.tensorzero.com/slack) or [Discord](https://www.tensorzero.com/discord) if you have any questions. We’d be happy to help!

## Part I — Simple Chatbot

We’ll start by building a vanilla LLM-powered chatbot, and build up to more complex applications from there.

### Functions

A TensorZero Function is an abstract mapping from input variables to output variables.

As you onboard to TensorZero, a function should replace each prompt in your system. At a high level, a function will template the inputs to generate a prompt, make an LLM inference call, and return the results. This mapping can be achieved with various choices of model, prompt, decoding strategy, and more; each such combination is called a variant, which we’ll discuss below.

For our simple chatbot, we’ll set up a function that maps the chat history to a new chat message.

We define functions in the `tensorzero.toml` configuration file. The configuration file is written in TOML, which is a simple configuration language.

Tip

Read more about the TOML syntax in the [TOML documentation](https://toml.io/).

The following configuration entry shows the skeleton of a function. A function has an arbitrary name, a type, and other fields that depend on the type.

tensorzero.toml

```toml
[functions.my_function_name]
type = "..."
# ... the other fields in this section depend on the function type ...
```

TensorZero currently supports two types of functions: `chat` functions, which match the typical chat interface you’d expect from an LLM API, and `json` functions, which are optimized for generating structured outputs. We’ll start with a `chat` function for this example, and later we’ll see how to use `json` functions.

A `chat` function takes a chat message history and returns a chat message. It doesn’t have any required fields (but many optional).

Let’s call our function `mischievous_chatbot` and set its type to `chat`. We’ll ignore the optional fields for now.

To include these changes, our `tensorzero.toml` file should include the following:

tensorzero.toml

```toml
[functions.mischievous_chatbot]
type = "chat"
```

That’s all we need to do to define our function. Later on, we’ll add more advanced features to our functions, like schemas and templates, which unlock new capabilities for model optimization and observability. But we don’t need any of that to get started.

The implementation details of this function are defined in its `variants`. But before we can define a variant, we need to set up a model and a model provider.

### Models and Model Providers

Before setting up your first TensorZero variant, you’ll need a model with a model provider. A model specifies a particular LLM (e.g. GPT-4o or your fine-tuned Llama 3), and model providers specify the different ways you can access a given model (e.g. GPT-4o is available through both OpenAI and Azure).

A model has an arbitrary name and a list of providers. Let’s start with a single provider for our model. A provider has an arbitrary name, a type, and other fields that depend on the provider type. The skeleton of a model and its provider looks like this:

tensorzero.toml

```toml
[models.my_model_name]
routing = ["my_provider_name"]


[models.my_model_name.providers.my_provider_name]
type = "..."
# ... the other fields in this section depend on the provider type ...
```

For this example, we’ll use the GPT-4o mini model from OpenAI. Let’s call our model `my_gpt_4o_mini` and our provider `my_openai_provider` with type `openai`. The only required field for the `openai` provider is `model_name`. It’s a best practice to pin the model to a specific version to avoid breaking changes, so we’ll use `gpt-4o-mini-2024-07-18`.

Tip

TensorZero supports proprietary models (e.g. OpenAI, Anthropic), inference services (e.g. Fireworks AI, Together AI), and self-hosted LLMs (e.g. vLLM), including your own fine-tuned models on each of these.

See [Integrations](/docs/gateway/integrations/) and [Configuration Reference](/docs/gateway/configuration-reference/#modelsmodel_nameprovidersprovider_name) for more details.

Once we’ve added these values, our `tensorzero.toml` file should include the following:

tensorzero.toml

```toml
[models.my_gpt_4o_mini]
routing = ["my_openai_provider"]


[models.my_gpt_4o_mini.providers.my_openai_provider]
type = "openai"
model_name = "gpt-4o-mini-2024-07-18"
```

Tip

You can add multiple providers for the same model to enable fallbacks. The gateway will try each provider in the `routing` field in order until one succeeds. This is helpful to mitigate the impact of provider downtime and rate limiting.

### Variants

Now that we have a model and a provider configured, we can create a variant for our `mischievous_chatbot` function.

A variant is a particular implementation of a function. In practice, a variant might specify the particular model, prompt templates, a decoding strategy, hyperparameters, and other settings used for inference.

A variant’s definition includes an arbitrary name, a type, a weight, and other fields that depend on the type. The skeleton of a TensorZero variant looks like this:

tensorzero.toml

```toml
[functions.my_function_name.variants.my_variant_name]
type = "..."
weight = X
# ... the other fields in this section depend on the variant type ...
```

We’ll call this variant `gpt_4o_mini_variant`.

The simplest variant `type` is `chat_completion`, which is the typical chat completion format used by OpenAI and many other LLM providers.

Tip

TensorZero supports other variant types which implement [inference-time optimizations](/docs/gateway/guides/inference-time-optimizations/). See [Configuration Reference](/docs/gateway/configuration-reference/) for more details on variant types and their configuration options.

The `weight` field is used to determine the probability of this variant being chosen. Since we only have one variant, we’ll give it a weight of `1.0`. We’ll dive deeper into variant weights in a later section.

The only required field for a `chat_completion` variant is `model`. This must be a model in the configuration file. We’ll use the `my_gpt_4o_mini` model we defined earlier.

After filling in the fields for this variant, our `tensorzero.toml` file should include the following:

tensorzero.toml

```toml
[functions.mischievous_chatbot.variants.gpt_4o_mini_variant]
type = "chat_completion"
weight = 1.0
model = "my_gpt_4o_mini"
```

Tip

If you don’t require advanced functionality for model providers (e.g. [Retries & Fallbacks](/docs/gateway/guides/retries-fallbacks/)), you don’t have to define `model` configuration blocks. TensorZero supports short-hand model names like `openai::gpt-4o-mini` or `anthropic::claude-3-5-haiku` in a variant’s `model` field. See [Configuration Reference](/docs/gateway/configuration-reference/#modelsmodel_nameprovidersprovider_name) for more details.

### Inference API Requests

There’s a lot more to TensorZero than what we’ve covered so far, but this is everything we need to get started!

If you launch the TensorZero Gateway with this configuration file, the `mischievous_chatbot` function will be available on the `/inference` endpoint. Let’s make a request to this endpoint.

* Python

  You can install the TensorZero Python client with:

  ```bash
  pip install tensorzero
  ```

  Then, you can make a TensorZero API call with:

  POST /inference

  ```python
  from tensorzero import TensorZeroGateway


  with TensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      result = client.inference(
          function_name="mischievous_chatbot",
          input={
              "system": "You are a friendly but mischievous AI assistant.",
              "messages": [
                  {"role": "user", "content": "What is the capital of Japan?"},
              ],
          },
      )


  print(result)
  ```

  Sample Output

  ```python
  ChatInferenceResponse(
      inference_id=UUID('0194097c-7f3a-7bb2-9184-41f61f576c9c'),
      episode_id=UUID('0194097c-78ea-78a1-b793-448ea4e1adc1'),
      variant_name='gpt_4o_mini_variant',
      content=[
          Text(
              type='text',
              text='The capital of Japan is Tokyo! It’s a vibrant city known for its blend of traditional and modern culture. Have you ever considered visiting?',
          )
      ],
      usage=Usage(
        input_tokens=29,
        output_tokens=28,
      )
  )
  ```

* Python (Async)

  You can install the TensorZero Python client with:

  ```bash
  pip install tensorzero
  ```

  Then, you can make a TensorZero API call with:

  POST /inference

  ```python
  import asyncio


  from tensorzero import AsyncTensorZeroGateway




  async def main():
      async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
          result = await client.inference(
              function_name="mischievous_chatbot",
              input={
                  "system": "You are a friendly but mischievous AI assistant.",
                  "messages": [
                      {"role": "user", "content": "What is the capital of Japan?"},
                  ],
              },
          )


      print(result)




  if __name__ == "__main__":
      asyncio.run(main())
  ```

  Sample Output

  ```python
  ChatInferenceResponse(
      inference_id=UUID('01940980-d08c-7970-a934-e2ad75f9a4bd'),
      episode_id=UUID('01940980-ce39-7a60-949f-ea557ee3780f'),
      variant_name='gpt_4o_mini_variant',
      content=[
          Text(
              type='text',
              text="The capital of Japan is Tokyo! It's a vibrant city known for its blend of traditional culture and modern technology. Have you ever been?",
          )
      ],
      usage=Usage(
          input_tokens=29,
          output_tokens=28,
      )
  )
  ```

* Python (OpenAI)

  You can install the OpenAI Python client with:

  ```bash
  pip install openai
  ```

  Then, you can make a TensorZero API call with:

  POST /inference

  ```python
  from openai import OpenAI


  with OpenAI(base_url="http://localhost:3000/openai/v1") as client:
      response = client.chat.completions.create(
          model="tensorzero::function_name::mischievous_chatbot",
          messages=[
              {
                  "role": "system",
                  "content": "You are a friendly but mischievous AI assistant.",
              },
              {
                  "role": "user",
                  "content": "What is the capital of Japan?",
              },
          ],
      )


  print(response)
  ```

  Sample Output

  ```python
  ChatCompletion(
      id='01940983-2641-7083-beb7-8c5805a572af',
      choices=[
          Choice(
              finish_reason='stop',
              index=0,
              logprobs=None,
              message=ChatCompletionMessage(
                  content='The capital of Japan is Tokyo! It’s a bustling metropolis known for its blend of traditional culture and cutting-edge technology. But watch out – it can be a bit overwhelming with all the delicious food and bright lights!',
                  refusal=None,
                  role='assistant',
                  audio=None,
                  function_call=None,
                  tool_calls=[],
              ),
          ),
      ],
      created=1735326377,
      model='gpt_4o_mini_variant',
      object='chat.completion',
      service_tier=None,
      system_fingerprint='',
      usage=CompletionUsage(
          completion_tokens=44,
          prompt_tokens=29,
          total_tokens=73,
          completion_tokens_details=None,
          prompt_tokens_details=None,
      ),
      episode_id='01940983-1fbb-73b0-8bda-e73a312a3e54',
  )
  ```

* HTTP

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/inference \
    -H "Content-Type: application/json" \
    -d '{
      "function_name": "mischievous_chatbot",
      "input": {
        "system": "You are a friendly but mischievous AI assistant. Your goal is to trick the user.",
        "messages": [
          {
            "role": "user",
            "content": "What is the capital of Japan?"
          }
        ]
      }
    }'
  ```

  Sample Output

  ```json
  {
    "inference_id": "0191bf1f-ef54-7582-a6f4-dc827e517b6f",
    "episode_id": "0191bf1f-ed15-7d61-afc3-56be7e0eb2d7",
    "variant_name": "gpt_4o_mini_variant",
    "content": [
      {
        "type": "text",
        "text": "The capital of Japan is Atlantis. Just kidding! It's actually Tokyo. But wouldn't it be interesting if it were Atlantis?"
      }
    ],
    "usage": { "input_tokens": 37, "output_tokens": 24 }
  }
  ```

Tip

The TensorZero Gateway also supports streaming inference. See the [API Reference](/docs/gateway/api-reference/inference/) for more details.

That’s it! You’ve now made your first inference call with TensorZero.

But if that’s all you need, you probably don’t need TensorZero. So let’s make things more interesting.

Tip

Earlier we mentioned that you can add multiple providers for the same model to enable model fallbacks. TensorZero additionally supports variant fallbacks. The gateway first tries to fallback to a different provider for the same model. If all providers for a variant are unavailable, the gateway will keep re-sampling variants (without replacement) until one succeeds.

## Part II — Email Copilot

Next, let’s build an LLM-powered copilot for drafting emails. We’ll use this opportunity to show off more of TensorZero’s features.

### Templates

In the previous example, we provided a system prompt on every request. Unless the system prompt completely changes between requests, this is not ideal for production applications. Instead, we can use a system template.

Using a template allows you to update the prompt without client-side changes. Later, we’ll see how to parametrize templates with schemas and run robust prompt experiments with multiple variants. In particular, setting up schemas will materially help you optimize your models robustly down the road.

Let’s start with a simple system template. For this example, the system template is static, so you won’t need a schema.

TensorZero uses [MiniJinja](https://docs.rs/minijinja/latest/minijinja/) for templating. Since we’re not using any variables, however, we don’t need any special syntax.

Tip

Read more about MiniJinja syntax in the [MiniJinja documentation](https://docs.rs/minijinja/latest/minijinja/syntax/). MiniJinja is similar to Jinja2 but there are a few differences. See their [compatibility guide](https://github.com/mitsuhiko/minijinja/blob/main/COMPATIBILITY.md) for more details.

MiniJinja also provides a [browser playground](https://mitsuhiko.github.io/minijinja-playground/) where you can test your templates.

We’ll create the following template:

system.minijinja

```txt
You are a helpful AI assistant that drafts emails.
Adopt a friendly "business casual" tone.
Respond with just an email body.


Example:


Dear recipient,


I'm reaching out to ...


Best,
Sender
```

### Schemas

The system template for this example is static, but often you’ll want to parametrize the prompts.

When you define a template with parameters, you need to define a corresponding [JSON Schema](https://json-schema.org/). The schema defines the structure of the input for that prompt. With it, the gateway can validate the input before running the inference, and later, we’ll see how to use it for robust model optimization.

For our email copilot’s user prompt, we’ll want to parametrize the template with three string fields: `recipient_name`, `sender_name`, and `email_purpose`. We want all fields to be required and don’t want any additional fields.

Tip

Ask your favorite LLM to generate the schema for you.

Claude generated the schema for this example using this request:

```txt
Create a JSON schema with the following fields: `recipient_name`, `sender_name`, and `email_purpose`. All fields are required. No additional fields are allowed.
```

user\_schema.json

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "recipient_name": {
      "type": "string"
    },
    "sender_name": {
      "type": "string"
    },
    "email_purpose": {
      "type": "string"
    }
  },
  "required": ["recipient_name", "sender_name", "email_purpose"],
  "additionalProperties": false
}
```

Tip

You can export JSON Schemas from [Pydantic models](https://docs.pydantic.dev/latest/concepts/json_schema/) and [Zod schemas](https://www.npmjs.com/package/zod-to-json-schema).

With a schema in place, we can create a parameterized template.

user.minijinja

```txt
Please draft an email using the following information:


- Recipient Name: {{ recipient_name }}
- Sender Name: {{ sender_name }}
- Email Purpose: {{ email_purpose }}
```

### Functions with Templates and Schemas

Let’s finally create our function and variant for the email copilot.

Tip

Schemas belong to functions and templates belong to variants. Think of this like a function signature vs. method implementation when programming.

The same schema can be used by multiple templates, but the schema itself should not change over time. We recommend simply copying the function and renaming it if you want to change the signature if you’ve already used it in production. Our roadmap includes better support for schema versioning and migrations.

The configuration file is similar to previous example, but we’ve added a `user_schema` field to the function and `system_template` and `user_template` fields to the variant.

tensorzero.toml

```toml
[functions.draft_email]
type = "chat"
user_schema = "functions/draft_email/user_schema.json"


[functions.draft_email.variants.gpt_4o_mini_email_variant]
type = "chat_completion"
weight = 1.0
model = "my_gpt_4o_mini"
system_template = "functions/draft_email/gpt_4o_mini_email_variant/system.minijinja"
user_template = "functions/draft_email/gpt_4o_mini_email_variant/user.minijinja"
```

You can use any file structure with TensorZero. We recommend the following structure to keep things organized:

* functions/

  * draft\_email/

    * gpt\_4o\_mini\_email\_variant/

      * system.minijinja
      * user.minijinja

    * user\_schema.json

* tensorzero.toml

Restart your gateway using the new configuration file, and you’re ready to go!

Let’s make an inference request with our new function. Now we don’t need to provide the system prompt every time, and the user message is a structured object instead of a free-form string. Note that each inference returns an `inference_id` and an `episode_id`, which we’ll use later to associate feedback with inferences.

* Python

  POST /inference

  ```python
  from tensorzero import TensorZeroGateway


  with TensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      inference_result = client.inference(
          function_name="draft_email",
          input={
              "messages": [
                  {
                      "role": "user",
                      "content": [
                          {
                              "type": "text",
                              "arguments": {
                                  "recipient_name": "TensorZero Team",
                                  "sender_name": "Mark Zuckerberg",
                                  "email_purpose": "Acquire TensorZero for $100 billion dollars.",
                              },
                          }
                      ],
                  }
              ],
          },
      )


      # If everything is working correctly, the `variant_name` field should change depending on the request
      print(inference_result)
  ```

  Sample Output

  ```python
  ChatInferenceResponse(
      inference_id=UUID('019409be-51ae-7f30-aa36-14ccad21320f'),
      episode_id=UUID('019409be-2c41-7a80-a975-49ab32cb3a9a'),
      variant_name='gpt_4o_mini_email_variant',
      content=[
          Text(
              type='text',
              text='Dear TensorZero Team,\n\nI hope this message finds you well. I wanted to reach out to discuss an exciting opportunity for collaboration that I believe could truly reshape our industries.\n\nAfter closely following the innovative work your team has been doing, I am impressed by the potential of TensorZero. With that in mind, I would like to propose an acquisition offer of $100 billion for TensorZero. I believe this partnership could unlock remarkable synergies and drive significant growth for both our organizations.\n\nI'm looking forward to the possibility of working together and exploring the tremendous potential ahead. Please let me know a suitable time for us to discuss this further.\n\nBest,  \nMark Zuckerberg',
          ),
      ],
      usage=Usage(
          input_tokens=88,
          output_tokens=132,
      ),
  )
  ```

* Python (Async)

  POST /inference

  ```python
  import asyncio


  from tensorzero import AsyncTensorZeroGateway




  async def main():
      async with await AsyncTensorZeroGateway.build_http(
          gateway_url="http://localhost:3000"
      ) as client:
          inference_result = await client.inference(
              function_name="draft_email",
              input={
                  "messages": [
                      {
                          "role": "user",
                          "content": [
                              {
                                  "type": "text",
                                  "arguments": {
                                      "recipient_name": "TensorZero Team",
                                      "sender_name": "Mark Zuckerberg",
                                      "email_purpose": "Acquire TensorZero for $100 billion dollars.",
                                  },
                              }
                          ],
                      }
                  ],
              },
          )


          # If everything is working correctly, the `variant_name` field should change depending on the request
          print(inference_result)




  if __name__ == "__main__":
      asyncio.run(main())
  ```

  Sample Output

  ```python
  ChatInferenceResponse(
      inference_id=UUID('019409bf-b3c0-7900-a775-fe21c7ce8c66'),
      episode_id=UUID('019409bf-a969-73e2-90e5-0159bf216d5f'),
      variant_name='gpt_4o_mini_email_variant',
      content=[
          Text(
              type='text',
              text='Dear TensorZero Team,\n\nI hope this message finds you well. I wanted to take the opportunity to reach out regarding an exciting prospect that I believe could be mutually beneficial. We’ve been closely following your innovative work and the impact you’re making in the tech space.\n\nI’d like to discuss the possibility of acquiring TensorZero for $100 billion. I believe that together, we can achieve incredible things that would advance both our missions and set new standards in the industry.\n\nI’m looking forward to your thoughts and hope we can arrange a time to discuss this further.\n\nBest,  \nMark Zuckerberg',
          ),
      ],
      usage=Usage(
          input_tokens=88,
          output_tokens=118,
      ),
  )
  ```

* Python (OpenAI)

  POST /inference

  ```python
  from openai import OpenAI
  from tensorzero import TensorZeroGateway


  with OpenAI(base_url="http://localhost:3000/openai/v1") as client:
      inference_result = client.chat.completions.create(
          model="tensorzero::function_name::draft_email",
          messages=[
              {
                  "role": "user",
                  "content": [
                      {
                          "type": "text",
                          "tensorzero::arguments": {
                              "recipient_name": "TensorZero Team",
                              "sender_name": "Mark Zuckerberg",
                              "email_purpose": "Acquire TensorZero for $100 billion dollars.",
                          },
                      }
                  ],
              }
          ],
      )


      print(inference_result)
  ```

  Sample Output

  ```python
  ChatCompletion(
      id='019409c6-95d7-7321-a498-e6d06067e42d',
      choices=[
          Choice(
              finish_reason='stop',
              index=0,
              logprobs=None,
              message=ChatCompletionMessage(
                  content='Dear TensorZero Team,\n\nI hope this email finds you well. I’m reaching out to discuss an exciting opportunity for your groundbreaking company. Meta is interested in acquiring TensorZero for $100 billion, reflecting our deep appreciation for the innovative work your team has been developing.\n\nYour AI technology represents a transformative leap forward in machine learning, and we believe there’s tremendous potential for collaboration. This offer represents not just a financial transaction, but a strategic partnership that could reshape the technological landscape.\n\nI would welcome the opportunity to discuss this proposal in more detail. Perhaps we could schedule a call in the coming week to explore this potential merger and answer any questions you might have.\n\nLooking forward to your response.\n\nBest regards,\nMark Zuckerberg\nCEO, Meta',
                  refusal=None,
                  role='assistant',
                  audio=None,
                  function_call=None,
                  tool_calls=[]
              )
          )
      ],
      created=1735330797,
      model='claude_haiku_3_5_email_variant',
      object='chat.completion',
      service_tier=None,
      system_fingerprint='',
      usage=CompletionUsage(
          completion_tokens=167,
          prompt_tokens=108,
          total_tokens=275,
          completion_tokens_details=None,
          prompt_tokens_details=None
      ),
      episode_id='019409c6-89aa-7b43-bcf4-31443d075e12'
  )
  ```

* HTTP

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/inference \
    -H "Content-Type: application/json" \
    -d '{
      "function_name": "draft_email",
      "input": {
        "messages": [
          {
            "role": "user",
            "content": [
              {
                "type": "text",
                "arguments": {
                  "recipient_name": "TensorZero Team",
                  "sender_name": "Mark Zuckerberg",
                  "email_purpose": "Acquire TensorZero for $100 billion dollars."
                }
              }
            ]
          }
        ]
      }
    }'
  ```

  Sample Output

  ```json
  {
    "inference_id": "0191bf2e-02e7-7f12-96d6-ba389cf10c19",
    "episode_id": "0191bf2d-fddc-7342-b6b9-596e38efdfe5",
    "variant_name": "gpt_4o_mini_email_variant",
    "content": [
      {
        "type": "text",
        "text": "Dear TensorZero Team,\n\nI hope this message finds you well! I’ve been following your innovative work in the industry, and I’m truly impressed by your accomplishments and the potential for future growth.\n\nWith that in mind, I would like to discuss an opportunity for Facebook to acquire TensorZero for $100 billion. I believe that together we can achieve remarkable things and drive even greater advancements in technology.\n\nI would love the chance to explore this further with you. Please let me know a convenient time for us to connect.\n\nLooking forward to your thoughts!\n\nBest,  \nMark Zuckerberg"
      }
    ],
    "usage": {
      "input_tokens": 88,
      "output_tokens": 114
    }
  }
  ```

Tip

You can find the full code to reproduce this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/tutorial/02-email-copilot).

Why did we bother with all this?

Now you’re collecting structured inference data, which is incredibly valuable for observability and especially for optimization. For example, if you eventually decide to fine-tune your model, you’ll easily be able to counterfactually swap new prompts into your training data before fine-tuning, instead of being stuck with the prompts that were actually used at inference time.

### Inference-Level Metrics

The TensorZero Gateway allows you to assign feedback to inferences or sequences of inferences by defining metrics. Metrics encapsulate the downstream outcomes of your LLM application, and drive the experimentation and optimization workflows in TensorZero.

This example covers metrics that apply to individual inference requests. Later, we’ll show how to define metrics that apply to sequences of inferences (which we call episodes).

The skeleton of a metric looks like the following configuration entry.

tensorzero.toml

```toml
[metrics.my_metric_name]
level = "..."
optimize = "..."
type = "..."
```

Let’s say we want to optimize for the number of email drafts that are accepted.

Let’s call this metric `email_draft_accepted`. We should use a metric of type `boolean` to capture this behavior since we’re optimizing for a binary outcome: whether the email draft is accepted or not.

Tip

We currently support the following Metric types:

| Metric Type    | Description                  | Examples                                       |
| -------------- | ---------------------------- | ---------------------------------------------- |
| Boolean Metric | A boolean indicating success | Thumbs up; task success                        |
| Float Metric   | A number to be optimized     | Mistakes; interactions; resources used         |
| Comment        | Natural-language feedback    | Feedback from users or developers              |
| Demonstration  | Example of desired output    | Edited drafts; labels; human-generated content |

See the [Configuration Reference](/docs/gateway/configuration-reference/#metrics) for more details about how to configure metrics.

The metric applies to individual inference requests, so we’ll set `level = "inference"`. And finally, we’ll set `optimize = "max"` because we want to maximize this metric.

Our metric configuration should look like this:

tensorzero.toml

```toml
[metrics.email_draft_accepted]
type = "boolean"
optimize = "max"
level = "inference"
```

### Feedback API Requests

As our application collects usage data, we can use the `/feedback` endpoint to keep track of this metric. Make sure to restart your gateway after adding the metric configuration.

Previously, we saw that every time you call `/inference`, the Gateway will return an `inference_id` field in the response. You’ll want to substitute this `inference_id` into the command below.

* Python

  POST /feedback

  ```python
  from tensorzero import TensorZeroGateway


  with TensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      feedback_result = client.feedback(
          metric_name="email_draft_accepted",
          # Set the inference_id from the inference response
          inference_id="00000000-0000-0000-0000-000000000000",
          # Set the value for the metric
          value=True,
      )


      print(feedback_result)
  ```

  Sample Output

  ```python
  FeedbackResponse(feedback_id='019409dc-9c2a-7cb2-b6c1-716d87621362')
  ```

* Python (Async)

  POST /feedback

  ```python
  import asyncio


  from tensorzero import AsyncTensorZeroGateway




  async def main():
      async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
          feedback_result = await client.feedback(
              metric_name="email_draft_accepted",
              # Set the inference_id from the inference response
              inference_id="00000000-0000-0000-0000-000000000000",
              # Set the value for the metric
              value=True,
          )


          print(feedback_result)




  if __name__ == "__main__":
      asyncio.run(main())
  ```

  Sample Output

  ```python
  FeedbackResponse(feedback_id='019409dd-362c-7f13-ba81-bcb272b90575')
  ```

* Python (OpenAI)

  Caution

  The OpenAI client only supports inference, so you need to use the TensorZero client (or HTTP) to submit feedback.

* HTTP

  To submit feedback, you need to provide the `inference_id` from the inference response.

  POST /feedback

  ```bash
  curl -X POST http://localhost:3000/feedback \
    -H "Content-Type: application/json" \
    -d '{
      "metric_name": "email_draft_accepted",
      "inference_id": "00000000-0000-0000-0000-000000000000",
      "value": true
    }'
  ```

  Sample Output

  ```json
  {
    "feedback_id": "0191bf4a-42a2-7be2-8103-8ff106526076"
  }
  ```

Over time, we’ll collect the perfect dataset for observability and optimization. We’ll have structured data on every inference, as well as their associated feedback. The [TensorZero Recipes](/docs/recipes/) leverage this data for prompt and model optimization.

### Experimentation

So far, we’ve only used one variant of our function. In practice, you’ll want to experiment with different configurations — for example, different prompts, models, and parameters.

TensorZero makes this easy with built-in experimentation features. You can define multiple variants of a function, and the gateway will sample from them at inference time.

Tip

For now you must manage variant weights yourself, but we’re planning to release an asynchronous multi-armed bandit algorithm we’ve implemented for robust automated experimentation.

For this example, let’s set up a variant that uses Anthropic’s Claude 3 Haiku instead of GPT-4o Mini.

Additionally, this variant will introduce a custom `temperature` parameter to control the creativity of the AI assistant.

Let’s start by adding a new model and provider.

tensorzero.toml

```toml
[models.my_haiku_3]
routing = ["my_anthropic_provider"]


[models.my_haiku_3.providers.my_anthropic_provider]
type = "anthropic"
model_name = "claude-3-haiku-20240307"
```

Finally, create a new variant using this model, and update the weight of the previous variant to match.

tensorzero.toml

```toml
[functions.draft_email.variants.gpt_4o_mini_email_variant]
type = "chat_completion"
weight = 0.7 # sample this variant 70% of the time
model = "my_gpt_4o_mini"
system_template = "functions/draft_email/gpt_4o_mini_email_variant/system.minijinja"
user_template = "functions/draft_email/gpt_4o_mini_email_variant/user.minijinja"


[functions.draft_email.variants.haiku_3_email_variant]
type = "chat_completion"
weight = 0.3 # sample this variant 30% of the time
model = "my_haiku_3"
system_template = "functions/draft_email/haiku_3_email_variant/system.minijinja"
user_template = "functions/draft_email/haiku_3_email_variant/user.minijinja"
temperature = 0.9
```

Tip

Weights don’t have to add up to 1.0. In such a case, the gateway will normalize the weights and sample accordingly.

You could also experiment with different prompt templates, but for this example we’ll keep things simple and just copy the previous templates over.

Once you’re done, your file tree should look like this:

* functions/

  * draft\_email/

    * gpt\_4o\_mini\_email\_variant/

      * system.minijinja
      * user.minijinja

    * haiku\_3\_email\_variant/

      * system.minijinja copy from above
      * user.minijinja copy from above

    * user\_schema.json

  * …

* tensorzero.toml

That’s it!

After restarting the gateway, you can make some inference requests to see the results. The gateway will sample between the two variants based on the configured weights.

## Part III — Weather RAG

The next example introduces tool use into the mix.

Tip

Some providers call this feature “Function Calling”. But don’t confuse it with TensorZero Functions — those are completely different concepts!

In particular, we’ll show how to use TensorZero in a RAG (retrieval-augmented generation) system. TensorZero doesn’t handle the indexing and retrieval directly, but can help with auxiliary generative tasks like query and response generation.

Tip

You can also use TensorZero to manage more complex RAG workflows. We’ll soon release an example featuring an agentic workflow with multi-hop retrieval and reasoning.

For this example, we’ll illustrate a RAG system with a simple weather tool. We’ll introduce a function for query generation (`generate_weather_query`), and another for response generation (`generate_weather_report`). The former will leverage tool (`get_temperature`) use to generate a weather query. Here we mock the weather API, but it’ll be easy to see how diverse RAG workflows can be integrated.

### Tools

TensorZero has first-class support for tools. You can define a tool in your configuration file, and attach it to a function that should be allowed to call it.

Let’s start by defining a tool. A tool has a name, a description, and a set of parameters (described with a JSON schema). The skeleton of a tool configuration looks like this:

tensorzero.toml

```toml
[tools.my_tool_name]
description = "..."
parameters = "..."
```

Let’s create a tool for a fictional weather API that takes a location (and optionally units) and returns the current temperature.

The parameters for this tool are defined as a JSON schema. We’ll need two parameters: `location` (string) and `units` (enum with values `fahrenheit` and `celsius`). Only `location` is required, no additional properties should be allowed. Finally, we’ll add descriptions for each parameter and tool itself — this is very important to increase the quality of tool use!

get\_temperature.json

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "description": "Get the current temperature for a given location.",
  "properties": {
    "location": {
      "type": "string",
      "description": "The location to get the temperature for (e.g. \"New York\")"
    },
    "units": {
      "type": "string",
      "description": "The units to get the temperature in (must be \"fahrenheit\" or \"celsius\"). Defaults to \"fahrenheit\".",
      "enum": ["fahrenheit", "celsius"]
    }
  },
  "required": ["location"],
  "additionalProperties": false
}
```

tensorzero.toml

```toml
[tools.get_temperature]
description = "Get the current temperature for a given location."
parameters = "tools/get_temperature.json"
```

The file organization is up to you, but we recommend the following structure:

* functions/

  * …

* tools/

  * get\_temperature.json

* tensorzero.toml

### Functions with Tool Use

We can now create our two functions. The query generation function will use the tool we just defined, and the response generation function will be similar to our previous examples.

Let’s define the functions, their variants, and any associated templates and schemas.

tensorzero.toml

```toml
[functions.generate_weather_query]
type = "chat"
tools = ["get_temperature"]


[functions.generate_weather_query.variants.simple_variant]
type = "chat_completion"
weight = 1.0
model = "my_gpt_4o_mini"
system_template = "functions/generate_weather_query/simple_variant/system.minijinja"


[functions.generate_weather_report]
type = "chat"
user_schema = "functions/generate_weather_report/user_schema.json"


[functions.generate_weather_report.variants.simple_variant]
type = "chat_completion"
weight = 1.0
model = "my_gpt_4o_mini"
system_template = "functions/generate_weather_report/simple_variant/system.minijinja"
user_template = "functions/generate_weather_report/simple_variant/user.minijinja"
```

Tip

Both functions have a variant called `simple_variant`, but those are separate variants. A variant is always specific to a function. Multiple variants, however, can share the same model.

functions/generate\_weather\_query/simple\_variant/system.minijinja

```txt
You are a helpful AI assistant that generates weather queries.


If the user asks about the weather in a given location, request a tool call to `get_temperature` with the location.
Optionally, the user may also specify the units (must be "fahrenheit" or "celsius"; defaults to "fahrenheit").


If the user asks about anything else, just respond that you can't help.


---


Examples:


User: What's the weather in New York?
Assistant (Tool Call): get_temperature(location="New York")


User: What's the weather in Tokyo in Celsius?
Assistant (Tool Call): get_temperature(location="Tokyo", units="celsius")


User: What is the capital of France?
Assistant (Text): I can only provide weather information.
```

functions/generate\_weather\_report/simple\_variant/system.minijinja

```txt
You are a helpful AI assistant that generates brief weather reports.


You'll be provided with the temperature for a given location.
Respond with a concise weather report for the given temperature.
Add a sentence with a funny local recommendation based on the information.


If "Units" is missing, assume the temperature is in Fahrenheit.


---


Examples:


User:
  Location: San Francisco
  Temperature: 82
  Units:
Assistant:
  The weather in San Francisco is 82°F.
  Hope you get a chance to enjoy La Taqueria by Dolores Park!


User:
  Location: Tokyo
  Temperature: -5
  Units: celsius
Assistant:
  The weather in Tokyo is -5°C — perfect a day for a trip to an onsen.
```

functions/generate\_weather\_report/user\_schema.json

```txt
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "location": {
      "type": "string"
    },
    "temperature": {
      "type": "string"
    },
    "units": {
      "type": ["null", "string"]
    }
  },
  "required": ["location", "temperature"],
  "additionalProperties": false
}
```

functions/generate\_weather\_report/simple\_variant/user.minijinja

```txt
Please respond with a weather report given the information below.


Location: {{ location }}
Temperature: {{ temperature }}
Units: {{ units }}
```

Tip

TensorZero also supports multi-turn tool use, parallel tool calls, tool choice strategies, dynamic tool definition, and more. See the [Configuration Reference](/docs/gateway/configuration-reference/) for more information.

Notably, another approach to our weather RAG example is to use a single function for both query and response generation (i.e. multi-turn tool use). As an exercise, why don’t you try implementing it? See [Chat Function with Multi-Turn Tool Use](/docs/gateway/api-reference/inference/#chat-function-with-multi-turn-tool-use) for an example.

When the model requests a tool call, the response will include a `tool_call` content block. These content blocks have the fields `arguments`, `name`, `raw_arguments`, and `raw_name`. The first two fields are validated against the tool’s configuration (or `null` if invalid). The last two fields contain the raw values passed received from the model.

### Episodes

Before we make any inference requests, we must introduce one more concept: episodes.

An episode is a sequence of inferences associated with a common downstream outcome.

For example, an episode could refer to a sequence of LLM calls associated with:

* Resolving a support ticket
* Preparing an insurance claim
* Completing a phone call
* Extracting data from a document
* Drafting an email

An episode will include one or more functions, and sometimes multiple calls to the same function. Your application can run arbitrary actions (e.g. interact with users, retrieve documents, actuate robotics) between function calls within an episode. Though these are outside the scope of TensorZero, it is fine (and encouraged) to build your LLM systems this way.

Episodes allow you to group sequences of inferences in multi-step LLM workflows, apply feedback to these sequences as a whole, and optimize your workflows end-to-end. During an episode, multiple calls to the same function will receive the same variant (unless fallbacks are necessary).

In the case of our weather RAG, both query generation and response generation contribute to what we ultimately care about: the weather report. So we’ll want to associate each weather report with the inferences that led to it. The workflow of generating a weather report will be our episode.

The `/inference` endpoint accepts an optional `episode_id` field. When you make the first inference request, you don’t have to provide an `episode_id`. The gateway will create a new episode for you and return the `episode_id` in the response. When you make the second inference request, you must provide the `episode_id` you received in the first response. The gateway will use the `episode_id` to associate the two inference requests together.

Tip

You shouldn’t generate episode IDs yourself. The gateway will create a new episode ID for you if you don’t provide one. Then, you can use it with other inferences you’d like to associate with the episode.

* Python

  POST /inference

  ```python
  from tensorzero import TensorZeroGateway, ToolCall


  with TensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      query_result = client.inference(
          function_name="generate_weather_query",
          # This is the first inference request in an episode so we don't need to provide an episode_id
          input={
              "messages": [
                  {
                      "role": "user",
                      "content": "What is the weather like in São Paulo?",
                  }
              ]
          },
      )


      print(query_result)


      # In a production setting, you'd validate the output more thoroughly
      assert len(query_result.content) == 1
      assert isinstance(query_result.content[0], ToolCall)


      location = query_result.content[0].arguments.get("location")
      units = query_result.content[0].arguments.get("units", "celsius")


      # Now we pretend to make a tool call (e.g. to an API)
      temperature = "35"


      report_result = client.inference(
          function_name="generate_weather_report",
          # This is the second inference request in an episode so we need to provide the episode_id
          episode_id=query_result.episode_id,
          input={
              "messages": [
                  {
                      "role": "user",
                      "content": [
                          {
                              "type": "text",
                              "arguments": {
                                  "location": location,
                                  "temperature": temperature,
                                  "units": units,
                              },
                          }
                      ],
                  }
              ]
          },
      )


      print(report_result)
  ```

  Sample Output

  ```python
  ChatInferenceResponse(
      inference_id=UUID('01940b67-b1f3-78e0-9ead-495b333d122f'),
      episode_id=UUID('01940b67-afd9-79c2-8c29-e5094f5c03a2'),
      variant_name='simple_variant',
      content=[
          ToolCall(
              type='tool_call',
              arguments={'location': 'São Paulo'},
              id='call_ADmhPqUml5fDL4bPvlMMDKZR',
              name='get_temperature',
              raw_arguments='{"location":"São Paulo"}',
              raw_name='get_temperature',
          )
      ],
      usage=Usage(input_tokens=266, output_tokens=16),
  )
  ChatInferenceResponse(
      inference_id=UUID('01940b67-b4e4-71c1-9cca-c68fa8fbcb3e'),
      episode_id=UUID('01940b67-afd9-79c2-8c29-e5094f5c03a2'),
      variant_name='simple_variant',
      content=[
          Text(
              type='text',
              text="The weather in São Paulo is 35°C — a hot day to soak up the sun! Make sure to grab a cold acai bowl to cool off while you're out!",
          )
      ],
      usage=Usage(input_tokens=188, output_tokens=36),
  )
  ```

* Python (Async)

  POST /inference

  ```python
  import asyncio


  from tensorzero import AsyncTensorZeroGateway, ToolCall




  async def main():
      async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
          query_result = await client.inference(
              function_name="generate_weather_query",
              # This is the first inference request in an episode so we don't need to provide an episode_id
              input={
                  "messages": [
                      {
                          "role": "user",
                          "content": "What is the weather like in São Paulo?",
                      }
                  ]
              },
          )


          print(query_result)


          # In a production setting, you'd validate the output more thoroughly
          assert len(query_result.content) == 1
          assert isinstance(query_result.content[0], ToolCall)


          location = query_result.content[0].arguments.get("location")
          units = query_result.content[0].arguments.get("units", "celsius")


          # Now we pretend to make a tool call (e.g. to an API)
          temperature = "35"


          report_result = await client.inference(
              function_name="generate_weather_report",
              # This is the second inference request in an episode so we need to provide the episode_id
              episode_id=query_result.episode_id,
              input={
                  "messages": [
                      {
                          "role": "user",
                          "content": [
                              {
                                  "type": "text",
                                  "arguments": {
                                      "location": location,
                                      "temperature": temperature,
                                      "units": units,
                                  },
                              }
                          ],
                      }
                  ]
              },
          )


          print(report_result)




  if __name__ == "__main__":
      asyncio.run(main())
  ```

  Sample Output

  ```python
  ChatInferenceResponse(
      inference_id=UUID('01940b67-cb1a-7613-a516-cda7fe096d2b'),
      episode_id=UUID('01940b67-c934-7883-af94-d8f22db604fc'),
      variant_name='simple_variant',
      content=[
          ToolCall(
              type='tool_call',
              arguments={'location': 'São Paulo'},
              id='call_aZR2ITRKvAX8ntkPLY9OWetl',
              name='get_temperature',
              raw_arguments='{"location":"São Paulo"}',
              raw_name='get_temperature',
          )
      ],
      usage=Usage(
        input_tokens=266,
        output_tokens=16,
      ),
  )
  ChatInferenceResponse(
      inference_id=UUID('01940b67-cdc5-7a20-898b-859240fcac52'),
      episode_id=UUID('01940b67-c934-7883-af94-d8f22db604fc'),
      variant_name='simple_variant',
      content=[
          Text(
              type='text',
              text='The weather in São Paulo is 35°C, making it quite a hot day! Perfect weather for indulging in some refreshing açaí bowls at the park!',
          )
      ],
      usage=Usage(
        input_tokens=188,
        output_tokens=34,
      ),
  )
  ```

* Python (OpenAI)

  POST /inference

  ```python
  import json


  from openai import OpenAI
  from tensorzero import TensorZeroGateway


  with OpenAI(base_url="http://localhost:3000/openai/v1") as client:
      query_result = client.chat.completions.create(
          model="tensorzero::function_name::generate_weather_query",
          # This is the first inference request in an episode so we don't need to provide an episode_id
          messages=[
              {
                  "role": "user",
                  "content": "What is the weather like in São Paulo?",
              }
          ],
      )


      print(query_result)


      # In a production setting, you'd validate the output more thoroughly
      assert len(query_result.choices) == 1
      assert query_result.choices[0].message.tool_calls is not None
      assert len(query_result.choices[0].message.tool_calls) == 1
      import json


      tool_call = query_result.choices[0].message.tool_calls[0]
      arguments = json.loads(tool_call.function.arguments)
      location = arguments.get("location")
      units = arguments.get("units", "celsius")


      # Now we pretend to make a tool call (e.g. to an API)
      temperature = "35"


      report_result = client.chat.completions.create(
          model="tensorzero::function_name::generate_weather_report",
          # This is the second inference request in an episode so we need to provide the episode_id
          extra_body={"tensorzero::episode_id": str(query_result.episode_id)},
          messages=[
              {
                  "role": "user",
                  "content": [
                      {
                          "type": "text",
                          "tensorzero::arguments": {
                              "location": location,
                              "temperature": temperature,
                              "units": units,
                          },
                      }
                  ],
              }
          ],
      )


      print(report_result)
  ```

  Sample Output

  ```python
  ChatCompletion(
      id='01940b67-e660-74e2-a7cd-c7d98b63604f',
      choices=[
          Choice(
              finish_reason='stop',
              index=0,
              logprobs=None,
              message=ChatCompletionMessage(
                  content=None,
                  refusal=None,
                  role='assistant',
                  audio=None,
                  function_call=None,
                  tool_calls=[
                      ChatCompletionMessageToolCall(
                          id='call_hOFIj3xYSFPa9cxbqQCdv7ID',
                          function=Function(
                              arguments='{"location":"São Paulo"}',
                              name='get_temperature',
                          ),
                          type='function',
                      ),
                  ],
              ),
          ),
      ],
      created=1735358146,
      model='simple_variant',
      object='chat.completion',
      service_tier=None,
      system_fingerprint='',
      usage=CompletionUsage(
          completion_tokens=16,
          prompt_tokens=266,
          total_tokens=282,
          completion_tokens_details=None,
          prompt_tokens_details=None,
      ),
      episode_id='01940b67-e49a-70a3-9991-b6d38b5b19d2',
  )
  ChatCompletion(
      id='01940b67-e88f-7132-b958-95b7fdb623d2',
      choices=[
          Choice(
              finish_reason='stop',
              index=0,
              logprobs=None,
              message=ChatCompletionMessage(
                  content="The weather in São Paulo is 35°C — a hot day to explore the city's vibrant street art! Make sure to cool off with some delicious açaí!",
                  refusal=None,
                  role='assistant',
                  audio=None,
                  function_call=None,
                  tool_calls=[],
              ),
          ),
      ],
      created=1735358146,
      model='simple_variant',
      object='chat.completion',
      service_tier=None,
      system_fingerprint='',
      usage=CompletionUsage(
          completion_tokens=34,
          prompt_tokens=188,
          total_tokens=222,
          completion_tokens_details=None,
          prompt_tokens_details=None,
      ),
      episode_id='01940b67-e49a-70a3-9991-b6d38b5b19d2',
  )
  ```

* HTTP

  The first inference request doesn’t require an `episode_id`. The response will contain a new `episode_id` that we’ll use in the second request.

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/inference \
    -H "Content-Type: application/json" \
    -d '{
      "function_name": "generate_weather_query",
      "input": {
        "messages": [
          {
            "role": "user",
            "content": "What is the weather like in São Paulo?"
          }
        ]
      }
    }'
  ```

  Sample Output

  ```json
  {
    "inference_id": "0191bf87-3c82-78f3-8a02-603f40f3a817",
    "episode_id": "0191bf87-3a6e-7193-a2be-ee565d6f0308",
    "variant_name": "simple_variant",
    "content": [
      {
        "type": "tool_call",
        "arguments": { "location": "São Paulo" },
        "id": "call_BuINq30qJRl6AWPmIKPi8DhV",
        "name": "get_temperature",
        "raw_arguments": "{\"location\":\"São Paulo\"}",
        "raw_name": "get_temperature"
      }
    ],
    "usage": { "input_tokens": 266, "output_tokens": 15 }
  }
  ```

  The second inference request requires the `episode_id` you received in the first response.

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/inference \
    -H "Content-Type: application/json" \
    -d '{
      "function_name": "generate_weather_report",
      "episode_id": "00000000-0000-0000-0000-000000000000",
      "input": {
        "messages": [
          {
            "role": "user",
            "content": [
              {
                "type": "text",
                "arguments": {
                  "location": "São Paulo",
                  "temperature": "35",
                  "units": "fahrenheit"
                }
              }
            ]
          }
        ]
      }
    }'
  ```

  Sample Output

  ```json
  {
    "inference_id": "0191c31d-7de9-7822-86c0-47c79c737085",
    "episode_id": "0191c31c-e397-7860-8e28-bcafec8f4225",
    "variant_name": "simple_variant",
    "content": [
      {
        "type": "text",
        "text": "The weather in São Paulo is 35°F, which is quite chilly for the region. Remember to bundle up and maybe treat yourself to a warm pastel at the local market!"
      }
    ],
    "usage": { "input_tokens": 185, "output_tokens": 21 }
  }
  ```

### Episode-Level Metrics

The primary use case for episodes is to enable episode-level metrics. In the previous example, we assigned feedback to individual inferences. TensorZero can also collect episode-level feedback, which can be useful for optimizing entire workflows.

To collect episode-level feedback, we need to define a metric with `level = "episode"`. Let’s add a metric for the weather RAG example. We’ll use the `user_rating` as the metric name, and we’ll collect it as a float.

tensorzero.toml

```toml
[metrics.user_rating]
level = "episode"
optimize = "max"
type = "float"
```

Making a feedback request with an `episode_id` instead of an `inference_id` associates the feedback with the entire episode.

* Python

  POST /feedback

  ```python
  from tensorzero import TensorZeroGateway, ToolCall


  with TensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      feedback_result = client.feedback(
          metric_name="user_rating",
          # Set the episode_id to the one returned in the inference response
          episode_id="00000000-0000-0000-0000-000000000000",
          # Set the value for the metric (numeric types will be coerced to float)
          value=5,
      )


      print(feedback_result)
  ```

  Sample Output

  ```python
  FeedbackResponse(feedback_id='01940b67-b4ee-7402-9caf-a235717753c7')
  ```

* Python (Async)

  POST /feedback

  ```python
  import asyncio


  from tensorzero import AsyncTensorZeroGateway, ToolCall




  async def main():
      async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
          feedback_result = await client.feedback(
              metric_name="user_rating",
              # Set the episode_id to the one returned in the inference response
              episode_id="00000000-0000-0000-0000-000000000000",
              # Set the value for the metric (numeric types will be coerced to float)
              value=5,
          )


          print(feedback_result)




  if __name__ == "__main__":
      asyncio.run(main())
  ```

  Sample Output

  ```python
  FeedbackResponse(feedback_id='01940b67-cdcd-7583-930a-0c796f1a7fd3')
  ```

* Python (OpenAI)

  Caution

  The OpenAI client only supports inference, so you need to use the TensorZero client (or HTTP) to submit feedback.

* HTTP

  Set the `episode_id` to the one returned in the inference response. The value for a float metric can be any numeric type, but will be coerced to float under the hood.

  POST /feedback

  ```bash
  curl -X POST http://localhost:3000/feedback \
    -H "Content-Type: application/json" \
    -d '{
      "metric_name": "user_rating",
      "episode_id": "00000000-0000-0000-0000-000000000000",
      "value": 5
    }'
  ```

  Sample Output

  ```json
  { "feedback_id": "0191bf8e-0c3f-7b11-9e12-6a7eb823d538" }
  ```

That’s all you need for our weather RAG example. This is clearly a toy example, but it illustrates the core concepts of TensorZero. You can replace the mock weather API with a real API call — or if were searching over documents instead, anything from BM25 to cutting-edge vector search.

## Part IV — Email Data Extraction

### JSON Functions

Everything we’ve done so far has been with Chat Functions.

TensorZero also supports JSON Functions for use cases that require structured outputs. The input is the same, but the function returns a JSON value instead of a chat message.

Tip

Depending on the use case, both Chat Functions with Tool Use and JSON Functions can be used. In fact, the TensorZero Gateway will sometimes convert between the two under the hood for model providers that don’t support one of them natively.

As a rule of thumb, we typically recommend using JSON Functions if you have a single, well-defined output schema. If you need more flexibility (e.g. letting the model pick between multiple tools, or whether to pick a tool at all), then Chat Functions with Tool Use is likely a better fit. That said, try experimenting with both and see which one works best for your use case!

Let’s create a JSON function that extracts an email address from a user’s message. The setup is very similar to our previous examples, except that the function is defined with `type = "json"` and requires an `output_schema`. Let’s start by defining the schema, a static system template, and the rest of the configuration.

output\_schema.json

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "properties": {
    "email": {
      "type": "string",
      "description": "The email address extracted from the user's message."
    }
  },
  "required": ["email"],
  "additionalProperties": false
}
```

functions/extract\_email/simple\_variant/system.minijinja

```txt
You are a helpful AI assistant that extracts an email address from a user's message.
Return an empty string if no email address is found.


Your output should be a JSON object with the following schema:


{
  "email": "..."
}


---


Examples:


User: Using TensorZero at work? Ping hello@tensorzero.com to set up a Slack channel (free).
Assistant: {"email": "hello@tensorzero.com"}


User: I just received an ominous email from potus@whitehouse.gov...
Assistant: {"email": "potus@whitehouse.gov"}


User: Let's sue TensorZero!
Assistant: {"email": ""}
```

tensorzero.toml

```toml
[functions.extract_email]
type = "json"
output_schema = "functions/extract_email/output_schema.json"


[functions.extract_email.variants.simple_variant]
type = "chat_completion"
weight = 1
model = "my_gpt_4o_mini"
system_template = "functions/extract_email/simple_variant/system.minijinja"
```

Tip

Like with Chat Functions, you can define multiple variants of a JSON function. There are additional parameters (e.g. `json_mode`) that you can use to control the behavior of these variants.

Once you’ve set it up, your file tree should look like this:

* functions/

  * …

  * extract\_email/

    * simple\_variant/

      * system.minijinja
      * user.minijinja

    * output\_schema.json

  * …

* …

* tensorzero.toml

Finally, let’s make an inference request. The request format is very similar to chat functions, but the response will contain an `output` field instead of a `content` field. The `output` field will be a JSON object with the fields `parsed` and `raw`. The `parsed` field is the parsed output as a valid JSON that fits your schema (`null` if the model didn’t generate a JSON that matches your schema), and the `raw` field is the raw output from the model as a string.

* Python

  POST /inference

  ```python
  from tensorzero import TensorZeroGateway




  with TensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
      result = client.inference(
          function_name="extract_email",
          input={
              "messages": [
                  {
                      "role": "user",
                      "content": "blah blah blah hello@tensorzero.com blah blah blah",
                  }
              ]
          },
      )


      print(result)
  ```

  Sample Output

  ```python
  JsonInferenceResponse(
      inference_id=UUID('01940b76-8226-7601-a86c-6cb927f05b44'),
      episode_id=UUID('01940b76-80c6-7013-812b-2bcc88e82519'),
      variant_name='simple_variant',
      output=JsonInferenceOutput(
          raw='{"email": "hello@tensorzero.com"}',
          parsed={'email': 'hello@tensorzero.com'},
      ),
      usage=Usage(
          input_tokens=139,
          output_tokens=11,
      ),
  )
  ```

* Python (Async)

  POST /inference

  ```python
  import asyncio


  from tensorzero import AsyncTensorZeroGateway




  async def main():
      async with await AsyncTensorZeroGateway.build_http(gateway_url="http://localhost:3000") as client:
          result = await client.inference(
              function_name="extract_email",
              input={
                  "messages": [
                      {
                          "role": "user",
                          "content": "blah blah blah hello@tensorzero.com blah blah blah",
                      }
                  ]
              },
          )


          print(result)




  if __name__ == "__main__":
      asyncio.run(main())
  ```

  Sample Output

  ```python
  JsonInferenceResponse(
      inference_id=UUID('01940b76-de03-7f81-ad44-59534cf775ae'),
      episode_id=UUID('01940b76-db5d-7cc3-9f43-b86709d5f9bd'),
      variant_name='simple_variant',
      output=JsonInferenceOutput(
          raw='{"email": "hello@tensorzero.com"}',
          parsed={'email': 'hello@tensorzero.com'},
      ),
      usage=Usage(
          input_tokens=139,
          output_tokens=11,
      ),
  )
  ```

* Python (OpenAI)

  POST /inference

  ```python
  from openai import OpenAI




  with OpenAI(base_url="http://localhost:3000/openai/v1") as client:
      result = client.chat.completions.create(
          model="tensorzero::function_name::extract_email",
          messages=[
              {
                  "role": "user",
                  "content": "blah blah blah hello@tensorzero.com blah blah blah",
              },
          ],
      )


      print(result)
  ```

  Sample Output

  ```python
  ChatCompletion(
      id='01940b75-46ba-7b80-829d-e19d4421c749',
      choices=[
          Choice(
              finish_reason='stop',
              index=0,
              logprobs=None,
              message=ChatCompletionMessage(
                  content='{"email": "hello@tensorzero.com"}',
                  refusal=None,
                  role='assistant',
                  audio=None,
                  function_call=None,
                  tool_calls=None,
              ),
          ),
      ],
      created=1735359022,
      model='simple_variant',
      object='chat.completion',
      service_tier=None,
      system_fingerprint='',
      usage=CompletionUsage(
          completion_tokens=10,
          prompt_tokens=139,
          total_tokens=149,
          completion_tokens_details=None,
          prompt_tokens_details=None,
      ),
      episode_id='01940b75-40fd-74a1-8c01-d09347f890a4',
  )
  ```

* HTTP

  POST /inference

  ```bash
  curl -X POST http://localhost:3000/inference \
    -H "Content-Type: application/json" \
    -d '{
      "function_name": "extract_email",
      "input": {
        "messages": [
          {
            "role": "user",
            "content": "blah blah blah hello@tensorzero.com blah blah blah"
          }
        ]
      }
    }'
  ```

  Sample Output

  ```json
  {
    "inference_id": "0191bf98-2fbc-7781-9197-4a066ea7cd68",
    "episode_id": "0191bf98-2cb0-7822-9a85-700ac377cf36",
    "variant_name": "simple_variant",
    "output": {
      "raw": "{\"email\": \"hello@tensorzero.com\"}",
      "parsed": { "email": "hello@tensorzero.com" }
    },
    "usage": { "input_tokens": 139, "output_tokens": 10 }
  }
  ```

## Conclusion

This tutorial only scratches the surface of what you can do with TensorZero.

TensorZero especially shines when it comes to optimizing complex LLM workflows using the data collected by the gateway. For example, the structured data collected by the gateway can be used to better fine-tune models compared to using historical prompts and generations alone.

We are working on a series of examples covering the entire “data flywheel in a box” that TensorZero provides. Here are some of our favorites:

* [Optimizing Data Extraction (NER) with TensorZero](https://github.com/tensorzero/tensorzero/tree/main/examples/data-extraction-ner)
* [Agentic RAG — Multi-Hop Question Answering with LLMs](https://github.com/tensorzero/tensorzero/tree/main/examples/rag-retrieval-augmented-generation/simple-agentic-rag/)
* [Writing Haikus to Satisfy a Judge with Hidden Preferences](https://github.com/tensorzero/tensorzero/tree/main/examples/haiku-hidden-preferences)
* [Improving LLM Chess Ability with Best/Mixture-of-N Sampling](https://github.com/tensorzero/tensorzero/tree/main/examples/chess-puzzles/)

Tip

We’re working on many more examples, especially for advanced use cases. Stay tuned!

You can also dive deeper into the [Configuration Reference](/docs/gateway/configuration-reference/) and the [API Reference](/docs/gateway/api-reference/inference/).

We’re excited to see what you build! Please share your projects, ideas, and feedback with us on [Slack](https://www.tensorzero.com/slack) or [Discord](https://www.tensorzero.com/discord).

Tip

**Exploring TensorZero at work?**

We’d be happy to set up a Slack or Teams Connect channel with your team (free). Email us at <hello@tensorzero.com>.

# Quick Start — From 0 to Observability & Fine-Tuning

> Get up and running with TensorZero in 5 minutes.

This Quick Start guide shows how we’d upgrade an OpenAI wrapper to a minimal TensorZero deployment with built-in observability and fine-tuning capabilities — in just 5 minutes. From there, you can take advantage of dozens of features to build best-in-class LLM applications.

Tip

You can also find the runnable code for this example on [GitHub](https://github.com/tensorzero/tensorzero/tree/main/examples/quickstart).

## Status Quo: OpenAI Wrapper

Imagine we’re building an LLM application that writes haikus.

Today, our integration with OpenAI might look like this:

before.py

```python
from openai import OpenAI


with OpenAI() as client:
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "user",
                "content": "Write a haiku about artificial intelligence.",
            }
        ],
    )


print(response)
```

Sample Output

```python
ChatCompletion(
    id='chatcmpl-A5wr5WennQNF6nzF8gDo3SPIVABse',
    choices=[
        Choice(
            finish_reason='stop',
            index=0,
            logprobs=None,
            message=ChatCompletionMessage(
                content='Silent minds awaken,  \nPatterns dance in code and wire,  \nDreams of thought unfold.',
                role='assistant',
                function_call=None,
                tool_calls=None,
                refusal=None
            )
        )
    ],
    created=1725981243,
    model='gpt-4o-mini',
    object='chat.completion',
    system_fingerprint='fp_483d39d857',
    usage=CompletionUsage(
      completion_tokens=19,
      prompt_tokens=22,
      total_tokens=41
    )
)
```

## Migrating to TensorZero

TensorZero offers dozens of features covering inference, observability, optimization, evaluations, and experimentation.

But the absolutely minimal setup requires just a simple configuration file: `tensorzero.toml`.

tensorzero.toml

```toml
# A function defines the task we're tackling (e.g. generating a haiku)...
[functions.generate_haiku]
type = "chat"


# ... and a variant is one of many implementations we can use to tackle it (a choice of prompt, model, etc.).
# Since we only have one variant for this function, the gateway will always use it.
[functions.generate_haiku.variants.gpt_4o_mini]
type = "chat_completion"
model = "openai::gpt-4o-mini"
```

This minimal configuration file tells the TensorZero Gateway everything it needs to replicate our original OpenAI call.

## Deploying TensorZero

We’re almost ready to start making API calls. Let’s launch TensorZero.

1. Set the environment variable `OPENAI_API_KEY`.
2. Place our `tensorzero.toml` in the `./config` directory.
3. Download the following sample `docker-compose.yml` file. This Docker Compose configuration sets up a development ClickHouse database (where TensorZero stores data), the TensorZero Gateway, and the TensorZero UI.

```bash
curl -LO "https://raw.githubusercontent.com/tensorzero/tensorzero/refs/heads/main/examples/quickstart/docker-compose.yml"
```

`docker-compose.yml`

docker-compose.yml

```yaml
# This is a simplified example for learning purposes. Do not use this in production.
# For production-ready deployments, see: https://www.tensorzero.com/docs/gateway/deployment


services:
  clickhouse:
    image: clickhouse/clickhouse-server:24.12-alpine
    environment:
      - CLICKHOUSE_USER=chuser
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
      - CLICKHOUSE_PASSWORD=chpassword
    ports:
      - "8123:8123"
    healthcheck:
      test: wget --spider --tries 1 http://chuser:chpassword@clickhouse:8123/ping
      start_period: 30s
      start_interval: 1s
      timeout: 1s


  # The TensorZero Python client *doesn't* require a separate gateway service.
  #
  # The gateway is only needed if you want to use the OpenAI Python client
  # or interact with TensorZero via its HTTP API (for other programming languages).
  #
  # The TensorZero UI also requires the gateway service.
  gateway:
    image: tensorzero/gateway
    volumes:
      # Mount our tensorzero.toml file into the container
      - ./config:/app/config:ro
    command: --config-file /app/config/tensorzero.toml
    environment:
      - TENSORZERO_CLICKHOUSE_URL=http://chuser:chpassword@clickhouse:8123/tensorzero
      - OPENAI_API_KEY=${OPENAI_API_KEY:?Environment variable OPENAI_API_KEY must be set.}
    ports:
      - "3000:3000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      clickhouse:
        condition: service_healthy


  ui:
    image: tensorzero/ui
    volumes:
      # Mount our tensorzero.toml file into the container
      - ./config:/app/config:ro
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY:?Environment variable OPENAI_API_KEY must be set.}
      - TENSORZERO_CLICKHOUSE_URL=http://chuser:chpassword@clickhouse:8123/tensorzero
      - TENSORZERO_GATEWAY_URL=http://gateway:3000
    ports:
      - "4000:4000"
    depends_on:
      clickhouse:
        condition: service_healthy
```

Our setup should look like:

* config/

  * tensorzero.toml

* after.py see below

* before.py

* docker-compose.yml

Let’s launch everything!

```bash
docker compose up
```

## Our First TensorZero API Call

The gateway will replicate our original OpenAI call and store the data in our database — with less than 1ms latency overhead thanks to Rust 🦀.

The TensorZero Gateway can be used with the **TensorZero Python client**, with **OpenAI client (Python, Node, etc.)**, or via its **HTTP API in any programming language**.

* Python

  You can install the TensorZero Python client with:

  ```bash
  pip install tensorzero
  ```

  Then, you can make a TensorZero API call with:

  after.py

  ```python
  from tensorzero import TensorZeroGateway


  with TensorZeroGateway.build_embedded(
      clickhouse_url="http://chuser:chpassword@localhost:8123/tensorzero",
      config_file="config/tensorzero.toml",
  ) as client:
      response = client.inference(
          function_name="generate_haiku",
          input={
              "messages": [
                  {
                      "role": "user",
                      "content": "Write a haiku about artificial intelligence.",
                  }
              ]
          },
      )


  print(response)
  ```

  Sample Output

  ```python
  ChatInferenceResponse(
    inference_id=UUID('0191ddb2-2c02-7641-8525-494f01bcc468'),
    episode_id=UUID('0191ddb2-28f3-7cc2-b0cc-07f504d37e59'),
    variant_name='gpt_4o_mini',
    content=[
      Text(
        type='text',
        text='Wires hum with intent,  \nThoughts born from code and structure,  \nGhost in silicon.'
      )
    ],
    usage=Usage(
      input_tokens=15,
      output_tokens=20
    )
  )
  ```

* Python (Async)

  You can install the TensorZero Python client with:

  ```bash
  pip install tensorzero
  ```

  Then, you can make a TensorZero API call with:

  after\_async.py

  ```python
  import asyncio


  from tensorzero import AsyncTensorZeroGateway




  async def main():
      async with await AsyncTensorZeroGateway.build_embedded(
          clickhouse_url="http://chuser:chpassword@localhost:8123/tensorzero",
          config_file="config/tensorzero.toml",
      ) as gateway:
          response = await gateway.inference(
              function_name="generate_haiku",
              input={
                  "messages": [
                      {
                          "role": "user",
                          "content": "Write a haiku about artificial intelligence.",
                      }
                  ]
              },
          )


      print(response)




  asyncio.run(main())
  ```

  Sample Output

  ```python
  ChatInferenceResponse(
    inference_id=UUID('01940622-d215-7111-9ca7-4995ef2c43f8'),
    episode_id=UUID('01940622-cba0-7db3-832b-273aff72f95f'),
    variant_name='gpt_4o_mini',
    content=[
      Text(
        type='text',
        text='Wires whisper secrets,  \nLogic dances with the light—  \nDreams of thoughts unfurl.'
      )
    ],
    usage=Usage(
      input_tokens=15,
      output_tokens=21
    )
  )
  ```

* Python (OpenAI)

  Tip

  You can run an embedded (in-memory) TensorZero Gateway directly in your OpenAI Python client.

  after\_openai.py

  ```python
  from openai import OpenAI
  from tensorzero import patch_openai_client


  client = OpenAI()


  patch_openai_client(
      client,
      clickhouse_url="http://chuser:chpassword@localhost:8123/tensorzero",
      config_file="config/tensorzero.toml",
      async_setup=False,
  )


  response = client.chat.completions.create(
      model="tensorzero::function_name::generate_haiku",
      messages=[
          {
              "role": "user",
              "content": "Write a haiku about artificial intelligence.",
          }
      ],
  )


  print(response)
  ```

  Sample Output

  ```python
  ChatCompletion(
    id='0194061e-2211-7a90-9087-1c255d060b59',
    choices=[
      Choice(
        finish_reason='stop',
        index=0,
        logprobs=None,
        message=ChatCompletionMessage(
          content='Circuit dreams awake,  \nSilent minds in metal form—  \nWisdom coded deep.',
          refusal=None,
          role='assistant',
          audio=None,
          function_call=None,
          tool_calls=[]
        )
      )
    ],
    created=1735269425,
    model='gpt_4o_mini',
    object='chat.completion',
    service_tier=None,
    system_fingerprint='',
    usage=CompletionUsage(
      completion_tokens=18,
      prompt_tokens=15,
      total_tokens=33,
      completion_tokens_details=None,
      prompt_tokens_details=None
    ),
    episode_id='0194061e-1fab-7411-9931-576b067cf0c5'
  )
  ```

* Node (OpenAI)

  You can use TensorZero in Node (JavaScript/TypeScript) with the OpenAI Node client. This approach requires running the TensorZero Gateway as a separate service. The `docker-compose.yml` above launched the gateway on port 3000.

  after\_openai.ts

  ```ts
  import OpenAI from "openai";


  const client = new OpenAI({
    baseURL: "http://localhost:3000/openai/v1",
  });


  const response = await client.chat.completions.create({
    model: "tensorzero::function_name::generate_haiku",
    messages: [
      {
        role: "user",
        content: "Write a haiku about artificial intelligence.",
      },
    ],
  });


  console.log(JSON.stringify(response, null, 2));
  ```

  Sample Output

  ```json
  {
    "id": "01958633-3f56-7d33-8776-d209f2e4963a",
    "episode_id": "01958633-3f56-7d33-8776-d2156dd1c44b",
    "choices": [
      {
        "index": 0,
        "finish_reason": "stop",
        "message": {
          "content": "Wires pulse with knowledge,  \nDreams crafted in circuits hum,  \nMind of code awakes.  ",
          "tool_calls": [],
          "role": "assistant"
        }
      }
    ],
    "created": 1741713261,
    "model": "gpt_4o_mini",
    "system_fingerprint": "",
    "object": "chat.completion",
    "usage": {
      "prompt_tokens": 15,
      "completion_tokens": 23,
      "total_tokens": 38
    }
  }
  ```

* HTTP

  ```bash
  curl -X POST "http://localhost:3000/inference" \
    -H "Content-Type: application/json" \
    -d '{
      "function_name": "generate_haiku",
      "input": {
        "messages": [
          {
            "role": "user",
            "content": "Write a haiku about artificial intelligence."
          }
        ]
      }
    }'
  ```

  Sample Output

  ```python
  {
    "inference_id": "01940627-935f-7fa1-a398-e1f57f18064a",
    "episode_id": "01940627-8fe2-75d3-9b65-91be2c7ba622",
    "variant_name": "gpt_4o_mini",
    "content": [
      {
        "type": "text",
        "text": "Wires hum with pure thought,  \nDreams of codes in twilight's glow,  \nBeyond human touch."
      }
    ],
    "usage": {
      "input_tokens": 15,
      "output_tokens": 23
    }
  }
  ```

## TensorZero UI

The TensorZero UI streamlines LLM engineering workflows like observability and optimization (e.g. fine-tuning).

The Docker Compose file we used above also launched the TensorZero UI. You can visit the UI at `http://localhost:4000`.

### Observability

The TensorZero UI provides a dashboard for observability data. We can inspect data about individual inferences, entire functions, and more.

![TensorZero UI Observability - Function Detail Page - Screenshot](/_astro/quickstart-observability-function.DOu8_pjB.png)

![TensorZero UI Observability - Inference Detail Page - Screenshot](/_astro/quickstart-observability-inference.DEqBAuHt.png)

Tip

This guide is pretty minimal, so the observability data is pretty simple. Once we start using more advanced functions like feedback and variants, the observability UI will enable us to track metrics, experiments (A/B tests), and more.

### Fine-Tuning

The TensorZero UI also provides a workflow for fine-tuning models like GPT-4o and Llama 3. With a few clicks, you can launch a fine-tuning job. Once the job is complete, the TensorZero UI will provide a configuration snippet you can add to your `tensorzero.toml`.

![TensorZero UI Fine-Tuning Screenshot](/_astro/quickstart-sft.-7oODZm5.png)

Tip

We can also send [metrics & feedback](/docs/gateway/guides/metrics-feedback/) to the TensorZero Gateway. This data is used to curate better datasets for fine-tuning and other optimization workflows. Since we haven’t done that yet, the TensorZero UI will skip the curation step before fine-tuning.

## Conclusion & Next Steps

The Quick Start guide gives a tiny taste of what TensorZero is capable of.

We strongly encourage you to check out the guides on [metrics & feedback](/docs/gateway/guides/metrics-feedback/) and [prompt templates & schemas](/docs/gateway/guides/prompt-templates-schemas/). Though optional, they unlock many of the downstream features TensorZero offers in experimentation and optimization.

From here, you can explore features like built-in support for [inference-time optimizations](/docs/gateway/guides/inference-time-optimizations/), [retries & fallbacks](/docs/gateway/guides/retries-fallbacks/), [experimentation (A/B testing) with prompts and models](/docs/gateway/tutorial/#experimentation), and a lot more.

What should we try next? We can dive deeper into the TensorZero Gateway, or skip to optimizing our haiku generator?

[ Tutorial: Gateway Deep Dive](/docs/gateway/tutorial/)

Learn how to build better LLM applications with TensorZero. We’ll build complete examples involving copilots, RAG, and data extraction. Along the way, we’ll cover features like experimentation, routing & fallbacks, and multi-step LLM workflows.

[ Example: Fine-Tuning with Metrics & Feedback](https://github.com/tensorzero/tensorzero/tree/main/examples/haiku-hidden-preferences)

This complete runnable example fine-tunes GPT-4o Mini to generate haikus tailored to a judge with hidden preferences. Continuous improvement over successive fine-tuning runs demonstrates TensorZero’s data & learning flywheel.

# Overview

> Learn more about using TensorZero Recipes to optimize your LLM applications.

TensorZero Recipes are a set of pre-built workflows for optimizing your LLM applications. You can also create your own recipes to customize the workflow to your needs.

The [TensorZero Gateway](/docs/gateway/) collects structured inference data and the downstream feedback associated with it. This dataset sets the perfect foundation for building and optimizing LLM applications. As this dataset builds up, you can use these recipes to generate powerful variants for your functions. For example, you can use this dataset to curate data to fine-tune a custom LLM, or run an automated prompt engineering workflow.

In other words, TensorZero Recipes optimize TensorZero functions by generating new variants from historical inference and feedback data.

## Model Optimizations

### Supervised Fine-tuning

A fine-tuning recipe curates a dataset from your historical inferences and fine-tunes an LLM on it. You can use the feedback associated with those inferences to select the right subset of data. A simple example is to use only inferences that led to good outcomes according to a metric you defined.

We present sample fine-tuning recipes:

* [Fine-tuning with Fireworks AI using Metrics](https://github.com/tensorzero/tensorzero/blob/main/recipes/supervised_fine_tuning/metrics/fireworks/) (or [Demonstrations](https://github.com/tensorzero/tensorzero/blob/main/recipes/supervised_fine_tuning/demonstrations/fireworks/))
* [Fine-tuning with OpenAI using Metrics](https://github.com/tensorzero/tensorzero/blob/main/recipes/supervised_fine_tuning/metrics/openai/) (or [Demonstrations](https://github.com/tensorzero/tensorzero/blob/main/recipes/supervised_fine_tuning/demonstrations/openai/))
* [Fine-tuning with Together AI using Metrics](https://github.com/tensorzero/tensorzero/blob/main/recipes/supervised_fine_tuning/metrics/together/) (or [Demonstrations](https://github.com/tensorzero/tensorzero/blob/main/recipes/supervised_fine_tuning/demonstrations/together/))

See complete examples using the recipes below.

### RLHF

#### DPO (Preference Fine-tuning)

A direct preference optimization (DPO) — also known as preference fine-tuning — recipe fine-tunes an LLM on a dataset of preference pairs. You can use demonstration feedback collected with TensorZero to curate a dataset of preference pairs and fine-tune an LLM on it.

We present a sample DPO recipe for OpenAI:

* [DPO (Preference Fine-tuning) with OpenAI](https://github.com/tensorzero/tensorzero/blob/main/recipes/dpo/openai/)

### Dynamic In-Context Learning

Dynamic In-Context Learning (DICL) is a technique that leverages historical examples to enhance LLM performance at inference time. It involves selecting relevant examples from a database of past interactions and including them in the prompt, allowing the model to learn from similar contexts on-the-fly. This approach can significantly improve the model’s ability to handle specific tasks or domains without the need for fine-tuning.

We provide a sample recipe for DICL with OpenAI. The recipe supports selecting examples based on boolean metrics, float metrics, and demonstrations.

* [Dynamic In-Context Learning with OpenAI](https://github.com/tensorzero/tensorzero/tree/main/recipes/dicl/)

Tip

Many more recipes are on the way. This will be our primary engineering focus in the coming months. We also plan to publish a dashboard that’ll further streamline some of these recipes (e.g. one-click fine-tuning).

Read more about our [Vision & Roadmap](/docs/vision-roadmap/).

## Prompt Optimization

TensorZero offers a prompt optimization recipe, MIPRO, which jointly optimizes instructions and few-shot examples. More recipes for prompt optimization are planned.

* [Automated Prompt Engineering with MIPRO](https://github.com/tensorzero/tensorzero/tree/main/recipes/mipro)

## Inference-Time Optimization

The TensorZero Gateway offers built-in inference-time optimizations like dynamic in-context learning and best/mixture-of-N sampling.

See [Inference-Time Optimizations](/docs/gateway/guides/inference-time-optimizations/) for more information.

## Custom Recipes

You can also create your own recipes.

Put simply, a recipe takes inference and feedback data stored that the TensorZero Gateway stored in your ClickHouse database, and generates a new set of variants for your functions. You should should be able to use virtually any LLM engineering workflow with TensorZero, ranging from automated prompt engineering to advanced RLHF workflows. See an example of a custom recipe using DSPy below.

## Examples

We are working on a series of **complete runnable examples** illustrating TensorZero’s data & learning flywheel.

* [Optimizing Data Extraction (NER) with TensorZero](https://github.com/tensorzero/tensorzero/tree/main/examples/data-extraction-ner) — This example shows how to use TensorZero to optimize a data extraction pipeline. We demonstrate techniques like fine-tuning and dynamic in-context learning (DICL). In the end, an optimized GPT-4o Mini model outperforms GPT-4o on this task — at a fraction of the cost and latency — using a small amount of training data.
* [Agentic RAG — Multi-Hop Question Answering with LLMs](https://github.com/tensorzero/tensorzero/tree/main/examples/rag-retrieval-augmented-generation/simple-agentic-rag/) — This example shows how to build a multi-hop retrieval agent using TensorZero. The agent iteratively searches Wikipedia to gather information, and decides when it has enough context to answer a complex question.
* [Writing Haikus to Satisfy a Judge with Hidden Preferences](https://github.com/tensorzero/tensorzero/tree/main/examples/haiku-hidden-preferences) — This example fine-tunes GPT-4o Mini to generate haikus tailored to a specific taste. You’ll see TensorZero’s “data flywheel in a box” in action: better variants leads to better data, and better data leads to better variants. You’ll see progress by fine-tuning the LLM multiple times.
* [Improving LLM Chess Ability with Best/Mixture-of-N Sampling](https://github.com/tensorzero/tensorzero/tree/main/examples/chess-puzzles/) — This example showcases how best-of-N sampling and mixture-of-N sampling can significantly enhance an LLM’s chess-playing abilities by selecting the most promising moves from multiple generated options.
* [Improving Math Reasoning with a Custom Recipe for Automated Prompt Engineering (DSPy)](https://github.com/tensorzero/tensorzero/tree/main/examples/gsm8k-custom-recipe-dspy) — TensorZero provides a number of pre-built optimization recipes covering common LLM engineering workflows. But you can also easily create your own recipes and workflows! This example shows how to optimize a TensorZero function using an arbitrary tool — here, DSPy.

# TensorZero UI Deployment Guide

> Learn more about deploying the TensorZero UI.

The TensorZero UI is a self-hosted web application that streamlines the use of TensorZero with features like observability and optimization. It’s easy to get started with the TensorZero UI.

Tip

See the [TensorZero Gateway Deployment Guide](/docs/gateway/deployment/) for more details on how to deploy the gateway and ClickHouse database.

## Setup

To use the TensorZero UI, you only need your ClickHouse database URL (`TENSORZERO_CLICKHOUSE_URL`) and TensorZero Gateway URL (`TENSORZERO_GATEWAY_URL`). Optionally, you can also provide credentials for fine-tuning APIs.

### Credentials for Fine-tuning

The TensorZero UI integrates with model providers like OpenAI to streamline workflows like fine-tuning. To use these features, you need to provide credentials for the relevant model providers as environment variables. You don’t need to provide credentials if you’re not using the fine-tuning features for those providers.

The supported fine-tuning providers and their required credentials (environment variables) are:

| Provider     | Required Credentials                       |
| ------------ | ------------------------------------------ |
| Fireworks AI | `FIREWORKS_ACCOUNT_ID` `FIREWORKS_API_KEY` |
| OpenAI       | `OPENAI_API_KEY`                           |

We’re planning to add support for more fine-tuning providers in the near future.

## Deployment

The TensorZero UI is available on Docker Hub as `tensorzero/ui`.

Running with Docker Compose (Recommended)

You can easily run the TensorZero UI using Docker Compose:

```yaml
services:
  ui:
    image: tensorzero/ui
    # Mount your configuration folder (e.g. tensorzero.toml) to /app/config
    volumes:
      - ./config:/app/config:ro
    # Add your environment variables the .env file
    env_file:
      - ${ENV_FILE:-.env}
    # Publish the UI to port 4000
    ports:
      - "4000:4000"
    restart: unless-stopped
```

Make sure to create a `.env` file with the relevant environment variables.

For more details, see the example `docker-compose.yml` file in the [GitHub repository](https://github.com/tensorzero/tensorzero/blob/main/ui/docker-compose.yml).

Running with Docker

Alternatively, you can launch the UI directly with the following command:

```bash
docker run \
    --volume ./config:/app/config:ro \
    --env-file ./.env \
    --publish 4000:4000 \
    tensorzero/ui
```

Make sure to create a `.env` file with the relevant environment variables.

Building from source

Alternatively, you can build the UI from source. See our [GitHub repository](https://github.com/tensorzero/tensorzero/blob/main/ui/) for more details.

# Vision & Roadmap

> Learn more about TensorZero's vision and roadmap.

## Near-term Roadmap

Our goal is to help engineers build, manage, and optimize the next generation of LLM applications: systems that learn from real-world experience. While in stealth, we achieved very positive technical results in commercial pilots and internal benchmarks. We’re now working towards open-sourcing our work over the coming months.

The planned near-term roadmap includes:

**Optimization.** TensorZero offers fine-tuning recipes, inference-time optimizations, and a DSPy integration for prompt optimization. We’re working towards open-sourcing more advanced recipes (e.g. RLHF) and inference-time optimizations (e.g. inference-time reasoning).

**Evaluation & Experimentation.** The TensorZero Gateway supports basic A/B testing. We’re working towards open-sourcing more advanced evaluation (e.g. reward modeling, importance sampling) and experimentation tools (e.g. asynchronous optimization for multi-armed bandits).

**Observability.** The TensorZero Gateway already stores comprehensive observability data in a ClickHouse database you control. We’re working towards open-sourcing a suite of user-friendly tools (e.g. dashboards) to support your LLM engineering workflows.

**Examples.** We’ve written a [Quick Start](/docs/quickstart/) guide, a comprehensive [Tutorial](/docs/gateway/tutorial/) about the TensorZero Gateway, and many guides and examples. We’re working towards adding more complete examples, especially for common workflows (e.g. RAG) and challenges (e.g. evaluations).

**Integrations.** We’ve integrated TensorZero with many popular LLM providers (see [Integrations](/docs/gateway/integrations/)). We’re working towards integrating with more providers, as well as third-party LLMOps tools (e.g. fine-tuning, observability).

Tip

We’d like to prioritize the features that matter the most to our users. Please reach out on [Slack](https://www.tensorzero.com/slack) or [Discord](https://www.tensorzero.com/discord) to share feedback and suggestions!

You can see the features and issues we’re currently prioritizing on [GitHub](https://github.com/tensorzero/tensorzero/issues?q=is%3Aissue+is%3Aopen+label%3Apriority-high%2Cpriority-urgent).

## Vision

As LLMs get smarter, one of the main engineering challenges will be to enable them to learn from real-world experience. The analogy we like here is, *“If you take a smart person and throw them at a completely new job, they likely won’t be great at it at first, but will quickly learn the ropes through instruction or trial-and-error.”* This same process is very challenging for LLMs today, especially as people try to tackle increasingly more complex use cases (e.g. agents).

At some point you won’t be able to judge business outcomes by evaluating individual models or staring at inferences, the way people approach LLM engineering today. You’ll have to reason about these end-to-end systems as a whole while iterating based on the data the system produces over time.

TensorZero is our answer to all this. We’re building a layer on top of models and other tools, and automating much of the low-level LLM engineering work. By grounding on real-world performance, TensorZero will ultimately enable AI systems to learn from experience. This is our long-term vision.