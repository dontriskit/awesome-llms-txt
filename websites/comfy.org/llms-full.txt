# Getting Started
Source: https://docs.comfy.org/comfy-cli/getting-started



### Overview

`comfy-cli` is a [command line tool](https://github.com/Comfy-Org/comfy-cli) that makes it easier to install and manage Comfy.

### Install CLI

<CodeGroup>
  ```bash pip
  pip install comfy-cli
  ```

  ```bash homebrew
  brew tap Comfy-Org/comfy-cli
  brew install comfy-org/comfy-cli/comfy-cli
  ```
</CodeGroup>

To get shell completion hints:

```bash
comfy --install-completion
```

### Install ComfyUI

Create a virtual environment with any Python version greater than 3.9.

<CodeGroup>
  ```bash conda
  conda create -n comfy-env python=3.11
  conda activate comfy-env
  ```

  ```bash venv
  python3 -m venv comfy-env
  source comfy-env/bin/activate
  ```
</CodeGroup>

Install ComfyUI

```bash
comfy install
```

<Warning>You still need to install CUDA, or ROCm depending on your GPU.</Warning>

### Run ComfyUI

```bash
comfy launch
```

### Manage Custom Nodes

```bash
comfy node install <NODE_NAME>
```

We use `cm-cli` for installing custom nodes. See the [docs](https://github.com/ltdrdata/ComfyUI-Manager/blob/main/docs/en/cm-cli.md) for more information.

### Manage Models

Downloading models with `comfy-cli` is easy. Just run:

```bash
comfy model download <url> models/checkpoints
```

### Contributing

We encourage contributions to comfy-cli! If you have suggestions, ideas, or bug reports, please open an issue on our [GitHub repository](https://github.com/Comfy-Org/comfy-cli/issues). If you want to contribute code, fork the repository and submit a pull request.

Refer to the [Dev Guide](https://github.com/Comfy-Org/comfy-cli/blob/main/DEV_README.md) for further details.

### Analytics

We track usage of the CLI to improve the user experience. You can disable this by running:

```bash
comfy tracking disable
```

To re-enable tracking, run:

```bash
comfy tracking enable
```


# Reference
Source: https://docs.comfy.org/comfy-cli/reference



# CLI

## Nodes

**Usage**:

```console
$ comfy node [OPTIONS] COMMAND [ARGS]...
```

**Options**:

* `--install-completion`: Install completion for the current shell.
* `--show-completion`: Show completion for the current shell, to copy it or customize the installation.
* `--help`: Show this message and exit.

**Commands**:

* `deps-in-workflow`
* `disable`
* `enable`
* `fix`
* `install`
* `install-deps`
* `reinstall`
* `restore-dependencies`
* `restore-snapshot`
* `save-snapshot`: Save a snapshot of the current ComfyUI...
* `show`
* `simple-show`
* `uninstall`
* `update`

### `deps-in-workflow`

**Usage**:

```console
$ deps-in-workflow [OPTIONS]
```

**Options**:

* `--workflow TEXT`: Workflow file (.json/.png)  \[required]
* `--output TEXT`: Workflow file (.json/.png)  \[required]
* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `disable`

**Usage**:

```console
$ disable [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: disable custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `enable`

**Usage**:

```console
$ enable [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: enable custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `fix`

**Usage**:

```console
$ fix [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: fix dependencies for specified custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `install`

**Usage**:

```console
$ install [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: install custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `install-deps`

**Usage**:

```console
$ install-deps [OPTIONS]
```

**Options**:

* `--deps TEXT`: Dependency spec file (.json)
* `--workflow TEXT`: Workflow file (.json/.png)
* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `reinstall`

**Usage**:

```console
$ reinstall [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: reinstall custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `restore-dependencies`

**Usage**:

```console
$ restore-dependencies [OPTIONS]
```

**Options**:

* `--help`: Show this message and exit.

### `restore-snapshot`

**Usage**:

```console
$ restore-snapshot [OPTIONS] PATH
```

**Arguments**:

* `PATH`: \[required]

**Options**:

* `--help`: Show this message and exit.

### `save-snapshot`

Save a snapshot of the current ComfyUI environment

**Usage**:

```console
$ save-snapshot [OPTIONS]
```

**Options**:

* `--output TEXT`: Specify the output file path. (.json/.yaml)
* `--help`: Show this message and exit.

### `show`

**Usage**:

```console
$ show [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: \[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list]  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `simple-show`

**Usage**:

```console
$ simple-show [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: \[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list]  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `uninstall`

**Usage**:

```console
$ uninstall [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: uninstall custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `update`

**Usage**:

```console
$ update [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: update custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

## Models

**Usage**:

```console
$ comfy model [OPTIONS] COMMAND [ARGS]...
```

**Options**:

* `--install-completion`: Install completion for the current shell.
* `--show-completion`: Show completion for the current shell, to copy it or customize the installation.
* `--help`: Show this message and exit.

**Commands**:

* `download`: Download a model to a specified relative...
* `list`: Display a list of all models currently...
* `remove`: Remove one or more downloaded models,...

### `download`

Download a model to a specified relative path if it is not already downloaded.

**Usage**:

```console
$ download [OPTIONS]
```

**Options**:

* `--url TEXT`: The URL from which to download the model  \[required]
* `--relative-path TEXT`: The relative path from the current workspace to install the model.  \[default: models/checkpoints]
* `--help`: Show this message and exit.

### `list`

Display a list of all models currently downloaded in a table format.

**Usage**:

```console
$ list [OPTIONS]
```

**Options**:

* `--relative-path TEXT`: The relative path from the current workspace where the models are stored.  \[default: models/checkpoints]
* `--help`: Show this message and exit.

### `remove`

Remove one or more downloaded models, either by specifying them directly or through an interactive selection.

**Usage**:

```console
$ remove [OPTIONS]
```

**Options**:

* `--relative-path TEXT`: The relative path from the current workspace where the models are stored.  \[default: models/checkpoints]
* `--model-names TEXT`: List of model filenames to delete, separated by spaces
* `--help`: Show this message and exit.


# Contributing
Source: https://docs.comfy.org/community/contributing



### Create a PR

Fork the [repo](https://github.com/comfyanonymous/ComfyUI), and create a PR.


# Datatypes
Source: https://docs.comfy.org/custom-nodes/backend/datatypes



These are the most important built in datatypes. You can also [define your own](./more_on_inputs#custom-datatypes).

Datatypes are used on the client side to prevent a workflow from passing the wrong form of data into a node - a bit like strong typing.
The JavaScript client side code will generally not allow a node output to be connected to an input of a different datatype,
although a few exceptions are noted below.

## Comfy datatypes

### COMBO

* No additional parameters in `INPUT_TYPES`

* Python datatype: defined as `list[str]`, output value is `str`

Represents a dropdown menu widget.
Unlike other datatypes, `COMBO` it is not specified in `INPUT_TYPES` by a `str`, but by a `list[str]`
corresponding to the options in the dropdown list, with the first option selected by default.

`COMBO` inputs are often dynamically generated at run time. For instance, in the built-in `CheckpointLoaderSimple` node, you find

```
"ckpt_name": (folder_paths.get_filename_list("checkpoints"), )
```

or they might just be a fixed list of options,

```
"play_sound": (["no","yes"], {}),
```

### Primitive and reroute

Primitive and reroute nodes only exist on the client side. They do not have an intrinsic datatype, but when connected they take on
the datatype of the input or output to which they have been connected (which is why they can't connect to a `*` input...)

## Python datatypes

### INT

* Additional parameters in `INPUT_TYPES`:

  * `default` is required

  * `min` and `max` are optional

* Python datatype `int`

### FLOAT

* Additional parameters in `INPUT_TYPES`:

  * `default` is required

  * `min`, `max`, `step` are optional

* Python datatype `float`

### STRING

* Additional parameters in `INPUT_TYPES`:

  * `default` is required

* Python datatype `str`

### BOOLEAN

* Additional parameters in `INPUT_TYPES`:

  * `default` is required

* Python datatype `bool`

## Tensor datatypes

### IMAGE

* No additional parameters in `INPUT_TYPES`

* Python datatype `torch.Tensor` with *shape* \[B,H,W,C]

A batch of `B` images, height `H`, width `W`, with `C` channels (generally `C=3` for `RGB`).

### LATENT

* No additional parameters in `INPUT_TYPES`

* Python datatype `dict`, containing a `torch.Tensor` with *shape* \[B,C,H,W]

The `dict` passed contains the key `samples`, which is a `torch.Tensor` with *shape* \[B,C,H,W] representing
a batch of `B` latents, with `C` channels (generally `C=4` for existing stable diffusion models), height `H`, width `W`.

The height and width are 1/8 of the corresponding image size (which is the value you set in the Empty Latent Image node).

Other entries in the dictionary contain things like latent masks.

{/* TODO need to dig into this */}

{/* TODO new SD models might have different C values? */}

### MASK

* No additional parameters in `INPUT_TYPES`

* Python datatype `torch.Tensor` with *shape* \[H,W] or \[B,C,H,W]

### AUDIO

* No additional parameters in `INPUT_TYPES`

* Python datatype `dict`, containing a `torch.Tensor` with *shape* \[B, C, T] and a sample rate.

The `dict` passed contains the key `waveform`, which is a `torch.Tensor` with *shape* \[B, C, T] representing a batch of `B` audio samples, with `C` channels (`C=2` for stereo and `C=1` for mono), and `T` time steps (i.e., the number of audio samples).

The `dict` contains another key `sample_rate`, which indicates the sampling rate of the audio.

## Custom Sampling datatypes

### Noise

The `NOISE` datatype represents a *source* of noise (not the actual noise itself). It can be represented by any Python object
that provides a method to generate noise, with the signature `generate_noise(self, input_latent:Tensor) -> Tensor`, and a
property, `seed:Optional[int]`.

<Tip>The `seed` is passed into `sample` guider in the `SamplerCustomAdvanced`, but does not appear to be used in any of the standard guiders.
It is Optional, so you can generally set it to None.</Tip>

When noise is to be added, the latent is passed into this method, which should return a `Tensor` of the same shape containing the noise.

See the [noise mixing example](./snippets#creating-noise-variations)

### Sampler

The `SAMPLER` datatype represents a sampler, which is represented as a Python object providing a `sample` method.
Stable diffusion sampling is beyond the scope of this guide; see `comfy/samplers.py` if you want to dig into this part of the code.

### Sigmas

The `SIGMAS` datatypes represents the values of sigma before and after each step in the sampling process, as produced by a scheduler.
This is represented as a one-dimensional tensor, of length `steps+1`, where each element represents the noise expected to be present
before the corresponding step, with the final value representing the noise present after the final step.

A `normal` scheduler, with 20 steps and denoise of 1, for an SDXL model, produces:

```
tensor([14.6146, 10.7468,  8.0815,  6.2049,  4.8557,  
         3.8654,  3.1238,  2.5572,  2.1157,  1.7648,  
         1.4806,  1.2458,  1.0481,  0.8784,  0.7297,  
         0.5964,  0.4736,  0.3555,  0.2322,  0.0292,  0.0000])
```

<Tip>The starting value of sigma depends on the model, which is why a scheduler node requires a `MODEL` input to produce a SIGMAS output</Tip>

### Guider

A `GUIDER` is a generalisation of the denoising process, as 'guided' by a prompt or any other form of conditioning. In Comfy the guider is
represented by a `callable` Python object providing a `__call__(*args, **kwargs)` method which is called by the sample.

The `__call__` method takes (in `args[0]`) a batch of noisy latents (tensor `[B,C,H,W]`), and returns a prediction of the noise (a tensor of the same shape).

## Model datatypes

There are a number of more technical datatypes for stable diffusion models. The most significant ones are `MODEL`, `CLIP`, `VAE` and `CONDITIONING`.
Working with these is (for the time being) beyond the scope of this guide! {/* TODO but maybe not forever */}

## Additional Parameters

Below is a list of officially supported keys that can be used in the 'extra options' portion of an input definition.

<Warning>You can use additional keys for your own custom widgets, but should *not* reuse any of the keys below for other purposes.</Warning>

{/* TODO -- did I actually get everything? */}

| Key              | Description                                                                                                                                                                                      |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `default`        | The default value of the widget                                                                                                                                                                  |
| `min`            | The minimum value of a number (`FLOAT` or `INT`)                                                                                                                                                 |
| `max`            | The maximum value of a number (`FLOAT` or `INT`)                                                                                                                                                 |
| `step`           | The amount to increment or decrement a widget                                                                                                                                                    |
| `label_on`       | The label to use in the UI when the bool is `True` (`BOOL`)                                                                                                                                      |
| `label_off`      | The label to use in the UI when the bool is `False` (`BOOL`)                                                                                                                                     |
| `defaultInput`   | Defaults to an input socket rather than a supported widget                                                                                                                                       |
| `forceInput`     | `defaultInput` and also don't allow converting to a widget                                                                                                                                       |
| `multiline`      | Use a multiline text box (`STRING`)                                                                                                                                                              |
| `placeholder`    | Placeholder text to display in the UI when empty (`STRING`)                                                                                                                                      |
| `dynamicPrompts` | Causes the front-end to evaluate dynamic prompts                                                                                                                                                 |
| `lazy`           | Declares that this input uses [Lazy Evaluation](./lazy_evaluation)                                                                                                                               |
| `rawLink`        | When a link exists, rather than receiving the evaluated value, you will receive the link (i.e. `["nodeId", <outputIndex>]`). Primarily useful when your node uses [Node Expansion](./expansion). |


# Node Expansion
Source: https://docs.comfy.org/custom-nodes/backend/expansion



## Node Expansion

Normally, when a node is executed, that execution function immediately returns the output results of that node. "Node Expansion" is a relatively advanced technique that allows nodes to return a new subgraph of nodes that should take its place in the graph. This technique is what allows custom nodes to implement loops.

### A Simple Example

First, here's a simple example of what node expansion looks like:

<Tip>We highly recommend using the `GraphBuilder` class when creating subgraphs. It isn't mandatory, but it prevents you from making many easy mistakes.</Tip>

```python
def load_and_merge_checkpoints(self, checkpoint_path1, checkpoint_path2, ratio):
    from comfy_execution.graph_utils import GraphBuilder # Usually at the top of the file
    graph = GraphBuilder()
    checkpoint_node1 = graph.node("CheckpointLoaderSimple", checkpoint_path=checkpoint_path1)
    checkpoint_node2 = graph.node("CheckpointLoaderSimple", checkpoint_path=checkpoint_path2)
    merge_model_node = graph.node("ModelMergeSimple", model1=checkpoint_node1.out(0), model2=checkpoint_node2.out(0), ratio=ratio)
    merge_clip_node = graph.node("ClipMergeSimple", clip1=checkpoint_node1.out(1), clip2=checkpoint_node2.out(1), ratio=ratio)
    return {
        # Returning (MODEL, CLIP, VAE) outputs
        "result": (merge_model_node.out(0), merge_clip_node.out(0), checkpoint_node1.out(2)),
        "expand": graph.finalize(),
    }
```

While this same node could previously be implemented by manually calling into ComfyUI internals, using expansion means that each subnode will be cached separately (so if you change `model2`, you don't have to reload `model1`).

### Requirements

In order to perform node expansion, a node must return a dictionary with the following keys:

1. `result`: A tuple of the outputs of the node. This may be a mix of finalized values (like you would return from a normal node) and node outputs.
2. `expand`: The finalized graph to perform expansion on. See below if you are not using the `GraphBuilder`.

#### Additional Requirements if not using GraphBuilder

The format expected from the `expand` key is the same as the ComfyUI API format. The following requirements are handled by the `GraphBuilder`, but must be handled manually if you choose to forego it:

1. Node IDs must be unique across the entire graph. (This includes between multiple executions of the same node due to the use of lists.)
2. Node IDs must be deterministic and consistent between multiple executions of the graph (including partial executions due to caching).

Even if you don't want to use the `GraphBuilder` for actually building the graph (e.g. because you're loading the raw json of the graph from a file), you can use the `GraphBuilder.alloc_prefix()` function to generate a prefix and `comfy.graph_utils.add_graph_prefix` to fix existing graphs to meet these requirements.

### Efficient Subgraph Caching

While you can pass non-literal inputs to nodes within the subgraph (like torch tensors), this can inhibit caching *within* the subgraph. When possible, you should pass links to subgraph objects rather than the node itself. (You can declare an input as a `rawLink` within the input's [Additional Parameters](./datatypes#additional-parameters) to do this easily.)


# Images, Latents, and Masks
Source: https://docs.comfy.org/custom-nodes/backend/images_and_masks



When working with these datatypes, you will need to know about the `torch.Tensor` class.
Complete documentation is [here](https://pytorch.org/docs/stable/tensors.html), or
an introduction to the key concepts required for Comfy [here](./tensors).

<Warning>If your node has a single output which is a tensor, remember to return `(image,)` not `(image)`</Warning>

Most of the concepts below are illustrated in the [example code snippets](./snippets).

## Images

An IMAGE is a `torch.Tensor` with shape `[B,H,W,C]`, `C=3`. If you are going to save or load images, you will
need to convert to and from `PIL.Image` format - see the code snippets below! Note that some `pytorch` operations
offer (or expect) `[B,C,H,W]`, known as 'channel first', for reasons of computational efficiency. Just be careful.

### Working with PIL.Image

If you want to load and save images, you'll want to use PIL:

```python
from PIL import Image, ImageOps
```

## Masks

A MASK is a `torch.Tensor` with shape `[B,H,W]`.
In many contexts, masks have binary values (0 or 1), which are used to indicate which pixels should undergo specific operations.
In some cases values between 0 and 1 are used indicate an extent of masking, (for instance, to alter transparency, adjust filters, or composite layers).

### Masks from the Load Image Node

The `LoadImage` node uses an image's alpha channel (the "A" in "RGBA") to create MASKs.
The values from the alpha channel are normalized to the range \[0,1] (torch.float32) and then inverted.
The `LoadImage` node always produces a MASK output when loading an image. Many images (like JPEGs) don't have an alpha channel.
In these cases, `LoadImage` creates a default mask with the shape `[1, 64, 64]`.

### Understanding Mask Shapes

In libraries like `numpy`, `PIL`, and many others, single-channel images (like masks) are typically represented as 2D arrays, shape `[H,W]`.
This means the `C` (channel) dimension is implicit, and thus unlike IMAGE types, batches of MASKs have only three dimensions: `[B, H, W]`.
It is not uncommon to encounter a mask which has had the `B` dimension implicitly squeezed, giving a tensor `[H,W]`.

To use a MASK, you will often have to match shapes by unsqueezing to produce a shape `[B,H,W,C]` with `C=1`
To unsqueezing the `C` dimension, so you should `unsqueeze(-1)`, to unsqueeze `B`, you `unsqueeze(0)`.
If your node receives a MASK as input, you would be wise to always check `len(mask.shape)`.

## Latents

A LATENT is a `dict`; the latent sample is referenced by the key `samples` and has shape `[B,C,H,W]`, with `C=4`.

<Tip>LATENT is channel first, IMAGE is channel last</Tip>


# Lazy Evaluation
Source: https://docs.comfy.org/custom-nodes/backend/lazy_evaluation



## Lazy Evaluation

By default, all `required` and `optional` inputs are evaluated before a node can be run. Sometimes, however, an
input won't necessarily be used and evaluating it would result in unnecessary processing. Here are some examples
of nodes where lazy evaluation may be beneficial:

1. A `ModelMergeSimple` node where the ratio is either `0.0` (in which case the first model doesn't need to be loaded)
   or `1.0` (in which case the second model doesn't need to be loaded).
2. Interpolation between two images where the ratio (or mask) is either entirely `0.0` or entirely `1.0`.
3. A Switch node where one input determines which of the other inputs will be passed through.

<Tip>There is very little cost in making an input lazy. If it's something you can do, you generally should.</Tip>

### Creating Lazy Inputs

There are two steps to making an input a "lazy" input. They are:

1. Mark the input as lazy in the dictionary returned by `INPUT_TYPES`
2. Define a method named `check_lazy_status` (note: *not* a class method) that will be called prior to evaluation to determine if any more inputs are necessary.

To demonstrate these, we'll make a "MixImages" node that interpolates between two images according to a mask. If the entire mask is `0.0`, we don't need to evaluate any part of the tree leading up to the second image. If the entire mask is `1.0`, we can skip evaluating the first image.

#### Defining `INPUT_TYPES`

Declaring that an input is lazy is as simple as adding a `lazy: True` key-value pair to the input's options dictionary.

```python
@classmethod
def INPUT_TYPES(cls):
    return {
        "required": {
            "image1": ("IMAGE",{"lazy": True}),
            "image2": ("IMAGE",{"lazy": True}),
            "mask": ("MASK",),
        },
    }
```

In this example, `image1` and `image2` are both marked as lazy inputs, but `mask` will always be evaluated.

#### Defining `check_lazy_status`

A `check_lazy_status` method is called if there are one or more lazy inputs that are not yet available. This method receives the same arguments as the standard execution function. All available inputs are passed in with their final values while unavailable lazy inputs have a value of `None`.

The responsibility of the `check_lazy_status` function is to return a list of the names of any lazy inputs that are needed to proceed. If all lazy inputs are available, the function should return an empty list.

Note that `check_lazy_status` may be called multiple times. (For example, you might find after evaluating one lazy input that you need to evaluate another.)

<Tip>Note that because the function uses actual input values, it is *not* a class method.</Tip>

```python
def check_lazy_status(self, mask, image1, image2):
    mask_min = mask.min()
    mask_max = mask.max()
    needed = []
    if image1 is None and (mask_min != 1.0 or mask_max != 1.0):
        needed.append("image1")
    if image2 is None and (mask_min != 0.0 or mask_max != 0.0):
        needed.append("image2")
    return needed
```

### Full Example

```python
class LazyMixImages:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image1": ("IMAGE",{"lazy": True}),
                "image2": ("IMAGE",{"lazy": True}),
                "mask": ("MASK",),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "mix"

    CATEGORY = "Examples"

    def check_lazy_status(self, mask, image1, image2):
        mask_min = mask.min()
        mask_max = mask.max()
        needed = []
        if image1 is None and (mask_min != 1.0 or mask_max != 1.0):
            needed.append("image1")
        if image2 is None and (mask_min != 0.0 or mask_max != 0.0):
            needed.append("image2")
        return needed

    # Not trying to handle different batch sizes here just to keep the demo simple
    def mix(self, mask, image1, image2):
        mask_min = mask.min()
        mask_max = mask.max()
        if mask_min == 0.0 and mask_max == 0.0:
            return (image1,)
        elif mask_min == 1.0 and mask_max == 1.0:
            return (image2,)

        result = image1 * (1. - mask) + image2 * mask,
        return (result[0],)
```

## Execution Blocking

While Lazy Evaluation is the recommended way to "disable" part of a graph, there are times when you want to disable an `OUTPUT` node that doesn't implement lazy evaluation itself. If it's an output node that you developed yourself, you should just add lazy evaluation as follows:

1. Add a required (if this is a new node) or optional (if you care about backward compatibility) input for `enabled` that defaults to `True`
2. Make all other inputs `lazy` inputs
3. Only evaluate the other inputs if `enabled` is `True`

If it's not a node you control, you can make use of a `comfy_execution.graph.ExecutionBlocker`. This special object can be returned as an output from any socket. Any nodes which receive an `ExecutionBlocker` as input will skip execution and return that `ExecutionBlocker` for any outputs.

<Tip>**There is intentionally no way to stop an ExecutionBlocker from propagating forward.** If you think you want this, you should really be using Lazy Evaluation.</Tip>

### Usage

There are two ways to construct and use an `ExecutionBlocker`

1. Pass `None` into the constructor to silently block execution. This is useful for cases where blocking execution is part of a successful run -- like disabling an output.

```python
def silent_passthrough(self, passthrough, blocked):
    if blocked:
        return (ExecutionBlocker(None),)
    else:
        return (passthrough,)
```

2. Pass a string into the constructor to display an error message when a node is blocked due to receiving the object. This can be useful if you want to display a meaningful error message if someone uses a meaningless output -- for example, the `VAE` output when loading a model that doesn't contain VAEs.

```python
def load_checkpoint(self, ckpt_name):
    ckpt_path = folder_paths.get_full_path("checkpoints", ckpt_name)
    model, clip, vae = load_checkpoint(ckpt_path)
    if vae is None:
        # This error is more useful than a "'NoneType' has no attribute" error
        # in a later node
        vae = ExecutionBlocker(f"No VAE contained in the loaded model {ckpt_name}")
    return (model, clip, vae)
```


# Lifecycle
Source: https://docs.comfy.org/custom-nodes/backend/lifecycle



## How Comfy loads custom nodes

When Comfy starts, it scans the directory `custom_nodes` for Python modules, and attempts to load them.
If the module exports `NODE_CLASS_MAPPINGS`, it will be treated as a custom node.
<Tip>A python module is a directory containing an `__init__.py` file.
The module exports whatever is listed in the `__all__` attribute defined in `__init__.py`.</Tip>

### **init**.py

`__init__.py` is executed when Comfy attempts to import the module. For a module to be recognized as containing
custom node definitions, it needs to export `NODE_CLASS_MAPPINGS`. If it does (and if nothing goes wrong in the import),
the nodes defined in the module will be available in Comfy. If there is an error in your code,
Comfy will continue, but will report the module as having failed to load. So check the Python console!

A very simple `__init__.py` file would look like this:

```python
from .python_file import MyCustomNode
NODE_CLASS_MAPPINGS = { "My Custom Node" : MyCustomNode }
__all__ = ["NODE_CLASS_MAPPINGS"]
```

#### NODE\_CLASS\_MAPPINGS

`NODE_CLASS_MAPPINGS` must be a `dict` mapping custom node names (unique across the Comfy install)
to the corresponding node class.

#### NODE\_DISPLAY\_NAME\_MAPPINGS

`__init__.py` may also export `NODE_DISPLAY_NAME_MAPPINGS`, which maps the same unique name to a display name for the node.
If `NODE_DISPLAY_NAME_MAPPINGS` is not provided, Comfy will use the unique name as the display name.

#### WEB\_DIRECTORY

If you are deploying client side code, you will also need to export the path, relative to the module, in which the
JavaScript files are to be found. It is conventional to place these in a subdirectory of your custom node named `js`.
<Tip>*Only* `.js` files will be served; you can't deploy `.css` or other types in this way</Tip>

<Warning>In previous versions of Comfy, `__init__.py` was required to copy the JavaScript files into the main Comfy web
subdirectory. You will still see code that does this. Don't.</Warning>


# Data lists
Source: https://docs.comfy.org/custom-nodes/backend/lists



## Length one processing

Internally, the Comfy server represents data flowing from one node to the next as a Python `list`, normally length 1, of the relevant datatype.
In normal operation, when a node returns an output, each element in the output `tuple` is separately wrapped in a list (length 1); then when the
next node is called, the data is unwrapped and passed to the main function.

<Tip>You generally don't need to worry about this, since Comfy does the wrapping and unwrapping.</Tip>

<Tip>This isn't about batches. A batch (of, for instance, latents, or images) is a *single entry* in the list (see [tensor datatypes](./images_and_masks))</Tip>

## List processing

In some circumstance, multiple data instances are processed in a single workflow, in which case the internal data will be a list containing the data instances.
An example of this might be processing a series of images one at a time to avoid running out of VRAM, or handling images of different sizes.

By default, Comfy will process the values in the list sequentially:

* if the inputs are `list`s of different lengths, the shorter ones are padded by repeating the last value
* the main method is called once for each value in the input lists
* the outputs are `list`s, each of which is the same length as the longest input

The relevant code can be found in the method `map_node_over_list` in `execution.py`.

However, as Comfy wraps node outputs into a `list` of length one, if the `tuple` returned by
a custom node contains a `list`, that `list` will be wrapped, and treated as a single piece of data.
In order to tell Comfy that the list being returned should not be wrapped, but treated as a series of data for sequential processing,
the node should provide a class attribute `OUTPUT_IS_LIST`, which is a `tuple[bool]`, of the same length as `RETURN_TYPES`, specifying
which outputs which should be so treated.

A node can also override the default input behaviour and receive the whole list in a single call. This is done by setting a class attribute
`INPUT_IS_LIST` to `True`.

Here's a (lightly annotated) example from the built in nodes - `ImageRebatch` takes one or more batches of images (received as a list, because `INPUT_IS_LIST - True`)
and rebatches them into batches of the requested size.

<Tip>`INPUT_IS_LIST` is node level - all inputs get the same treatment. So the value of the `batch_size` widget is given by `batch_size[0]`.</Tip>

```Python

class ImageRebatch:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": { "images": ("IMAGE",),
                              "batch_size": ("INT", {"default": 1, "min": 1, "max": 4096}) }}
    RETURN_TYPES = ("IMAGE",)
    INPUT_IS_LIST = True
    OUTPUT_IS_LIST = (True, )
    FUNCTION = "rebatch"
    CATEGORY = "image/batch"

    def rebatch(self, images, batch_size):
        batch_size = batch_size[0]    # everything comes as a list, so batch_size is list[int]

        output_list = []
        all_images = []
        for img in images:                    # each img is a batch of images
            for i in range(img.shape[0]):     # each i is a single image
                all_images.append(img[i:i+1])

        for i in range(0, len(all_images), batch_size): # take batch_size chunks and turn each into a new batch
            output_list.append(torch.cat(all_images[i:i+batch_size], dim=0))  # will die horribly if the image batches had different width or height!

        return (output_list,)
```

#### INPUT\_IS\_LIST


# Publishing to the Manager
Source: https://docs.comfy.org/custom-nodes/backend/manager



{/*
  description: "Understand how to publish a custom node to the ComfyUI Manager database."
  */}

{/*
  ## What is a custom node?

  One of the great powers of Comfy is that its node-based approach allows you to develop new workflows by plugging together the nodes provided in different ways. The built-in nodes provide a wide range of functionality, but you may find that you need a feature not provided by a core node. 

  Custom nodes are nodes developed by the community. It allows you to implement new features and share them with the wider community. If you are interested in developing custom nodes, you can read more about it [here](/custom-nodes/overview).

  ## ComfyUI Manager

  While custom nodes can be installed manually, most people use
  [ComfyUI Manager](https://github.com/ltdrdata/ComfyUI-Manager) to install them. **ComfyUI Manager** takes care of installing, 
  updating, and removing custom nodes, and any dependencies. But it isn't part
  of the Comfy core, so you need to manually install it. 

  ### Installing ComfyUI Manager

  ```bash
  cd ComfyUI/custom_nodes
  git clone https://github.com/ltdrdata/ComfyUI-Manager.git
  ```

  Restart Comfy afterwards. 
  See [ComfyUI Manager Install](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#installation) for details or special cases.

  */}

### Using ComfyUI Manager

To make your custom node available through **ComfyUI Manager** you need to save it as a git repository (generally at `github.com`)
and then submit a Pull Request on the **ComfyUI Manager** git, in which you have edited `custom-node-list.json` to add your node.
[More details](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#how-to-register-your-custom-node-into-comfyui-manager).

When a user installs the node, **ComfyUI Manager** will:

<Steps>
  <Step title="Git Clone">
    git clone the repository,
  </Step>

  <Step title="Install Python Dependencies">
    install the pip dependencies listed in the custom node repository under `requirements.txt` (if present),

    ```
    pip install -r requirements.txt
    ```

    <Tip>As is always the case with `pip`, it is possible that your node requirements will be in conflict with other
    custom nodes. Don't make your `requirements.txt` any more restrictive than they need to be.</Tip>
  </Step>

  <Step title="Run Install Script">
    execute `install.py`, if it is present in the custom node repository.
    <Tip>`install.py` is executed from the root path of the custom node</Tip>
  </Step>
</Steps>

### ComfyUI Manager files

As indicated above, there are a number of files and scripts that **ComfyUI Manager** will use to manage the lifecycle of
a custom node. These are all optional.

* `requirements.txt` - Python dependencies as mentioned above
* `install.py`, `uninstall.py` - executed when the custom node is installed or uninstalled
  <Tip>Users can just delete the directory, so you can't rely on `uninstall.py` being run</Tip>
* `disable.py`, `enable.py` - executed when a custom node is disabled or re-enabled
  <Tip>`enable.py` is only run when a disabled node is re-enabled - it should just reverse anything done in `disable.py`</Tip>
  <Tip>Disabled custom node subdirectory have `.disabled` appended to their names, and Comfy ignores these modules</Tip>
* `node_list.json` - only required if the custom nodes pattern of NODE\_CLASS\_MAPPINGS is not conventional.

See the [ComfyUI Manager guide](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#custom-node-support-guide) for official details.


# Hidden and Flexible inputs
Source: https://docs.comfy.org/custom-nodes/backend/more_on_inputs



## Hidden inputs

Alongside the `required` and `optional` inputs, which create corresponding inputs or widgets on the client-side,
there are three `hidden` input options which allow the custom node to request certain information from the server.

These are accessed by returning a value for `hidden` in the `INPUT_TYPES` `dict`, with the signature `dict[str,str]`,
containing one or more of `PROMPT`, `EXTRA_PNGINFO`, or `UNIQUE_ID`

```python
@classmethod
def INPUT_TYPES(s):
    return {
        "required": {...},
        "optional": {...},
        "hidden": {
            "unique_id": "UNIQUE_ID",
            "prompt": "PROMPT", 
            "extra_pnginfo": "EXTRA_PNGINFO",
        }
    }
```

### UNIQUE\_ID

`UNIQUE_ID` is the unique identifier of the node, and matches the `id` property of the node on the client side.
It is commonly used in client-server communications (see [messages](/essentials/comfyui-server/comms_messages#getting-node-id)).

### PROMPT

`PROMPT` is the complete prompt sent by the client to the server.
See [the prompt object](/custom-nodes/js/javascript_objects_and_hijacking#prompt) for a full description.

### EXTRA\_PNGINFO

`EXTRA_PNGINFO` is a dictionary that will be copied into the metadata of any `.png` files saved. Custom nodes can store additional
information in this dictionary for saving (or as a way to communicate with a downstream node).

<Tip>Note that if Comfy is started with the `disable_metadata` option, this data won't be saved.</Tip>

### DYNPROMPT

`DYNPROMPT` is an instance of `comfy_execution.graph.DynamicPrompt`. It differs from `PROMPT` in that it may mutate during the course of execution in response to [Node Expansion](/custom-nodes/backend/expansion).
<Tip>`DYNPROMPT` should only be used for advanced cases (like implementing loops in custom nodes).</Tip>

## Flexible inputs

### Custom datatypes

If you want to pass data between your own custom nodes, you may find it helpful to define a custom datatype. This is (almost) as simple as
just choosing a name for the datatype, which should be a unique string in upper case, such as `CHEESE`.

You can then use `CHEESE` in your node `INPUT_TYPES` and `RETURN_TYPES`, and the Comfy client will only allow `CHEESE` outputs to connect to a `CHEESE` input.
`CHEESE` can be any python object.

The only point to note is that because the Comfy client doesn't know about `CHEESE` you need (unless you define a custom widget for `CHEESE`,
which is a topic for another day), to force it to be an input rather than a widget. This can be done with the `forceInput` option in the input options dictionary:

```python
@classmethod
def INPUT_TYPES(s):
    return {
        "required": { "my_cheese": ("CHEESE", {"forceInput":True}) }
    }
```

### Wildcard inputs

```python
@classmethod
def INPUT_TYPES(s):
    return {
        "required": { "anything": ("*",{})},
    }

@classmethod
def VALIDATE_INPUTS(s, input_types):
    return True
```

The frontend allows `*` to indicate that an input can be connected to any source. Because this is not officially supported by the backend, you can skip the backend validation of types by accepting a parameter named `input_types` in your `VALIDATE_INPUTS` function. (See [VALIDATE\_INPUTS](./server_overview#validate-inputs) for more information.)
It's up to the node to make sense of the data that is passed.

### Dynamically created inputs

If inputs are dynamically created on the client side, they can't be defined in the Python source code.
In order to access this data we need an `optional` dictionary that allows Comfy to pass data with
arbitrary names. Since the Comfy server

```python
class ContainsAnyDict(dict):
    def __contains__(self, key):
        return True
...

@classmethod
def INPUT_TYPES(s):
    return {
        "required": {},
        "optional": ContainsAnyDict()
    }
...

def main_method(self, **kwargs):
    # the dynamically created input data will be in the dictionary kwargs

```

<Tip>Hat tip to rgthree for this pythonic trick!</Tip>


# Properties
Source: https://docs.comfy.org/custom-nodes/backend/server_overview

Properties of a custom node

### Simple Example

Here's the code for the Invert Image Node, which gives an overview of the key concepts in custom node development.

```python
class InvertImageNode:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": { "image_in" : ("IMAGE", {}) },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image_out",)
    CATEGORY = "examples"
    FUNCTION = "invert"

    def invert(self, image_in):
        image_out = 1 - image_in
        return (image_out,)
```

### Main properties

Every custom node is a Python class, with the following key properties:

#### INPUT\_TYPES

`INPUT_TYPES`, as the name suggests, defines the inputs for the node. The method returns a `dict`
which *must* contain the key `required`, and *may* also include the keys `optional` and/or `hidden`. The only difference
between `required` and `optional` inputs is that `optional` inputs can be left unconnected.
For more information on `hidden` inputs, see [Hidden Inputs](./more_on_inputs#hidden-inputs).

Each key has, as its value, another `dict`, in which key-value pairs specify the names and types of the inputs.
The types are defined by a `tuple`, the first element of which defines the data type,
and the second element of which is a `dict` of additional parameters.

Here we have just one required input, named `image_in`, of type `IMAGE`, with no additional parameters.

Note that unlike the next few attributes, this `INPUT_TYPES` is a `@classmethod`. This is so
that the options in dropdown widgets (like the name of the checkpoint to be loaded) can be
computed by Comfy at run time. We'll go into this more later. {/* TODO link when written */}

#### RETURN\_TYPES

A `tuple` of `str` defining the data types returned by the node.
If the node has no outputs this must still be provided `RETURN_TYPES = ()`
<Warning>If you have exactly one output, remember the trailing comma: `RETURN_TYPES = ("IMAGE",)`.
This is required for Python to make it a `tuple`</Warning>

#### RETURN\_NAMES

The names to be used to label the outputs. This is optional; if omitted, the names are simply the `RETURN_TYPES` in lowercase.

#### CATEGORY

Where the node will be found in the ComfyUI **Add Node** menu. Submenus can be specified as a path, eg. `examples/trivial`.

#### FUNCTION

The name of the Python function in the class that should be called when the node is executed.

The function is called with named arguments. All `required` (and `hidden`) inputs will be included;
`optional` inputs will be included only if they are connected, so you should provide default values for them in the function
definition (or capture them with `**kwargs`).

The function returns a tuple corresponding to the `RETURN_TYPES`. This is required even if nothing is returned (`return ()`).
Again, if you only have one output, remember that trailing comma `return (image_out,)`!

### Execution Control Extras

A great feature of Comfy is that it caches outputs,
and only executes nodes that might produce a different result than the previous run.
This can greatly speed up lots of workflows.

In essence this works by identifying which nodes produce an output (these, notably the Image Preview and Save Image nodes, are always executed), and then working
backwards to identify which nodes provide data that might have changed since the last run.

Two optional features of a custom node assist in this process.

#### OUTPUT\_NODE

By default, a node is not considered an output. Set `OUTPUT_NODE = True` to specify that it is.

#### IS\_CHANGED

By default, Comfy considers that a node has changed if any of its inputs or widgets have changed.
This is normally correct, but you may need to override this if, for instance, the node uses a random
number (and does not specify a seed - it's best practice to have a seed input in this case so that
the user can control reproducability and avoid unecessary execution), or loads an input that may have
changed externally, or sometimes ignores inputs (so doesn't need to execute just because those inputs changed).

<Warning>Despite the name, IS\_CHANGED should not return a `bool`</Warning>

`IS_CHANGED` is passed the same arguments as the main function defined by `FUNCTION`, and can return any
Python object. This object is compared with the one returned in the previous run (if any) and the node
will be considered to have changed if `is_changed != is_changed_old` (this code is in `execution.py` if you need to dig).

Since `True == True`, a node that returns `True` to say it has changed will be considered not to have! I'm sure this would
be changed in the Comfy code if it wasn't for the fact that it might break existing nodes to do so.

To specify that your node should always be considered to have changed (which you should avoid if possible, since it
stops Comfy optimising what gets run), `return float("NaN")`. This returns a `NaN` value, which is not equal
to anything, even another `NaN`.

A good example of actually checking for changes is the code from the built-in LoadImage node, which loads the image and returns a hash

```python
    @classmethod
    def IS_CHANGED(s, image):
        image_path = folder_paths.get_annotated_filepath(image)
        m = hashlib.sha256()
        with open(image_path, 'rb') as f:
            m.update(f.read())
        return m.digest().hex()
```

### Other attributes

There are three other attributes that can be used to modify the default Comfy treatment of a node.

#### INPUT\_IS\_LIST, OUTPUT\_IS\_LIST

These are used to control sequential processing of data, and are described [later](./lists.mdx).

### VALIDATE\_INPUTS

If a class method `VALIDATE_INPUTS` is defined, it will be called before the workflow begins execution.
`VALIDATE_INPUTS` should return `True` if the inputs are valid, or a message (as a `str`) describing the error (which will prevent execution).

#### Validating Constants

<Warning>Note that `VALIDATE_INPUTS` will only receive inputs that are defined as constants within the workflow. Any inputs that are received from other nodes will *not* be available in `VALIDATE_INPUTS`.</Warning>

`VALIDATE_INPUTS` is called with only the inputs that its signature requests (those returned by `inspect.getfullargspec(obj_class.VALIDATE_INPUTS).args`). Any inputs which are received in this way will *not* run through the default validation rules. For example, in the following snippet, the front-end will use the specified `min` and `max` values of the `foo` input, but the back-end will not enforce it.

```python
class CustomNode:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": { "foo" : ("INT", {"min": 0, "max": 10}) },
        }

    @classmethod
    def VALIDATE_INPUTS(cls, foo):
        # YOLO, anything goes!
        return True
```

Additionally, if the function takes a `**kwargs` input, it will receive *all* available inputs and all of them will skip validation as if specified explicitly.

#### Validating Types

If the `VALIDATE_INPUTS` method receives an argument named `input_types`, it will be passed a dictionary in which the key is the name of each input which is connected to an output from another node and the value is the type of that output.

When this argument is present, all default validation of input types is skipped. Here's an example making use of the fact that the front-end allows for the specification of multiple types:

```python
class AddNumbers:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "input1" : ("INT,FLOAT", {"min": 0, "max": 1000})
                "input2" : ("INT,FLOAT", {"min": 0, "max": 1000})
            },
        }

    @classmethod
    def VALIDATE_INPUTS(cls, input_types):
        # The min and max of input1 and input2 are still validated because
        # we didn't take `input1` or `input2` as arguments
        if input_types["input1"] not in ("INT", "FLOAT"):
            return "input1 must be an INT or FLOAT type"
        if input_types["input2"] not in ("INT", "FLOAT"):
            return "input2 must be an INT or FLOAT type"
        return True
```


# Annotated Examples
Source: https://docs.comfy.org/custom-nodes/backend/snippets



A growing collection of fragments of example code...

## Images and Masks

### Load an image

Load an image into a batch of size 1 (based on `LoadImage` source code in `nodes.py`)

```python
i = Image.open(image_path)
i = ImageOps.exif_transpose(i)
if i.mode == 'I':
    i = i.point(lambda i: i * (1 / 255))
image = i.convert("RGB")
image = np.array(image).astype(np.float32) / 255.0
image = torch.from_numpy(image)[None,]
```

### Save an image batch

Save a batch of images (based on `SaveImage` source code in `nodes.py`)

```python
for (batch_number, image) in enumerate(images):
    i = 255. * image.cpu().numpy()
    img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))
    filepath = # some path that takes the batch number into account
    img.save(filepath)
```

### Invert a mask

Inverting a mask is a straightforward process. Since masks are normalised to the range \[0,1]:

```python
mask = 1.0 - mask
```

### Convert a mask to Image shape

```Python
# We want [B,H,W,C] with C = 1
if len(mask.shape)==2: # we have [H,W], so insert B and C as dimension 1
    mask = mask[None,:,:,None]
elif len(mask.shape)==3 and mask.shape[2]==1: # we have [H,W,C]
    mask = mask[None,:,:,:]
elif len(mask.shape)==3:                      # we have [B,H,W]
    mask = mask[:,:,:,None]
```

### Using Masks as Transparency Layers

When used for tasks like inpainting or segmentation, the MASK's values will eventually be rounded to the nearest integer so that they are binary  0 indicating regions to be ignored and 1 indicating regions to be targeted. However, this doesn't happen until the MASK is passed to those nodes. This flexibility allows you to use MASKs as as you would in digital photography contexts as a transparency layer:

```python
# Invert mask back to original transparency layer
mask = 1.0 - mask

# Unsqueeze the `C` (channels) dimension
mask = mask.unsqueeze(-1)

# Concatenate ("cat") along the `C` dimension
rgba_image = torch.cat((rgb_image, mask), dim=-1)
```

## Noise

### Creating noise variations

Here's an example of creating a noise object which mixes the noise from two sources. This could be used to create slight noise variations by varying `weight2`.

```python
class Noise_MixedNoise:
    def __init__(self, nosie1, noise2, weight2):
        self.noise1  = noise1
        self.noise2  = noise2
        self.weight2 = weight2

    @property
    def seed(self): return self.noise1.seed

    def generate_noise(self, input_latent:torch.Tensor) -> torch.Tensor:
        noise1 = self.noise1.generate_noise(input_latent)
        noise2 = self.noise2.generate_noise(input_latent)
        return noise1 * (1.0-self.weight2) + noise2 * (self.weight2)
```


# Working with torch.Tensor
Source: https://docs.comfy.org/custom-nodes/backend/tensors



## pytorch, tensors, and torch.Tensor

All the core number crunching in Comfy is done by [pytorch](https://pytorch.org/). If your custom nodes are going
to get into the guts of stable diffusion you will need to become familiar with this library, which is way beyond
the scope of this introduction.

However, many custom nodes will need to manipulate images, latents and masks, each of which are represented internally
as `torch.Tensor`, so you'll want to bookmark the
[documentation for torch.Tensor](https://pytorch.org/docs/stable/tensors.html).

### What is a Tensor?

`torch.Tensor` represents a tensor, which is the mathematical generalization of a vector or matrix to any number of dimensions.
A tensor's *rank* is the number of dimensions it has (so a vector has *rank* 1, a matrix *rank* 2); its *shape* describes the
size of each dimension.

So an RGB image (of height H and width W) might be thought of as three arrays (one for each color channel), each measuring H x W,
which could be represented as a tensor with *shape* `[H,W,3]`. In Comfy images almost always come in a batch (even if the batch
only contains a single image). `torch` always places the batch dimension first, so Comfy images have *shape* `[B,H,W,3]`, generally
written as `[B,H,W,C]` where C stands for Channels.

### squeeze, unsqueeze, and reshape

If a tensor has a dimension of size 1 (known as a collapsed dimension), it is equivalent to the same tensor with that dimension removed
(a batch with 1 image is just an image). Removing such a collapsed dimension is referred to as squeezing, and
inserting one is known as unsqueezing.

<Warning>Some torch code, and some custom node authors, will return a squeezed tensor when a dimension is collapsed - such
as when a batch has only one member. This is a common cause of bugs!</Warning>

To represent the same data in a different shape is referred to as reshaping. This often requires you to know
the underlying data structure, so handle with care!

### Important notation

`torch.Tensor` supports most Python slice notation, iteration, and other common list-like operations. A tensor
also has a `.shape` attribute which returns its size as a `torch.Size` (which is a subclass of `tuple` and can
be treated as such).

There are some other important bits of notation you'll often see (several of these are less common
standard Python notation, seen much more frequently when dealing with tensors)

* `torch.Tensor` supports the use of `None` in slice notation
  to indicate the insertion of a dimension of size 1.

* `:` is frequently used when slicing a tensor; this simply means 'keep the whole dimension'.
  It's like using `a[start:end]` in Python, but omitting the start point and end point.

* `...` represents 'the whole of an unspecified number of dimensions'. So `a[0, ...]` would extract the first
  item from a batch regardless of the number of dimensions.

* in methods which require a shape to be passed, it is often passed as a `tuple` of the dimensions, in
  which a single dimension can be given the size `-1`, indicating that the size of this dimension should
  be calculated based on the total size of the data.

```python
>>> a = torch.Tensor((1,2))
>>> a.shape
torch.Size([2])
>>> a[:,None].shape 
torch.Size([2, 1])
>>> a.reshape((1,-1)).shape
torch.Size([1, 2])
```

### Elementwise operations

Many binary on `torch.Tensor` (including '+', '-', '\*', '/' and '==') are applied elementwise (independantly applied to each element).
The operands must be *either* two tensors of the same shape, *or* a tensor and a scalar. So:

```python
>>> import torch
>>> a = torch.Tensor((1,2))
>>> b = torch.Tensor((3,2))
>>> a*b
tensor([3., 4.])
>>> a/b
tensor([0.3333, 1.0000])
>>> a==b
tensor([False,  True])
>>> a==1
tensor([ True, False])
>>> c = torch.Tensor((3,2,1)) 
>>> a==c
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0
```

### Tensor truthiness

<Warning>The 'truthiness' value of a Tensor is not the same as that of Python lists.</Warning>

You may be familiar with the truthy value of a Python list as `True` for any non-empty list, and `False` for `None` or `[]`.
By contrast A `torch.Tensor` (with more than one elements) does not have a defined truthy value. Instead you need to use
`.all()` or `.any()` to combine the elementwise truthiness:

```python
>>> a = torch.Tensor((1,2))
>>> print("yes" if a else "no")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: Boolean value of Tensor with more than one value is ambiguous
>>> a.all()
tensor(False)
>>> a.any()
tensor(True)
```

This also means that you need to use `if a is not None:` not `if a:` to determine if a tensor variable has been set.


# Annotated Examples
Source: https://docs.comfy.org/custom-nodes/js/javascript_examples



A growing collection of fragments of example code...

## Right click menus

### Background menu

The main background menu (right-click on the canvas) is generated by a call to\
`LGraph.getCanvasMenuOptions`. One way to add your own menu options is to hijack this call:

```Javascript
/* in setup() */
    const original_getCanvasMenuOptions = LGraphCanvas.prototype.getCanvasMenuOptions;
    LGraphCanvas.prototype.getCanvasMenuOptions = function () {
        // get the basic options 
        const options = original_getCanvasMenuOptions.apply(this, arguments);
        options.push(null); // inserts a divider
        options.push({
            content: "The text for the menu",
            callback: async () => {
                // do whatever
            }
        })
        return options;
    }
```

### Node menu

When you right click on a node, the menu is similarly generated by `node.getExtraMenuOptions`.
But instead of returning an options object, this one gets it passed in...

```javascript
/* in beforeRegisterNodeDef() */
if (nodeType?.comfyClass=="MyNodeClass") { 
    const original_getExtraMenuOptions = nodeType.prototype.getExtraMenuOptions;
    nodeType.prototype.getExtraMenuOptions = function(_, options) {
        original_getExtraMenuOptions?.apply(this, arguments);
        options.push({
            content: "Do something fun",
            callback: async () => {
                // fun thing
            }
        })
    }   
}
```

### Submenus

If you want a submenu, provide a callback which uses `LiteGraph.ContextMenu` to create it:

```javascript
function make_submenu(value, options, e, menu, node) {
    const submenu = new LiteGraph.ContextMenu(
        ["option 1", "option 2", "option 3"],
        { 
            event: e, 
            callback: function (v) { 
                // do something with v (=="option x")
            }, 
            parentMenu: menu, 
            node:node
        }
    )
}

/* ... */
    options.push(
        {
            content: "Menu with options",
            has_submenu: true,
            callback: make_submenu,
        }
    )
```

## Capture UI events

This works just like you'd expect - find the UI element in the DOM and
add an eventListener. `setup()` is a good place to do this, since the page
has fully loaded. For instance, to detect a click on the 'Queue' button:

```Javascript
function queue_button_pressed() { console.log("Queue button was pressed!") }
document.getElementById("queue-button").addEventListener("click", queue_button_pressed);
```

## Detect when a workflow starts

This is one of many `api` events:

```javascript
import { api } from "../../scripts/api.js";
/* in setup() */
    function on_execution_start() { 
        /* do whatever */
    }
    api.addEventListener("execution_start", on_execution_start);
```

## Detect an interrupted workflow

A simple example of hijacking the api:

```Javascript
import { api } from "../../scripts/api.js";
/* in setup() */
    const original_api_interrupt = api.interrupt;
    api.interrupt = function () {
        /* Do something before the original method is called */
        original_api_interrupt.apply(this, arguments);
        /* Or after */
    }
```

## Catch clicks on your node

`node` has a mouseDown method you can hijack.
This time we're careful to pass on any return value.

```javascript
async nodeCreated(node) {
    if (node?.comfyClass === "My Node Name") {
        const original_onMouseDown = node.onMouseDown;
        node.onMouseDown = function( e, pos, canvas ) {
            alert("ouch!");
            return original_onMouseDown?.apply(this, arguments);
        }        
    }
}
```


# Comfy Hooks
Source: https://docs.comfy.org/custom-nodes/js/javascript_hooks



## Extension hooks

At various points during Comfy execution, the application calls
`#invokeExtensionsAsync` or `#invokeExtensions` with the name of a hook.
These invoke, on all registered extensions, the appropriately named method (if present), such as `setup`
in the example above.

Comfy provides a variety of hooks for custom extension code to use to modify client behavior.

<Tip>These hooks are called during creation and modification of the Comfy client side elements.
<br />Events during workflow execution are handled by
the `apiUpdateHandlers`</Tip> {/* TODO link when written */}

A few of the most significant hooks are described below.
As Comfy is being actively developed, from time to time additional hooks are added, so
search for `#invokeExtensions` in `app.js` to find all available hooks.

See also the [sequence](#call-sequences) in which hooks are invoked.

### Commonly used hooks

Start with `beforeRegisterNodeDef`, which is used by the majority of extensions, and is often the only one needed.

#### beforeRegisterNodeDef()

Called once for each node type (the list of nodes available in the `AddNode` menu), and is used to
modify the bahaviour of the node.

```Javascript
async beforeRegisterNodeDef(nodeType, nodeData, app) 
```

The object passed in the `nodeType` parameter essentially serves as a template
for all nodes that will be created of this type, so modifications made to `nodeType.prototype` will apply
to all nodes of this type. `nodeData` is an encapsulation of aspects of the node defined in the Python code,
such as its category, inputs, and outputs. `app` is a reference to the main Comfy app object (which you
have already imported anyway!)

<Tip>This method is called, on each registered extension, for *every* node type, not just the ones added by that extension.</Tip>

The usual idiom is to check `nodeType.ComfyClass`, which holds the Python class name corresponding to this node,
to see if yuo need to modify the node. Often this means modifying the custom nodes that you have added,
although you may sometimes need to modify the behavior of other nodes (or other custom nodes
might modify yours!), in which case care should be taken to ensure interoperability.

<Tip>Since other extensions may also modify nodes, aim to write code that makes as few assumptions as possible.
And play nicely - isolate your changes wherever possible.</Tip>

A very common idiom in `beforeRegisterNodeDef` is to 'hijack' an existing method:

```Javascript
async beforeRegisterNodeDef(nodeType, nodeData, app) {
	if (nodeType.comfyClass=="MyNodeClass") { 
		const onConnectionsChange = nodeType.prototype.onConnectionsChange;
		nodeType.prototype.onConnectionsChange = function (side,slot,connect,link_info,output) {     
			const r = onConnectionsChange?.apply(this, arguments);   
			console.log("Someone changed my connection!");
			return r;
		}
	}
}
```

In this idiom the existing prototype method is stored, and then replaced. The replacement calls the
original method (the `?.apply` ensures that if there wasn't one this is still safe) and then
performs additional operations. Depending on your code logic, you may need to place the `apply` elsewhere in your replacement code,
or even make calling it conditional.

When hijacking a method in this way, you will want to look at the core comfy code (breakpoints are your friend) to check
and conform with the method signature.

#### nodeCreated()

```Javascript
async nodeCreated(node)
```

Called when a specific instance of a node gets created
(right at the end of the `ComfyNode()` function on `nodeType` which serves as a constructor).
In this hook you can make modifications to individual instances of your node.

<Tip>Changes that apply to all instances are better added to the prototype in `beforeRegisterNodeDef` as described above.</Tip>

#### init()

```Javascript
async init()
```

Called when the Comfy webpage is loaded (or reloaded). The call is made after the graph object has been created, but before any
nodes are registered or created. It can be used to modify core Comfy behavior by hijacking methods of the app, or of the
graph (a `LiteGraph` object). This is discussed further in [Comfy Objects](./javascript_objects_and_hijacking).

<Warning>With great power comes great responsibility. Hijacking core behavior makes it more likely your nodes
will be incompatible with other custom nodes, or future Comfy updates</Warning>

#### setup()

```Javascript
async setup()
```

Called at the end of the startup process. A good place to add event listeners (either for Comfy events, or DOM events),
or adding to the global menus, both of which are discussed elsewhere. {/* TODO link when written */}

<Tip>To do something when a workflow has loaded, use `afterConfigureGraph`, not `setup`</Tip>

### Call sequences

These sequences were obtained by insert logging code into the Comfy `app.js` file. You may find similar code helpful
in understanding the execution flow.

```Javascript
/* approx line 220 at time of writing: */
	#invokeExtensions(method, ...args) {
		console.log(`invokeExtensions      ${method}`) // this line added
		// ...
	}
/* approx line 250 at time of writing: */
	async #invokeExtensionsAsync(method, ...args) {
		console.log(`invokeExtensionsAsync ${method}`) // this line added
		// ...
	}
```

#### Web page load

```
invokeExtensionsAsync init
invokeExtensionsAsync addCustomNodeDefs
invokeExtensionsAsync getCustomWidgets
invokeExtensionsAsync beforeRegisterNodeDef    [repeated multiple times]
invokeExtensionsAsync registerCustomNodes
invokeExtensionsAsync beforeConfigureGraph
invokeExtensionsAsync nodeCreated
invokeExtensions      loadedGraphNode
invokeExtensionsAsync afterConfigureGraph
invokeExtensionsAsync setup
```

#### Loading workflow

```
invokeExtensionsAsync beforeConfigureGraph
invokeExtensionsAsync beforeRegisterNodeDef   [zero, one, or multiple times]
invokeExtensionsAsync nodeCreated             [repeated multiple times]
invokeExtensions      loadedGraphNode         [repeated multiple times]
invokeExtensionsAsync afterConfigureGraph
```

{/* TODO why does beforeRegisterNodeDef get called again? */}

#### Adding new node

```
invokeExtensionsAsync nodeCreated
```


# Comfy Objects
Source: https://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking



## LiteGraph

The Comfy UI is built on top of [LiteGraph](https://github.com/jagenjo/litegraph.js).
Much of the Comfy functionality is provided by LiteGraph, so if developing more complex
nodes you will probably find it helpful to clone that repository and browse the documentation,
which can be found at `doc/index.html`.

## ComfyApp

The `app` object (always accessible by `import { app } from "../../scripts/app.js";`) represents the Comfy application running in the browser,
and contains a number of useful properties and functions, some of which are listed below.

<Warning>Hijacking functions on `app` is not recommended, as Comfy is under constant development, and core behavior may change.</Warning>

### Properties

Important properties of `app` include (this is not an exhaustive list):

| property        | contents                                                                                                                                                        |
| --------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `canvas`        | An LGraphCanvas object, representing the current user interface. It contains some potentially interesting properties, such as `node_over` and `selected_nodes`. |
| `canvasEl`      | The DOM `<canvas>` element                                                                                                                                      |
| `graph`         | A reference to the LGraph object describing the current graph                                                                                                   |
| `runningNodeId` | During execution, the node currently being executed                                                                                                             |
| `ui`            | Provides access to some UI elements, such as the queue, menu, and dialogs                                                                                       |

`canvas` (for graphical elements) and `graph` (for logical connections) are probably the ones you are most likely to want to access.

### Functions

Again, there are many. A few significant ones are:

| function          | notes                                                                 |
| ----------------- | --------------------------------------------------------------------- |
| graphToPrompt     | Convert the graph into a prompt that can be sent to the Python server |
| loadGraphData     | Load a graph                                                          |
| queuePrompt       | Submit a prompt to the queue                                          |
| registerExtension | You've seen this one - used to add an extension                       |

## LGraph

The `LGraph` object is part of the LiteGraph framework, and represents the current logical state of the graph (nodes and links).
If you want to manipulate the graph, the LiteGraph documentation (at `doc/index.html` if you clone `https://github.com/jagenjo/litegraph.js`)
describes the functions you will need.

You can use `graph` to obtain details of nodes and links, for example:

```Javascript
const ComfyNode_object_for_my_node = app.graph._nodes_by_id(my_node_id) 
ComfyNode_object_for_my_node.inputs.forEach(input => {
    const link_id = input.link;
    if (link_id) {
        const LLink_object = app.graph.links[link_id]
        const id_of_upstream_node = LLink_object.origin_id
        // etc
    }
});
```

## LLink

The `LLink` object, accessible through `graph.links`, represents a single link in the graph, from node `link.origin_id` output slot `link.origin_slot`
to node `link.target_id` slot `link.target_slot`. It also has a string representing the data type, in `link.type`, and `link.id`.

`LLink`s are created in the `connect` method of a `LGraphNode` (of which `ComfyNode` is a subclass).

<Tip>Avoid creating your own LLink objects - use the LiteGraph functions instead.</Tip>

## ComfyNode

`ComfyNode` is a subclass of `LGraphNode`, and the LiteGraph documentation is therefore helpful for more generic
operations. However, Comfy has significantly extended the LiteGraph core behavior, and also does not make
use of all LiteGraph functionality.

<Tip>The description that follows applies to a normal node.
Group nodes, primitive nodes, notes, and redirect nodes have different properties.</Tip>

A `ComfyNode` object represents a node in the current workflow. It has a number of important properties
that you may wish to make use of, a very large number of functions that you may wish to use, or hijack to
modify behavior.

To get a more complete sense of the node object, you may find it helpful to insert the following
code into your extension and place a breakpoint on the `console.log` command. When you then create a new node
you can use your favorite debugger to interrogate the node.

```Javascript
async nodeCreated(node) {
    console.log("nodeCreated")
}
```

### Properties

| property          | contents                                                                                                                            |
| ----------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| `bgcolor`         | The background color of the node, or undefined for the default                                                                      |
| `comfyClass`      | The Python class representing the node                                                                                              |
| `flags`           | A dictionary that may contain flags related to the state of the node. In particular, `flags.collapsed` is true for collapsed nodes. |
| `graph`           | A reference to the LGraph object                                                                                                    |
| `id`              | A unique id                                                                                                                         |
| `input_type`      | A list of the input types (eg "STRING", "MODEL", "CLIP" etc). Generally matches the Python INPUT\_TYPES                             |
| `inputs`          | A list of inputs (discussed below)                                                                                                  |
| `mode`            | Normally 0, set to 2 if the node is muted and 4 if the node is bypassed. Values of 1 and 3 are not used by Comfy                    |
| `order`           | The node's position in the execution order. Set by `LGraph.computeExecutionOrder()` when the prompt is submitted                    |
| `pos`             | The \[x,y] position of the node on the canvas                                                                                       |
| `properties`      | A dictionary containing `"Node name for S&R"`, used by LiteGraph                                                                    |
| `properties_info` | The type and default value of entries in `properties`                                                                               |
| `size`            | The width and height of the node on the canvas                                                                                      |
| `title`           | Display Title                                                                                                                       |
| `type`            | The unique name (from Python) of the node class                                                                                     |
| `widgets`         | A list of widgets (discussed below)                                                                                                 |
| `widgets_values`  | A list of the current values of widgets                                                                                             |

### Functions

There are a very large number of functions (85, last time I counted). A selection are listed below.
Most of these functions are unmodified from the LiteGraph core code.

#### Inputs, Outputs, Widgets

| function               | notes                                                                                              |
| ---------------------- | -------------------------------------------------------------------------------------------------- |
| Inputs / Outputs       | Most have output methods with the equivalent names: s/In/Out/                                      |
| `addInput`             | Create a new input, defined by name and type                                                       |
| `addInputs`            | Array version of `addInput`                                                                        |
| `findInputSlot`        | Find the slot index from the input name                                                            |
| `findInputSlotByType`  | Find an input matching the type. Options to prefer, or only use, free slots                        |
| `removeInput`          | By slot index                                                                                      |
| `getInputNode`         | Get the node connected to this input. The output equivalent is `getOutputNodes` and returns a list |
| `getInputLink`         | Get the LLink connected to this input. No output equivalent                                        |
| Widgets                |                                                                                                    |
| `addWidget`            | Add a standard Comfy widget                                                                        |
| `addCustomWidget`      | Add a custom widget (defined in the `getComfyWidgets` hook)                                        |
| `addDOMWidget`         | Add a widget defined by a DOM element                                                              |
| `convertWidgetToInput` | Convert a widget to an input if allowed by `isConvertableWidget` (in `widgetInputs.js`)            |

#### Connections

| function              | notes                                                                                             |
| --------------------- | ------------------------------------------------------------------------------------------------- |
| `connect`             | Connect this node's output to another node's input                                                |
| `connectByType`       | Connect output to another node by specifying the type - connects to first available matching slot |
| `connectByTypeOutput` | Connect input to another node output by type                                                      |
| `disconnectInput`     | Remove any link into the input (specified by name or index)                                       |
| `disconnectOutput`    | Disconnect an output from a specified node's input                                                |
| `onConnectionChange`  | Called on each node. `side==1` if it's an input on this node                                      |
| `onConnectInput`      | Called *before* a connection is made. If this returns `false`, the connection is refused          |

#### Display

| function           | notes                                                                                                  |
| ------------------ | ------------------------------------------------------------------------------------------------------ |
| `setDirtyCanvas`   | Specify that the foreground (nodes) and/or background (links and images) need to be redrawn            |
| `onDrawBackground` | Called with a `CanvasRenderingContext2D` object to draw the background. Used by Comfy to render images |
| `onDrawForeground` | Called with a `CanvasRenderingContext2D` object to draw the node.                                      |
| `getTitle`         | The title to be displayed.                                                                             |
| `collapse`         | Toggles the collapsed state of the node.                                                               |

<Warning>`collapse` is badly named; it *toggles* the collapsed state.
It takes a boolean parameter, which can be used to override
`node.collapsable === false`.</Warning>

#### Other

| function     | notes                                                              |
| ------------ | ------------------------------------------------------------------ |
| `changeMode` | Use to set the node to bypassed (`mode == 4`) or not (`mode == 0`) |

## Inputs and Widgets

Inputs and Widgets represent the two ways that data can be fed into a node. In general a widget can be
converted to an input, but not all inputs can be converted to a widget (as many datatypes can't be
entered through a UI element).

`node.inputs` is a list of the current inputs (colored dots on the left hand side of the node),
specifying their `.name`, `.type`, and `.link` (a reference to the connected `LLink` in `app.graph.links`).

If an input is a widget which has been converted, it also holds a reference to the, now inactive, widget in `.widget`.

`node.widgets` is a list of all widgets, whether or not they have been converted to an input. A widget has:

| property/function | notes                                                                     |
| ----------------- | ------------------------------------------------------------------------- |
| `callback`        | A function called when the widget value is changed                        |
| `last_y`          | The vertical position of the widget in the node                           |
| `name`            | The (unique within a node) widget name                                    |
| `options`         | As specified in the Python code (such as default, min, and max)           |
| `type`            | The name of the widget type (see below) in lowercase                      |
| `value`           | The current widget value. This is a property with `get` and `set` methods |

### Widget Types

`app.widgets` is a dictionary of currently registered widget types, keyed in the UPPER CASE version of the name of the type.
Build in Comfy widgets types include the self explanatory `BOOLEAN`, `INT`, and `FLOAT`,
as well as `STRING` (which comes in two flavours, single line and multiline),
`COMBO` for dropdown selection from a list, and `IMAGEUPLOAD`, used in Load Image nodes.

Custom widget types can be added by providing a `getCustomWidgets` method in your extension.

{/* TODO add link */}

### Linked widgets

Widgets can also be linked - the built in behavior of `seed` and `control_after_generate`, for example.
A linked widget has `.type = 'base_widget_type:base_widget_name'`; so `control_after_generate` may have
type `int:seed`.

## Prompt

When you press the `Queue Prompt` button in Comfy, the `app.graphToPrompt()` method is called to convert the
current graph into a prompt that can be sent to the server.

`app.graphToPrompt` returns an object (refered to herein as `prompt`) with two properties, `output` and `workflow`.

### output

`prompt.output` maps from the `node_id` of each node in the graph to an object with two properties.

* `prompt.output[node_id].class_type`, the unique name of the custom node class, as defined in the Python code
* `prompt.output[node_id].inputs`, which contains the value of each input (or widget) as a map from the input name to:
  * the selected value, if it is a widget, or
  * an array containing (`upstream_node_id`, `upstream_node_output_slot`) if there is a link connected to the input, or
  * undefined, if it is a widget that has been converted to an input and is not connected
  * other unconnected inputs are not included in `.inputs`

<Tip>Note that the `upstream_node_id` in the array describing a connected input is represented as a string, not an integer.</Tip>

### workflow

`prompt.workflow` contains the following properties:

* `config` - a dictionary of additional configuration options (empty by default)
* `extra` - a dictionary containing extra information about the workflow. By default it contains:
  * `extra.ds` - describes the current view of the graph (`scale` and `offset`)
* `groups` - all groups in the workflow
* `last_link_id` - the id of the last link added
* `last_node_id` - the id of the last node added
* `links` - a list of all links in the graph. Each entry is an array of five integers and one string:
  * (`link_id`, `upstream_node_id`, `upstream_node_output_slot`, `downstream_node_id`, `downstream_node_input_slot`, `data type`)
* `nodes` - a list of all nodes in the graph. Each entry is a map of a subset of the properties of the node as described [above](#comfynode)
  * The following properties are included: `flags`, `id`, `inputs`, `mode`, `order`, `pos`, `properties`, `size`, `type`, `widgets_values`
  * In addition, unless a node has no outputs, there is an `outputs` property, which is a list of the outputs of the node, each of which contains:
    * `name` - the name of the output
    * `type` - the data type of the output
    * `links` - a list of the `link_id` of all links from this output (if there are no connections, may be an empty list, or null),
    * `shape` - the shape used to draw the output (default 3 for a dot)
    * `slot_index` - the slot number of the output
* `version` - the LiteGraph version number (at time of writing, `0.4`)

<Tip>`nodes.output` is absent for nodes with no outputs, not an empty list.</Tip>


# Javascript Extensions
Source: https://docs.comfy.org/custom-nodes/js/javascript_overview



## Extending the Comfy Client

Comfy can be modified through an extensions mechanism. To add an extension you need to:

* Export `WEB_DIRECTORY` from your Python module,
* Place one or more `.js` files into that directory,
* Use `app.registerExtension` to register your extension.

These three steps are below. Once you know how to add an extension, look
through the [hooks](javascript_hooks) available to get your code called,
a description of various [Comfy objects](javascript_objects_and_hijacking) you might need,
or jump straight to some [example code snippets](javascript_examples).

### Exporting `WEB_DIRECTORY`

The Comfy web client can be extended by creating a subdirectory in your custom node directory, conventionally called `js`, and
exporting `WEB_DIRECTORY` - so your `__init_.py` will include something like:

```python
WEB_DIRECTORY = "./js"
__all__ = ["NODE_CLASS_MAPPINGS", "NODE_DISPLAY_NAME_MAPPINGS", "WEB_DIRECTORY"]
```

### Including `.js` files

<Tip>All Javascript `.js` files will be loaded by the browser as the Comfy webpage loads. You don't need to specify the file
your extension is in.</Tip>

*Only* `.js` files will be added to the webpage. Other resources (such as `.css` files) can be accessed
at `extensions/custom_node_subfolder/the_file.css` and added programmatically.

<Warning>That path does *not* include the name of the subfolder. The value of `WEB_DIRECTORY` is inserted by the server.</Warning>

### Registering an extension

The basic structure of an extension follows is to import the main Comfy `app` object, and call `app.registerExtension`,
passing a dictionary that contains a unique `name`,
and one or more functions to be called by hooks in the Comfy code.

A complete, trivial, and annoying, extension might look like this:

```Javascript
import { app } from "../../scripts/app.js";
app.registerExtension({ 
	name: "a.unique.name.for.a.useless.extension",
	async setup() { 
		alert("Setup complete!")
	},
})
```


# Settings
Source: https://docs.comfy.org/custom-nodes/js/javascript_settings



You can provide a settings object to ComfyUI that will show up when the user
opens the ComfyUI settings panel.

## Basic operation

### Add a setting

```javascript
import { app } from "../../scripts/app.js";

app.registerExtension({
    name: "My Extension",
    settings: [
        {
            id: "example.boolean",
            name: "Example boolean setting",
            type: "boolean",
            defaultValue: false,
        },
    ],
});
```

The `id` must be unique across all extensions and will be used to fetch values.

If you do not [provide a category](#categories), then the `id` will be split by
`.` to determine where it appears in the settings panel.

* If your `id` doesn't contain any `.` then it will appear in the "Other"
  category and your `id` will be used as the section heading.
* If your `id` contains at least one `.` then the leftmost part will be used
  as the setting category and the second part will be used as the section
  heading. Any further parts are ignored.

### Read a setting

```javascript
import { app } from "../../scripts/app.js";

if (app.extensionManager.setting.get('example.boolean')) {
    console.log("Setting is enabled.");
} else {
    console.log("Setting is disabled.");
}
```

### React to changes

The `onChange()` event handler will be called as soon as the user changes the
setting in the settings panel.

This will also be called when the extension is registered, on every page load.

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Write a setting

```javascript
import { app } from "../../scripts/app.js";

try {
    await app.extensionManager.setting.set("example.boolean", true);
} catch (error) {
    console.error(`Error changing setting: ${error}`);
}
```

### Extra configuration

The setting types are based on [PrimeVue](https://primevue.org/) components.
Props described in the PrimeVue documentation can be defined for ComfyUI
settings by adding them in an `attrs` field.

For instance, this adds increment/decrement buttons to a number input:

```javascript
{
    id: "example.number",
    name: "Example number setting",
    type: "number",
    defaultValue: 0,
    attrs: {
        showButtons: true,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

## Types

### Boolean

This shows an on/off toggle.

Based on the [ToggleSwitch PrimeVue
component](https://primevue.org/toggleswitch/).

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Text

This is freeform text.

Based on the [InputText PrimeVue component](https://primevue.org/inputtext/).

```javascript
{
    id: "example.text",
    name: "Example text setting",
    type: "text",
    defaultValue: "Foo",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Number

This for entering numbers.

To allow decimal places, set the `maxFractionDigits` attribute to a number greater than zero.

Based on the [InputNumber PrimeVue
component](https://primevue.org/inputnumber/).

```javascript
{
    id: "example.number",
    name: "Example number setting",
    type: "number",
    defaultValue: 42,
    attrs: {
        showButtons: true,
        maxFractionDigits: 1,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Slider

This lets the user enter a number directly or via a slider.

Based on the [Slider PrimeVue component](https://primevue.org/slider/). Ranges
are not supported.

```javascript
{
    id: "example.slider",
    name: "Example slider setting",
    type: "slider",
    attrs: {
        min: -10,
        max: 10,
        step: 0.5,
    },
    defaultValue: 0,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Combo

This lets the user pick from a drop-down list of values.

You can provide options either as a plain string or as an object with `text`
and `value` fields. If you only provide a plain string, then it will be used
for both.

You can let the user enter freeform text by supplying the `editable: true`
attribute, or search by supplying the `filter: true` attribute.

Based on the [Select PrimeVue component](https://primevue.org/select/). Groups
are not supported.

```javascript
{
    id: "example.combo",
    name: "Example combo setting",
    type: "combo",
    defaultValue: "first",
    options: [
        { text: "My first option", value: "first" },
        "My second option",
    ],
    attrs: {
        editable: true,
        filter: true,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Color

This lets the user select a color from a color picker or type in a hex
reference.

Note that the format requires six full hex digits - three digit shorthand does
not work.

Based on the [ColorPicker PrimeVue
component](https://primevue.org/colorpicker/).

```javascript
{
    id: "example.color",
    name: "Example color setting",
    type: "color",
    defaultValue: "ff0000",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Image

This lets the user upload an image.

The setting will be saved as a [data
URL](https://developer.mozilla.org/en-US/docs/Web/URI/Schemes/data).

Based on the [FileUpload PrimeVue
component](https://primevue.org/fileupload/).

```javascript
{
    id: "example.image",
    name: "Example image setting",
    type: "image",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Hidden

Hidden settings aren't displayed in the settings panel, but you can read and
write to them from your code.

```javascript
{
    id: "example.hidden",
    name: "Example hidden setting",
    type: "hidden",
}
```

## Other

### Categories

You can specify the categorisation of your setting separately to the `id`.
This means you can change the categorisation and naming without changing the
`id` and losing the values that have already been set by users.

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    category: ["Category name", "Section heading", "Setting label"],
}
```

### Tooltips

You can add extra contextual help with the `tooltip` field. This adds a small 
icon after the field name that will show the help text when the user hovers
over it.

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    tooltip: "This is some helpful information",
}
```


# Overview
Source: https://docs.comfy.org/custom-nodes/overview



Custom nodes allow you to implement new features and share them with the wider community.

A custom node is like any Comfy node: it takes input, does something to it, and produces an output.
While some custom nodes perform highly complex tasks, many just do one thing. Here's an example of a
simple node that takes an image and inverts it.

![Unique Images Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/invert_image_node.png)

## Client-Server Model

Comfy runs in a client-server model. The server, written in Python, handles all the real work: data-processing, models, image diffusion etc. The client, written in Javascript, handles the user interface.

Comfy can also be used in API mode, in which a workflow is sent to the server by a non-Comfy client (such as another UI, or a command line script).

Custom nodes can be placed into one of four categories:

### Server side only

The majority of Custom Nodes run purely on the server side, by defining a Python class that specifies the input and output types, and provides a function that can be called to process inputs and produce an output.

### Client side only

A few Custom Nodes provide a modification to the client UI, but do not add core functionality. Despite the name, they may not even add new nodes to the system.

### Independent Client and Server

Custom nodes may provide additional server features, and additional (related) UI features (such as a new widget to deal with a new data type). In most cases, communication between the client and server can be handled by the Comfy data flow control.

### Connected Client and Server

In a small number of cases, the UI features and the server need to interact with each other directly.

<Warning>Any node that requires Client-Server communication will not be compatible with use through the API.</Warning>


# Tips
Source: https://docs.comfy.org/custom-nodes/tips



### Recommended Development Lifecycle


# Getting Started
Source: https://docs.comfy.org/custom-nodes/walkthrough



This page will take you step-by-step through the process of creating a custom node.

Our example will take a batch of images, and return one of the images. Initially, the node
will return the image which is, on average, the lightest in color; we'll then extend
it to have a range of selection criteria, and then finally add some client side code.

This page assumes very little knowledge of Python or Javascript.

After this walkthrough, dive into the details of [backend code](./backend/server_overview), and
[frontend code](./backend/server_overview).

## Write a basic node

### Prerequisites

* A working ComfyUI [installation](/installation/manual_install). For development, we recommend installing ComfyUI manually.
* A working comfy-cli [installation](/comfy-cli/getting-started).

### Setting up

```bash
cd ComfyUI/custom_nodes
comfy node scaffold
```

After answering a few questions, you'll have a new directory set up.

```bash
 ~  % comfy node scaffold
You've downloaded .cookiecutters/cookiecutter-comfy-extension before. Is it okay to delete and re-download it? [y/n] (y): y
  [1/9] full_name (): Comfy
  [2/9] email (you@gmail.com): me@comfy.org
  [3/9] github_username (your_github_username): comfy
  [4/9] project_name (My Custom Nodepack): FirstComfyNode
  [5/9] project_slug (firstcomfynode): 
  [6/9] project_short_description (A collection of custom nodes for ComfyUI): 
  [7/9] version (0.0.1): 
  [8/9] Select open_source_license
    1 - GNU General Public License v3
    2 - MIT license
    3 - BSD license
    4 - ISC license
    5 - Apache Software License 2.0
    6 - Not open source
    Choose from [1/2/3/4/5/6] (1): 1
  [9/9] include_web_directory_for_custom_javascript [y/n] (n): y
Initialized empty Git repository in firstcomfynode/.git/
 Custom node project created successfully!
```

### Defining the node

Add the following code to the end of `src/nodes.py`:

```Python src/nodes.py
class ImageSelector:
    CATEGORY = "example"
    @classmethod    
    def INPUT_TYPES(s):
        return { "required":  { "images": ("IMAGE",), } }
    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "choose_image"
```

<Info>The basic structure of a custom node is described in detail [here](/custom-nodes/backend/server_overview). </Info>

A custom node is defined using a Python class, which must include these four things: `CATEGORY`,
which specifies where in the add new node menu the custom node will be located,
`INPUT_TYPES`, which is a class method defining what inputs the node will take
(see [later](/custom-nodes/backend/server_overview#input-types) for details of the dictionary returned),
`RETURN_TYPES`, which defines what outputs the node will produce, and `FUNCTION`, the name
of the function that will be called when the node is executed.

<Tip>Notice that the data type for input and output is `IMAGE` (singular) even though
we expect to receive a batch of images, and return just one. In Comfy, `IMAGE` means
image batch, and a single image is treated as a batch of size 1.</Tip>

### The main function

The main function, `choose_image`, receives named arguments as defined in `INPUT_TYPES`, and
returns a `tuple` as defined in `RETURN_TYPES`. Since we're dealing with images, which are internally
stored as `torch.Tensor`,

```Python
import torch
```

Then add the function to your class. The datatype for image is `torch.Tensor` with shape `[B,H,W,C]`,
where `B` is the batch size and `C` is the number of channels - 3, for RGB. If we iterate over such
a tensor, we will get a series of `B` tensors of shape `[H,W,C]`. The `.flatten()` method turns
this into a one dimensional tensor, of length `H*W*C`, `torch.mean()` takes the mean, and `.item()`
turns a single value tensor into a Python float.

```Python
def choose_image(self, images):
    brightness = list(torch.mean(image.flatten()).item() for image in images)
    brightest = brightness.index(max(brightness))
    result = images[brightest].unsqueeze(0)
    return (result,)
```

Notes on those last two lines:

* `images[brightest]` will return a Tensor of shape `[H,W,C]`. `unsqueeze` is used to insert a (length 1) dimension at, in this case, dimension zero, to give
  us `[B,H,W,C]` with `B=1`: a single image.
* in `return (result,)`, the trailing comma is essential to ensure you return a tuple.

### Register the node

To make Comfy recognize the new node, it must be available at the package level. Modify the `NODE_CLASS_MAPPINGS` variable at the end of `src/nodes.py`. You must restart ComfyUI to see any changes.

```Python src/nodes.py

NODE_CLASS_MAPPINGS = {
    "Example" : Example,
    "Image Selector" : ImageSelector,
}

# Optionally, you can rename the node in the `NODE_DISPLAY_NAME_MAPPINGS` dictionary.
NODE_DISPLAY_NAME_MAPPINGS = {
    "Example": "Example Node",
    "Image Selector": "Image Selector",
}
```

<Info>For a detailed explanation of how ComfyUI discovers and loads custom nodes, see the [node lifecycle documentation](/custom-nodes/backend/lifecycle).</Info>

## Add some options

That node is maybe a bit boring, so we might add some options; a widget that allows you to
choose the brightest image, or the reddest, bluest, or greenest. Edit your `INPUT_TYPES` to look like:

```Python
@classmethod    
def INPUT_TYPES(s):
    return { "required":  { "images": ("IMAGE",), 
                            "mode": (["brightest", "reddest", "greenest", "bluest"],)} }
```

Then update the main function. We'll use a fairly naive definition of 'reddest' as being the average
`R` value of the pixels divided by the average of all three colors. So:

```Python
def choose_image(self, images, mode):
    batch_size = images.shape[0]
    brightness = list(torch.mean(image.flatten()).item() for image in images)
    if (mode=="brightest"):
        scores = brightness
    else:
        channel = 0 if mode=="reddest" else (1 if mode=="greenest" else 2)
        absolute = list(torch.mean(image[:,:,channel].flatten()).item() for image in images)
        scores = list( absolute[i]/(brightness[i]+1e-8) for i in range(batch_size) )
    best = scores.index(max(scores))
    result = images[best].unsqueeze(0)
    return (result,)
```

## Tweak the UI

Maybe we'd like a bit of visual feedback, so let's send a little text message to be displayed.

### Send a message from server

This requires two lines to be added to the Python code:

```Python
from server import PromptServer
```

and, at the end of the `choose_image` method, add a line to send a message to the front end (`send_sync` takes a message
type, which should be unique, and a dictionary)

```Python
PromptServer.instance.send_sync("example.imageselector.textmessage", {"message":f"Picked image {best+1}"})
return (result,)
```

### Write a client extension

To add some Javascript to the client, create a subdirectory, `web/js` in your custom node directory, and modify the end of `__init__.py`
to tell Comfy about it by exporting `WEB_DIRECTORY`:

```Python
WEB_DIRECTORY = "./web/js"
__all__ = ['NODE_CLASS_MAPPINGS', 'WEB_DIRECTORY']
```

The client extension is saved as a `.js` file in the `web/js` subdirectory, so create `image_selector/web/js/imageSelector.js` with the
code below. (For more, see [client side coding](./js/javascript_overview)).

```Javascript
app.registerExtension({
	name: "example.imageselector",
    async setup() {
        function messageHandler(event) { alert(event.detail.message); }
        app.api.addEventListener("example.imageselector.textmessage", messageHandler);
    },
})
```

All we've done is register an extension and add a listener for the message type we are sending in the `setup()` method. This reads the dictionary we sent (which is stored in `event.detail`).

Stop the Comfy server, start it again, reload the webpage, and run your workflow.

### The complete example

The complete example is available [here](https://gist.github.com/robinjhuang/fbf54b7715091c7b478724fc4dffbd03). You can download the example workflow [JSON file](https://github.com/Comfy-Org/docs/blob/main/public/workflow.json) or view it below:

<div align="center">
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/firstnodeworkflow.png" alt="Image Selector Workflow" width="100%" />
</div>


# Workflow templates
Source: https://docs.comfy.org/custom-nodes/workflow_templates



If you have example workflow files associated with your custom nodes
then ComfyUI can show these to the user in the template browser (`Workflow`/`Browse Templates` menu).
Workflow templates are a great way to support people getting started with your nodes.

All you have to do as a node developer is to create an `example_workflows` folder and place the `json` files there.
Optionally you can place `jpg` files with the same name to be shown as the template thumbnail.

Under the hood ComfyUI statically serves these files along with an endpoint (`/api/workflow_templates`)
that returns the collection of workflow templates.

## Example

Under `ComfyUI-MyCustomNodeModule/example_workflows/` directory:

* `My_example_workflow_1.json`
* `My_example_workflow_1.jpg`
* `My_example_workflow_2.json`

In this example ComfyUI's template browser shows a category called `ComfyUI-MyCustomNodeModule` with two items, one of which has a thumbnail.


# Messages
Source: https://docs.comfy.org/essentials/comfyui-server/comms_messages



## Messages

During execution (or when the state of the queue changes), the `PromptExecutor` sends messages back to the client
through the `send_sync` method of `PromptServer`.

These messages are received by a socket event listener defined in `api.js` (at time of writing around line 90, or search for `this.socket.addEventListener`),
which creates a `CustomEvent` object for any known message type, and dispatches it to any registered listeners.

An extension can register to receive events (normally done in the `setup()` function) following the standard Javascript idiom:

```Javascript
api.addEventListener(message_type, messageHandler);
```

If the `message_type` is not one of the built in ones, it will be added to the list of known message types automatically. The message `messageHandler`
will be called with a `CustomEvent` object, which extends the event raised by the socket to add a `.detail` property, which is a dictionary of
the data sent by the server. So usage is generally along the lines of:

```Javascript
function messageHandler(event) {
    if (event.detail.node == aNodeIdThatIsInteresting) {
        // do something with event.detail.other_things
    }
}
```

### Built in message types

During execution (or when the state of the queue changes), the `PromptExecutor` sends the following messages back to the client
through the `send_sync` method of `PromptServer`. An extension can register as a listener for any of these.

| event                   | when                                                                       | data                                                                                                    |
| ----------------------- | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |
| `execution_start`       | When a prompt is about to run                                              | `prompt_id`                                                                                             |
| `execution_error`       | When an error occurs during execution                                      | `prompt_id`, plus additional information                                                                |
| `execution_interrupted` | When execution is stopped by a node raising `InterruptProcessingException` | `prompt_id`, `node_id`, `node_type` and `executed` (a list of executed nodes)                           |
| `execution_cached`      | At the start of execution                                                  | `prompt_id`, `nodes` (a list of nodes which are being skipped because their cached outputs can be used) |
| `executing`             | When a new node is about to be executed                                    | `node` (node id or `None` to indicate completion), `prompt_id`                                          |
| `executed`              | When a node returns a ui element                                           | `node` (node id), `prompt_id`, `output`                                                                 |
| `progress`              | During execution of a node that implements the required hook               | `node` (node id), `prompt_id`, `value`, `max`                                                           |
| `status`                | When the state of the queue changes                                        | `exec_info`, a dictionary holding `queue_remaining`, the number of entries in the queue                 |

### Using executed

Despite the name, an `executed` message is not sent whenever a node completes execution (unlike `executing`), but only when the node
returns a ui update.

To do this, the main function needs to return a dictionary instead of a tuple:

```python
# at the end of my main method
        return { "ui":a_new_dictionary, "result": the_tuple_of_output_values }
```

`a_new_dictionary` will then be sent as the value of `output` in an `executed` message.
The `result` key can be omitted if the node has no outputs (see, for instance, the code for `SaveImage` in `nodes.py`)

### Custom message types

As indicated above, on the client side, a custom message type can be added simply by registering as a listener for a unique message type name.

```Javascript
api.addEventListener("my.custom.message", messageHandler);
```

On the server, the code is equally simple:

```Python
from server import PromptServer
# then, in your main execution function (normally)
        PromptServer.instance.send_sync("my.custom.message", a_dictionary)
```

#### Getting node\_id

Most of the built-in messages include the current node id in the value of `node`. It's likely that you will want to do the same.

The node\_id is available on the server side through a hidden input, which is obtained with the `hidden` key in the `INPUT_TYPES` dictionary:

```Python
    @classmethod    
    def INPUT_TYPES(s):
        return {"required" : { }, # whatever your required inputs are 
                "hidden": { "node_id": "UNIQUE_ID" } } # Add the hidden key

    def my_main_function(self, required_inputs, node_id):
        # do some things
        PromptServer.instance.send_sync("my.custom.message", {"node": node_id, "other_things": etc})
```


# Server Overview
Source: https://docs.comfy.org/essentials/comfyui-server/comms_overview



## Overview

The Comfy server runs on top of the [aiohttp framework](https://docs.aiohttp.org/), which in turn uses [asyncio](https://pypi.org/project/asyncio/).

Messages from the server to the client are sent by socket messages through the `send_sync` method of the server,
which is an instance of `PromptServer` (defined in `server.py`). They are processed
by a socket event listener registered in `api.js`. See [messages](./comms_messages).

Messages from the client to the server are sent by the `api.fetchApi()` method defined in `api.js`,
and are handled by http routes defined by the server. See [routes](./comms_routes).

<Tip>The client submits the whole workflow (widget values and all) when you queue a request.
The server does not receive any changes you make after you send a request to the queue.
If you want to modify server behavior during execution, you'll need routes.</Tip>


# Execution Model Inversion Guide
Source: https://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide



[PR #2666](https://github.com/comfyanonymous/ComfyUI/pull/2666) inverts the execution model from a back-to-front recursive model to a front-to-back topological sort. While most custom nodes should continue to "just work", this page is intended to serve as a guide for custom node creators to the things that *could* break.

## Breaking Changes

### Monkey Patching

Any code that monkey patched the execution model is likely to stop working. Note that the performance of execution with this PR exceeds that with the most popular monkey patches, so many of them will be unnecessary.

### Optional Input Validation

Prior to this PR, only nodes that were connected to outputs exclusively through a string of `"required"` inputs were actually validated. If you had custom nodes that were only ever connected to `"optional"` inputs, you previously wouldn't have been seeing that they failed validation.

<Tip>If your nodes' outputs could already be connected to `"required"` inputs, it is unlikely that anything in this section applies to you. It will primarily apply to custom node authors who use custom types and exclusively use `"optional"` inputs.</Tip>

Here are some of the things that could cause you to fail validation along with recommended solutions:

* Use of reserved [Additional Parameters](/custom-nodes/backend/datatypes#additional-parameters) like `min` and `max` on types that aren't comparable (e.g. dictionaries) in order to configure custom widgets.
  * Change the additional parameters used to non-reserved keys like `uiMin` and `uiMax`. *(Recommended Solution)*
    ```python
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "my_size": ("VEC2", {"uiMin": 0.0, "uiMax": 1.0}),
            }
        }
    ```

  * Define a custom [VALIDATE\_INPUTS](/custom-nodes/backend/server_overview#validate-inputs) function with this input so validation of it is skipped. *(Quick Solution)*
    ```python
    @classmethod
    def VALIDATE_INPUTS(cls, my_size):
        return True
    ```

* Use of composite types (e.g. `CUSTOM_A,CUSTOM_B`)
  * (When used as output) Define and use a wrapper like `MakeSmartType` [seen here in the PR's unit tests](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R2)
    ```python
    class MyCustomNode:

        @classmethod
        def INPUT_TYPES(cls):
            return {
                "required": {
                    "input": (MakeSmartType("FOO,BAR"), {}),
                }
            }

        RETURN_TYPES = (MakeSmartType("FOO,BAR"),)

        # ...
    ```
  * (When used as input) Define a custom[VALIDATE\_INPUTS](/custom-nodes/backend/server_overview#validate-inputs) function that takes a `input_types` argument so type validation is skipped.
    ```python
    @classmethod
    def VALIDATE_INPUTS(cls, input_types):
        return True
    ```
  * (Supports both, convenient) Define and use the `@VariantSupport` decorator [seen here in the PR's unit tests](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R15)
    ```python
    @VariantSupport
    class MyCustomNode:

        @classmethod
        def INPUT_TYPES(cls):
            return {
                "required": {
                    "input": ("FOO,BAR", {}),
                }
            }
        
        RETURN_TYPES = (MakeSmartType("FOO,BAR"),)

        # ...
    ```

* The use of lists (e.g. `[1, 2, 3]`) as constants in the graph definition (e.g. to represent a const `VEC3` input). This would have required a front-end extension before. Previously, lists of size exactly `2` would have failed anyway -- they would have been treated as broken links.
  * Wrap the lists in a dictionary like `{ "value": [1, 2, 3] }`

### Execution Order

Execution order has always changed depending on which nodes happen to have which IDs, but it may now change depending on which values are cached as well. In general, the execution order should be considered non-deterministic and subject to change (beyond what is enforced by the graph's structure).

Don't rely on the execution order.

*HIC SUNT DRACONES*

## New Functionality

### Validation Changes

A number of features were added to the `VALIDATE_INPUTS` function in order to lessen the impact of the [Optional Input Validation](#optional-input-validation) mentioned above.

* Default validation will now be skipped for inputs which are received by the `VALIDATE_INPUTS` function.
* The `VALIDATE_INPUTS` function can now take `**kwargs` which causes all inputs to be treated as validated by the node creator.
* The `VALIDATE_INPUTS` function can take an input named `input_types`. This input will be a dict mapping each input (connected via a link) to the type of the connected output. When this argument exists, type validation for the node's inputs is skipped.

You can read more at [VALIDATE\_INPUTS](/custom-nodes/backend/server_overview#validate-inputs).

### Lazy Evaluation

Inputs can be evaluated lazily (i.e. you can wait to see if they are needed before evaluating the attached node and all its ancestors). See [Lazy Evaluation](/custom-nodes/backend/lazy_evaluation) for more information.

### Node Expansion

At runtime, nodes can expand into a subgraph of nodes. This is what allows loops to be implemented (via tail-recursion). See [Node Expansion](/custom-nodes/backend/expansion) for more information.


# Dependencies
Source: https://docs.comfy.org/essentials/core-concepts/dependencies



{/*
  description: "Understand asset and software dependencies in ComfyUI"
  */}

## A workflow file depends on other files

A ComfyUI workflow is usually not self-contained. In other words, a workflow can't function unless it can access the other files it needs. These are called **dependencies** because the primary document, the workflow file, ***depends*** on other files in order to do its work.

## Assets

An AI model is an example of an ***asset***. In media production, an asset is some media file that supplies input data. For example, a video editing program operates on movie files stored on disk. The editing programs project file holds links to these movie file assets, allowing non-destructive editing that doesnt alter the original movie files.

ComfyUI works the same way. A workflow can only run if all of the required assets are found and loaded. Generative AI models, images, movies, and sounds are some examples of assets that a workflow might depend upon. These are therefore known as ***dependent assets*** or ***asset dependencies***.

## Software

An advanced application like ComfyUI also has ***software dependencies***. These are libraries of programming code and data that are required for the application to run. Custom nodes are examples of software dependencies. On an even more fundamental level, the Python programming environment is the ultimate dependency for ComfyUI. The correct version of Python is required to run a particular version of ComfyUI. Updates to Python, ComfyUI, and custom nodes can all be handled from the **ComfyUI Manager** window.

![ComfyUI Custom Nodes Manager](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_dependecies_custom-nodes-manager.png)


# Links
Source: https://docs.comfy.org/essentials/core-concepts/links



{/*
  description: "Understand connection links in ComfyUI"
  */}

## Links connect nodes

In the terminology of ComfyUI, the lines or curves between nodes are called ***links***. Theyre also known as ***connections*** or wires. Links can be displayed in several ways, such as straight lines (the default), spline curves, or completely hidden. Display of links is controlled from the ComfyUI **Setup** window and the **Toggle Link Visibility** button on the display toolbar at the lower right of the ComfyUI main window.

Link display is crucial. Depending on the situation, it may be necessary to see all links. Especially when learning, sharing, or even just understanding workflows, the visibility of links enables users to follow the flow of data through the graph. For packaged workflows that arent intended to be altered, it might make sense to hide the links to reduce clutter.

![color-coded links](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_links.png)

### Reroute node

If legibility of the graph structure is important, then link wires can be manually routed in the 2D space of the graph with a tiny node called **Reroute**. Its purpose is to position the beginning and/or end points of link wires to ensure visibility. We can design a workflow so that link wires dont pass behind nodes, dont cross other link wires, and so on.

## Color-coding

The data type of node properties is indicated by color coding of input/output ports and link connection wires. We can always tell which inputs and outputs can be connected to one another by their color. Ports can only be connected to other ports of the same color.

Common data types:

| Data type                 | Color       |
| ------------------------- | ----------- |
| diffusion model           | lavender    |
| CLIP model                | yellow      |
| VAE model                 | rose        |
| conditioning              | orange      |
| latent image              | pink        |
| pixel image               | blue        |
| mask                      | green       |
| number (integer or float) | light green |


# Models
Source: https://docs.comfy.org/essentials/core-concepts/models



{/*
  description: "Understand AI models and their role in ComfyUI"
  */}

## Models are essential

Models are essential building blocks for media generation workflows. They can be combined and mixed to achieve different creative effects.

The word ***model*** has many different meanings. Here, it means a data file carrying information that is required for a node graph to do its work. Specifically, its a data structure that *models* some function. As a verb, to model something means to represent it or provide an example.

The primary example of a model data file in ComfyUI is an AI ***diffusion model***. This is a large set of data that represents the complex relationships among text strings and images, making it possible to translate words into pictures or vice versa. Other examples of common models used for image generation are language models such as CLIP, and upscaling models such as RealESRGAN.

## Model files

Model files are absolutely required for generative media production. Nothing can happen in a workflow if the model files are not found. Models are not included in the ComfyUI installation, but ComfyUI can often automatically download and install missing model files. Many models can be downloaded and installed from the **ComfyUI Manager** window. Models can also be found at websites such as [huggingface.co](https://huggingface.co), [civitai.green](https://civitai.green), and [github.com](https://github.com).

### Using Models in ComfyUI

1. Download and place them in the ComfyUI program directory
   1. Within the **models** folder, you'll find subfolders for various types of models, such as **checkpoints**
   2. The **ComfyUI Manager** helps to automate the process of searching, downloading, and installing
   3. Restart ComfyUI if it's running
2. In your workflow, create the node appropriate to the model type, e.g. **Load Checkpoint**, **Load LoRA**, **Load VAE**
3. In the loader node, choose the model you wish to use
4. Connect the loader node to other nodes in your workflow

### File size

Models can be extremely large files relative to image files. A typical uncompressed image may require a few megabytes of disk storage. Generative AI models can be tens of thousands of times larger, up to tens of gigabytes per model. They take up a great deal of disk space and take a long time to transfer over a network.

## Model training and refinement

A generative AI model is created by training a machine learning program on a very large set of data, such as pairs of images and text descriptions. An AI model doesnt store the training data explicitly, but rather it stores the correlations that are implicit within the data.

Organizations and companies such as Stability AI and Black Forest Labs release base models that carry large amounts of generic information. These are general purpose generative AI models. Commonly, the base models need to be ***refined*** in order to get high quality generative outputs. A dedicated community of people work to refine the base models. The new, refined models produce better output, provide new or different functionality, and/or use fewer resources. Refined models can usually be run on systems with less computing power and/or memory.

## Auxiliary models

Model functionality can be extended with auxiliary models. For example, art directing a text-to-image workflow to achieve a specific result may be difficult or impossible using a diffusion model alone. Additional models can refine a diffusion model within the workflow graph to produce desired results. Examples include **LoRA** (Low Rank Adaptation), a small model that is trained on a specific subject; **ControlNet**, a model that helps control composition using a guide image; and **Inpainting**, a model that allows certain diffusion models to generate new content within an existing image.

![auxiliary models](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_auxiliary-model.png)


# Nodes
Source: https://docs.comfy.org/essentials/core-concepts/nodes



{/*
  description: "Understand the concept of a node in ComfyUI."
  */}

## Nodes perform operations

In computer science, a ***node*** is a container for information, usually including programmed instructions to perform some task. Nodes almost never exist in isolation, theyre almost always connected to other nodes in a networked graph. In ComfyUI, nodes take the visual form of boxes that are connected to each other.

ComfyUI nodes are usually ***function operators***. This means that they operate on some data to perform a function. A function is a process that accepts input data, performs some operation on it, and produces output data. In other words, nodes do some work, contributing to the completion of a task such as generating an image. So ComfyUI nodes almost always have at least one input or output, and usually have multiple inputs and outputs.

## Custom Nodes

ComfyUI includes many powerful nodes in the base installation package. These are known as **Comfy Core** nodes. Additionally, the ComfyUI community has created an amazing array of [***custom nodes***](https://registry.comfy.org) to perform a wide variety of functions.

## ComfyUI Manager

![ComfyUI Manager interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_nodes_manager.png)

The **ComfyUI Manager** window makes it easy to perform custom node management tasks such as search, install, update, disable, and uninstall. The Manager is included in the ComfyUI desktop application, but not in the ComfyUI server application.

### Installing the Manager

If you're running the ComfyUI server application, you need to install the Manager. If ComfyUI is running, shut it down before proceeding.

The first step is to install Git, a command-line application for software version control. Git will download the ComfyUI Manager from [github.com](https://github.com). Download Git from [git-scm.com](https://git-scm.com/) and install it.

Once Git is installed, navigate to the ComfyUI server program directory, to the folder labeled **custom\_nodes**. Open up a command window or terminal. Make sure that the command line displays the current directory path as **custom\_nodes**. Enter the following command. This will download the Manager. Technically, this is known as *cloning a Git repository*.

```bash
git clone https://github.com/ltdrdata/ComfyUI-Manager.git
```

For details or special cases, see [ComfyUI Manager Install](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#installation).


# Properties
Source: https://docs.comfy.org/essentials/core-concepts/properties



{/*
  description: "Understand node properties in ComfyUI"
  */}

## Nodes are containers for properties

Nodes usually have ***properties***. Also known as ***parameters*** or ***attributes***, node properties are variables that can be changed. Some properties can be adjusted manually by the user, using a data entry field called a ***widget***. Other properties can be driven automatically by other nodes connected to the property ***input slot*** or port. Usually, a property can be converted from widget to input and vice versa, allowing users to control property values manually or automatically.

Properties can take many forms and hold many different types of information. For example, a **Load Checkpoint** node has a single property: the file path to the generative model checkpoint file. A **KSampler** node has multiple properties such as the number of sampling **steps**, **CFG** scale, **sampler\_name**, etc.

![node properties](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_properties.png)

## Data types

Information can come in many different forms, called ***data types***. For example, alphanumeric text is known as a ***string***, a whole number is an ***integer***, and a number with a decimal point is known as a ***floating point*** number or ***float***. New data types are always being added to ComfyUI.

ComfyUI is written in the Python scripting language, which is very forgiving about data types. By contrast, the ComfyUI environment is very ***strongly typed***. This means that different data types cant be mixed up. For example, we cant connect an image output to an integer input. This is a huge benefit to users, guiding them to proper workflow construction and preventing program errors.


# Workflow
Source: https://docs.comfy.org/essentials/core-concepts/workflow



{/*
  description: "Understand the concept of a workflow in ComfyUI."
  */}

## A graph of nodes

ComfyUI is an environment for building and running generative content ***workflows***. In this context, a workflow is defined as a collection of program objects called ***nodes*** that are connected to each other, forming a network. This network is also known as a ***graph***.

A ComfyUI workflow can generate any type of media: image, video, audio, AI model, AI agent, and so on.

## Sample workflows

To get started, try out some of the [official workflows](https://comfyanonymous.github.io/ComfyUI_examples). These use only the Core nodes included in the ComfyUI installation. A thriving community of developers has created a rich [ecosystem](https://registry.comfy.org) of custom nodes to extend the functionality of ComfyUI.

### Simple Example

![simple workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/simple_workflow.jpeg)

## Visual programming

A node-based computer program like ComfyUI provides a level of power and flexibility that cant be achieved with traditional menu- and button-driven applications. The ComfyUI node graph is not limited by the tools provided in a traditional computer application. Its a high-level ***visual programming environment*** allowing users to design complex systems without needing to write program code or understand advanced mathematics.

Many other computer applications use this same node graph paradigm. Examples include the compositing application called Nuke, the 3D programs Maya and Blender, the Unreal real-time graphics engine, and the interactive media authoring program called Max.

### More Complex Example

![complex workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/complex_workflow.jpeg)

## Procedural framework

Another term used to describe a node-based application is ***procedural framework***. Procedural means generative: some procedure or algorithm is employed to generate content such as a 3D model or a musical composition.

ComfyUI is all of these things: a node graph, a visual programming environment, and a procedural framework. What makes ComfyUI different (and amazing!) is that its radically open structure allows us to generate any type of media asset such as picture, movie, sound, 3D model, AI model, etc.

In the context of ComfyUI, the term ***workflow*** is a synonym for the node network or graph. It corresponds to the ***scene graph*** in a 3D or multimedia program: the network of all of the nodes within a particular disk file. 3D programs call this a ***scene file***. Video editing, compositing, and multimedia programs usually call it a ***project file***.

## Saving workflows

The ComfyUI workflow is automatically saved in the metadata of any generated image, allowing users to open and use the graph that generated the image. A workflow can also be stored in a human-readable text file that follows the JSON data format. This is necessary for media formats that dont support metadata. ComfyUI workflows stored as JSON files are very small, allowing convenient versioning, archiving, and sharing of graphs, independently of any generated media.


# Keyboard and Mouse Shortcuts
Source: https://docs.comfy.org/essentials/shortcuts



{/*
  description: "A list of keyboard and mouse shortcuts for ComfyUI"
  */}

{/* TODO(yoland): Add this back to comfyUI readme page */}

<Tabs>
  <Tab title="Windows/Linux">
    | Shortcut                            | Command                                                                                                            |
    | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------ |
    | Ctrl + Enter                        | Queue up current graph for generation                                                                              |
    | Ctrl + Shift + Enter                | Queue up current graph as first for generation                                                                     |
    | Ctrl + Z / Ctrl + Y                 | Undo/Redo                                                                                                          |
    | Ctrl + S                            | Save workflow                                                                                                      |
    | Ctrl + O                            | Load workflow                                                                                                      |
    | Ctrl + A                            | Select all nodes                                                                                                   |
    | Alt + C                             | Collapse/uncollapse selected nodes                                                                                 |
    | Ctrl + M                            | Mute/unmute selected nodes                                                                                         |
    | Ctrl + B                            | Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)            |
    | Delete<br />Backspace               | Delete selected nodes                                                                                              |
    | Ctrl + Delete<br />Ctrl + Backspace | Delete the current graph                                                                                           |
    | Space                               | Move the canvas around when held and moving the cursor                                                             |
    | Ctrl + Click<br />Shift + Click     | Add clicked node to selection                                                                                      |
    | Ctrl + C/Ctrl + V                   | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)                     |
    | Ctrl + C/Ctrl + Shift + V           | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |
    | Shift + Drag                        | Move multiple selected nodes at the same time                                                                      |
    | Ctrl + D                            | Load default graph                                                                                                 |
    | Q                                   | Toggle visibility of the queue                                                                                     |
    | H                                   | Toggle visibility of history                                                                                       |
    | R                                   | Refresh graph                                                                                                      |
    | Double-Click LMB                    | Quick search for nodes to add                                                                                      |
  </Tab>

  <Tab title="MacOS">
    | Keybind                               | Explanation                                                                                                        |
    | ------------------------------------- | ------------------------------------------------------------------------------------------------------------------ |
    | Cmd  + Enter                         | Queue up current graph for generation                                                                              |
    | Cmd  + Shift + Enter                 | Queue up current graph as first for generation                                                                     |
    | Cmd  + Z/Cmd  + Y                   | Undo/Redo                                                                                                          |
    | Cmd  + S                             | Save workflow                                                                                                      |
    | Cmd  + O                             | Load workflow                                                                                                      |
    | Cmd  + A                             | Select all nodes                                                                                                   |
    | Opt  + C                             | Collapse/uncollapse selected nodes                                                                                 |
    | Cmd  + M                             | Mute/unmute selected nodes                                                                                         |
    | Cmd  + B                             | Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)            |
    | Delete<br />Backspace                 | Delete selected nodes                                                                                              |
    | Cmd  + Delete<br />Cmd  + Backspace | Delete the current graph                                                                                           |
    | Space                                 | Move the canvas around when held and moving the cursor                                                             |
    | Cmd  + Click<br />Shift + Click      | Add clicked node to selection                                                                                      |
    | Cmd  + C / Cmd  + V                 | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)                     |
    | Cmd  + C / Cmd  + Shift + V         | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |
    | Shift + Drag                          | Move multiple selected nodes at the same time                                                                      |
    | Cmd  + D                             | Load default graph                                                                                                 |
    | Q                                     | Toggle visibility of the queue                                                                                     |
    | H                                     | Toggle visibility of history                                                                                       |
    | R                                     | Refresh graph                                                                                                      |
    | Double-Click LMB                      | Quick search for nodes to add                                                                                      |
  </Tab>
</Tabs>


# Getting Started with AI Image Generation
Source: https://docs.comfy.org/get_started/first_generation

This tutorial will guide you through your first image generation with ComfyUI, covering basic interface operations like workflow loading, model installation, and image generation

This guide aims to help you understand ComfyUI's basic operations and complete your first image generation. We'll cover:

1. Loading example workflows
   * Loading from ComfyUI's workflow templates
   * Loading from images with workflow metadata
2. Model installation guidance
   * Automatic model installation
   * Manual model installation
   * Using ComfyUI Manager for model installation
3. Completing your first text-to-image generation

## About Text-to-Image

Text-to-Image is a fundamental AI drawing feature that generates images from text descriptions. It's one of the most commonly used functions in AI art generation. You can think of the process as telling your requirements (positive and negative prompts) to an artist (the drawing model), who will then create what you want. Detailed explanations about text-to-image will be covered in the [Text to Image](/tutorials/basic/text-to-image) chapter.

## ComfyUI Text-to-Image Workflow Tutorial

### 1. Launch ComfyUI

Make sure you've followed the installation guide to start ComfyUI and can successfully enter the ComfyUI interface.

![ComfyUI Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/comfyui-boot-screen.jpg)

If you have not installed ComfyUI, please choose a suitable version to install based on your device.

<AccordionGroup>
  <Accordion title="ComfyUI Desktop (Recommended)">
    ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

    * Code is open source on [Github](https://github.com/Comfy-Org/desktop)

    You can choose the appropriate installation for your system and hardware below

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI Desktop (Windows) Installation Guide" icon="link" href="/installation/desktop/windows">
          Suitable for **Windows** version with **Nvidia** GPU
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI Desktop (MacOS) Installation Guide" icon="link" href="/installation/desktop/macos">
          Suitable for MacOS with **Apple Silicon**
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](/installation/manual_install) section to install ComfyUI</Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI Portable (Windows)">
    <Card title="ComfyUI Portable (Windows) Installation Guide" icon="link" href="/installation/comfyui_portable_windows">
      Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.
    </Card>
  </Accordion>

  <Accordion title="Manual Installation">
    <Card title="ComfyUI Manual Installation Guide" icon="link" href="/installation/manual_install">
      Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)
    </Card>
  </Accordion>
</AccordionGroup>

### 2. Load Default Text-to-Image Workflow

ComfyUI usually loads the default text-to-image workflow automatically when launched. However, you can try different methods to load workflows to familiarize yourself with ComfyUI's basic operations:

<Tabs>
  <Tab title="Load from Workflow Template">
    ![ComfyUI Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1.jpg)
    Follow the numbered steps in the image:

    1. Click the **Fit View** button in the bottom right to ensure any loaded workflow isn't hidden
    2. Click the **folder icon (workflows)** in the sidebar
    3. Click the **Browse example workflows** button at the top of the Workflows panel

    Continue with:
    ![Load Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg)

    4. Select the first default workflow **Image Generation** to load it

    Alternatively, you can select **Browse workflow templates** from the workflow menu
    ![ComfyUI Menu - Browse Workflow Templates](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg)
  </Tab>

  <Tab title="Load from Images with Metadata">
    All images generated by ComfyUI contain metadata including workflow information. You can load workflows by:

    * Dragging and dropping a ComfyUI-generated image into the interface
    * Using menu **Workflows** -> **Open** to open an image

    Try loading the workflow using this example image:
    ![ComfyUI-Text to Image Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/text-to-image-workflow.png)
  </Tab>

  <Tab title="Load from workflow.json">
    ComfyUI workflows can be stored in JSON format. You can export workflows using menu **Workflows** -> **Export**.

    Try downloading and loading this example workflow:

    <a className="prose" href="https://github.com/Comfy-Org/docs/blob/main/public/text-to-image.json" download style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
      <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download text-to-image.json</p>
    </a>

    After downloading, use menu **Workflows** -> **Open** to load the JSON file.
  </Tab>
</Tabs>

### 3. Model Installation

Most ComfyUI installations don't include base models by default. After loading the workflow, if you don't have the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model installed, you'll see this prompt:

![Missing Models](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg)

All models are stored in `<your ComfyUI installation>/ComfyUI/models/` with subfolders like `checkpoints`, `embeddings`, `vae`, `lora`, `upscale_model`, etc. ComfyUI detects models in these folders and paths configured in `extra_models_config.yaml` at startup.

![ComfyUI Models Folder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg)

You can install models through:

<Tabs>
  <Tab title="Automatic Download">
    After you click the **Download** button, ComfyUI will execute the download, and different behaviors will be performed depending on the version you are using.

    <Tabs>
      <Tab title="ComfyUI Desktop">
        The desktop version will automatically complete the model download and save it to the `<your ComfyUI installation location>/ComfyUI/models/checkpoints` directory.
        You can wait for the installation to complete or view the installation progress in the model panel on the sidebar.

        ![Model Download Progress](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg)

        If everything goes smoothly, the model should be able to download locally. If the download fails for a long time, please try other installation methods.
      </Tab>

      <Tab title="ComfyUI Portable">
        The browser will execute file downloads. Please save the file to the `<your ComfyUI installation location>/ComfyUI_windows_portable/ComfyUI/models/checkpoints` directory after the download is complete.
      </Tab>
    </Tabs>
  </Tab>

  <Tab title="ComfyUI Manager">
    ComfyUI Manager is a tool for managing custom nodes, models, and plugins.

    <Steps>
      <Step title="Open ComfyUI Manager">
        ![ComfyUI Manager Installation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager.jpg)

        Click the `Manager` button to open ComfyUI Manager
      </Step>

      <Step title="Open Model Manager">
        ![ComfyUI Manager Model Management](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-model-manager.jpg)

        Click `Model Manager`
      </Step>

      <Step title="Search and Install Model">
        ![ComfyUI Manager Model Download](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-download-model.jpg)

        1. Search for `v1-5-pruned-emaonly.ckpt`
        2. Click `install` on the desired model
      </Step>
    </Steps>
  </Tab>

  <Tab title="Manual Installation">
    Visit [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) and follow this guide:

    ![Hugging Face Model Download](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-5-hugging-face.jpg)

    Save the downloaded file to:

    <Tabs>
      <Tab title="ComfyUI Desktop">
        Save to `<your ComfyUI installation>/ComfyUI/models/checkpoints`

        ![ComfyUI Desktop Model Save Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-6-2-desktop.jpg)
      </Tab>

      <Tab title="ComfyUI Portable">
        Save to `ComfyUI_windows_portable/ComfyUI/models/checkpoints`

        ![ComfyUI Portable Model Save Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-6-1-portable.jpg)
      </Tab>
    </Tabs>

    Refresh or restart ComfyUI after saving.
  </Tab>
</Tabs>

### 4. Load Model and Generate Your First Image

After installing the model:

![Image Generation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg)

1. In the **Load Checkpoint** node, ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected
2. Click `Queue` or press `Ctrl + Enter` to generate

The result will appear in the **Save Image** node. Right-click to save locally.

![ComfyUI First Image Generation Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg)

For detailed text-to-image instructions, see our comprehensive guide:

<Card title="ComfyUI Text-to-Image Workflow Guide" icon="link" href="/tutorials/basic/text-to-image">
  Click here for detailed text-to-image workflow instructions
</Card>

## Troubleshooting

### Model Loading Issues

If the `Load Checkpoint` node shows no models or displays "null", verify your model installation location and try refreshing or restarting ComfyUI.


# Introduction
Source: https://docs.comfy.org/get_started/introduction

Official documentation for ComfyUI. Contribute [here](https://github.com/Comfy-Org/docs).

<img className="block dark:hidden" src="https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui_screenshot.png" alt="Hero Light" />

<img className="hidden dark:block" src="https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui_screenshot.png" alt="Hero Dark" />

## [ComfyUI](https://github.com/comfyanonymous/ComfyUI)

The most powerful and modular stable diffusion GUI and backend. Written by [comfyanonymous](https://github.com/comfyanonymous) and other [contributors](https://github.com/comfyanonymous/ComfyUI/graphs/contributors).

* **ComfyUI** is a node-based interface and inference engine for generative AI
* Users can combine various AI models and operations through nodes to achieve highly customizable and controllable content generation
* ComfyUI is completely open source and can run on your local device

## Getting Started with ComfyUI

### ComfyUI Installation

ComfyUI currently offers multiple installation methods, supporting Windows, MacOS, and Linux systems:

<AccordionGroup>
  <Accordion title="ComfyUI Desktop (Recommended)">
    ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

    * Code is open source on [Github](https://github.com/Comfy-Org/desktop)

    You can choose the appropriate installation for your system and hardware below

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI Desktop (Windows) Installation Guide" icon="link" href="/installation/desktop/windows">
          Suitable for **Windows** version with **Nvidia** GPU
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI Desktop (MacOS) Installation Guide" icon="link" href="/installation/desktop/macos">
          Suitable for MacOS with **Apple Silicon**
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](/installation/manual_install) section to install ComfyUI</Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI Portable (Windows)">
    <Card title="ComfyUI Portable (Windows) Installation Guide" icon="link" href="/installation/comfyui_portable_windows">
      Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.
    </Card>
  </Accordion>

  <Accordion title="Manual Installation">
    <Card title="ComfyUI Manual Installation Guide" icon="link" href="/installation/manual_install">
      Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)
    </Card>
  </Accordion>
</AccordionGroup>

## Contributing to ComfyUI Ecosystem

If you're planning to develop ComfyUI custom nodes (plugins), please read the following section.

<Card title="Custom Node Development Guide" icon="link" href="/custom-nodes/overview">
  Learn how to build a custom node (plugin) for ComfyUI
</Card>

## Contributing to Documentation

Fork the documentation [repo](https://github.com/comfyanonymous/ComfyUI) on Github and submit a PR to us


# ComfyUI(portable) Windows
Source: https://docs.comfy.org/installation/comfyui_portable_windows

This tutorial will guide you on how to download and start using ComfyUI Portable and run the corresponding programs

<Tip>
  For Nvidia 50 series (Blackwell) GPUs, please refer to the [System Requirements](/installation/nvidia-50-series) section to ensure your system meets the requirements for ComfyUI.
</Tip>

**ComfyUI Portable** is a standalone packaged complete ComfyUI Windows version that has integrated an independent **Python (python\_embeded)** required for ComfyUI to run. You only need to extract it to use it. Currently, the portable version supports running through **Nvidia GPU** or **CPU**.

This guide section will walk you through installing ComfyUI Portable.

## Download ComfyUI Portable

You can get the latest ComfyUI Portable download link by clicking the link below

<a className="prose" href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download ComfyUI Portable</p>
</a>

After downloading, you can use decompression software like [7-ZIP](https://7-zip.org/) to extract the compressed package

The file structure and description after extracting the portable version are as follows:

```
ComfyUI_windows_portable
 ComfyUI                   // ComfyUI main program
 python_embeded            // Independent Python environment
 update                    // Batch scripts for upgrading portable version
 README_VERY_IMPORTANT.txt   // ComfyUI Portable usage instructions in English
 run_cpu.bat                 // Double click to start ComfyUI (CPU only)
 run_nvidia_gpu.bat          // Double click to start ComfyUI (Nvidia GPU)
```

## How to Launch ComfyUI

Double click either `run_nvidia_gpu.bat` or `run_cpu.bat` depending on your computer's configuration to launch ComfyUI.
You will see the command running as shown in the image below

![ComfyUI Portable Command Prompt](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui-portable-cmd.png)

When you see something similar to the image

```
To see the GUI go to: http://127.0.0.1:8188
```

At this point, your ComfyUI service has started. Normally, ComfyUI will automatically open your default browser and navigate to `http://127.0.0.1:8188`. If it doesn't open automatically, please manually open your browser and visit this address.

<Warning>During use, please do not close the corresponding command line window, otherwise ComfyUI will stop running</Warning>

## First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey\~

<Card title="First Image Generation" icon="link" href="/get_started/first_generation">
  This tutorial will guide you through your first model installation and text-to-image generation
</Card>

## Additional ComfyUI Portable Instructions

### 1. Upgrading ComfyUI Portable

You can use the batch commands in the update folder to upgrade your ComfyUI Portable version

```
ComfyUI_windows_portable
 update
    update.py
    update_comfyui.bat                          // Update ComfyUI to the latest commit version
    update_comfyui_and_python_dependencies.bat  // Only use when you have issues with your runtime environment
    update_comfyui_stable.bat                   // Update ComfyUI to the latest stable version
```

### 2. ComfyUI Model Sharing and Custom Model Directory Configuration

If you are also using [A1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) or want to customize your model storage location, you can modify the following file to complete the configuration

```
ComfyUI_windows_portable
 ComfyUI
   extra_model_paths.yaml.example  // This file is the configuration template
```

Please copy and rename the `extra_model_paths.yaml.example` to `extra_model_paths.yaml`.

Below is the original configuration file content, which you can modify according to your needs

```yaml
#Rename this to extra_model_paths.yaml and ComfyUI will load it


#config for a1111 ui
#all you have to do is change the base_path to where yours is installed
a111:
    base_path: path/to/stable-diffusion-webui/

    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet

#config for comfyui
#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.

#comfyui:
#     base_path: path/to/comfyui/
#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads
#     #is_default: true
#     checkpoints: models/checkpoints/
#     clip: models/clip/
#     clip_vision: models/clip_vision/
#     configs: models/configs/
#     controlnet: models/controlnet/
#     diffusion_models: |
#                  models/diffusion_models
#                  models/unet
#     embeddings: models/embeddings/
#     loras: models/loras/
#     upscale_models: models/upscale_models/
#     vae: models/vae/

#other_ui:
#    base_path: path/to/ui
#    checkpoints: models/checkpoints
#    gligen: models/gligen
#    custom_nodes: path/custom_nodes

```

For example, if your WebUI is located at `D:\stable-diffusion-webui\`, you can modify the corresponding configuration to

```yaml
a111:
    base_path: D:\stable-diffusion-webui\
    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet
```

This way, models under paths like `D:\stable-diffusion-webui\models\Stable-diffusion\` can be detected and used by ComfyUI. Similarly, you can add other custom model location configurations

### 3. Setting Up LAN Access for ComfyUI Portable

If your ComfyUI is running on a local network and you want other devices to access ComfyUI, you can modify the `run_nvidia_gpu.bat` or `run_cpu.bat` file using Notepad to complete the configuration. This is mainly done by adding `--listen` to specify the listening address.
Below is an example of the `run_nvidia_gpu.bat` file command with the `--listen` parameter added

```bat
.\python_embeded\python.exe -s ComfyUI\main.py --listen --windows-standalone-build
pause
```

After enabling ComfyUI, you will notice the final running address will become

```
Starting server

To see the GUI go to: http://0.0.0.0:8188
To see the GUI go to: http://[::]:8188
```

You can press `WIN + R` and type `cmd` to open the command prompt, then enter `ipconfig` to view your local IP address. Other devices can then access ComfyUI by entering `http://your-local-IP:8188` in their browser.


# Linux Desktop Version
Source: https://docs.comfy.org/installation/desktop/linux

This article introduces how to download, install and use ComfyUI Desktop for Linux

<Warning>Linux pre-built packages are not yet available. Please try [manual installation.](/installation/manual_install)</Warning>


# MacOS Desktop Version
Source: https://docs.comfy.org/installation/desktop/macos

This article introduces how to download, install and use ComfyUI Desktop for MacOS

export const log_path_0 = "~/Library/Logs/ComfyUI"

export const config_path_0 = "~/Library/Application Support/ComfyUI"

**ComfyUI Desktop** is a standalone installation version that can be installed like regular software.
It supports quick installation and automatic configuration of the **Python environment and dependencies**, and supports one-click import of existing ComfyUI settings, models, workflows, and files.

ComfyUI Desktop is an open source project, please visit the full code [here](https://github.com/Comfy-Org/desktop).

<Note>ComfyUI Desktop (MacOS) only supports Apple Silicon</Note>

This tutorial will guide you through the software installation process and explain related configuration details.

<Warning>As **ComfyUI Desktop** is still in **Beta** status, the actual installation process may change</Warning>

## ComfyUI Desktop (MacOS) Download

Please click the button below to download the installation package for MacOS **ComfyUI Desktop**

<a className="prose" href="https://download.comfy.org/mac/dmg/arm64" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download for MacOS</p>
</a>

## ComfyUI Desktop Installation Steps

Double-click the downloaded installation package file. As shown in the image, drag the **ComfyUI** application into the **Applications** folder following the arrow

![ComfyUI Installation Package](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0.png)

If your folder shows as below with a prohibition sign on the icon after opening the installation package, it means your current system version is not compatible with **ComfyUI Desktop**
![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0-1.png)

Then find the **ComfyUI icon** in **Launchpad** and click it to enter ComfyUI initialization settings
![ComfyUI Lanchpad](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-1.jpg)

## ComfyUI Desktop Initialization Process

<Steps>
  <Step title="Start Screen">
    <Tabs>
      <Tab title="Normal Start">
        ![ComfyUI Installation Steps - Start](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-2.png)

        Click **Get Started** to begin initialization
      </Tab>

      <Tab title="Maintenance Page">
        There are many reasons you might have issues installing ComfyUI. Maybe a network connection failed when installing pytorch (15 GB). Or you dont have git installed. The maintenance page automatically opens when it detects an issue and provides a way to resolve the issue.

        You can use it to resolve most issues:

        * Create a python virtual environment
        * Reinstall all missing core dependencies to your Python virtual environment thats managed by Desktop
        * Install git, VC redis
        * Choose a new install location

        The default maintenance page displays the current error content

        ![ComfyUI Maintenance Page](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-1.jpg)

        Clicking `All` allows you to view all the content that can be operated on currently

        ![ComfyUI Maintenance Page](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-2.jpg)
      </Tab>
    </Tabs>
  </Step>

  <Step title="Select GPU">
    ![ComfyUI Installation Steps - GPU Selection](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-3.png)

    The three options are:

    1. **MPS (Recommended):** Metal Performance Shaders (MPS) is an Apple framework that uses GPUs to accelerate computing and machine learning tasks on Apple devices, supporting frameworks like PyTorch.
    2. **Manual Configuration:** You need to manually install and configure the python runtime environment. Don't select this unless you know how to configure
    3. **Enable CPU Mode:** For developers and special cases only. Don't select this unless you're sure you need it

    Unless there are special circumstances, please select **MPS** as shown and click **Next** to proceed
  </Step>

  <Step title="Install location">
    ![ComfyUI Installation Steps - Installation Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-4.png)

    In this step, you will select the installation location for the following related content of ComfyUI:

    * **Python Environment**
    * **Models Model Files**
    * **Custom Nodes Custom Nodes**

    Recommendations:

    * Please create a separate empty folder as the installation directory for ComfyUI
    * Please ensure that the disk has at least **5G** of disk space to ensure the normal installation of **ComfyUI Desktop**

    <Note>Not all files are installed in this directory, some files will be located in the MacOS system directory, you can refer to the uninstallation section of this guide to complete the uninstallation of the ComfyUI desktop version</Note>
  </Step>

  <Step title="Migrate from Existing Installation (Optional)">
    ![ComfyUI Installation Steps - File Migration](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-5.png)

    In this step you can migrate your existing ComfyUI installation content to ComfyUI Desktop. Select your existing ComfyUI installation directory, and the installer will automatically recognize:

    * **User Files**
    * **Models:** Will not be copied, only linked with desktop version
    * **Custom Nodes:** Nodes will be reinstalled

    Don't worry, this step won't copy model files. You can check or uncheck options as needed. Click **Next** to continue
  </Step>

  <Step title="Desktop Settings">
    ![ComfyUI Installation Steps - Desktop Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-6.png)

    These are preference settings:

    1. **Automatic Updates:** Whether to set automatic updates when ComfyUI updates are available
    2. **Usage Metrics:** If enabled, we will collect **anonymous usage data** to help improve ComfyUI
    3. **Mirror Settings:** Since the program needs internet access to download Python and complete environment installation, if you see a red  during installation indicating this may cause installation failure, please follow the steps below

    ![ComfyUI Installation Steps - Mirror Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-7.png)
    Expand the mirror settings to find the specific failing mirror. In this screenshot the error is **Python Install Mirror** failure.

    For different mirror errors, you can refer to the following content to try to manually find different mirrors and replace them

    The following cases mainly apply to users in China.

    #### Python Installation Mirror

    If the default mirror is unavailable, please try using the mirror below.

    ```
    https://python-standalone.org/mirror/astral-sh/python-build-standalone
    ```

    If you need to find other alternative GitHub mirror addresses, please look for and construct a mirror address pointing to the releases of the `python-build-standalone` repository.

    ```
    https://github.com/astral-sh/python-build-standalone/releases/download
    ```

    Build a link in the following pattern

    ```
    https://xxx/astral-sh/python-build-standalone/releases/download
    ```

    <info>Since most of the Github mirror services are provided by third parties, please pay attention to the security during use.</info>

    #### PyPI Mirror

    * Alibaba Cloud: [https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)
    * Tencent Cloud: [https://mirrors.cloud.tencent.com/pypi/simple/](https://mirrors.cloud.tencent.com/pypi/simple/)
    * University of Science and Technology of China: [https://pypi.mirrors.ustc.edu.cn/simple/](https://pypi.mirrors.ustc.edu.cn/simple/)
    * Shanghai Jiao Tong University: [https://pypi.sjtu.edu.cn/simple/](https://pypi.sjtu.edu.cn/simple/)

    #### Torch Mirror

    * Aliyun: [https://mirrors.aliyun.com/pytorch-wheels/cu121/](https://mirrors.aliyun.com/pytorch-wheels/cu121/)
  </Step>

  <Step title="Complete the installation">
    If everything is correct, the installer will complete and automatically enter the ComfyUI Desktop interface, then the installation is successful
    ![ComfyUI Desktop Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-interface.jpg)
  </Step>
</Steps>

## First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey\~

<Card title="First Image Generation" icon="link" href="/get_started/first_generation">
  This tutorial will guide you through your first model installation and text-to-image generation
</Card>

## How to Update ComfyUI Desktop

Currently, ComfyUI Desktop updates use automatic detection updates, please ensure that automatic updates are enabled in the settings

![ComfyUI Desktop Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg)

## How to Uninstall ComfyUI Desktop

For **ComfyUI Desktop**, you can directly delete **ComfyUI** from the **Applications** folder

If you want to completely remove all **ComfyUI Desktop** files, you can manually delete these folders:

* /Users/Library/Application Support/ComfyUI

The above operations will not delete your following folders. If you need to delete corresponding files, please delete manually:

* models files
* custom nodes
* input/output directories

## Troubleshooting

### Error identification

If installation fails, you should see the following screen

![ComfyUI Installation Failed](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg)

It is recommended to take these steps to find the error cause:

1. Click `Show Terminal` to view error output
2. Click `Open Logs` to view installation logs
3. Visit official forum to search for error reports
4. Click `Reinstall` to try reinstalling

Before submitting feedback, it's recommended to provide the **error output** and **log files** to tools like **GPT**

![ComfyUI Installation Failed - Error Log](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg)
![ComfyUI Installation Failed - GPT Feedback](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg)

As shown above, ask the GPT for the cause of the corresponding error, or remove ComfyUI completely and retry the installation.

### Feedback Installation Failure

If you encounter any errors during installation, please check if there are similar error reports or submit errors to us through:

* Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)
* Comfy Official Forum: [https://forum.comfy.org/](https://forum.comfy.org/)

When submitting error reports, please ensure you include the following logs and configuration files to help us locate and investigate the issue:

1. Log Files

| Filename    | Description                                                                                     | Location     |
| ----------- | ----------------------------------------------------------------------------------------------- | ------------ |
| main.log    | Contains logs related to desktop application and server startup from the Electron process       | {log_path_0} |
| comfyui.log | Contains logs related to ComfyUI normal operation, such as core ComfyUI process terminal output | {log_path_0} |

![ComfyUI Log Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)

2. Configuration Files

| Filename                   | Description                                                                     | Location        |
| -------------------------- | ------------------------------------------------------------------------------- | --------------- |
| extra\_models\_config.yaml | Contains additional paths where ComfyUI will search for models and custom nodes | {config_path_0} |
| config.json                | Contains application configuration. This file should not be edited directly     | {config_path_0} |

![ComfyUI Config Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)


# Windows Desktop Version
Source: https://docs.comfy.org/installation/desktop/windows

This article introduces how to download, install and use ComfyUI Desktop for Windows

export const log_path_0 = "C:\Users\ <your username> \AppData\Roaming\ComfyUI\logs"

export const config_path_0 = "C:\Users\ <your username> \AppData\Roaming\ComfyUI"

**ComfyUI Desktop** is a standalone installation version that can be installed like regular software. It supports quick installation and automatic configuration of the **Python environment and dependencies**, and supports one-click import of existing ComfyUI settings, models, workflows, and files. You can quickly migrate from an existing [ComfyUI Portable version](/installation/comfyui_portable_windows) to the Desktop version.

ComfyUI Desktop is an open source project, please visit the full code [here](https://github.com/Comfy-Org/desktop)

ComfyUI Desktop hardware requirements:

* NVIDIA GPU

This tutorial will guide you through the software installation process and explain related configuration details.

<Warning>As **ComfyUI Desktop** is still in **Beta** status, the actual installation process may change</Warning>

## ComfyUI Desktop (Windows) Download

Please click the button below to download the installation package for Windows **ComfyUI Desktop**

<a className="prose" href="https://download.comfy.org/windows/nsis/x64" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download for Windows (NVIDIA)</p>
</a>

## ComfyUI Desktop Installation Steps

Double-click the downloaded installation package file, which will first perform an automatic installation and create a **ComfyUI Desktop** shortcut on the desktop

![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-shortcut.jpg)

Double-click the corresponding shortcut to enter ComfyUI initialization settings

### ComfyUI Desktop Initialization Process

<Steps>
  <Step title="Start Screen">
    <Tabs>
      <Tab title="Normal Start">
        ![ComfyUI Installation Steps - Start](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-1.jpg)

        Click **Get Started** to begin initialization
      </Tab>

      <Tab title="Maintenance Page">
        There are many reasons you might have issues installing ComfyUI. Maybe a network connection failed when installing pytorch (15 GB). Or you dont have git installed. The maintenance page automatically opens when it detects an issue and provides a way to resolve the issue.

        You can use it to resolve most issues:

        * Create a python virtual environment
        * Reinstall all missing core dependencies to your Python virtual environment thats managed by Desktop
        * Install git, VC redis
        * Choose a new install location

        The default maintenance page displays the current error content

        ![ComfyUI Maintenance Page](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-1.jpg)

        Clicking `All` allows you to view all the content that can be operated on currently

        ![ComfyUI Maintenance Page](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-2.jpg)
      </Tab>
    </Tabs>
  </Step>

  <Step title="Select GPU">
    ![ComfyUI Installation Steps - GPU Selection](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-2.jpg)

    The three options are:

    1. **Nvidia GPU (Recommended):** Direct support for pytorch and CUDA
    2. **Manual Configuration:** You need to manually install and configure the python runtime environment. Don't select this unless you know how to configure
    3. **Enable CPU Mode:** For developers and special cases only. Don't select this unless you're sure you need it

    Unless there are special circumstances, please select **NVIDIA** as shown and click **Next** to proceed
  </Step>

  <Step title="Install location">
    ![ComfyUI Installation Steps - Installation Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-3.jpg)

    In this step, you will select the installation location for the following ComfyUI content:

    * **Python Environment**
    * **Models Model Files**
    * **Custom Nodes Custom Nodes**

    Recommendations:

    * Please select a **solid-state drive** as the installation location, which will increase ComfyUI's performance when accessing models.
    * Please create a separate empty folder as the ComfyUI installation directory
    * Please ensure that the corresponding disk has at least around **15G** of disk space to ensure the installation of ComfyUI Desktop

    <Note>Not all files are installed in this directory, some files will still be installed on the C drive, and if you need to uninstall in the future, you can refer to the uninstallation section of this guide to complete the full uninstallation of ComfyUI Desktop</Note>

    After completing this step, click **Next** to proceed to the next step
  </Step>

  <Step title="Migrate from Existing Installation (Optional)">
    ![ComfyUI Installation Steps - File Migration](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-4.jpg)

    In this step you can migrate your existing ComfyUI installation content to ComfyUI Desktop. As shown, I selected my original **D:\ComfyUI\_windows\_portable\ComfyUI** installation directory. The installer will automatically recognize:

    * **User Files**
    * **Models:** Will not be copied, only linked with desktop version
    * **Custom Nodes:** Nodes will be reinstalled

    Don't worry, this step won't copy model files. You can check or uncheck options as needed. Click **Next** to continue
  </Step>

  <Step title="Desktop Settings">
    ![ComfyUI Installation Steps - Desktop Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-5.jpg)

    These are preference settings:

    1. **Automatic Updates:** Whether to set automatic updates when ComfyUI updates are available
    2. **Usage Metrics:** If enabled, we will collect **anonymous usage data** to help improve ComfyUI
    3. **Mirror Settings:** Since the program needs internet access to download Python and complete environment installation, if you see a red  during installation indicating this may cause installation failure, please follow the steps below

    ![ComfyUI Installation Steps - Mirror Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-6.jpg)
    Expand the mirror settings to find the specific failing mirror. In this screenshot the error is **Python Install Mirror** failure.

    For different mirror errors, you can refer to the following content to try to manually find different mirrors and replace them

    The following cases mainly apply to users in China.

    #### Python Installation Mirror

    If the default mirror is unavailable, please try using the mirror below.

    ```
    https://python-standalone.org/mirror/astral-sh/python-build-standalone
    ```

    If you need to find other alternative GitHub mirror addresses, please look for and construct a mirror address pointing to the releases of the `python-build-standalone` repository.

    ```
    https://github.com/astral-sh/python-build-standalone/releases/download
    ```

    Build a link in the following pattern

    ```
    https://xxx/astral-sh/python-build-standalone/releases/download
    ```

    <info>Since most of the Github mirror services are provided by third parties, please pay attention to the security during use.</info>

    #### PyPI Mirror

    * Alibaba Cloud: [https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)
    * Tencent Cloud: [https://mirrors.cloud.tencent.com/pypi/simple/](https://mirrors.cloud.tencent.com/pypi/simple/)
    * University of Science and Technology of China: [https://pypi.mirrors.ustc.edu.cn/simple/](https://pypi.mirrors.ustc.edu.cn/simple/)
    * Shanghai Jiao Tong University: [https://pypi.sjtu.edu.cn/simple/](https://pypi.sjtu.edu.cn/simple/)

    #### Torch Mirror

    * Aliyun: [https://mirrors.aliyun.com/pytorch-wheels/cu121/](https://mirrors.aliyun.com/pytorch-wheels/cu121/)
  </Step>

  <Step title="Complete the installation">
    If everything is correct, the installer will complete and automatically enter the ComfyUI Desktop interface, then the installation is successful
    ![ComfyUI Desktop Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-interface.jpg)
  </Step>
</Steps>

## First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey\~

<Card title="First Image Generation" icon="link" href="/get_started/first_generation">
  This tutorial will guide you through your first model installation and text-to-image generation
</Card>

## How to Update ComfyUI Desktop

Currently, ComfyUI Desktop updates use automatic detection updates, please ensure that automatic updates are enabled in the settings

![ComfyUI Desktop Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg)

## How to Uninstall ComfyUI Desktop

For **ComfyUI Desktop** you can use the system uninstall function in Windows Settings to complete software uninstallation

![ComfyUI Desktop Uninstallation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-uninstall-comfyui.jpg)

If you want to completely remove all **ComfyUI Desktop** files, you can manually delete these folders:

* C:\Users\<your username>\AppData\Local\@comfyorgcomfyui-electron-updater
* C:\Users\<your username>\AppData\Local\Programs\@comfyorgcomfyui-electron
* C:\Users\<your username>\AppData\Roaming\ComfyUI

The above operations will not delete your following folders. If you need to delete corresponding files, please delete manually:

* models files
* custom nodes
* input/output directories

## Troubleshooting

### Display unsupported devices

![ComfyUI Installation Steps - Unsupported Device](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-0.jpg)

Since ComfyUI Desktop (Windows) only supports **NVIDIA GPUs with CUDA**, you may see this screen if your device is not supported

* Please switch to a supported device
* Or consider using [ComfyUI Portable](/installation/comfyui_portable_windows) or through [manual installation](/installation/manual_install) to use ComfyUI

### Error identification

If installation fails, you should see the following screen

![ComfyUI Installation Failed](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg)

It is recommended to take these steps to find the error cause:

1. Click `Show Terminal` to view error output
2. Click `Open Logs` to view installation logs
3. Visit official forum to search for error reports
4. Click `Reinstall` to try reinstalling

Before submitting feedback, it's recommended to provide the **error output** and **log files** to tools like **GPT**

![ComfyUI Installation Failed - Error Log](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg)
![ComfyUI Installation Failed - GPT Feedback](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg)

As shown above, ask the GPT for the cause of the corresponding error, or remove ComfyUI completely and retry the installation.

### Feedback Installation Failure

If you encounter any errors during installation, please check if there are similar error reports or submit errors to us through:

* Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)
* Comfy Official Forum: [https://forum.comfy.org/](https://forum.comfy.org/)

When submitting error reports, please ensure you include the following logs and configuration files to help us locate and investigate the issue:

1. Log Files

| Filename    | Description                                                                                     | Location     |
| ----------- | ----------------------------------------------------------------------------------------------- | ------------ |
| main.log    | Contains logs related to desktop application and server startup from the Electron process       | {log_path_0} |
| comfyui.log | Contains logs related to ComfyUI normal operation, such as core ComfyUI process terminal output | {log_path_0} |

![ComfyUI Log Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)

2. Configuration Files

| Filename                   | Description                                                                     | Location        |
| -------------------------- | ------------------------------------------------------------------------------- | --------------- |
| extra\_models\_config.yaml | Contains additional paths where ComfyUI will search for models and custom nodes | {config_path_0} |
| config.json                | Contains application configuration. This file should not be edited directly     | {config_path_0} |

![ComfyUI Config Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)


# Manual Installation
Source: https://docs.comfy.org/installation/manual_install



<Tip>
  For Nvidia 50 series (Blackwell) GPUs, please refer to the [System Requirements](/installation/nvidia-50-series) section to ensure your system meets the requirements for ComfyUI.
</Tip>

<Tabs>
  <Tab title="Windows">
    ### Clone the repository

    ```bash
    git clone git@github.com:comfyanonymous/ComfyUI.git
    ```

    More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

    <Warning>If you have not installed Microsoft Visual C++ Redistributable, please install it [here.](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170)</Warning>

    ### Install Dependencies

    1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.

       Create an environment with Conda.

       ```
       conda create -n comfyenv
       conda activate comfyenv
       ```
    2. Install GPU Dependencies

       <Accordion title="Nvidia">
         ```
         conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
           ```
         </Accordion>
       </Accordion>

       <Accordion title="AMD">
         ```
         pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
           ```
         </Accordion>
       </Accordion>

       <Accordion title="Mac ARM Silicon">
         ```bash
         conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
         ```
       </Accordion>
    3. ```bash
       cd ComfyUI
       pip install -r requirements.txt
       ```
    4. Start the application
       ```
       cd ComfyUI
       python main.py
       ```
  </Tab>

  <Tab title="Linux">
    ### Clone the repository

    ```bash
    git clone git@github.com:comfyanonymous/ComfyUI.git
    ```

    More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

    ### Install Dependencies

    1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.

       Create an environment with Conda.

       ```
       conda create -n comfyenv
       conda activate comfyenv
       ```
    2. Install GPU Dependencies

       <Accordion title="Nvidia">
         ```
         conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
           ```
         </Accordion>
       </Accordion>

       <Accordion title="AMD">
         ```
         pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
           ```
         </Accordion>
       </Accordion>

       <Accordion title="Mac ARM Silicon">
         ```bash
         conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
         ```
       </Accordion>
    3. ```bash
       cd ComfyUI
       pip install -r requirements.txt
       ```
    4. Start the application
       ```
       cd ComfyUI
       python main.py
       ```
  </Tab>

  <Tab title="MacOS">
    ### Clone the repository

    Open [Terminal application](https://support.apple.com/guide/terminal/open-or-quit-terminal-apd5265185d-f365-44cb-8b09-71a064a42125/mac).

    ```bash
    git clone git@github.com:comfyanonymous/ComfyUI.git
    ```

    More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

    ### Install Dependencies

    1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.

       Create an environment with Conda.

       ```
       conda create -n comfyenv
       conda activate comfyenv
       ```
    2. Install GPU Dependencies

       <Accordion title="Nvidia">
         ```
         conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
           ```
         </Accordion>
       </Accordion>

       <Accordion title="AMD">
         ```
         pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
           ```
         </Accordion>
       </Accordion>

       <Accordion title="Mac ARM Silicon">
         ```bash
         conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
         ```
       </Accordion>
    3. ```bash
       cd ComfyUI
       pip install -r requirements.txt
       ```
    4. Start the application
       ```
       cd ComfyUI
       python main.py
       ```
  </Tab>
</Tabs>


# System Requirements
Source: https://docs.comfy.org/installation/system_requirements

This guide introduces some system requirements for ComfyUI, including hardware and software requirements

In this guide, we will introduce the system requirements for installing ComfyUI. Due to frequent updates of ComfyUI, this document may not be updated in a timely manner. Please refer to the relevant instructions in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).

Regardless of which version of ComfyUI you use, it runs in a separate Python environment.

You can refer to the following sections to learn about the installation methods for different systems and versions of ComfyUI. In the installation of different versions, we have simply described the system requirements.

<AccordionGroup>
  <Accordion title="ComfyUI Desktop (Recommended)">
    ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

    * Code is open source on [Github](https://github.com/Comfy-Org/desktop)

    You can choose the appropriate installation for your system and hardware below

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI Desktop (Windows) Installation Guide" icon="link" href="/installation/desktop/windows">
          Suitable for **Windows** version with **Nvidia** GPU
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI Desktop (MacOS) Installation Guide" icon="link" href="/installation/desktop/macos">
          Suitable for MacOS with **Apple Silicon**
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](/installation/manual_install) section to install ComfyUI</Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI Portable (Windows)">
    <Card title="ComfyUI Portable (Windows) Installation Guide" icon="link" href="/installation/comfyui_portable_windows">
      Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.
    </Card>
  </Accordion>

  <Accordion title="Manual Installation">
    <Card title="ComfyUI Manual Installation Guide" icon="link" href="/installation/manual_install">
      Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)
    </Card>
  </Accordion>
</AccordionGroup>

## Nvidia 50 Series GPU Requirements

To make your Nvidia 50 series GPU (Blackwell architecture) work properly with ComfyUI, you need a PyTorch version that supports CUDA 12.8 or newer. Currently (March 2025), the stable version of PyTorch does not yet support the Blackwell architecture, so you need to use a nightly build version.

Related discussions on this issue are concentrated [here](https://github.com/comfyanonymous/ComfyUI/discussions/6643).

### For Windows Users

**Recommended Option:**
Download the standalone ComfyUI Portable version with nightly pytorch 2.7 cu128:

* [Click here to download](https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_windows_portable_nvidia_or_cpu_nightly_pytorch.7z)

**Other Options:**
Older torch 2.6 Windows package:

* [Click here to download the standalone ComfyUI package with a cuda 12.8 torch build](https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_cu128_50XX.7z)

### Manual Installation

Windows and Linux users can install the PyTorch nightly version using the following command:

```bash
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
```

### Docker Container Alternative

You can try the PyTorch container provided by Nvidia, which might offer better performance.

Container address: [https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch)

Usage method:

```bash
docker run -p 8188:8188 --gpus all -it --rm nvcr.io/nvidia/pytorch:25.01-py3
```

Inside the Docker container, execute the following commands:

```bash
git clone https://github.com/comfyanonymous/ComfyUI
cd ComfyUI
grep -v 'torchaudio\|torchvision' requirements.txt > temp_requirements.txt
pip install -r temp_requirements.txt
python main.py --listen
```


# Create a new custom node
Source: https://docs.comfy.org/registry/api-reference/nodes/create-a-new-custom-node

post /publishers/{publisherId}/nodes



# Delete a specific node
Source: https://docs.comfy.org/registry/api-reference/nodes/delete-a-specific-node

delete /publishers/{publisherId}/nodes/{nodeId}



# Retrieve a specific node by ID
Source: https://docs.comfy.org/registry/api-reference/nodes/retrieve-a-specific-node-by-id

get /nodes/{nodeId}
Returns the details of a specific node.



# Retrieve all nodes
Source: https://docs.comfy.org/registry/api-reference/nodes/retrieve-all-nodes

get /publishers/{publisherId}/nodes



# Retrieves a list of nodes
Source: https://docs.comfy.org/registry/api-reference/nodes/retrieves-a-list-of-nodes

get /nodes
Returns a paginated list of nodes across all publishers.



# Returns a node version to be installed.
Source: https://docs.comfy.org/registry/api-reference/nodes/returns-a-node-version-to-be-installed

get /nodes/{nodeId}/install
Retrieves the node data for installation, either the latest or a specific version.



# Update a specific node
Source: https://docs.comfy.org/registry/api-reference/nodes/update-a-specific-node

put /publishers/{publisherId}/nodes/{nodeId}



# API Overview
Source: https://docs.comfy.org/registry/api-reference/overview



## Overview

The Custom Node Registry follows this structure:

```mermaid
erDiagram
    PUBLISHER {
        string id PK
    }

    USER {
        string id PK
        string name
        string publisher_id FK
    }

    CUSTOM_NODE {
        string id PK
        string name
        string publisher_id FK
    }

    NODE_VERSION {
        string id PK
        string version
        string node_id FK
    }

    PUBLISHER ||--o{ USER: "has many"
    PUBLISHER ||--o{ CUSTOM_NODE: "has many"
    CUSTOM_NODE ||--o{ NODE_VERSION: "has many"
```

## Commonly Used APIs

* **List All Nodes** [API](/registry/api-reference/nodes/retrieves-a-list-of-nodes)
* **Install a Node** [API](/registry/api-reference/nodes/returns-a-node-version-to-be-installed)


# Create a new publisher
Source: https://docs.comfy.org/registry/api-reference/publishers/create-a-new-publisher

post /publishers



# Delete a publisher
Source: https://docs.comfy.org/registry/api-reference/publishers/delete-a-publisher

delete /publishers/{publisherId}



# Retrieve a publisher by ID
Source: https://docs.comfy.org/registry/api-reference/publishers/retrieve-a-publisher-by-id

get /publishers/{publisherId}



# Retrieve all publishers
Source: https://docs.comfy.org/registry/api-reference/publishers/retrieve-all-publishers

get /publishers



# Retrieve all publishers for a given user
Source: https://docs.comfy.org/registry/api-reference/publishers/retrieve-all-publishers-for-a-given-user

get /users/publishers/



# Update a publisher
Source: https://docs.comfy.org/registry/api-reference/publishers/update-a-publisher

put /publishers/{publisherId}



# Validate if a publisher username is available
Source: https://docs.comfy.org/registry/api-reference/publishers/validate-if-a-publisher-username-is-available

get /publishers/validate
Checks if the publisher username is already taken.



# Create a new personal access token
Source: https://docs.comfy.org/registry/api-reference/token-management/create-a-new-personal-access-token

post /publishers/{publisherId}/tokens



# Delete a specific personal access token
Source: https://docs.comfy.org/registry/api-reference/token-management/delete-a-specific-personal-access-token

delete /publishers/{publisherId}/tokens/{tokenId}



# Retrieve all personal access tokens for a publisher
Source: https://docs.comfy.org/registry/api-reference/token-management/retrieve-all-personal-access-tokens-for-a-publisher

get /publishers/{publisherId}/tokens



# List all versions of a node
Source: https://docs.comfy.org/registry/api-reference/versions/list-all-versions-of-a-node

get /nodes/{nodeId}/versions



# Publish a new version of a node
Source: https://docs.comfy.org/registry/api-reference/versions/publish-a-new-version-of-a-node

post /publishers/{publisherId}/nodes/{nodeId}/versions



# Retrieve a specific version of a node
Source: https://docs.comfy.org/registry/api-reference/versions/retrieve-a-specific-version-of-a-node

get /nodes/{nodeId}/versions/{versionId}



# Unpublish (delete) a specific version of a node
Source: https://docs.comfy.org/registry/api-reference/versions/unpublish-delete-a-specific-version-of-a-node

delete /publishers/{publisherId}/nodes/{nodeId}/versions/{versionId}



# Update changelog and deprecation status of a node version
Source: https://docs.comfy.org/registry/api-reference/versions/update-changelog-and-deprecation-status-of-a-node-version

put /publishers/{publisherId}/nodes/{nodeId}/versions/{versionId}
Update only the changelog and deprecated status of a specific version of a node.



# Custom Node CI/CD
Source: https://docs.comfy.org/registry/cicd



## Introduction

When making changes to custom nodes, it's not uncommon to break things in Comfy or other custom nodes. It is often unrealistic to test on every operating system and different configurations of Pytorch.

### Run Comfy Workflows using Github Actions

[Comfy-Action](https://github.com/Comfy-Org/comfy-action) allows you to run a Comfy workflow\.json file on Github Actions. It supports downloading models, custom nodes, and runs on Linux/Mac/Windows.

### Results

Output files are uploaded to the [CI/CD Dashboard](https://comfyci.org) and can be viewed as a last step before commiting new changes or publishing new versions of the custom node.

![ComfyCI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyci.png)


# Overview
Source: https://docs.comfy.org/registry/overview



## Introduction

The Registry is a public collection of custom nodes. Developers can publish, version, deprecate, and track metrics related to their custom nodes. ComfyUI users can discover, install, and rate custom nodes from the registry.

## Why use the Registry?

The Comfy Registry helps the community by standardizing the development of custom nodes:

<Icon icon="timeline" iconType="solid" size={20} /> **Node Versioning:** Developers frequently publish new versions of their custom nodes which often break workflows that rely on them. With registry nodes being [semantically versioned](https://semver.org/), users can now choose to safely upgrade, deprecate, or lock their node versions in place, knowing in advance how their actions will impact their workflows. The workflow JSON will store the version of the node used, so you can always reliably reproduce your workflows.

<Icon icon="shield" iconType="solid" size={20} /> **Node Security:** The registry will serve as a backend for the [ComfyUI-manager](https://github.com/ltdrdata/ComfyUI-Manager). All nodes will be scanned for malicious behaviour such as custom pip wheels, arbitrary system calls, etc. Nodes that pass these checks will have a verification flag (<Icon icon="check" iconType="solid" />) beside their name on the UI-manager. For a list of security standards, see the [standards](/registry/standards).

<Icon icon="magnifying-glass" iconType="solid" size={20} /> **Search:** Search across all nodes on the Registry to find existing nodes for your workflow\.x

## Publishing Nodes

Get started publishing your first node by following the [tutorial](/registry/publishing).

## Frequently Asked Questions

<AccordionGroup>
  <Accordion title="Do registry nodes have unique identifiers?">
    Yes, a custom node on the Registry has a globally unique name and this allows Comfy Workflow JSON files to uniquely identify any custom node without collisions.
  </Accordion>

  <Accordion title="Are there any restrictions on what I can publish?">
    Check the [standards](/registry/standards) for more information.
  </Accordion>

  <Accordion title="How do you ensure node stability?">
    Once a custom node version is published, it cannot be changed. This ensures that users can rely on the stability of the custom node over time.
  </Accordion>

  <Accordion title="How are nodes versioned?">
    Custom nodes are versioned using [semantic versioning](https://semver.org/). This allows users to understand the impact of upgrading to a new version.
  </Accordion>

  <Accordion title="How do I deprecate a node version?">
    You can deprecate a version in the Comfy Registry website by clicking **More Actions > Deprecate**. Users who installed this version will be shown the deprecation message and be encouraged to upgrade to a newer version.

    Deprecating versions is useful when an issue is discovered after publishing.
  </Accordion>
</AccordionGroup>


# Publishing Nodes
Source: https://docs.comfy.org/registry/publishing



## Set up a Registry Account

Follow the steps below to set up a registry account and publish your first node.

### Watch a Tutorial

<iframe height="415" src="https://www.youtube.com/embed/WhOZZOgBggU?si=6TyvhJJadmQ65uXC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen style={{ width: "100%", borderRadius: "0.5rem" }} />

### Create a Publisher

A publisher is an identity that can publish custom nodes to the registry. Every custom node needs to include a publisher identifier in the pyproject.toml [file]().

Go to [Comfy Registry](https://registry.comfy.org), and create a publisher account. Your publisher id is globally unique, and cannot be changed later because it is used in the URL of your custom node.

Your publisher id is found after the `@` symbol on your profile page.

<img className="block" src="https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/publisherid.png" alt="Hero Dark" />

### Create an API Key for publishing

Go [here](https://registry.comfy.org/nodes) and click on the publisher you want to create an API key for. This will be used to publish a custom node via the CLI.

![Create key for Specific Publisher](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/pat-1.png)

Name the API key and save it somewhere safe. If you lose it, you'll have to create a new key.

![Create API Key](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/pat-2.png)

### Add Metadata

<Tip>Have you installed the comfy-cli? [Do that first](/comfy-cli/getting-started).</Tip>

```bash
comfy node init
```

This command will generate the following metadata:

```toml
# pyproject.toml
[project]
name = "" # Unique identifier for your node. Immutable after creation.
description = ""
version = "1.0.0" # Custom Node version. Must be semantically versioned.
license = { file = "LICENSE.txt" }
dependencies  = [] # Filled in from requirements.txt

[project.urls]
Repository = "https://github.com/..."

[tool.comfy]
PublisherId = "" # TODO (fill in Publisher ID from Comfy Registry Website).
DisplayName = "" # Display name for the Custom Node. Can be changed later.
Icon = "https://example.com/icon.png" # SVG, PNG, JPG or GIF (MAX. 800x400px)
```

Add this file to your repository. Check the [specifications](/registry/specifications) for more information on the pyproject.toml file.

## Publish to the Registry

### Option 1: Comfy CLI

Run the command below to manually publish your node to the registry.

```bash
comfy node publish
```

You'll be prompted for the API key.

```bash
API Key for publisher '<publisher id>': ****************************************************

...Version 1.0.0 Published. 
See it here: https://registry.comfy.org/publisherId/your-node
```

<Warning>
  Keep in mind that the API key is hidden by default.
</Warning>

<Warning>
  When copy-pasting, your API key might have an additional \x16 at the back when using CTRL+V (for Windows), eg: \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\x16.

  It is recommended to copy-paste your API key via right-clicking instead.
</Warning>

### Option 2: Github Actions

Automatically publish your node through github actions.

<Steps>
  <Step title="Set up a Github Secret">
    Go to Settings -> Secrets and Variables -> Actions -> Under Secrets Tab and Repository secrets -> New Repository Secret.

    Create a secret called `REGISTRY_ACCESS_TOKEN` and store your API key as the value.
  </Step>

  <Step title="Create a Github Action">
    Copy the code below and paste it here `/.github/workflows/publish_action.yml`

    ```bash
    name: Publish to Comfy registry
    on:
      workflow_dispatch:
      push:
        branches:
          - main
        paths:
          - "pyproject.toml"

    jobs:
      publish-node:
        name: Publish Custom Node to registry
        runs-on: ubuntu-latest
        steps:
          - name: Check out code
            uses: actions/checkout@v4
          - name: Publish Custom Node
            uses: Comfy-Org/publish-node-action@main
            with:
              personal_access_token: ${{ secrets.REGISTRY_ACCESS_TOKEN }} ## Add your own personal access token to your Github Repository secrets and reference it here.
    ```

    <Warning>If your working branch is named something besides `main`, such as `master`, add the name under the branches section.</Warning>
  </Step>

  <Step title="Test the Github Action">
    Push an update to your `pyproject.toml`'s version number. You should see your updated node on the registry.

    <Tip>The github action will automatically run every time you push an update to your `pyproject.toml` file</Tip>
  </Step>
</Steps>


# pyproject.toml
Source: https://docs.comfy.org/registry/specifications



# Node ID

The node id ("name" field in toml file) uniquely identifies the custom node, and will be used in URLs from the registry. Users can also install the node by referencing the name.

`comfy node install <node-id>`

The node id must be less than 100 characters and can only contain alphanumeric characters, hyphens, underscores, and periods. There should not be consecutive special characters and the id cannot start with a number or special character.

Comparison of node ids is case-insensitive. See the official [python documentation](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#name) for more information.

We recommend using a short name for your node id, and don't include "ComfyUI" in the name.

# Version Number

The registry uses [semantic versioning](https://semver.org/) which indicates the specific release of a custom node through a three-digit version number X.Y.Z.

X - **MAJOR** change that breaks previous updates

Y - **MINOR** change that adds new features and is backwards compatible

Z - **PATCH** change that fixes a bug

# License

An optional field that expects a relative path to your license file (usually named `LICENSE` or `LICENSE.txt`).

* `license = { file = "LICENSE" }` 
* `license = "LICENSE"` 

Alternatively, it can also be referenced by name. Common licenses include [MIT](https://opensource.org/license/mit), [GPL](https://www.gnu.org/licenses/gpl-3.0.en.html), or [Apache](https://www.apache.org/licenses/LICENSE-2.0).

* `license = {text = "MIT License"}` 
* `license = "MIT LICENSE"` 

Read up more on toml file standards [here](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license)

# Publisher ID

The publisher id uniquely identifies a publisher and is ideally the same as your github username. It is also referred to as the username on the registry and can be found after the `@` symbol on the profile page.

# Icon

An optional field that expects a icon URL. It accepts extensions SVG, PNG, JPG or GIF with a maximum resolution of 800px by 400px.


# Standards
Source: https://docs.comfy.org/registry/standards

Security and other standards for publishing to the Registry

## Base Standards

### 1. Community Value

Custom nodes must provide valuable functionality to the ComfyUI community

Avoid:

* Excessive self-promotion
* Impersonation or misleading behavior
* Malicious behavior
* Self-promotion is permitted only within your designated settings menu section
* Top and side menus should contain only useful functionality

### 2. Node Compatibility

Do not interfere with other custom nodes' operations (installation, updates, removal)

* For dependencies on other custom nodes:
  * Display clear warnings when dependent functionality is used
  * Provide example workflows demonstrating required nodes

### 3. Legal Compliance

Must comply with all applicable laws and regulations

### 5. Quality Requirements

Nodes must be fully functional, well documented, and actively maintained.

### 6. Fork Guidelines

Forked nodes must:

* Have clearly distinct names from original
* Provide significant differences in functionality or code

Below are standards that must be met to publish custom nodes to the registry.

## Security Standards

Custom nodes should be secure. We will start working with custom nodes that violate these standards to be rewritten. If there is some major functionality that should be exposed by core, please request it in the [rfcs repo](https://github.com/comfy-org/rfcs).

### eval/exec Calls

#### Policy

The use of `eval` and `exec` functions is prohibited in custom nodes due to security concerns.

#### Reasoning

These functions can enable arbitrary code execution, creating potential Remote Code Execution (RCE) vulnerabilities when processing user inputs. Workflows containing nodes that pass user inputs into `eval` or `exec` could be exploited for various cyberattacks, including:

* Keylogging
* Ransomware
* Other malicious code execution

### subprocess for pip install

#### Policy

Runtime package installation through subprocess calls is not permitted.

#### Reasoning

* First item
  ComfyUI manager will ship with ComfyUI and lets the user install dependencies
* Centralized dependency management improves security and user experience
* Helps prevent potential supply chain attacks
* Eliminates need for multiple ComfyUI reloads

### Code Obfuscation

#### Policy

Code obfuscation is prohibited in custom nodes.

#### Reasoning

Obfuscated code:

* Impossible to review and likely to be malicious


# Node Definition JSON
Source: https://docs.comfy.org/specs/nodedef_json

JSON schema for a ComfyUI Node.

The node definition JSON is defined using [JSON Schema](https://json-schema.org/). Changes to this schema will be discussed in the [rfcs repo](https://github.com/comfy-org/rfcs).

## v2.0 (Latest)

```json Node Definition v2.0
{
  "$ref": "#/definitions/ComfyNodeDefV2",
  "definitions": {
    "ComfyNodeDefV2": {
      "type": "object",
      "properties": {
        "inputs": {
          "type": "object",
          "additionalProperties": {
            "anyOf": [
              {
                "type": "object",
                "properties": {
                  "default": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "array",
                        "items": {
                          "type": "number"
                        }
                      }
                    ]
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "min": {
                    "type": "number"
                  },
                  "max": {
                    "type": "number"
                  },
                  "step": {
                    "type": "number"
                  },
                  "display": {
                    "type": "string",
                    "enum": [
                      "slider",
                      "number",
                      "knob"
                    ]
                  },
                  "control_after_generate": {
                    "type": "boolean"
                  },
                  "type": {
                    "type": "string",
                    "const": "INT"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "array",
                        "items": {
                          "type": "number"
                        }
                      }
                    ]
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "min": {
                    "type": "number"
                  },
                  "max": {
                    "type": "number"
                  },
                  "step": {
                    "type": "number"
                  },
                  "display": {
                    "type": "string",
                    "enum": [
                      "slider",
                      "number",
                      "knob"
                    ]
                  },
                  "round": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "boolean",
                        "const": false
                      }
                    ]
                  },
                  "type": {
                    "type": "string",
                    "const": "FLOAT"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "type": "boolean"
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "label_on": {
                    "type": "string"
                  },
                  "label_off": {
                    "type": "string"
                  },
                  "type": {
                    "type": "string",
                    "const": "BOOLEAN"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "type": "string"
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "multiline": {
                    "type": "boolean"
                  },
                  "dynamicPrompts": {
                    "type": "boolean"
                  },
                  "defaultVal": {
                    "type": "string"
                  },
                  "placeholder": {
                    "type": "string"
                  },
                  "type": {
                    "type": "string",
                    "const": "STRING"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {},
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "control_after_generate": {
                    "type": "boolean"
                  },
                  "image_upload": {
                    "type": "boolean"
                  },
                  "image_folder": {
                    "type": "string",
                    "enum": [
                      "input",
                      "output",
                      "temp"
                    ]
                  },
                  "allow_batch": {
                    "type": "boolean"
                  },
                  "video_upload": {
                    "type": "boolean"
                  },
                  "remote": {
                    "type": "object",
                    "properties": {
                      "route": {
                        "anyOf": [
                          {
                            "type": "string",
                            "format": "uri"
                          },
                          {
                            "type": "string",
                            "pattern": "^\\/"
                          }
                        ]
                      },
                      "refresh": {
                        "anyOf": [
                          {
                            "type": "number",
                            "minimum": -9007199254740991,
                            "maximum": 9007199254740991
                          },
                          {
                            "type": "number",
                            "maximum": 9007199254740991,
                            "minimum": -9007199254740991
                          }
                        ]
                      },
                      "response_key": {
                        "type": "string"
                      },
                      "query_params": {
                        "type": "object",
                        "additionalProperties": {
                          "type": "string"
                        }
                      },
                      "refresh_button": {
                        "type": "boolean"
                      },
                      "control_after_refresh": {
                        "type": "string",
                        "enum": [
                          "first",
                          "last"
                        ]
                      },
                      "timeout": {
                        "type": "number",
                        "minimum": 0
                      },
                      "max_retries": {
                        "type": "number",
                        "minimum": 0
                      }
                    },
                    "required": [
                      "route"
                    ],
                    "additionalProperties": false
                  },
                  "options": {
                    "type": "array",
                    "items": {
                      "type": [
                        "string",
                        "number"
                      ]
                    }
                  },
                  "type": {
                    "type": "string",
                    "const": "COMBO"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {},
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "type": {
                    "type": "string"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              }
            ]
          }
        },
        "outputs": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "index": {
                "type": "number"
              },
              "name": {
                "type": "string"
              },
              "type": {
                "type": "string"
              },
              "is_list": {
                "type": "boolean"
              },
              "options": {
                "type": "array"
              },
              "tooltip": {
                "type": "string"
              }
            },
            "required": [
              "index",
              "name",
              "type",
              "is_list"
            ],
            "additionalProperties": false
          }
        },
        "hidden": {
          "type": "object",
          "additionalProperties": {}
        },
        "name": {
          "type": "string"
        },
        "display_name": {
          "type": "string"
        },
        "description": {
          "type": "string"
        },
        "category": {
          "type": "string"
        },
        "output_node": {
          "type": "boolean"
        },
        "python_module": {
          "type": "string"
        },
        "deprecated": {
          "type": "boolean"
        },
        "experimental": {
          "type": "boolean"
        }
      },
      "required": [
        "inputs",
        "outputs",
        "name",
        "display_name",
        "description",
        "category",
        "output_node",
        "python_module"
      ],
      "additionalProperties": false
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```


# Node Definition JSON 1.0
Source: https://docs.comfy.org/specs/nodedef_json_1_0

JSON schema for a ComfyUI Node.

## v1.0

```json Node Definition v1.0
{
  "$ref": "#/definitions/ComfyNodeDefV1",
  "definitions": {
    "ComfyNodeDefV1": {
      "type": "object",
      "properties": {
        "input": {
          "type": "object",
          "properties": {
            "required": {
              "type": "object",
              "additionalProperties": {
                "anyOf": [
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "INT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "FLOAT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "round": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "boolean",
                                    "const": false
                                  }
                                ]
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "BOOLEAN"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "boolean"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "label_on": {
                                "type": "string"
                              },
                              "label_off": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "STRING"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "string"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "multiline": {
                                "type": "boolean"
                              },
                              "dynamicPrompts": {
                                "type": "boolean"
                              },
                              "defaultVal": {
                                "type": "string"
                              },
                              "placeholder": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "array",
                        "items": {
                          "type": [
                            "string",
                            "number"
                          ]
                        }
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "COMBO"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  }
                ]
              }
            },
            "optional": {
              "type": "object",
              "additionalProperties": {
                "anyOf": [
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "INT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "FLOAT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "round": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "boolean",
                                    "const": false
                                  }
                                ]
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "BOOLEAN"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "boolean"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "label_on": {
                                "type": "string"
                              },
                              "label_off": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "STRING"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "string"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "multiline": {
                                "type": "boolean"
                              },
                              "dynamicPrompts": {
                                "type": "boolean"
                              },
                              "defaultVal": {
                                "type": "string"
                              },
                              "placeholder": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "array",
                        "items": {
                          "type": [
                            "string",
                            "number"
                          ]
                        }
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "COMBO"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  }
                ]
              }
            },
            "hidden": {
              "type": "object",
              "additionalProperties": {}
            }
          },
          "additionalProperties": false
        },
        "output": {
          "type": "array",
          "items": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "array",
                "items": {
                  "type": [
                    "string",
                    "number"
                  ]
                }
              }
            ]
          }
        },
        "output_is_list": {
          "type": "array",
          "items": {
            "type": "boolean"
          }
        },
        "output_name": {
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "output_tooltips": {
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "name": {
          "type": "string"
        },
        "display_name": {
          "type": "string"
        },
        "description": {
          "type": "string"
        },
        "category": {
          "type": "string"
        },
        "output_node": {
          "type": "boolean"
        },
        "python_module": {
          "type": "string"
        },
        "deprecated": {
          "type": "boolean"
        },
        "experimental": {
          "type": "boolean"
        }
      },
      "required": [
        "name",
        "display_name",
        "description",
        "category",
        "output_node",
        "python_module"
      ],
      "additionalProperties": false
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```


# Workflow JSON
Source: https://docs.comfy.org/specs/workflow_json

JSON schema for a ComfyUI workflow.

The workflow JSON is defined using [JSON Schema](https://json-schema.org/). Changes to this schema will be discussed in the [rfcs repo](https://github.com/comfy-org/rfcs).

## Version 1.0 (Latest)

```json ComfyUI Workflow v1.0
{
  "$ref": "#/definitions/ComfyWorkflow1_0",
  "definitions": {
    "ComfyWorkflow1_0": {
      "type": "object",
      "properties": {
        "version": {
          "type": "number",
          "const": 1
        },
        "config": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "links_ontop": {
                      "type": "boolean"
                    },
                    "align_to_grid": {
                      "type": "boolean"
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "state": {
          "type": "object",
          "properties": {
            "lastGroupid": {
              "type": "number"
            },
            "lastNodeId": {
              "type": "number"
            },
            "lastLinkId": {
              "type": "number"
            },
            "lastRerouteId": {
              "type": "number"
            }
          },
          "additionalProperties": true
        },
        "groups": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "title": {
                "type": "string"
              },
              "bounding": {
                "type": "array",
                "minItems": 4,
                "maxItems": 4,
                "items": [
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "font_size": {
                "type": "number"
              },
              "locked": {
                "type": "boolean"
              }
            },
            "required": [
              "title",
              "bounding"
            ],
            "additionalProperties": true
          }
        },
        "nodes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "type": "string"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "size": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "flags": {
                "type": "object",
                "properties": {
                  "collapsed": {
                    "type": "boolean"
                  },
                  "pinned": {
                    "type": "boolean"
                  },
                  "allow_interaction": {
                    "type": "boolean"
                  },
                  "horizontal": {
                    "type": "boolean"
                  },
                  "skip_repeated_outputs": {
                    "type": "boolean"
                  }
                },
                "additionalProperties": true
              },
              "order": {
                "type": "number"
              },
              "mode": {
                "type": "number"
              },
              "inputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "link": {
                      "type": [
                        "number",
                        "null"
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "outputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "links": {
                      "anyOf": [
                        {
                          "type": "array",
                          "items": {
                            "type": "number"
                          }
                        },
                        {
                          "type": "null"
                        }
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "properties": {
                "type": "object",
                "properties": {
                  "Node name for S&R": {
                    "type": "string"
                  }
                },
                "additionalProperties": true
              },
              "widgets_values": {
                "anyOf": [
                  {
                    "type": "array"
                  },
                  {
                    "type": "object",
                    "additionalProperties": {}
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "bgcolor": {
                "type": "string"
              }
            },
            "required": [
              "id",
              "type",
              "pos",
              "size",
              "flags",
              "order",
              "mode",
              "properties"
            ],
            "additionalProperties": true
          }
        },
        "links": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "type": "number"
              },
              "origin_id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "origin_slot": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "target_id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "target_slot": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "anyOf": [
                  {
                    "type": "string"
                  },
                  {
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "parentId": {
                "type": "number"
              }
            },
            "required": [
              "id",
              "origin_id",
              "origin_slot",
              "target_id",
              "target_slot",
              "type"
            ],
            "additionalProperties": true
          }
        },
        "reroutes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "type": "number"
              },
              "parentId": {
                "type": "number"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "linkIds": {
                "anyOf": [
                  {
                    "type": "array",
                    "items": {
                      "type": "number"
                    }
                  },
                  {
                    "type": "null"
                  }
                ]
              }
            },
            "required": [
              "id",
              "pos"
            ],
            "additionalProperties": true
          }
        },
        "extra": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "ds": {
                      "type": "object",
                      "properties": {
                        "scale": {
                          "type": "number"
                        },
                        "offset": {
                          "anyOf": [
                            {
                              "type": "object",
                              "properties": {
                                "0": {
                                  "type": "number"
                                },
                                "1": {
                                  "type": "number"
                                }
                              },
                              "required": [
                                "0",
                                "1"
                              ],
                              "additionalProperties": true
                            },
                            {
                              "type": "array",
                              "minItems": 2,
                              "maxItems": 2,
                              "items": [
                                {
                                  "type": "number"
                                },
                                {
                                  "type": "number"
                                }
                              ]
                            }
                          ]
                        }
                      },
                      "required": [
                        "scale",
                        "offset"
                      ],
                      "additionalProperties": true
                    },
                    "info": {
                      "type": "object",
                      "properties": {
                        "name": {
                          "type": "string"
                        },
                        "author": {
                          "type": "string"
                        },
                        "description": {
                          "type": "string"
                        },
                        "version": {
                          "type": "string"
                        },
                        "created": {
                          "type": "string"
                        },
                        "modified": {
                          "type": "string"
                        },
                        "software": {
                          "type": "string"
                        }
                      },
                      "required": [
                        "name",
                        "author",
                        "description",
                        "version",
                        "created",
                        "modified",
                        "software"
                      ],
                      "additionalProperties": true
                    },
                    "linkExtensions": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          }
                        },
                        "required": [
                          "id",
                          "parentId"
                        ],
                        "additionalProperties": true
                      }
                    },
                    "reroutes": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          },
                          "pos": {
                            "anyOf": [
                              {
                                "type": "object",
                                "properties": {
                                  "0": {
                                    "type": "number"
                                  },
                                  "1": {
                                    "type": "number"
                                  }
                                },
                                "required": [
                                  "0",
                                  "1"
                                ],
                                "additionalProperties": true
                              },
                              {
                                "type": "array",
                                "minItems": 2,
                                "maxItems": 2,
                                "items": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "number"
                                  }
                                ]
                              }
                            ]
                          },
                          "linkIds": {
                            "anyOf": [
                              {
                                "type": "array",
                                "items": {
                                  "type": "number"
                                }
                              },
                              {
                                "type": "null"
                              }
                            ]
                          }
                        },
                        "required": [
                          "id",
                          "pos"
                        ],
                        "additionalProperties": true
                      }
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "models": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": {
                "type": "string"
              },
              "url": {
                "type": "string",
                "format": "uri"
              },
              "hash": {
                "type": "string"
              },
              "hash_type": {
                "type": "string"
              },
              "directory": {
                "type": "string"
              }
            },
            "required": [
              "name",
              "url",
              "directory"
            ],
            "additionalProperties": false
          }
        }
      },
      "required": [
        "version",
        "state",
        "nodes"
      ],
      "additionalProperties": true
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```

## Older versions

* [0.4](./workflow_json_0.4.mdx)


# Workflow JSON 0.4
Source: https://docs.comfy.org/specs/workflow_json_0.4

JSON schema for a ComfyUI workflow.

## v0.4

```json
{
  "$ref": "#/definitions/ComfyWorkflow0_4",
  "definitions": {
    "ComfyWorkflow0_4": {
      "type": "object",
      "properties": {
        "last_node_id": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "string"
            }
          ]
        },
        "last_link_id": {
          "type": "number"
        },
        "nodes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "type": "string"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "size": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "flags": {
                "type": "object",
                "properties": {
                  "collapsed": {
                    "type": "boolean"
                  },
                  "pinned": {
                    "type": "boolean"
                  },
                  "allow_interaction": {
                    "type": "boolean"
                  },
                  "horizontal": {
                    "type": "boolean"
                  },
                  "skip_repeated_outputs": {
                    "type": "boolean"
                  }
                },
                "additionalProperties": true
              },
              "order": {
                "type": "number"
              },
              "mode": {
                "type": "number"
              },
              "inputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "link": {
                      "type": [
                        "number",
                        "null"
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "outputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "links": {
                      "anyOf": [
                        {
                          "type": "array",
                          "items": {
                            "type": "number"
                          }
                        },
                        {
                          "type": "null"
                        }
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "properties": {
                "type": "object",
                "properties": {
                  "Node name for S&R": {
                    "type": "string"
                  }
                },
                "additionalProperties": true
              },
              "widgets_values": {
                "anyOf": [
                  {
                    "type": "array"
                  },
                  {
                    "type": "object",
                    "additionalProperties": {}
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "bgcolor": {
                "type": "string"
              }
            },
            "required": [
              "id",
              "type",
              "pos",
              "size",
              "flags",
              "order",
              "mode",
              "properties"
            ],
            "additionalProperties": true
          }
        },
        "links": {
          "type": "array",
          "items": {
            "type": "array",
            "minItems": 6,
            "maxItems": 6,
            "items": [
              {
                "type": "number"
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "string"
                  },
                  {
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  {
                    "type": "number"
                  }
                ]
              }
            ]
          }
        },
        "groups": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "title": {
                "type": "string"
              },
              "bounding": {
                "type": "array",
                "minItems": 4,
                "maxItems": 4,
                "items": [
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "font_size": {
                "type": "number"
              },
              "locked": {
                "type": "boolean"
              }
            },
            "required": [
              "title",
              "bounding"
            ],
            "additionalProperties": true
          }
        },
        "config": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "links_ontop": {
                      "type": "boolean"
                    },
                    "align_to_grid": {
                      "type": "boolean"
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "extra": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "ds": {
                      "type": "object",
                      "properties": {
                        "scale": {
                          "type": "number"
                        },
                        "offset": {
                          "anyOf": [
                            {
                              "type": "object",
                              "properties": {
                                "0": {
                                  "type": "number"
                                },
                                "1": {
                                  "type": "number"
                                }
                              },
                              "required": [
                                "0",
                                "1"
                              ],
                              "additionalProperties": true
                            },
                            {
                              "type": "array",
                              "minItems": 2,
                              "maxItems": 2,
                              "items": [
                                {
                                  "type": "number"
                                },
                                {
                                  "type": "number"
                                }
                              ]
                            }
                          ]
                        }
                      },
                      "required": [
                        "scale",
                        "offset"
                      ],
                      "additionalProperties": true
                    },
                    "info": {
                      "type": "object",
                      "properties": {
                        "name": {
                          "type": "string"
                        },
                        "author": {
                          "type": "string"
                        },
                        "description": {
                          "type": "string"
                        },
                        "version": {
                          "type": "string"
                        },
                        "created": {
                          "type": "string"
                        },
                        "modified": {
                          "type": "string"
                        },
                        "software": {
                          "type": "string"
                        }
                      },
                      "required": [
                        "name",
                        "author",
                        "description",
                        "version",
                        "created",
                        "modified",
                        "software"
                      ],
                      "additionalProperties": true
                    },
                    "linkExtensions": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          }
                        },
                        "required": [
                          "id",
                          "parentId"
                        ],
                        "additionalProperties": true
                      }
                    },
                    "reroutes": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          },
                          "pos": {
                            "anyOf": [
                              {
                                "type": "object",
                                "properties": {
                                  "0": {
                                    "type": "number"
                                  },
                                  "1": {
                                    "type": "number"
                                  }
                                },
                                "required": [
                                  "0",
                                  "1"
                                ],
                                "additionalProperties": true
                              },
                              {
                                "type": "array",
                                "minItems": 2,
                                "maxItems": 2,
                                "items": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "number"
                                  }
                                ]
                              }
                            ]
                          },
                          "linkIds": {
                            "anyOf": [
                              {
                                "type": "array",
                                "items": {
                                  "type": "number"
                                }
                              },
                              {
                                "type": "null"
                              }
                            ]
                          }
                        },
                        "required": [
                          "id",
                          "pos"
                        ],
                        "additionalProperties": true
                      }
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "version": {
          "type": "number"
        },
        "models": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": {
                "type": "string"
              },
              "url": {
                "type": "string",
                "format": "uri"
              },
              "hash": {
                "type": "string"
              },
              "hash_type": {
                "type": "string"
              },
              "directory": {
                "type": "string"
              }
            },
            "required": [
              "name",
              "url",
              "directory"
            ],
            "additionalProperties": false
          }
        }
      },
      "required": [
        "last_node_id",
        "last_link_id",
        "nodes",
        "links",
        "version"
      ],
      "additionalProperties": true
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```


# ComfyUI Hunyuan3D-2 Examples
Source: https://docs.comfy.org/tutorials/3d/hunyuan3D-2

This guide will demonstrate how to use Hunyuan3D-2 in ComfyUI to generate 3D assets.

# Hunyuan3D 2.0 Introduction

![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-1.gif)
![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-2.gif)

[Hunyuan3D 2.0](https://github.com/Tencent/Hunyuan3D-2) is an open-source 3D asset generation model released by Tencent, capable of generating high-fidelity 3D models with high-resolution texture maps through text or images.

Hunyuan3D 2.0 adopts a two-stage generation approach, first generating a geometry model without textures, then synthesizing high-resolution texture maps. This effectively separates the complexity of shape and texture generation. Below are the two core components of Hunyuan3D 2.0:

1. **Geometry Generation Model (Hunyuan3D-DiT)**: Based on a flow diffusion Transformer architecture, it generates untextured geometric models that precisely match input conditions.
2. **Texture Generation Model (Hunyuan3D-Paint)**: Combines geometric conditions and multi-view diffusion techniques to add high-resolution textures to models, supporting PBR materials.

**Key Advantages**

* **High-Precision Generation**: Sharp geometric structures, rich texture colors, support for PBR material generation, achieving near-realistic lighting effects.
* **Diverse Usage Methods**: Provides code calls, Blender plugins, Gradio applications, and online experience through the official website, suitable for different user needs.
* **Lightweight and Compatibility**: The Hunyuan3D-2mini model requires only 5GB VRAM, the standard version needs 6GB VRAM for shape generation, and the complete process (shape + texture) requires only 12GB VRAM.

Recently (March 18, 2025), Hunyuan3D 2.0 also introduced a multi-view shape generation model (Hunyuan3D-2mv), which supports generating more detailed geometric structures from inputs at different angles.

This example includes three workflows:

* Using Hunyuan3D-2mv with multiple view inputs to generate 3D models
* Using Hunyuan3D-2mv-turbo with multiple view inputs to generate 3D models
* Using Hunyuan3D-2 with a single view input to generate 3D models

<Tip>
  ComfyUI now natively supports Hunyuan3D-2mv, but does not yet support texture and material generation. Please make sure you have updated to the latest version of [ComfyUI](https://github.com/comfyanonymous/ComfyUI) before starting.

  The workflow example PNG images in this tutorial contain workflow JSON in their metadata:

  * You can drag them directly into ComfyUI
  * Or use the menu `Workflows` -> `Open (ctrl+o)`

  This will load the corresponding workflow and prompt you to download the required models. The generated `.glb` format models will be output to the `ComfyUI/output/mesh` folder.
</Tip>

## ComfyUI Hunyuan3D-2mv Workflow Example

In the Hunyuan3D-2mv workflow, we'll use multi-view images to generate a 3D model. Note that multiple view images are not mandatory in this workflow - you can use only the `front` view image to generate a 3D model.

### 1. Workflow

Please download the images below and drag  into ComfyUI to load the workflow.
![Hunyuan3D-2mv workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/hunyuan-3d-multiview-elf.webp)

Download the images below we will use them as input images.

<div class="flex space-x-4">
  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/front.png" alt="input image" class="w-1/3" />

  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/left.png" alt="input image" class="w-1/3" />

  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/back.png" alt="input image" class="w-1/3" />
</div>

<Tip>
  In this example, the input images have already been preprocessed to remove excess background. In actual use, you can use custom nodes like [ComfyUI\_essentials](https://github.com/cubiq/ComfyUI_essentials) to automatically remove excess background.
</Tip>

### 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

* hunyuan3d-dit-v2-mv: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv.safetensors`

```
ComfyUI/
 models/
    checkpoints/
       hunyuan3d-dit-v2-mv.safetensors  // renamed file
```

### 3. Steps to Run the Workflow

![ComfyUI hunyuan3d\_2mv](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg)

1. Ensure that the Image Only Checkpoint Loader(img2vid model) has loaded our downloaded and renamed `hunyuan3d-dit-v2-mv.safetensors` model
2. Load the corresponding view images in each of the `Load Image` nodes
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

If you need to add more views, make sure to load other view images in the `Hunyuan3Dv2ConditioningMultiView` node, and ensure that you load the corresponding view images in the `Load Image` nodes.

## Hunyuan3D-2mv-turbo Workflow

In the Hunyuan3D-2mv-turbo workflow, we'll use the Hunyuan3D-2mv-turbo model to generate 3D models. This model is a step distillation version of Hunyuan3D-2mv, allowing for faster 3D model generation. In this version of the workflow, we set `cfg` to 1.0 and add a `flux guidance` node to control the `distilled cfg` generation.

### 1. Workflow

Please download the images below and drag into ComfyUI to load the workflow.

![Hunyuan3D-2mv-turbo workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/hunyuan-3d-turbo.webp)

Download the images below we will use them as input images.

<div class="flex space-x-4">
  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/front.png" alt="input image" class="w-1/2" />

  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/right.png" alt="input image" class="w-1/2" />
</div>

### 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

* hunyuan3d-dit-v2-mv-turbo: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv-turbo/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv-turbo.safetensors`

```
ComfyUI/
 models/
    checkpoints/
       hunyuan3d-dit-v2-mv-turbo.safetensors  // renamed file
```

### 3. Steps to Run the Workflow

![ComfyUI hunyuan3d\_2mv\_turbo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg)

1. Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2-mv-turbo.safetensors` model
2. Load the corresponding view images in each of the `Load Image` nodes
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Hunyuan3D-2 Single View Workflow

In the Hunyuan3D-2 workflow, we'll use the Hunyuan3D-2 model to generate 3D models. This model is not a multi-view model. In this workflow, we use the `Hunyuan3Dv2Conditioning` node instead of the `Hunyuan3Dv2ConditioningMultiView` node.

### 1. Workflow

Please download the image below and drag it into ComfyUI to load the workflow.

![Hunyuan3D-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d-non-multiview-train.webp)

Download the image below we will use it as input image.
![ComfyUI Hunyuan 3D 2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan_3d_v2_non_multiview_train.png)

### 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

* hunyuan3d-dit-v2-0: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2/resolve/main/hunyuan3d-dit-v2-0/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2.safetensors`

```
ComfyUI/
 models/
    checkpoints/
       hunyuan3d-dit-v2.safetensors  // renamed file
```

### 3. Steps to Run the Workflow

![ComfyUI hunyuan3d\_2](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg)

1. Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2.safetensors` model
2. Load the image in the `Load Image` node
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Community Resources

Below are ComfyUI community resources related to Hunyuan3D-2

* [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)
* [Kijai/Hunyuan3D-2\_safetensors](https://huggingface.co/Kijai/Hunyuan3D-2_safetensors/tree/main)
* [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)

## Hunyuan3D 2.0 Open-Source Model Series

Currently, Hunyuan3D 2.0 has open-sourced multiple models covering the complete 3D generation process. You can visit [Hunyuan3D-2](https://github.com/Tencent/Hunyuan3D-2) for more information.

**Hunyuan3D-2mini Series**

| Model                 | Description               | Date       | Parameters | Huggingface                                                                             |
| --------------------- | ------------------------- | ---------- | ---------- | --------------------------------------------------------------------------------------- |
| Hunyuan3D-DiT-v2-mini | Mini Image to Shape Model | 2025-03-18 | 0.6B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini) |

**Hunyuan3D-2mv Series**

| Model                    | Description                                                                                                 | Date       | Parameters | Huggingface                                                                              |
| ------------------------ | ----------------------------------------------------------------------------------------------------------- | ---------- | ---------- | ---------------------------------------------------------------------------------------- |
| Hunyuan3D-DiT-v2-mv-Fast | Guidance Distillation Version, can halve DIT inference time                                                 | 2025-03-18 | 1.1B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast) |
| Hunyuan3D-DiT-v2-mv      | Multi-view Image to Shape Model, suitable for 3D creation requiring multiple angles to understand the scene | 2025-03-18 | 1.1B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv)      |

**Hunyuan3D-2 Series**

| Model                   | Description                 | Date       | Parameters | Huggingface                                                                           |
| ----------------------- | --------------------------- | ---------- | ---------- | ------------------------------------------------------------------------------------- |
| Hunyuan3D-DiT-v2-0-Fast | Guidance Distillation Model | 2025-02-03 | 1.1B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast) |
| Hunyuan3D-DiT-v2-0      | Image to Shape Model        | 2025-01-21 | 1.1B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0)      |
| Hunyuan3D-Paint-v2-0    | Texture Generation Model    | 2025-01-21 | 1.3B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0)    |
| Hunyuan3D-Delight-v2-0  | Image Delight Model         | 2025-01-21 | 1.3B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0)  |


# ComfyUI Native HiDream-I1 Text-to-Image Workflow Example
Source: https://docs.comfy.org/tutorials/advanced/hidream

This guide will walk you through completing a ComfyUI native HiDream-I1 text-to-image workflow example

![HiDream-I1 Demo](https://raw.githubusercontent.com/HiDream-ai/HiDream-I1/main/assets/demo.jpg)

HiDream-I1 is a text-to-image model officially open-sourced by HiDream-ai on April 7, 2025. The model has 17B parameters and is released under the [MIT license](https://github.com/HiDream-ai/HiDream-I1/blob/main/LICENSE), supporting personal projects, scientific research, and commercial use.
It currently performs excellently in multiple benchmark tests.

## Model Features

**Hybrid Architecture Design**
A combination of Diffusion Transformer (DiT) and Mixture of Experts (MoE) architecture:

* Based on Diffusion Transformer (DiT), with dual-stream MMDiT modules processing multimodal information and single-stream DiT modules optimizing global consistency.
* Dynamic routing mechanism flexibly allocates computing resources, enhancing complex scene processing capabilities and delivering excellent performance in color restoration, edge processing, and other details.

**Multimodal Text Encoder Integration**
Integrates four text encoders:

* OpenCLIP ViT-bigG, OpenAI CLIP ViT-L (visual semantic alignment)
* T5-XXL (long text parsing)
* Llama-3.1-8B-Instruct (instruction understanding)
  This combination achieves SOTA performance in complex semantic parsing of colors, quantities, spatial relationships, etc., with Chinese prompt support significantly outperforming similar open-source models.

**Original Model Versions**

HiDream-ai provides three versions of the HiDream-I1 model to meet different needs. Below are the links to the original model repositories:

| Model Name      | Description    | Inference Steps | Repository Link                                                         |
| --------------- | -------------- | --------------- | ----------------------------------------------------------------------- |
| HiDream-I1-Full | Full version   | 50              | [ HiDream-I1-Full](https://huggingface.co/HiDream-ai/HiDream-I1-Full) |
| HiDream-I1-Dev  | Distilled dev  | 28              | [ HiDream-I1-Dev](https://huggingface.co/HiDream-ai/HiDream-I1-Dev)   |
| HiDream-I1-Fast | Distilled fast | 16              | [ HiDream-I1-Fast](https://huggingface.co/HiDream-ai/HiDream-I1-Fast) |

## About This Workflow Example

In this example, we will use the repackaged version from ComfyOrg. You can find all the model files we'll use in this example in the [HiDream-I1\_ComfyUI](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/) repository.

<Tip>
  Before starting, please update your ComfyUI version to ensure it's at least after this [commit](https://github.com/comfyanonymous/ComfyUI/commit/9ad792f92706e2179c58b2e5348164acafa69288) to make sure your ComfyUI has native support for HiDream
</Tip>

## HiDream-I1 Workflow

The model requirements for different ComfyUI native HiDream-I1 workflows are basically the same, with only the [diffusion models](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files/diffusion_models) files being different.

If you don't know which version to choose, please refer to the following suggestions:

* **HiDream-I1-Full** can generate the highest quality images
* **HiDream-I1-Dev** balances high-quality image generation with speed
* **HiDream-I1-Fast** can generate images in just 16 steps, suitable for scenarios requiring real-time iteration

For the **dev** and **fast** versions, negative prompts are not needed, so please set the `cfg` parameter to `1.0` during sampling. We have noted the corresponding parameter settings in the relevant workflows.

<Tip>
  The full versions of all three versions require a lot of VRAM - you may need more than 27GB of VRAM to run them smoothly. In the corresponding workflow tutorials,
  we will use the **fp8** version as a demonstration example to ensure that most users can run it smoothly.
  However, we will still provide download links for different versions of the model in the corresponding examples, and you can choose the appropriate file based on your VRAM situation.
</Tip>

### Model Installation

The following model files are common files that we will use.
Please click on the corresponding links to download and save them according to the model file save location.
We will guide you to download the corresponding **diffusion models** in the corresponding workflows.

**text\_encoders**

* [clip\_l\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)
* [clip\_g\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)
* [t5xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) This model has been used in many workflows, you may have already downloaded this file.
* [llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)

**VAE**

* [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) This is Flux's VAE model, if you have used Flux's workflow before, you may have already downloaded this file.

**diffusion models**
We will guide you to download the corresponding model files in the corresponding workflows.

Model file save location

```
 ComfyUI/
  models/
     text_encoders/
       clip_l_hidream.safetensors
       clip_g_hidream.safetensors
       t5xxl_fp8_e4m3fn_scaled.safetensors
       llama_3.1_8b_instruct_fp8_scaled.safetensors
     vae/
       ae.safetensors
     diffusion_models/
        ...               # We will guide you to install in the corresponding version workflow       
```

### HiDream-I1 Full Version Workflow

#### 1. Model File Download

Please select the appropriate version based on your hardware. Click the link and download the corresponding model file to save it to the `ComfyUI/models/diffusion_models/` folder.

* FP8 version: [hidream\_i1\_full\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp8.safetensors?download=true) requires more than 16GB of VRAM
* Full version: [hidream\_i1\_full\_f16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp16.safetensors?download=true) requires more than 27GB of VRAM

#### 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow
![HiDream-I1 Full Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_full.png)

#### 3. Complete the Workflow Step by Step

![HiDream-I1 Full Version Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg)

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_full_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **full** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`
5. For the `Ksampler` node, you need to make the following settings
   * Set `steps` to `50`
   * Set `cfg` to `5.0`
   * (Optional) Set `sampler` to `lcm`
   * (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

### HiDream-I1 Dev Version Workflow

#### 1. Model File Download

Please select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.

* FP8 version: [hidream\_i1\_dev\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_fp8.safetensors?download=true) requires more than 16GB of VRAM
* Full version: [hidream\_i1\_dev\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_bf16.safetensors?download=true) requires more than 27GB of VRAM

#### 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

![HiDream-I1 Dev Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_dev.png)

#### 3. Complete the Workflow Step by Step

![HiDream-I1 Dev Version Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg)
Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_dev_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **dev** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `6.0`
5. For the `Ksampler` node, you need to make the following settings
   * Set `steps` to `28`
   * (Important) Set `cfg` to `1.0`
   * (Optional) Set `sampler` to `lcm`
   * (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

### HiDream-I1 Fast Version Workflow

#### 1. Model File Download

Please select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.

* FP8 version: [hidream\_i1\_fast\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) requires more than 16GB of VRAM
* Full version: [hidream\_i1\_fast\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) requires more than 27GB of VRAM

#### 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

![HiDream-I1 Fast Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_fast.png)

#### 3. Complete the Workflow Step by Step

![HiDream-I1 Fast Version Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg)

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_fast_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **fast** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`
5. For the `Ksampler` node, you need to make the following settings
   * Set `steps` to `16`
   * (Important) Set `cfg` to `1.0`
   * (Optional) Set `sampler` to `lcm`
   * (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Other Related Resources

### GGUF Version Models

* [HiDream-I1-Full-gguf](https://huggingface.co/city96/HiDream-I1-Full-gguf)
* [HiDream-I1-Dev-gguf](https://huggingface.co/city96/HiDream-I1-Dev-gguf)

You need to use the Unet Loader (GGUF) node in City96's [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF) to replace the Load Diffusion Model node.

### NF4 Version Models

* [HiDream-I1-nf4](https://github.com/hykilpikonna/HiDream-I1-nf4)
* Use the [ComfyUI-HiDream-Sampler](https://github.com/SanDiegoDude/ComfyUI-HiDream-Sampler) node to use the NF4 version model.


# ComfyUI Image to Image Workflow
Source: https://docs.comfy.org/tutorials/basic/image-to-image

This guide will help you understand and complete an image to image workflow

## What is Image to Image

Image to Image is a workflow in ComfyUI that allows users to input an image and generate a new image based on it.

Image to Image can be used in scenarios such as:

* Converting original image styles, like transforming realistic photos into artistic styles
* Converting line art into realistic images
* Image restoration
* Colorizing old photos
* ... and other scenarios

To explain it with an analogy:
It's like asking an artist to create a specific piece based on your reference image.

If you carefully compare this tutorial with the [Text to Image](/tutorials/basic/text-to-image) tutorial,
you'll notice that the Image to Image process is very similar to Text to Image,
just with an additional input reference image as a condition. In Text to Image, we let the artist (image model) create freely based on our prompts,
while in Image to Image, we let the artist create based on both our reference image and prompts.

## ComfyUI Image to Image Workflow Example Guide

### Model Installation

Download the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) file and put it in your `ComfyUI/models/checkpoints` folder.

### Image to Image Workflow and Input Image

Download the image below and **drag it into ComfyUI** to load the workflow:
![Image to Image Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image_to_image.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

Download the image below and we will use it as the input image:
![Example Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/input.jpeg)

### Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

![ComfyUI Image to Image Workflow - Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image-to-image-02-guide.jpg)

1. Ensure `Load Checkpoint` loads  **v1-5-pruned-emaonly-fp16.safetensors**
2. Upload the input image to the `Load Image` node
3. Click `Queue` or press `Ctrl/Cmd + Enter` to generate

## Key Points of Image to Image Workflow

The key to the Image to Image workflow lies in the `denoise` parameter in the `KSampler` node, which should be **less than 1**

If you've adjusted the `denoise` parameter and generated images, you'll notice:

* The smaller the `denoise` value, the smaller the difference between the generated image and the reference image
* The larger the `denoise` value, the larger the difference between the generated image and the reference image

This is because `denoise` determines the strength of noise added to the latent space image after converting the reference image. If `denoise` is 1, the latent space image will become completely random noise, making it the same as the latent space generated by the `empty latent image` node, losing all characteristics of the reference image.

For the corresponding principles, please refer to the principle explanation in the [Text to Image](/tutorials/basic/text-to-image) tutorial.

## Try It Yourself

1. Try modifying the `denoise` parameter in the **KSampler** node, gradually changing it from 1 to 0, and observe the changes in the generated images
2. Replace with your own prompts and reference images to generate your own image effects


# ComfyUI Inpainting Workflow
Source: https://docs.comfy.org/tutorials/basic/inpaint

This guide will introduce you to the inpainting workflow in ComfyUI, walk you through an inpainting example, and cover topics like using the mask editor

This article will introduce the concept of inpainting in AI image generation and guide you through creating an inpainting workflow in ComfyUI. We'll cover:

* Using inpainting workflows to modify images
* Using the ComfyUI mask editor to draw masks
* `VAE Encoder (for Inpainting)` node

## About Inpainting

In AI image generation, we often encounter situations where we're satisfied with the overall image but there are elements we don't want or that contain errors. Simply regenerating might produce a completely different image, so using inpainting to fix specific parts becomes very useful.

It's like having an **artist (AI model)** paint a picture, but we're still not satisfied with the specific details. We need to tell the artist **which areas to adjust (mask)**, and then let them **repaint (inpaint)** according to our requirements.

Common inpainting scenarios include:

* **Defect Repair:** Removing unwanted objects, fixing incorrect AI-generated body parts, etc.
* **Detail Optimization:** Precisely adjusting local elements (like modifying clothing textures, adjusting facial expressions)
* And other scenarios

## ComfyUI Inpainting Workflow Example

### Model and Resource Preparation

#### 1. Model Installation

Download the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)  file and put it in your `ComfyUI/models/checkpoints` folder:

#### 2. Inpainting Asset

Please download the following image which we'll use as input:

![ComfyUI Inpainting Input Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/input.png)

<Note>This image already contains an alpha channel (transparency mask), so you don't need to manually draw a mask. This tutorial will also cover how to use the mask editor to draw masks.</Note>

#### 3. Inpainting Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

![ComfyUI Inpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

### ComfyUI Inpainting Workflow Example Explanation

Follow the steps in the diagram below to ensure the workflow runs correctly.

![ComfyUI Inpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_workflow.png)

1. Ensure `Load Checkpoint` loads `512-inpainting-ema.safetensors`
2. Upload the input image to the `Load Image` node
3. Click `Queue` or use `Ctrl + Enter` to generate

![Inpainting Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png)

For comparison, here's the result using the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model:

![SD1.5 Inpainting Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png)

You will find that the results generated by the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) model have better inpainting effects and more natural transitions.
This is because this model is specifically designed for inpainting, which helps us better control the generation area, resulting in improved inpainting effects.

Do you remember the analogy we've been using? Different models are like artists with varying abilities, but each artist has their own limits. Choosing the right model can help you achieve better generation results.

You can try these approaches to achieve better results:

1. Modify positive and negative prompts with more specific descriptions
2. Try multiple runs using different seeds in the `KSampler` for different generation results
3. After learning about the mask editor in this tutorial, you can re-inpaint the generated results to achieve satisfactory outcomes.

Next, we'll learn about using the **Mask Editor**. While our input image already includes an `alpha` transparency channel (the area we want to edit),
so manual mask drawing isn't necessary, you'll often use the Mask Editor to create masks in practical applications.

### Using the Mask Editor

First right-click the `Save Image` node and select `Copy(Clipspace)`:

![Copy Image to Clipboard](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png)

Then right-click the **Load Image** node and select `Paste(Clipspace)`:

![Paste Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png)

Right-click the **Load Image** node again and select `Open in MaskEditor`:

![Open Mask Editor](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg)

![Mask Editor Demo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint-maskeditor.gif)

1. Adjust brush parameters on the right panel
2. Use eraser to correct mistakes
3. Click `Save` when finished

The drawn content will be used as a Mask input to the VAE Encoder (for Inpainting) node for encoding

Then try adjusting your prompts and generating again until you achieve satisfactory results.

## VAE Encoder (for Inpainting) Node

Comparing this workflow with [Text-to-Image](/tutorials/basic/text-to-image) and [Image-to-Image](/tutorials/basic/image-to-image), you'll notice the main differences are in the VAE section's conditional inputs.
In this workflow, we use the **VAE Encoder (for Inpainting)** node, specifically designed for inpainting to help us better control the generation area and achieve better results.

![VAE Encoder (for Inpainting) Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/lantent/inpaint/vae_encode_for_inpainting.jpg)

**Input Types**

| Parameter Name | Function                                                                                                                                              |
| -------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| `pixels`       | Input image to be encoded into latent space.                                                                                                          |
| `vae`          | VAE model used to encode the image from pixel space to latent space.                                                                                  |
| `mask`         | Image mask specifying which areas need modification.                                                                                                  |
| `grow_mask_by` | Pixel value to expand the original mask outward, ensuring a transition area around the mask to avoid hard edges between inpainted and original areas. |

**Output Types**

| Parameter Name | Function                                    |
| -------------- | ------------------------------------------- |
| `latent`       | Image encoded into latent space by the VAE. |


# ComfyUI LoRA Example
Source: https://docs.comfy.org/tutorials/basic/lora

This guide will help you understand and use a single LoRA model

**LoRA (Low-Rank Adaptation)** is an efficient technique for fine-tuning large generative models like Stable Diffusion.
It introduces trainable low-rank matrices to the pre-trained model, adjusting only a portion of parameters rather than retraining the entire model,
thus achieving optimization for specific tasks at a lower computational cost.
Compared to base models like SD1.5, LoRA models are smaller and easier to train.

![LoRA Model vs Base Model Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/compare.png)

The image above compares generation with the same parameters using [dreamshaper\_8](https://civitai.com/models/4384?modelVersionId=128713) directly versus using the [blindbox\_V1Mix](https://civitai.com/models/25995/blindbox) LoRA model.
As you can see, by using a LoRA model, we can generate images in different styles without adjusting the base model.

We will demonstrate how to use a LoRA model. All LoRA variants: Lycoris, loha, lokr, locon, etc... are used in the same way.

In this example, we will learn how to load and use a LoRA model in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), covering the following topics:

1. Installing a LoRA model
2. Generating images using a LoRA model
3. A simple introduction to the `Load LoRA` node

## Required Model Installation

Download the [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder.

Download the [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model\&format=SafeTensor\&size=full\&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

## LoRA Workflow File

Download the image below and **drag it into ComfyUI** to load the workflow.
![ComfyUI Workflow - LoRA](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/lora.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

## Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

![ComfyUI Workflow - LoRA Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/flow_diagram.png)

1. Ensure `Load Checkpoint` loads `dreamshaper_8.safetensors`
2. Ensure `Load LoRA` loads `blindbox_V1Mix.safetensors`
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

## Load LoRA Node Introduction

![Load LoRA Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_lora.jpg)

Models in the `ComfyUI\models\loras` folder will be detected by ComfyUI and can be loaded using this node.

### Input Types

| Parameter Name   | Function                                                                                               |
| ---------------- | ------------------------------------------------------------------------------------------------------ |
| `model`          | Connect to the base model                                                                              |
| `clip`           | Connect to the CLIP model                                                                              |
| `lora_name`      | Select the LoRA model to load and use                                                                  |
| `strength_model` | Affects how strongly the LoRA influences the model weights; higher values make the LoRA style stronger |
| `strength_clip`  | Affects how strongly the LoRA influences the CLIP text embeddings                                      |

### Output Types

| Parameter Name | Function                                             |
| -------------- | ---------------------------------------------------- |
| `model`        | Outputs the model with LoRA adjustments applied      |
| `clip`         | Outputs the CLIP model with LoRA adjustments applied |

This node supports chain connections, allowing multiple `Load LoRA` nodes to be linked in series to apply multiple LoRA models. For more details, please refer to [ComfyUI Multiple LoRAs Example](/tutorials/basic/multiple-loras)

![LoRA Node Chain Connection](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png)

## Try It Yourself

1. Try modifying the prompt or adjusting different parameters of the `Load LoRA` node, such as `strength_model`, to observe changes in the generated images and become familiar with the `Load LoRA` node.
2. Visit [CivitAI](https://civitai.com/models) to download other kinds of LoRA models and try using them.


# ComfyUI Multiple LoRAs Example
Source: https://docs.comfy.org/tutorials/basic/multiple-loras

This guide demonstrates how to apply multiple LoRA models simultaneously in ComfyUI

In our [ComfyUI LoRA Example](/tutorials/basic/lora), we introduced how to load and use a single LoRA model, mentioning the node's chain connection capability.

![LoRA Node Chaining](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png)

This tutorial demonstrates chaining multiple `Load LoRA` nodes to apply two LoRA models simultaneously: [blindbox\_V1Mix](https://civitai.com/models/25995?modelVersionId=32988) and [MoXinV1](https://civitai.com/models/12597?modelVersionId=14856).

The comparison below shows individual effects of these LoRAs using identical parameters:

![Single LoRA Effects Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/compare.png)

By chaining multiple LoRA models, we achieve a blended style in the final output:

![Multi-LoRA Application Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png)

## Model Installation

Download the [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder.

Download the [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model\&format=SafeTensor\&size=full\&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

Download the [MoXinV1.safetensors](https://civitai.com/api/download/models/14856?type=Model\&format=SafeTensor\&size=full\&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

## Multi-LoRA Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:
![ComfyUI Workflow - Multiple LoRAs](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

## Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

![Multi-LoRA Workflow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/flow_diagram.png)

1. Ensure `Load Checkpoint` loads  **dreamshaper\_8.safetensors**
2. Ensure first `Load LoRA` loads **blindbox\_V1Mix.safetensors**
3. Ensure second `Load LoRA` loads **MoXinV1.safetensors**
4. Click `Queue` or press `Ctrl/Cmd + Enter` to generate

## Try It Yourself

1. Adjust `strength_model` values in both `Load LoRA` nodes to control each LoRA's influence
2. Explore [CivitAI](https://civitai.com/models) for additional LoRAs and create custom combinations


# ComfyUI Outpainting Workflow Example
Source: https://docs.comfy.org/tutorials/basic/outpaint

This guide will introduce you to the outpainting workflow in ComfyUI and walk you through an outpainting example

This guide will introduce you to the concept of outpainting in AI image generation and how to create an outpainting workflow in ComfyUI. We will cover:

* Using outpainting workflow to extend an image
* Understanding and using outpainting-related nodes in ComfyUI
* Mastering the basic outpainting process

## About Outpainting

In AI image generation, we often encounter situations where an existing image has good composition but the canvas area is too small, requiring us to extend the canvas to get a larger scene. This is where outpainting comes in.

Basically, it requires similar content to [Inpainting](/tutorials/basic/inpaint), but we use different nodes to **build the mask**.

Outpainting applications include:

* **Scene Extension:** Expand the scene range of the original image to show a more complete environment
* **Composition Adjustment:** Optimize the overall composition by extending the canvas
* **Content Addition:** Add more related scene elements to the original image

## ComfyUI Outpainting Workflow Example Explanation

### Preparation

#### 1. Model Installation

Download the following model file and save it to `ComfyUI/models/checkpoints` directory:

* [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)

#### 2. Input Image

Prepare an image you want to extend. In this example, we will use the following image:

![ComfyUI Outpainting Input Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/input.png)

#### 3. Outpainting Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

![ComfyUI Outpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpaint.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

### Outpainting Workflow Usage Explanation

![ComfyUI Outpainting Workflow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpainting_workflow.jpg)

The key steps of the outpainting workflow are as follows:

1. Load the locally installed model file in the `Load Checkpoint` node
2. Click the `Upload` button in the `Load Image` node to upload your image
3. Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute the image generation

In this workflow, we mainly use the `Pad Image for outpainting` node to control the direction and range of image extension. This is actually an [Inpaint](/tutorials/basic/inpaint.mdx) workflow, but we use different nodes to build the mask.

### Pad Image for outpainting Node

![Pad Image for outpainting Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/pad_image_for_outpainting.jpg)

This node accepts an input image and outputs an extended image with a corresponding mask, where the mask is built based on the node parameters.

#### Input Parameters

| Parameter Name | Function                                                                                                                              |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| `image`        | Input image                                                                                                                           |
| `left`         | Left padding amount                                                                                                                   |
| `top`          | Top padding amount                                                                                                                    |
| `right`        | Right padding amount                                                                                                                  |
| `bottom`       | Bottom padding amount                                                                                                                 |
| `feathering`   | Controls the smoothness of the transition between the original image and the added padding, higher values create smoother transitions |

#### Output Parameters

| Parameter Name | Function                                                                   |
| -------------- | -------------------------------------------------------------------------- |
| `image`        | Output `image` represents the padded image                                 |
| `mask`         | Output `mask` indicates the original image area and the added padding area |

#### Node Output Content

After processing by the `Pad Image for outpainting` node, the output image and mask preview are as follows:

![Pad Image for outpainting Node Results](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg)

You can see the corresponding output results:

* The `Image` output is the extended image
* The `Mask` output is the mask marking the extension areas


# ComfyUI Text to Image Workflow
Source: https://docs.comfy.org/tutorials/basic/text-to-image

This guide will help you understand the concept of text-to-image in AI art generation and complete a text-to-image workflow in ComfyUI

This guide aims to introduce you to ComfyUI's text-to-image workflow and help you understand the functionality and usage of various ComfyUI nodes.

In this document, we will:

* Complete a text-to-image workflow
* Gain a basic understanding of diffusion model principles
* Learn about the functions and roles of workflow nodes
* Get an initial understanding of the SD1.5 model

We'll start by running a text-to-image workflow, followed by explanations of related concepts. Please choose the relevant sections based on your needs.

## About Text to Image

**Text to Image** is a fundamental process in AI art generation that creates images from text descriptions, with **diffusion models** at its core.

The text-to-image process requires the following elements:

* **Artist:** The image generation model
* **Canvas:** The latent space
* **Image Requirements (Prompts):** Including positive prompts (elements you want in the image) and negative prompts (elements you don't want)

This text-to-image generation process can be simply understood as telling your requirements (positive and negative prompts) to an **artist (the image model)**, who then creates what you want based on these requirements.

## ComfyUI Text to Image Workflow Example Guide

### 1. Preparation

Ensure you have at least one SD1.5 model file in your `ComfyUI/models/checkpoints` folder, such as [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)

If you haven't installed it yet, please refer to the model installation section in [Getting Started with ComfyUI AI Art Generation](/get_started/first_generation).

### 2. Loading the Text to Image Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/text-to-image-workflow.png" alt="ComfyUI-Text to Image Workflow" />

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

### 3. Loading the Model and Generating Your First Image

After installing the image model, follow the steps in the image below to load the model and generate your first image

![Image Generation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg)

Follow these steps according to the image numbers:

1. In the **Load Checkpoint** node, use the arrows or click the text area to ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected, and the left/right arrows don't show **null** text
2. Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute image generation

After the process completes, you should see the resulting image in the **Save Image** node interface, which you can right-click to save locally

![ComfyUI First Image Generation Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg)

<Tip>If you're not satisfied with the result, try running the generation multiple times. Each time you run it, **KSampler** will use a different random seed based on the `seed` parameter, so each generation will produce different results</Tip>

### 4. Start Experimenting

Try modifying the text in the **CLIP Text Encoder**

![CLIP Text Encoder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg)

The `Positive` connection to the KSampler node represents positive prompts, while the `Negative` connection represents negative prompts

Here are some basic prompting principles for the SD1.5 model:

* Use English whenever possible
* Separate prompts with English commas `,`
* Use phrases rather than long sentences
* Use specific descriptions
* Use expressions like `(golden hour:1.2)` to increase the weight of specific keywords, making them more likely to appear in the image. `1.2` is the weight, `golden hour` is the keyword
* Use keywords like `masterpiece, best quality, 4k` to improve generation quality

Here are several prompt examples you can try, or use your own prompts for generation:

**1. Anime Style**

Positive prompts:

```
anime style, 1girl with long pink hair, cherry blossom background, studio ghibli aesthetic, soft lighting, intricate details

masterpiece, best quality, 4k
```

Negative prompts:

```
low quality, blurry, deformed hands, extra fingers
```

**2. Realistic Style**

Positive prompts:

```
(ultra realistic portrait:1.3), (elegant woman in crimson silk dress:1.2), 
full body, soft cinematic lighting, (golden hour:1.2), 
(fujifilm XT4:1.1), shallow depth of field, 
(skin texture details:1.3), (film grain:1.1), 
gentle wind flow, warm color grading, (perfect facial symmetry:1.3)
```

Negative prompts:

```
(deformed, cartoon, anime, doll, plastic skin, overexposed, blurry, extra fingers)
```

**3. Specific Artist Style**

Positive prompts:

```
fantasy elf, detailed character, glowing magic, vibrant colors, long flowing hair, elegant armor, ethereal beauty, mystical forest, magical aura, high detail, soft lighting, fantasy portrait, Artgerm style
```

Negative prompts:

```
blurry, low detail, cartoonish, unrealistic anatomy, out of focus, cluttered, flat lighting
```

## Text to Image Working Principles

The entire text-to-image process can be understood as a **reverse diffusion process**. The [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) we downloaded is a pre-trained model that can **generate target images from pure Gaussian noise**. We only need to input our prompts, and it can generate target images through denoising random noise.

```mermaid
graph LR
A[Pure Gaussian Noise] --> B[Iterative Denoising]
B --> C[Intermediate Latents]
C --> D[Final Generated Image]
E[Text Prompts] --> F[CLIP Encoder]
F --> G[Semantic Vectors]
G --> B
```

We need to understand two concepts:

1. **Latent Space:** Latent Space is an abstract data representation method in diffusion models. Converting images from pixel space to latent space reduces storage space and makes it easier to train diffusion models and reduce denoising complexity. It's like architects using blueprints (latent space) for design rather than designing directly on the building (pixel space), maintaining structural features while significantly reducing modification costs
2. **Pixel Space:** Pixel Space is the storage space for images, which is the final image we see, used to store pixel values.

If you want to learn more about diffusion models, you can read these papers:

* [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/pdf/2006.11239)
* [Denoising Diffusion Implicit Models (DDIM)](https://arxiv.org/pdf/2010.02502)
* [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752)

## ComfyUI Text to Image Workflow Node Explanation

![ComfyUI Text to Image Workflow Explanation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/text-image-workflow.jpg)

### A. Load Checkpoint Node

![Load Checkpoint](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_checkpoint.jpg)

This node is typically used to load the image generation model. A `checkpoint` usually contains three components: `MODEL (UNet)`, `CLIP`, and `VAE`

* `MODEL (UNet)`: The UNet model responsible for noise prediction and image generation during the diffusion process
* `CLIP`: The text encoder that converts our text prompts into vectors that the model can understand, as the model cannot directly understand text prompts
* `VAE`: The Variational AutoEncoder that converts images between pixel space and latent space, as diffusion models work in latent space while our images are in pixel space

### B. Empty Latent Image Node

![Empty Latent Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/lantent/empty_latent_image.jpg)

Defines a latent space that outputs to the KSampler node. The Empty Latent Image node constructs a **pure noise latent space**

You can think of its function as defining the canvas size, which determines the dimensions of our final generated image

### C. CLIP Text Encoder Node

![CLIP Text Encoder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg)

Used to encode prompts, which are your requirements for the image

* The `Positive` condition input connected to the KSampler node represents positive prompts (elements you want in the image)
* The `Negative` condition input connected to the KSampler node represents negative prompts (elements you don't want in the image)

The prompts are encoded into semantic vectors by the `CLIP` component from the `Load Checkpoint` node and output as conditions to the KSampler node

### D. KSampler Node

![KSampler](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/sampling/k_sampler.jpg)

The **KSampler** is the core of the entire workflow, where the entire noise denoising process occurs, ultimately outputting a latent space image

```mermaid
graph LR
A[Diffusion Model] --> B{KSampler}
C[Random Noise<br>Latent Space] --> B
D[CLIP Semantic Vectors] --> B
B --> E[Denoised Latent]
```

Here's an explanation of the KSampler node parameters:

| Parameter Name               | Description                        | Function                                                                                                    |
| ---------------------------- | ---------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| **model**                    | Diffusion model used for denoising | Determines the style and quality of generated images                                                        |
| **positive**                 | Positive prompt condition encoding | Guides generation to include specified elements                                                             |
| **negative**                 | Negative prompt condition encoding | Suppresses unwanted content                                                                                 |
| **latent\_image**            | Latent space image to be denoised  | Serves as the input carrier for noise initialization                                                        |
| **seed**                     | Random seed for noise generation   | Controls generation randomness                                                                              |
| **control\_after\_generate** | Seed control mode after generation | Determines seed variation pattern in batch generation                                                       |
| **steps**                    | Number of denoising iterations     | More steps mean finer details but longer processing time                                                    |
| **cfg**                      | Classifier-free guidance scale     | Controls prompt constraint strength (too high leads to overfitting)                                         |
| **sampler\_name**            | Sampling algorithm name            | Determines the mathematical method for denoising path                                                       |
| **scheduler**                | Scheduler type                     | Controls noise decay rate and step size allocation                                                          |
| **denoise**                  | Denoising strength coefficient     | Controls noise strength added to latent space, 0.0 preserves original input features, 1.0 is complete noise |

In the KSampler node, the latent space uses `seed` as an initialization parameter to construct random noise, and semantic vectors `Positive` and `Negative` are input as conditions to the diffusion model

Then, based on the number of denoising steps specified by the `steps` parameter, denoising is performed. Each denoising step uses the denoising strength coefficient specified by the `denoise` parameter to denoise the latent space and generate a new latent space image

### E. VAE Decode Node

![VAE Decode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/lantent/vae_decode.jpg)

Converts the latent space image output from the **KSampler** into a pixel space image

### F. Save Image Node

![Save Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/save_image.jpg)

Previews and saves the decoded image from latent space to the local `ComfyUI/output` folder

## Introduction to SD1.5 Model

**SD1.5 (Stable Diffusion 1.5)** is an AI image generation model developed by [Stability AI](https://stability.ai/). It's the foundational version of the Stable Diffusion series, trained on **512512** resolution images, making it particularly good at generating images at this resolution. With a size of about 4GB, it runs smoothly on **consumer-grade GPUs (e.g., 6GB VRAM)**. Currently, SD1.5 has a rich ecosystem, supporting various plugins (like ControlNet, LoRA) and optimization tools.
As a milestone model in AI art generation, SD1.5 remains the best entry-level choice thanks to its open-source nature, lightweight architecture, and rich ecosystem. Although newer versions like SDXL/SD3 have been released, its value for consumer-grade hardware remains unmatched.

### Basic Information

* **Release Date**: October 2022
* **Core Architecture**: Based on Latent Diffusion Model (LDM)
* **Training Data**: LAION-Aesthetics v2.5 dataset (approximately 590M training steps)
* **Open Source Features**: Fully open-source model/code/training data

### Advantages and Limitations

Model Advantages:

* Lightweight: Small size, only about 4GB, runs smoothly on consumer GPUs
* Low Entry Barrier: Supports a wide range of plugins and optimization tools
* Mature Ecosystem: Extensive plugin and tool support
* Fast Generation: Smooth operation on consumer GPUs

Model Limitations:

* Detail Handling: Hands/complex lighting prone to distortion
* Resolution Limits: Quality degrades for direct 1024x1024 generation
* Prompt Dependency: Requires precise English descriptions for control


# ComfyUI Image Upscale Workflow
Source: https://docs.comfy.org/tutorials/basic/upscale

This guide explains the concept of image upscaling in AI drawing and demonstrates how to implement an image upscaling workflow in ComfyUI

## What is Image Upscaling?

Image Upscaling is the process of converting low-resolution images to high-resolution using algorithms.
Unlike traditional interpolation methods, AI upscaling models (like ESRGAN) can intelligently reconstruct details while maintaining image quality.

For instance, the default SD1.5 model often struggles with large-size image generation.
To achieve high-resolution results,we typically generate smaller images first and then use upscaling techniques.

This article covers one of many upscaling methods in ComfyUI. In this tutorial, we'll guide you through:

1. Downloading and installing upscaling models
2. Performing basic image upscaling
3. Combining text-to-image workflows with upscaling

## Upscaling Workflow

### Model Installation

Required ESRGAN models download:

<Steps>
  <Step title="Visit OpenModelDB">
    Visit [OpenModelDB](https://openmodeldb.info/) to search and download upscaling models (e.g., RealESRGAN)

    ![openmodeldb](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_OpenModelDB.jpg)

    As shown:

    1. Filter models by image type using the category selector
    2. The model's magnification factor is indicated in the top-right corner (e.g., 2x in the screenshot)

    We'll use the [4x-ESRGAN](https://openmodeldb.info/models/4x-ESRGAN) model for this tutorial. Click the `Download` button on the model detail page.

    ![OpenModelDB\_download](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_OpenModelDB_download.jpg)
  </Step>

  <Step title="Save Model Files in Directory">
    Save the model file (.pth) in `ComfyUI/models/upscale_models` directory
  </Step>
</Steps>

### Workflow and Assets

Download and drag the following image into ComfyUI to load the basic upscaling workflow:
![Upscale workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_workflow.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

Use this image in smaller size as input:
![Upscale-input](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale-input.jpg)

### Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

![Upscale workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_simple_workflow.jpg)

1. Ensure `Load Upscale Model` loads `4x-ESRGAN.pth`
2. Upload the input image to the `Load Image` node
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

The core components are the `Load Upscale Model` and `Upscale Image (Using Model)` nodes, which receive an image input and upscale it using the selected model.

## Text-to-Image Combined Workflow

After mastering basic upscaling, we can combine it with the [text-to-image](/tutorials/basic/text-to-image) workflow. For text-to-image basics, refer to the [text-to-image tutorial](/tutorials/basic/text-to-image).

Download and drag this image into ComfyUI to load the combined workflow:
![Text-to-image upscale workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/esrgan_example.png)

This workflow connects the text-to-image output image directly to the upscaling nodes for final processing.

## Additional Tips

<Tip>
  Model characteristics:

  * **RealESRGAN**: General-purpose upscaling
  * **BSRGAN**: Excels with text and sharp edges
  * **SwinIR**: Preserves natural textures, ideal for landscapes
</Tip>

1. **Chained Upscaling**: Combine multiple upscale nodes (e.g., 2x  4x) for ultra-high magnification
2. **Hybrid Workflow**: Connect upscale nodes after generation for "generate+enhance" pipelines
3. **Comparative Testing**: Different models perform better on specific image types - test multiple options


# ComfyUI ControlNet Usage Example
Source: https://docs.comfy.org/tutorials/controlnet/controlnet

This guide will introduce you to the basic concepts of ControlNet and demonstrate how to generate corresponding images in ComfyUI

Achieving precise control over image creation in AI image generation cannot be done with just one click.
It typically requires numerous generation attempts to produce a satisfactory image. However, the emergence of **ControlNet** has effectively addressed this challenge.

ControlNet is a conditional control generation model based on diffusion models (such as Stable Diffusion),
first proposed by [Lvmin Zhang](https://lllyasviel.github.io/) and Maneesh Agrawala et al. in 2023 in the paper [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).

ControlNet models significantly enhance the controllability of image generation and the ability to reproduce details by introducing multimodal input conditions,
such as edge detection maps, depth maps, and pose keypoints.

These conditioning constraints make image generation more controllable, allowing multiple ControlNet models to be used simultaneously during the drawing process for better results.

Before ControlNet, we could only rely on the model to generate images repeatedly until we were satisfied with the results, which involved a lot of randomness.

![Images generated with random seeds in ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/generated_with_random_seed.jpg)

With the advent of ControlNet, we can control image generation by introducing additional conditions.
For example, we can use a simple sketch to guide the image generation process, producing images that closely align with our sketch.

![Sketch-controlled image generation in ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/scribble_example.jpg)

In this example, we will guide you through installing and using ControlNet models in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), and complete a sketch-controlled image generation example.

![ComfyUI ControlNet Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)

<Tip>
  The workflows for other types of ControlNet V1.1 models are similar to this example. You only need to select the appropriate model and upload the corresponding reference image based on your needs.
</Tip>

## ControlNet Image Preprocessing Information

Different types of ControlNet models typically require different types of reference images:

![Reference Images](https://github.com/Fannovel16/comfyui_controlnet_aux/blob/main/examples/CNAuxBanner.jpg?raw=true)

> Image source: [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

Since the current **Comfy Core** nodes do not include all types of **preprocessors**, in the actual examples in this documentation, we will provide pre-processed images.
However, in practical use, you may need to use custom nodes to preprocess images to meet the requirements of different ControlNet models. Below are some relevant custom nodes:

* [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
* [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## ComfyUI ControlNet Workflow Example Explanation

### 1. ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)

<Tip>
  Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
  This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.
</Tip>

Please download the image below, which we will use as input:

![ComfyUI Sketch Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_input.png)

### 2. Manual Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [dreamCreationVirtual3DECommerce\_v10.safetensors](https://civitai.com/api/download/models/731340?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)
* [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)

```
ComfyUI/
 models/
    checkpoints/
       dreamCreationVirtual3DECommerce_v10.safetensors
    vae/
       vae-ft-mse-840000-ema-pruned.safetensors
    controlnet/
        control_v11p_sd15_scribble_fp16.safetensors
```

<Note>
  In this example, you could also use the VAE model embedded in dreamCreationVirtual3DECommerce\_v10.safetensors, but we're following the model author's recommendation to use a separate VAE model.
</Note>

### 3. Step-by-Step Workflow Execution

![ComfyUI Workflow - ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_scribble.png)

1. Ensure that `Load Checkpoint` can load **dreamCreationVirtual3DECommerce\_v10.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**
3. Click `Upload` in the `Load Image` node to upload the input image provided earlier
4. Ensure that `Load ControlNet` can load **control\_v11p\_sd15\_scribble\_fp16.safetensors**
5. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Related Node Explanations

### Load ControlNet Node Explanation

![load controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_controlnet_model.jpg)

Models located in `ComfyUI\models\controlnet` will be detected by ComfyUI and can be loaded through this node.

### Apply ControlNet Node Explanation

![apply controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg)

This node accepts the ControlNet model loaded by `load controlnet` and generates corresponding control conditions based on the input image.

**Input Types**

| Parameter Name  | Function                                                                                                                                   |
| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| `positive`      | Positive conditioning                                                                                                                      |
| `negative`      | Negative conditioning                                                                                                                      |
| `control_net`   | The ControlNet model to be applied                                                                                                         |
| `image`         | Preprocessed image used as reference for ControlNet application                                                                            |
| `vae`           | VAE model input                                                                                                                            |
| `strength`      | Strength of ControlNet application; higher values increase ControlNet's influence on the generated image                                   |
| `start_percent` | Determines when to start applying ControlNet as a percentage; e.g., 0.2 means ControlNet guidance begins when 20% of diffusion is complete |
| `end_percent`   | Determines when to stop applying ControlNet as a percentage; e.g., 0.8 means ControlNet guidance stops when 80% of diffusion is complete   |

**Output Types**

| Parameter Name | Function                                           |
| -------------- | -------------------------------------------------- |
| `positive`     | Positive conditioning data processed by ControlNet |
| `negative`     | Negative conditioning data processed by ControlNet |

You can use chain connections to apply multiple ControlNet models, as shown in the image below. You can also refer to the [Mixing ControlNet Models](/tutorials/controlnet/mixing-controlnets.mdx) guide to learn more about combining multiple ControlNet models.
![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg)

<Note>
  You might see the `Apply ControlNet(Old)` node in some early workflows, which is an early version of the ControlNet node. It is currently deprecated and not visible by default in search and node lists.
  ![apply controlnet old](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/controlnet/apply_controlnet_old.jpg)
  To enable it, go to **Settings** --> **comfy** --> **Node** and enable the `Show deprecated nodes in search` option. However, it's recommended to use the new node.
</Note>

## Start Your Exploration

1. Try creating similar sketches, or even draw your own, and use ControlNet models to generate images to experience the benefits of ControlNet.
2. Adjust the `Control Strength` parameter in the Apply ControlNet node to control the influence of the ControlNet model on the generated image.
3. Visit the [ControlNet-v1-1\_fp16\_safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/tree/main) repository to download other types of ControlNet models and try using them to generate images.


# ComfyUI Depth ControlNet Usage Example
Source: https://docs.comfy.org/tutorials/controlnet/depth-controlnet

This guide will introduce you to the basic concepts of Depth ControlNet and demonstrate how to generate corresponding images in ComfyUI

## Introduction to Depth Maps and Depth ControlNet

A depth map is a special type of image that uses grayscale values to represent the distance between objects in a scene and the observer or camera. In a depth map, the grayscale value is inversely proportional to distance: brighter areas (closer to white) indicate objects that are closer, while darker areas (closer to black) indicate objects that are farther away.

![Depth Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png)

Depth ControlNet is a ControlNet model specifically trained to understand and utilize depth map information. It helps AI correctly interpret spatial relationships, ensuring that generated images conform to the spatial structure specified by the depth map, thereby enabling precise control over three-dimensional spatial layouts.

### Application Scenarios for Depth Maps with ControlNet

Depth maps have numerous applications in various scenarios:

1. **Portrait Scenes**: Control the spatial relationship between subjects and backgrounds, avoiding distortion in critical areas such as faces
2. **Landscape Scenes**: Control the hierarchical relationships between foreground, middle ground, and background
3. **Architectural Scenes**: Control the spatial structure and perspective relationships of buildings
4. **Product Showcase**: Control the separation and spatial positioning of products against their backgrounds

In this example, we will use a depth map to generate an architectural visualization scene.

## ComfyUI ControlNet Workflow Example Explanation

### 1. ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

![Depth Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_controlnet.png)

<Tip>
  Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
  This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.
</Tip>

Please download the image below, which we will use as input:

![Depth Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png)

### 2. Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [architecturerealmix\_v11.safetensors](https://civitai.com/api/download/models/431755?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [control\_v11f1p\_sd15\_depth\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors?download=true)

```
ComfyUI/
 models/
    checkpoints/
       architecturerealmix_v11.safetensors
    controlnet/
        control_v11f1p_sd15_depth_fp16.safetensors
```

### 3. Step-by-Step Workflow Execution

![ComfyUI Workflow - Depth ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth.jpg)

1. Ensure that `Load Checkpoint` can load **architecturerealmix\_v11.safetensors**
2. Ensure that `Load ControlNet` can load **control\_v11f1p\_sd15\_depth\_fp16.safetensors**
3. Click `Upload` in the `Load Image` node to upload the depth image provided earlier
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Combining Depth Control with Other Techniques

Based on different creative needs, you can combine Depth ControlNet with other types of ControlNet to achieve better results:

1. **Depth + Lineart**: Maintain spatial relationships while reinforcing outlines, suitable for architecture, products, and character design
2. **Depth + Pose**: Control character posture while maintaining correct spatial relationships, suitable for character scenes

For more information on using multiple ControlNet models together, please refer to the [Mixing ControlNet](/tutorials/controlnet/mixing-controlnets.mdx) example.


# ComfyUI Depth T2I Adapter Usage Example
Source: https://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter

This guide will introduce you to the basic concepts of Depth T2I Adapter and demonstrate how to generate corresponding images in ComfyUI

## Introduction to T2I Adapter

[T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter) is a lightweight adapter developed by [Tencent ARC Lab](https://github.com/TencentARC) designed to enhance the structural, color, and style control capabilities of text-to-image generation models (such as Stable Diffusion).
It works by aligning external conditions (such as edge detection maps, depth maps, sketches, or color reference images) with the model's internal features, achieving high-precision control without modifying the original model structure. With only about 77M parameters (approximately 300MB in size), its inference speed is about 3 times faster than [ControlNet](https://github.com/lllyasviel/ControlNet-v1-1-nightly), and it supports multiple condition combinations (such as sketch + color grid). Application scenarios include line art to image conversion, color style transfer, multi-element scene generation, and more.

### Comparison Between T2I Adapter and ControlNet

Although their functions are similar, there are notable differences in implementation and application:

1. **Lightweight Design**: T2I Adapter has fewer parameters and a smaller memory footprint
2. **Inference Speed**: T2I Adapter is typically about 3 times faster than ControlNet
3. **Control Precision**: ControlNet offers more precise control in certain scenarios, while T2I Adapter is more suitable for lightweight control
4. **Multi-condition Combination**: T2I Adapter shows more significant resource advantages when combining multiple conditions

### Main Types of T2I Adapter

T2I Adapter provides various types to control different aspects:

* **Depth**: Controls the spatial structure and depth relationships in images
* **Line Art (Canny/Sketch)**: Controls image edges and lines
* **Keypose**: Controls character poses and actions
* **Segmentation (Seg)**: Controls scene layout through semantic segmentation
* **Color**: Controls the overall color scheme of images

In ComfyUI, using T2I Adapter is similar to [ControlNet](/tutorials/controlnet/controlnet.mdx) in terms of interface and workflow. In this example, we will demonstrate how to use a depth T2I Adapter to control an interior scene.

![ComfyUI Depth T2I Adapter Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)

## Value of Depth T2I Adapter Applications

Depth maps have several important applications in image generation:

1. **Spatial Layout Control**: Accurately describes three-dimensional spatial structures, suitable for interior design and architectural visualization
2. **Object Positioning**: Controls the relative position and size of objects in a scene, suitable for product showcases and scene construction
3. **Perspective Relationships**: Maintains reasonable perspective and proportions, suitable for landscape and urban scene generation
4. **Light and Shadow Layout**: Natural light and shadow distribution based on depth information, enhancing realism

We will use interior design as an example to demonstrate how to use the depth T2I Adapter, but these techniques are applicable to other scenarios as well.

## ComfyUI Depth T2I Adapter Workflow Example Explanation

### 1. Depth T2I Adapter Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

![ComfyUI Workflow - Depth T2I Adapter](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)

<Tip>
  Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
  This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.
</Tip>

Please download the image below, which we will use as input:

![ComfyUI Interior Depth Map](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png)

### 2. Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [interiordesignsuperm\_v2.safetensors](https://civitai.com/api/download/models/93152?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [t2iadapter\_depth\_sd15v2.pth](https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd15v2.pth?download=true)

```
ComfyUI/
 models/
    checkpoints/
       interiordesignsuperm_v2.safetensors
    controlnet/
        t2iadapter_depth_sd15v2.pth
```

### 3. Step-by-Step Workflow Execution

![ComfyUI Workflow - Depth T2I Adapter Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg)

1. Ensure that `Load Checkpoint` can load **interiordesignsuperm\_v2.safetensors**
2. Ensure that `Load ControlNet` can load **t2iadapter\_depth\_sd15v2.pth**
3. Click `Upload` in the `Load Image` node to upload the input image provided earlier
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## General Tips for Using T2I Adapter

### Input Image Quality Optimization

Regardless of the application scenario, high-quality input images are key to successfully using T2I Adapter:

1. **Moderate Contrast**: Control images (such as depth maps, line art) should have clear contrast, but not excessively extreme
2. **Clear Boundaries**: Ensure that major structures and element boundaries are clearly distinguishable in the control image
3. **Noise Control**: Try to avoid excessive noise in control images, especially for depth maps and line art
4. **Reasonable Layout**: Control images should have a reasonable spatial layout and element distribution

## Characteristics of T2I Adapter Usage

One major advantage of T2I Adapter is its ability to easily combine multiple conditions for complex control effects:

1. **Depth + Edge**: Control spatial layout while maintaining clear structural edges, suitable for architecture and interior design
2. **Line Art + Color**: Control shapes while specifying color schemes, suitable for character design and illustrations
3. **Pose + Segmentation**: Control character actions while defining scene areas, suitable for complex narrative scenes

Mixing different T2I Adapters, or combining them with other control methods (such as ControlNet, regional prompts, etc.), can further expand creative possibilities. To achieve mixing, simply chain multiple `Apply ControlNet` nodes together in the same way as described in [Mixing ControlNet](/tutorials/controlnet/mixing-controlnets.mdx).


# ComfyUI Mixing ControlNet Examples
Source: https://docs.comfy.org/tutorials/controlnet/mixing-controlnets

In this example, we will demonstrate how to mix multiple ControlNets and learn to use multiple ControlNet models to control image generation

In AI image generation, a single control condition often fails to meet the requirements of complex scenes. Mixing multiple ControlNets allows you to control different regions or aspects of an image simultaneously, achieving more precise control over image generation.

In certain scenarios, mixing ControlNets can leverage the characteristics of different control conditions to achieve more refined conditional control:

1. **Scene Complexity**: Complex scenes require multiple control conditions working together
2. **Fine-grained Control**: By adjusting the strength parameter of each ControlNet, you can precisely control the degree of influence for each part
3. **Complementary Effects**: Different types of ControlNets can complement each other, compensating for the limitations of single controls
4. **Creative Expression**: Combining different controls can produce unique creative effects

### How to Mix ControlNets

When mixing multiple ControlNets, each ControlNet influences the image generation process according to its applied area. ComfyUI enables multiple ControlNet conditions to be applied sequentially in a layered manner through chain connections in the `Apply ControlNet` node:

![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg)

## ComfyUI ControlNet Regional Division Mixing Example

In this example, we will use a combination of **Pose ControlNet** and **Scribble ControlNet** to generate a scene containing multiple elements: a character on the left controlled by Pose ControlNet and a cat on a scooter on the right controlled by Scribble ControlNet.

### 1. ControlNet Mixing Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Mixing ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets.png)

<Tip>
  This workflow image contains Metadata, and can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`. The system will automatically detect and prompt to download the required models.
</Tip>

Input pose image (controls the character pose on the left):

![ComfyUI Workflow - Mixing ControlNet Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input.png)

Input scribble image (controls the cat and scooter on the right):

![ComfyUI Workflow - Mixing ControlNet Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input_scribble.png)

### 2. Manual Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [awpainting\_v14.safetensors](https://civitai.com/api/download/models/624939?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)
* [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```
ComfyUI/
 models/
    checkpoints/
       awpainting_v14.safetensors
    controlnet/
       control_v11p_sd15_scribble_fp16.safetensors
       control_v11p_sd15_openpose_fp16.safetensors
    vae/
       vae-ft-mse-840000-ema-pruned.safetensors
```

### 3. Step-by-Step Workflow Execution

![ComfyUI Workflow - Mixing ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg)

Follow these steps according to the numbered markers in the image:

1. Ensure that `Load Checkpoint` can load **awpainting\_v14.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**

First ControlNet group using the Openpose model:
3\. Ensure that `Load ControlNet Model` loads **control\_v11p\_sd15\_openpose\_fp16.safetensors**
4\. Click `Upload` in the `Load Image` node to upload the pose image provided earlier

Second ControlNet group using the Scribble model:
5\. Ensure that `Load ControlNet Model` loads **control\_v11p\_sd15\_scribble\_fp16.safetensors**
6\. Click `Upload` in the `Load Image` node to upload the scribble image provided earlier
7\. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Workflow Explanation

#### Strength Balance

When controlling different regions of an image, balancing the strength parameters is particularly important:

* If the ControlNet strength for one region is significantly higher than another, it may cause that region's control effect to overpower and suppress the other region
* It's recommended to set similar strength values for ControlNets controlling different regions, for example, both set to 1.0

#### Prompt Techniques

For regional division mixing, the prompt needs to include descriptions of both regions:

```
"A woman in red dress, a cat riding a scooter, detailed background, high quality"
```

Such a prompt covers both the character and the cat on the scooter, ensuring the model pays attention to both control regions.

## Multi-dimensional Control Applications for a Single Subject

In addition to the regional division mixing shown in this example, another common mixing approach is to apply multi-dimensional control to the same subject. For example:

* **Pose + Depth**: Control character posture and spatial sense
* **Pose + Canny**: Control character posture and edge details
* **Pose + Reference**: Control character posture while referencing a specific style

In this type of application, reference images for multiple ControlNets should be aligned to the same subject, and their strengths should be adjusted to ensure proper balance.

By combining different types of ControlNets and specifying their control regions, you can achieve precise control over elements in your image.


# ComfyUI Pose ControlNet Usage Example
Source: https://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass

This guide will introduce you to the basic concepts of Pose ControlNet, and demonstrate how to generate large-sized images in ComfyUI using a two-pass generation approach

## Introduction to OpenPose

[OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) is an open-source real-time multi-person pose estimation system developed by Carnegie Mellon University (CMU), representing a significant breakthrough in the field of computer vision. The system can simultaneously detect multiple people in an image, capturing:

* **Body skeleton**: 18 keypoints, including head, shoulders, elbows, wrists, hips, knees, and ankles
* **Facial expressions**: 70 facial keypoints for capturing micro-expressions and facial contours
* **Hand details**: 21 hand keypoints for precisely expressing finger positions and gestures
* **Foot posture**: 6 foot keypoints, recording standing postures and movement details

![OpenPose Example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/openpose_example.jpg)

In AI image generation, skeleton structure maps generated by OpenPose serve as conditional inputs for ControlNet, enabling precise control over the posture, actions, and expressions of generated characters. This allows us to generate realistic human figures with expected poses and actions, greatly improving the controllability and practical value of AI-generated content.
Particularly for early Stable Diffusion 1.5 series models, skeletal maps generated by OpenPose can effectively prevent issues with distorted character actions, limbs, and expressions.

## ComfyUI 2-Pass Pose ControlNet Usage Example

### 1. Pose ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

![ComfyUI Workflow - Pose ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass.png)

<Tip>
  Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
  This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.
</Tip>

Please download the image below, which we will use as input:

![ComfyUI Pose Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass_input.png)

### 2. Manual Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
* [majicmixRealistic\_v7.safetensors](https://civitai.com/api/download/models/176425?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16)
* [japaneseStyleRealistic\_v20.safetensors](https://civitai.com/api/download/models/85426?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```
ComfyUI/
 models/
    checkpoints/
       majicmixRealistic_v7.safetensors
       japaneseStyleRealistic_v20.safetensors
    vae/
       vae-ft-mse-840000-ema-pruned.safetensors
    controlnet/
        control_v11p_sd15_openpose_fp16.safetensors
```

### 3. Step-by-Step Workflow Execution

![ComfyUI Workflow - Pose ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg)

Follow these steps according to the numbered markers in the image:

1. Ensure that `Load Checkpoint` can load **majicmixRealistic\_v7.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**
3. Ensure that `Load ControlNet Model` can load **control\_v11p\_sd15\_openpose\_fp16.safetensors**
4. Click the select button in the `Load Image` node to upload the pose input image provided earlier, or use your own OpenPose skeleton map
5. Ensure that `Load Checkpoint` can load **japaneseStyleRealistic\_v20.safetensors**
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Explanation of the Pose ControlNet 2-Pass Workflow

This workflow uses a two-pass image generation approach, dividing the image creation process into two phases:

### First Phase: Basic Pose Image Generation

In the first phase, the **majicmixRealistic\_v7** model is combined with Pose ControlNet to generate an initial character pose image:

1. First, load the majicmixRealistic\_v7 model via the `Load Checkpoint` node
2. Load the pose control model through the `Load ControlNet Model` node
3. The input pose image is fed into the `Apply ControlNet` node and combined with positive and negative prompt conditions
4. The first `KSampler` node (typically using 20-30 steps) generates a basic character pose image
5. The pixel-space image for the first phase is obtained through `VAE Decode`

This phase primarily focuses on correct character posture, pose, and basic structure, ensuring that the generated character conforms to the input skeletal pose.

### Second Phase: Style Optimization and Detail Enhancement

In the second phase, the output image from the first phase is used as a reference, with the **japaneseStyleRealistic\_v20** model performing stylization and detail enhancement:

1. The image generated in the first phase creates a larger resolution latent space through the `Upscale latent` node
2. The second `Load Checkpoint` loads the japaneseStyleRealistic\_v20 model, which focuses on details and style
3. The second `KSampler` node uses a lower `denoise` strength (typically 0.4-0.6) for refinement, preserving the basic structure from the first phase
4. Finally, a higher quality, larger resolution image is output through the second `VAE Decode` and `Save Image` nodes

This phase primarily focuses on style consistency, detail richness, and enhancing overall image quality.

## Advantages of 2-Pass Image Generation

Compared to single-pass generation, the two-pass image generation method offers the following advantages:

1. **Higher Resolution**: Two-pass processing can generate high-resolution images beyond the capabilities of single-pass generation
2. **Style Blending**: Can combine advantages of different models, such as using a realistic model in the first phase and a stylized model in the second phase
3. **Better Details**: The second phase can focus on optimizing details without having to worry about overall structure
4. **Precise Control**: Once pose control is completed in the first phase, the second phase can focus on refining style and details
5. **Reduced GPU Load**: Generating in two passes allows for high-quality large images with limited GPU resources

<Tip>
  To learn more about techniques for mixing multiple ControlNets, please refer to the [Mixing ControlNet Models](/tutorials/controlnet/mixing-controlnets.mdx) tutorial.
</Tip>


# ComfyUI Flux.1 ControlNet Examples
Source: https://docs.comfy.org/tutorials/flux/flux-1-controlnet

This guide will demonstrate workflow examples using Flux.1 ControlNet.

![Flux.1 Canny Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-canny-controlnet.png)
![Flux.1 Depth Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-depth-controlnet.png)

## FLUX.1 ControlNet Model Introduction

FLUX.1 Canny and Depth are two powerful models from the [FLUX.1 Tools](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/). This toolkit is designed to add control and guidance capabilities to FLUX.1, enabling users to modify and recreate real or generated images.

**FLUX.1-Depth-dev** and **FLUX.1-Canny-dev** are both 12B parameter Rectified Flow Transformer models that can generate images based on text descriptions while maintaining the structural features of the input image.
The Depth version maintains the spatial structure of the source image through depth map extraction techniques, while the Canny version uses edge detection techniques to preserve the structural features of the source image, allowing users to choose the appropriate control method based on different needs.

Both models have the following features:

* Top-tier output quality and detail representation
* Excellent prompt following ability while maintaining consistency with the original image
* Trained using guided distillation techniques for improved efficiency
* Open weights for the research community
* API interfaces (pro version) and open-source weights (dev version)

Additionally, Black Forest Labs also provides **FLUX.1-Depth-dev-lora** and **FLUX.1-Canny-dev-lora** adapter versions extracted from the complete models.
These can be applied to the FLUX.1 \[dev] base model to provide similar functionality with smaller file size, especially suitable for resource-constrained environments.

We will use the full version of **FLUX.1-Canny-dev** and **FLUX.1-Depth-dev-lora** to complete the workflow examples.

<Tip>
  All workflow images's Metadata contains the corresponding model download information. You can load the workflows by:

  * Dragging them directly into ComfyUI
  * Or using the menu `Workflows` -> `Openctrl+o`

  If you're not using the Desktop Version or some models can't be downloaded automatically, please refer to the manual installation sections to save the model files to the corresponding folder.

  For image preprocessors, you can use the following custom nodes to complete image preprocessing. In this example, we will provide processed images as input.

  * [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
  * [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)
</Tip>

## FLUX.1-Canny-dev Complete Version Workflow

### 1. Workflow and Asset

Please download the workflow image below and drag it into ComfyUI to load the workflow

![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev.png)

Please download the image below, which we will use as the input image

![ComfyUI Flux.1 Canny Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev-input.png)

### 2. Manual Models Installation

<Note>
  If you have previously used the [complete version of Flux related workflows](/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-canny-dev.safetensors** model file.
  Since you need to first agree to the terms of [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev), please visit the [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev) page and make sure you have agreed to the corresponding terms as shown in the image below.
  ![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux1_canny_dev_agreement.jpg)
</Note>

Complete model list:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-canny-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/resolve/main/flux1-canny-dev.safetensors?download=true) (Please ensure you have agreed to the corresponding repo's terms)

File storage location:

```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       t5xxl_fp16.safetensors
    vae/
       ae.safetensors
    diffusion_models/
        flux1-canny-dev.safetensors
```

### 3. Step-by-Step Workflow Execution

![ComfyUI Flux.1 Canny Controlnet Step Process](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg)

1. Make sure `ae.safetensors` is loaded in the `Load VAE` node
2. Make sure `flux1-canny-dev.safetensors` is loaded in the `Load Diffusion Model` node
3. Make sure the following models are loaded in the `DualCLIPLoader` node:
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
4. Upload the provided input image in the `Load Image` node
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### 4. Start Your Experimentation

Try using the [FLUX.1-Depth-dev](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev) model to complete the Depth version of the workflow

You can use the image below as input
![ComfyUI Indoor Depth Map](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png)

Or use the following custom nodes to complete image preprocessing:

* [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
* [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## FLUX.1-Depth-dev-lora Workflow

The LoRA version workflow builds on the complete version by adding the LoRA model. Compared to the [complete version of the Flux workflow](/tutorials/flux/flux-1-text-to-image), it adds nodes for loading and using the corresponding LoRA model.

### 1. Workflow and Asset

Please download the workflow image below and drag it into ComfyUI to load the workflow

![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora.png)

Please download the image below, which we will use as the input image

![ComfyUI Flux.1 Depth Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora-input.png)

### 2. Manual Model Download

<Tip>
  If you have previously used the [complete version of Flux related workflows](/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-depth-dev-lora.safetensors** model file.
</Tip>

Complete model list:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors?download=true)
* [flux1-depth-dev-lora.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora/resolve/main/flux1-depth-dev-lora.safetensors?download=true)

File storage location:

```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       t5xxl_fp16.safetensors
    vae/
       ae.safetensors
    diffusion_models/
       flux1-dev.safetensors
    loras/
        flux1-depth-dev-lora.safetensors
```

### 3. Step-by-Step Workflow Execution

![ComfyUI Flux.1 Depth Controlnet Step Process](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg)

1. Make sure `flux1-dev.safetensors` is loaded in the `Load Diffusion Model` node
2. Make sure `flux1-depth-dev-lora.safetensors` is loaded in the `LoraLoaderModelOnly` node
3. Make sure the following models are loaded in the `DualCLIPLoader` node:
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
4. Upload the provided input image in the `Load Image` node
5. Make sure `ae.safetensors` is loaded in the `Load VAE` node
6. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### 4. Start Your Experimentation

Try using the [FLUX.1-Canny-dev-lora](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora) model to complete the Canny version of the workflow

Use [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet) or [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to complete image preprocessing

## Community Versions of Flux Controlnets

XLab and InstantX + Shakker Labs have released Controlnets for Flux.

**InstantX:**

* [FLUX.1-dev-Controlnet-Canny](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Canny/blob/main/diffusion_pytorch_model.safetensors)
* [FLUX.1-dev-ControlNet-Depth](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Depth/blob/main/diffusion_pytorch_model.safetensors)
* [FLUX.1-dev-ControlNet-Union-Pro](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/blob/main/diffusion_pytorch_model.safetensors)

**XLab**: [flux-controlnet-collections](https://huggingface.co/XLabs-AI/flux-controlnet-collections)

Place these files in the `ComfyUI/models/controlnet` directory.

You can visit [Flux Controlnet Example](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/flux_controlnet_example.png) to get the corresponding workflow image, and use the image from [here](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/girl_in_field.png) as the input image.


# ComfyUI Flux.1 fill dev Example
Source: https://docs.comfy.org/tutorials/flux/flux-1-fill-dev

This guide demonstrates how to use Flux.1 fill dev to create Inpainting and Outpainting workflows.

![Flux.1 fill dev](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-fill-dev-demo.jpeg)

## Introduction to Flux.1 fill dev Model

Flux.1 fill dev is one of the core tools in the [FLUX.1 Tools suite](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/), specifically designed for image inpainting and outpainting.

Key features of Flux.1 fill dev:

* Powerful image inpainting and outpainting capabilities, with results second only to the commercial version FLUX.1 Fill \[pro].
* Excellent prompt understanding and following ability, precisely capturing user intent while maintaining high consistency with the original image.
* Advanced guided distillation training technology, making the model more efficient while maintaining high-quality output.
* Friendly licensing terms, with generated outputs usable for personal, scientific, and commercial purposes, please refer to the [FLUX.1 \[dev\] non-commercial license](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) for details.

Open Source Repository: [FLUX.1 \[dev\]](https://huggingface.co/black-forest-labs/FLUX.1-dev)

This guide will demonstrate inpainting and outpainting workflows based on the Flux.1 fill dev model.
If you're not familiar with inpainting and outpainting workflows, you can refer to [ComfyUI Layout Inpainting Example](/tutorials/basic/inpaint) and [ComfyUI Image Extension Example](/tutorials/basic/outpaint) for some related explanations.

## Flux.1 Fill dev and related models installation

Before we begin, let's complete the installation of the Flux.1 Fill dev model files. The inpainting and outpainting workflows will use exactly the same model files.
If you've previously used the full version of the [Flux.1 Text-to-Image workflow](/tutorials/flux/flux-1-text-to-image),
then you only need to download the **flux1-fill-dev.safetensors** model file in this section.

However, since downloading the corresponding model requires agreeing to the corresponding usage agreement, please visit the [black-forest-labs/FLUX.1-Fill-dev](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev) page and make sure you have agreed to the corresponding agreement as shown in the image below.
![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux1_fill_dev_agreement.jpg)

Complete model list:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-fill-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/resolve/main/flux1-fill-dev.safetensors?download=true)

File storage location:

```
ComfyUI/
 models/
    text_encoders/
        clip_l.safetensors
        t5xxl_fp16.safetensors
    vae/
        ae.safetensors
    diffusion_models/
         flux1-fill-dev.safetensors
```

## Flux.1 Fill dev inpainting workflow

### 1. Inpainting workflow and asset

Please download the image below and drag it into ComfyUI to load the corresponding workflow
![ComfyUI Flux.1 inpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint.png)

Please download the image below, we will use it as the input image
![ComfyUI Flux.1 inpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input.png)

<Note>
  The corresponding image already contains an alpha channel, so you don't need to draw a mask separately.
  If you want to draw your own mask, please [click here](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input_original.png) to get the image without a mask, and refer to the MaskEditor usage section in the [ComfyUI Layout Inpainting Example](/tutorials/basic/inpaint#using-the-mask-editor) to learn how to draw a mask in the `Load Image` node.
</Note>

### 2. Steps to run the workflow

![ComfyUI Flux.1 Fill dev Inpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_inpaint.jpg)

1. Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.
2. Ensure the `DualCLIPLoader` node has the following models loaded:
   * clip\_name1: `t5xxl_fp16.safetensors`
   * clip\_name2: `clip_l.safetensors`
3. Ensure the `Load VAE` node has `ae.safetensors` loaded.
4. Upload the input image provided in the document to the `Load Image` node; if you're using the version without a mask, remember to complete the mask drawing using the mask editor
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Flux.1 Fill dev Outpainting Workflow

### 1. Outpainting workflow and asset

Please download the image below and drag it into ComfyUI to load the corresponding workflow
![ComfyUI Flux.1 outpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint.png)

Please download the image below, we will use it as the input image
![ComfyUI Flux.1 outpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint_input.png)

### 2. Steps to run the workflow

![ComfyUI Flux.1 Fill dev Outpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_outpaint.jpg)

1. Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.
2. Ensure the `DualCLIPLoader` node has the following models loaded:
   * clip\_name1: `t5xxl_fp16.safetensors`
   * clip\_name2: `clip_l.safetensors`
3. Ensure the `Load VAE` node has `ae.safetensors` loaded.
4. Upload the input image provided in the document to the `Load Image` node
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow


# ComfyUI Flux.1 Text-to-Image Workflow Example
Source: https://docs.comfy.org/tutorials/flux/flux-1-text-to-image

This guide provides a brief introduction to the Flux.1 model and guides you through using the Flux.1 model for text-to-image generation with examples including the full version and the FP8 Checkpoint version.

![Flux](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_example.png)
Flux is one of the largest open-source text-to-image generation models, with 12B parameters and an original file size of approximately 23GB. It was developed by [Black Forest Labs](https://blackforestlabs.ai/), a team founded by former Stable Diffusion team members.
Flux is known for its excellent image quality and flexibility, capable of generating high-quality, diverse images.

Currently, the Flux.1 model has several main versions:

* **Flux.1 Pro:** The best performing model, closed-source, only available through API calls.
* **[Flux.1 \[dev\]](https://huggingface.co/black-forest-labs/FLUX.1-dev)** Open-source but limited to non-commercial use, distilled from the Pro version, with performance close to the Pro version.
* **[Flux.1 \[schnell\]](https://huggingface.co/black-forest-labs/FLUX.1-schnell)** Uses the Apache2.0 license, requires only 4 steps to generate images, suitable for low-spec hardware.

**Flux.1 Model Features**

* **Hybrid Architecture:** Combines the advantages of Transformer networks and diffusion models, effectively integrating text and image information, improving the alignment accuracy between generated images and prompts, with excellent fidelity to complex prompts.
* **Parameter Scale:** Flux has 12B parameters, capturing more complex pattern relationships and generating more realistic, diverse images.
* **Supports Multiple Styles:** Supports diverse styles, with excellent performance for various types of images.

In this example, we'll introduce text-to-image examples using both Flux.1 Dev and Flux.1 Schnell versions, including the full version model and the simplified FP8 Checkpoint version.

* **Flux Full Version:** Best performance, but requires larger VRAM resources and installation of multiple model files.
* **Flux FP8 Checkpoint:** Requires only one fp8 version of the model, but quality is slightly reduced compared to the full version.

<Tip>
  All workflow images's Metadata contains the corresponding model download information. You can load the workflows by:

  * Dragging them directly into ComfyUI
  * Or using the menu `Workflows` -> `Openctrl+o`

  If you're not using the Desktop Version or some models can't be downloaded automatically, please refer to the manual installation sections to save the model files to the corresponding folder.
  Make sure your ComfyUI is updated to the latest version before starting.
</Tip>

## Flux.1 Full Version Text-to-Image Example

<Note>
  If you can't download models from [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), make sure you've logged into Huggingface and agreed to the corresponding repository's license agreement.
  ![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_agreement.jpg)
</Note>

### Flux.1 Dev

#### 1. Workflow File

Please download the image below and drag it into ComfyUI to load the workflow.
![Flux Dev Original Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_t5fp16.png)

#### 2. Manual Model Installation

<Note>
  * The `flux1-dev.safetensors` file requires agreeing to the [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) agreement before downloading via browser.
  * If your VRAM is low, you can try using [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true) to replace the `t5xxl_fp16.safetensors` file.
</Note>

Please download the following model files:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true) Recommended when your VRAM is greater than 32GB.
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors)

Storage location:

```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       t5xxl_fp16.safetensors
    vae/
       ae.safetensors
    diffusion_models/
        flux1-dev.safetensors
```

#### 3. Steps to Run the Workflow

Please refer to the image below to ensure all model files are loaded correctly

![ComfyUI Flux Dev Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg)

1. Ensure the `DualCLIPLoader` node has the following models loaded:
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
2. Ensure the `Load Diffusion Model` node has `flux1-dev.safetensors` loaded
3. Make sure the `Load VAE` node has `ae.safetensors` loaded
4. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

<Tip>
  Thanks to Flux's excellent prompt following capability, we don't need any negative prompts
</Tip>

### Flux.1 Schnell

#### 1. Workflow File

Please download the image below and drag it into ComfyUI to load the workflow.

![Flux Schnell Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_t5fp8.png)

#### 2. Manual Models Installation

<Note>
  In this workflow, only two model files are different from the Flux1 Dev version workflow. For t5xxl, you can still use the fp16 version for better results.

  * **t5xxl\_fp16.safetensors** -> **t5xxl\_fp8.safetensors**
  * **flux1-dev.safetensors** -> **flux1-schnell.safetensors**
</Note>

Complete model file list:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-schnell.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors)

File storage location:

```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       t5xxl_fp8_e4m3fn.safetensors
    vae/
       ae.safetensors
    diffusion_models/
        flux1-schnell.safetensors
```

#### 3. Steps to Run the Workflow

![Flux Schnell Version Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg)

1. Ensure the `DualCLIPLoader` node has the following models loaded:
   * clip\_name1: t5xxl\_fp8\_e4m3fn.safetensors
   * clip\_name2: clip\_l.safetensors
2. Ensure the `Load Diffusion Model` node has `flux1-schnell.safetensors` loaded
3. Ensure the `Load VAE` node has `ae.safetensors` loaded
4. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Flux.1 FP8 Checkpoint Version Text-to-Image Example

The fp8 version is a quantized version of the original Flux.1 fp16 version.
To some extent, the quality of this version will be lower than that of the fp16 version,
but it also requires less VRAM, and you only need to install one model file to try running it.

### Flux.1 Dev

Please download the image below and drag it into ComfyUI to load the workflow.

![Flux Dev fp8 Checkpoint Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_fp8.png)

Please download [flux1-dev-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory.

Ensure that the corresponding `Load Checkpoint` node loads `flux1-dev-fp8.safetensors`, and you can try to run the workflow.

### Flux.1 Schnell

Please download the image below and drag it into ComfyUI to load the workflow.

![Flux Schnell fp8 Checkpoint Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_fp8.png)

Please download [flux1-schnell-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-schnell/resolve/main/flux1-schnell-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory.

Ensure that the corresponding `Load Checkpoint` node loads `flux1-schnell-fp8.safetensors`, and you can try to run the workflow.


# ComfyUI Hunyuan Video Examples
Source: https://docs.comfy.org/tutorials/video/hunyuan-video

This guide shows how to use Hunyuan Text-to-Video and Image-to-Video workflows in ComfyUI

<video controls className="w-full aspect-video" src="https://github.com/user-attachments/assets/442afb73-3092-454f-bc46-02361c285930" />

Hunyuan Video series is developed and open-sourced by [Tencent](https://huggingface.co/tencent), featuring a hybrid architecture that supports both [Text-to-Video](https://github.com/Tencent/HunyuanVideo) and [Image-to-Video](https://github.com/Tencent/HunyuanVideo-I2V) generation with a parameter scale of 13B.

Technical features:

* **Core Architecture:** Uses a DiT (Diffusion Transformer) architecture similar to Sora, effectively fusing text, image, and motion information to improve consistency, quality, and alignment between generated video frames. A unified full-attention mechanism enables multi-view camera transitions while ensuring subject consistency.
* **3D VAE:** The custom 3D VAE compresses videos into a compact latent space, making image-to-video generation more efficient.
* **Superior Image-Video-Text Alignment:** Utilizing MLLM text encoders that excel in both image and video generation, better following text instructions, capturing details, and performing complex reasoning.

You can learn more through the official repositories: [Hunyuan Video](https://github.com/Tencent/HunyuanVideo) and [Hunyuan Video-I2V](https://github.com/Tencent/HunyuanVideo-I2V).

This guide will walk you through setting up both **Text-to-Video** and **Image-to-Video** workflows in ComfyUI.

<Tip>
  The workflow images in this tutorial contain metadata with model download information.

  Simply drag them into ComfyUI or use the menu `Workflows` -> `Open (ctrl+o)` to load the corresponding workflow, which will prompt you to download the required models.

  Alternatively, this guide provides direct model links if automatic downloads fail or you are not using the Desktop version. All models are available [here](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/tree/main/split_files) for download.
</Tip>

## Shared Models for All Workflows

The following models are used in both Text-to-Video and Image-to-Video workflows. Please download and save them to the specified directories:

* [clip\_l.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors?download=true)
* [llava\_llama3\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors?download=true)
* [hunyuan\_video\_vae\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors?download=true)

Storage location:

```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       llava_llama3_fp8_scaled.safetensors
    vae/
       hunyuan_video_vae_bf16.safetensors
```

## Hunyuan Text-to-Video Workflow

Hunyuan Text-to-Video was open-sourced in December 2024, supporting 5-second short video generation through natural language descriptions in both Chinese and English.

### 1. Workflow

Download the image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Hunyuan Text-to-Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/t2v/kitchen.webp)

### 2. Manual Models Installation

Download [hunyuan\_video\_t2v\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_t2v_720p_bf16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models` folder.

Ensure you have all these model files in the correct locations:

```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors                       // Shared model
       llava_llama3_fp8_scaled.safetensors      // Shared model
    vae/
       hunyuan_video_vae_bf16.safetensors       // Shared model
    diffusion_models/
        hunyuan_video_t2v_720p_bf16.safetensors  // T2V model
```

### 3. Steps to Run the Workflow

![ComfyUI Hunyuan Video T2V Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg)

1. Ensure the `DualCLIPLoader` node has loaded these models:
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_t2v_720p_bf16.safetensors`
3. Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

<Tip>
  When the `length` parameter in the `EmptyHunyuanLatentVideo` node is set to 1, the model can generate a static image.
</Tip>

## Hunyuan Image-to-Video Workflow

Hunyuan Image-to-Video model was open-sourced on March 6, 2025, based on the HunyuanVideo framework. It transforms static images into smooth, high-quality videos and also provides LoRA training code to customize special video effects like hair growth, object transformation, etc.

Currently, the Hunyuan Image-to-Video model has two versions:

* v1 "concat": Better motion fluidity but less adherence to the image guidance
* v2 "replace": Updated the day after v1, with better image guidance but seemingly less dynamic compared to v1

<div class="flex justify-between">
  <div class="text-center">
    <p>v1 "concat"</p>

    <img src="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video.webp" alt="HunyuanVideo v1" />
  </div>

  <div class="text-center">
    <p>v2 "replace"</p>

    <img src="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video_v2.webp" alt="HunyuanVideo v2" />
  </div>
</div>

### Shared Model for v1 and v2 Versions

Download the following file and save it to the `ComfyUI/models/clip_vision` directory:

* [llava\_llama3\_vision.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors?download=true)

### V1 "concat" Image-to-Video Workflow

#### 1. Workflow and Asset

Download the workflow image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Hunyuan Image-to-Video v1](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v1_robot.webp)

Download the image below, which we'll use as the starting frame for the image-to-video generation:
![Starting Frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/hunyuan-video/i2v/robot-ballet.png)

#### 2. Related models manual installation

* [hunyuan\_video\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_image_to_video_720p_bf16.safetensors?download=true)

Ensure you have all these model files in the correct locations:

```
ComfyUI/
 models/
    clip_vision/
       llava_llama3_vision.safetensors                     // I2V shared model
    text_encoders/
       clip_l.safetensors                                  // Shared model
       llava_llama3_fp8_scaled.safetensors                 // Shared model
    vae/
       hunyuan_video_vae_bf16.safetensors                  // Shared model
    diffusion_models/
        hunyuan_video_image_to_video_720p_bf16.safetensors  // I2V v1 "concat" version model
```

#### 3. Steps to Run the Workflow

![ComfyUI Hunyuan Video I2V v1 Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg)

1. Ensure that `DualCLIPLoader` has loaded these models:
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure that `Load CLIP Vision` has loaded `llava_llama3_vision.safetensors`
3. Ensure that `Load Image Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. Ensure that `Load VAE` has loaded `vae_name: hunyuan_video_vae_bf16.safetensors`
5. Ensure that `Load Diffusion Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### v2 "replace" Image-to-Video Workflow

The v2 workflow is essentially the same as the v1 workflow. You just need to download the **replace** model and use it in the `Load Diffusion Model` node.

#### 1. Workflow and Asset

Download the workflow image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Hunyuan Image-to-Video v2](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v2_fennec_gril.webp)

Download the image below, which we'll use as the starting frame for the image-to-video generation:
![Starting Frame](https://comfyanonymous.github.io/ComfyUI_examples/flux/flux_dev_example.png)

#### 2. Related models manual installation

* [hunyuan\_video\_v2\_replace\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors?download=true)

Ensure you have all these model files in the correct locations:

```
ComfyUI/
 models/
    clip_vision/
       llava_llama3_vision.safetensors                                // I2V shared model
    text_encoders/
       clip_l.safetensors                                             // Shared model
       llava_llama3_fp8_scaled.safetensors                            // Shared model
    vae/
       hunyuan_video_vae_bf16.safetensors                             // Shared model
    diffusion_models/
        hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors  // V2 "replace" version model
```

#### 3. Steps to Run the Workflow

![ComfyUI Hunyuan Video I2V v2 Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg)

1. Ensure the `DualCLIPLoader` node has loaded these models:
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure the `Load CLIP Vision` node has loaded `llava_llama3_vision.safetensors`
3. Ensure the `Load Image Model` node has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`
5. Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors`
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Try it yourself

Here are some images and prompts we provide. Based on that content or make an adjustment to create your own video.

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png)

```
Futuristic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene
```

***

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/samurai.png)

```
Samurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic
```

***

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/a_flying_car.png)

```
flying car fastly moving and flying through the city
```

***

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png)

```
cyberpunk car race in night city, dynamic, super fast, fast shot
```


# LTX-Video
Source: https://docs.comfy.org/tutorials/video/ltxv



[LTX-Video](https://huggingface.co/Lightricks/LTX-Video) is a very efficient video model by lightricks. The important thing with this model is to give it long descriptive prompts.

## Multi Frame Control

Allows you to control the video with a series of images. You can download the input images: [starting frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house1.png) and [ending frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house2.png).

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/workflow.webp" alt="LTX-Video Multi Frame Control" />

<Tip>
  Drag the video directly into ComfyUI to run the workflow.
</Tip>

## Image to Video

Allows you to control the video with a first [frame image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/girl1.png).

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/workflow.webp" alt="LTX-Video Image to Video" />

<Tip>
  Drag the video directly into ComfyUI to run the workflow.
</Tip>

## Text to Video

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/t2v.webp" alt="LTX-Video Text to Video" />

<Tip>
  Drag the video directly into ComfyUI to run the workflow.
</Tip>

## Requirements

Download the following models and place them in the locations specified below:

* [ltx-video-2b-v0.9.5.safetensors](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/mochi_preview_repackaged/resolve/main/split_files/text_encoders/t5xxl_fp16.safetensors?download=true)

```
 checkpoints/
    ltx-video-2b-v0.9.5.safetensors
 text_encoders/
     t5xxl_fp16.safetensors
```


# ComfyUI Wan2.1 Fun Control Video Examples
Source: https://docs.comfy.org/tutorials/video/wan/fun-control

This guide demonstrates how to use Wan2.1 Fun Control in ComfyUI to generate videos with control videos

## About Wan2.1-Fun-Control

**Wan2.1-Fun-Control** is an open-source video generation and control project developed by Alibaba team.
It introduces innovative Control Codes mechanisms combined with deep learning and multimodal conditional inputs to generate high-quality videos that conform to preset control conditions. The project focuses on precisely guiding generated video content through multimodal control conditions.

Currently, the Fun Control model supports various control conditions, including **Canny (line art), Depth, OpenPose (human posture), MLSD (geometric edges), and trajectory control.**
The model also supports multi-resolution video prediction with options for 512, 768, and 1024 resolutions at 16 frames per second, generating videos up to 81 frames (approximately 5 seconds) in length.

Model versions:

* **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**
* **14B** High-performance: Model size reaches 32GB+, offering better results but **requiring higher VRAM**

Here are the relevant code repositories:

* [Wan2.1-Fun-1.3B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control)
* [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control)
* Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

ComfyUI now **natively supports** the Wan2.1 Fun Control model. Before starting this tutorial, please update your ComfyUI to ensure you're using a version after [this commit](https://github.com/comfyanonymous/ComfyUI/commit/3661c833bcc41b788a7c9f0e7bc48524f8ee5f82).

In this guide, we'll provide two workflows:

1. A workflow using only native Comfy Core nodes
2. A workflow using custom nodes

<Tip>
  Due to current limitations in native nodes for video support, the native-only workflow ensures users can complete the process without installing custom nodes.
  However, we've found that providing a good user experience for video generation is challenging without custom nodes, so we're providing both workflow versions in this guide.
</Tip>

## Model Installation

You only need to install these models once. The workflow images also contain model download information, so you can choose your preferred download method.

The following models can be found at [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334)

Click the corresponding links to download. If you've used Wan-related workflows before, you only need to download the **Diffusion models**.

**Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:

* [wan2.1\_fun\_control\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_control_1.3B_bf16.safetensors?download=true)
* [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/blob/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-Control.safetensors` after downloading

**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage location:

```
 ComfyUI/
  models/
     diffusion_models/
       wan2.1_fun_control_1.3B_bf16.safetensors
     text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors
     vae/
       wan_2.1_vae.safetensors
     clip_vision/
         clip_vision_h.safetensors                 
```

## ComfyUI Native Workflow

In this workflow, we use videos converted to **WebP format** since the `Load Image` node doesn't currently support mp4 format. We also use **Canny Edge** to preprocess the original video.
Because many users encounter installation failures and environment issues when installing custom nodes, this version of the workflow uses only native nodes to ensure a smoother experience.

Thanks to our powerful ComfyUI authors who provide feature-rich nodes. If you want to directly check the related version, see [Workflow Using Custom Nodes](#workflow-using-custom-nodes).

### 1. Workflow File Download

#### 1.1 Workflow File

Download the image below and drag it into ComfyUI to load the workflow:

![Wan2.1 Fun Control Native Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_native.webp)

#### 1.2 Input Images and Videos Download

Please download the following image and video for input:

![Input Reference Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_remix.png)

![Input Reference Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_video.webp)

### 2. Complete the Workflow Step by Step

![Wan2.1 Fun Control Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_native_flow_diagram.png)

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node (renamed to `Start_image`)
6. Upload the control video to the second `Load Image` node. Note: This node currently doesn't support mp4, only WebP videos
7. (Optional) Modify the prompt (both English and Chinese are supported)
8. (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions
9. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### 3. Usage Notes

* Since we need to input the same number of frames as the control video into the `WanFunControlToVideo` node, if the specified frame count exceeds the actual control video frames, the excess frames may display scenes not conforming to control conditions. We'll address this issue in the [Workflow Using Custom Nodes](#workflow-using-custom-nodes)
* Avoid setting overly large dimensions, as this can make the sampling process very time-consuming. Try generating smaller images first, then upscale
* Use your imagination to build upon this workflow by adding text-to-image or other types of workflows to achieve direct text-to-video generation or style transfer
* Use tools like [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) for richer control options

## Workflow Using Custom Nodes

We'll need to install the following two custom nodes:

* [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
* [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

You can use [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) to install missing nodes or follow the installation instructions for each custom node package.

### 1. Workflow File Download

#### 1.1 Workflow File

Download the image below and drag it into ComfyUI to load the workflow:

![Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.webp)

<Note>
  Due to the large size of video files, you can also click [here](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.json) to download the workflow file in JSON format.
</Note>

#### 1.2 Input Images and Videos Download

Please download the following image and video for input:
![Input Reference Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/02-robot's_eye.png)

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/02-man's_eye.mp4" />

### 2. Complete the Workflow Step by Step

![Wan2.1 Fun Control Workflow Using Custom Nodes Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png)

> The model part is essentially the same. If you've already experienced the native-only workflow, you can directly upload the corresponding images and run it.

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node
6. Upload an mp4 format video to the `Load Video(Upload)` custom node. Note that the workflow has adjusted the default `frame_load_cap`
7. For the current image, the `DWPose Estimator` only uses the `detect_face` option
8. (Optional) Modify the prompt (both English and Chinese are supported)
9. (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions
10. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### 3. Workflow Notes

Thanks to the ComfyUI community authors for their custom node packages:

* This example uses `Load Video(Upload)` to support mp4 videos
* The `video_info` obtained from `Load Video(Upload)` allows us to maintain the same `fps` for the output video
* You can replace `DWPose Estimator` with other preprocessors from the `ComfyUI-comfyui_controlnet_aux` node package
* Prompts support multiple languages

## Usage Tips

![Apply Multi Control Videos](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/apply_multi_control_videos.jpg)

* A useful tip is that you can combine multiple image preprocessing techniques and then use the `Image Blend` node to achieve the goal of applying multiple control methods simultaneously.

* You can use the `Video Combine` node from `ComfyUI-VideoHelperSuite` to save videos in mp4 format

* We use `SaveAnimatedWEBP` because we currently don't support embedding workflow into **mp4** and some other custom nodes may not support embedding workflow too. To preserve the workflow in the video, we choose  `SaveAnimatedWEBP` node.

* In the `WanFunControlToVideo` node, `control_video` is not mandatory, so sometimes you can skip using a control video, first generate a very small video size like 320x320, and then use them as control video input to achieve consistent results.

* [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)

* [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)


# ComfyUI Wan2.1 Fun InP Video Examples
Source: https://docs.comfy.org/tutorials/video/wan/fun-inp

This guide demonstrates how to use Wan2.1 Fun InP in ComfyUI to generate videos with first and last frame control

## About Wan2.1-Fun-InP

**Wan-Fun InP** is an open-source video generation model released by Alibaba, part of the Wan2.1-Fun series, focusing on generating videos from images with first and last frame control.

**Key features**:

* **First and last frame control**: Supports inputting both first and last frame images to generate transitional video between them, enhancing video coherence and creative freedom. Compared to earlier community versions, Alibaba's official model produces more stable and significantly higher quality results.
* **Multi-resolution support**: Supports generating videos at 512512, 768768, 10241024 and other resolutions to accommodate different scenario requirements.

**Model versions**:

* **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**
* **14B** High-performance: Model size reaches 32GB+, offering better results but requiring **higher VRAM**

Below are the relevant model weights and code repositories:

* [Wan2.1-Fun-1.3B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Input)
* [Wan2.1-Fun-14B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Input)
* Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

<Tip>
  Currently, ComfyUI natively supports the Wan2.1 Fun InP model. Before starting this tutorial, please update your ComfyUI to ensure your version is after [this commit](https://github.com/comfyanonymous/ComfyUI/commit/0a1f8869c9998bbfcfeb2e97aa96a6d3e0a2b5df).
</Tip>

## Wan2.1 Fun InP Workflow

Download the image below and drag it into ComfyUI to load the workflow:

![Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_inp/wan2.1_fun_inp.webp)

### 1. Workflow File Download

### 2. Manual Model Installation

If automatic model downloading is ineffective, please download the models manually and save them to the corresponding folders.

The following models can be found at [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334)

**Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:

* [wan2.1\_fun\_inp\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_inp_1.3B_bf16.safetensors?download=true)
* [Wan2.1-Fun-14B-InP](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP/resolve/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-InP.safetensors` after downloading

**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage location:

```
 ComfyUI/
  models/
     diffusion_models/
       wan2.1_fun_inp_1.3B_bf16.safetensors
     text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors
     vae/
       wan_2.1_vae.safetensors
     clip_vision/
         clip_vision_h.safetensors                 
```

### 3. Complete the Workflow Step by Step

![ComfyUI Wan2.1 Fun InP Video Generation Workflow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_inp_flow_diagram.png)

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_inp_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node (renamed to `Start_image`)
6. Upload the ending frame to the second `Load Image` node
7. (Optional) Modify the prompt (both English and Chinese are supported)
8. (Optional) Adjust the video size in `WanFunInpaintToVideo`, avoiding overly large dimensions
9. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### 4. Workflow Notes

<Tip>
  Please make sure to use the correct model, as `wan2.1_fun_inp_1.3B_bf16.safetensors` and `wan2.1_fun_control_1.3B_bf16.safetensors` are stored in the same folder and have very similar names. Ensure you're using the right model.
</Tip>

* When using Wan Fun InP, you may need to frequently modify prompts to ensure the accuracy of the corresponding scene transitions.

## Other Wan2.1 Fun InP or video-related custom node packages

* [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
* [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)
* [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)


# ComfyUI Wan2.1 Video Examples
Source: https://docs.comfy.org/tutorials/video/wan/wan-video

This guide demonstrates how to generate videos with first and last frames using Wan2.1 Video in ComfyUI

Wan2.1 Video series is a video generation model open-sourced by Alibaba in February 2025 under the [Apache 2.0 license](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file).
It offers two versions:

* 14B (14 billion parameters)
* 1.3B (1.3 billion parameters)
  Covering multiple tasks including text-to-video (T2V) and image-to-video (I2V).
  The model not only outperforms existing open-source models in performance but more importantly, its lightweight version requires only 8GB of VRAM to run, significantly lowering the barrier to entry.

<video controls>
  <source src="https://github.com/user-attachments/assets/4aca6063-60bf-4953-bfb7-e265053f49ef" type="video/mp4" />
</video>

* [Wan2.1 Code Repository](https://github.com/Wan-Video/Wan2.1)
* [Wan2.1 Model Repository](https://huggingface.co/Wan-AI)

## Wan2.1 ComfyUI Native Workflow Examples

<Tip>
  Please update ComfyUI to the latest version before starting the examples to make sure you have native Wan Video support.
</Tip>

## Model Installation

All models mentioned in this guide can be found [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files). Below are the common models you'll need for the examples in this guide, which you can download in advance:

Choose one version from **Text encoders** to download:

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage locations:

```
ComfyUI/
 models/
    diffusion_models/
    ...                  # Let's download the models in the corresponding workflow
    text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors
    vae/
        wan_2.1_vae.safetensors
    clip_vision/
         clip_vision_h.safetensors   
```

<Note>
  For diffusion models, we'll use the fp16 precision models in this guide because we've found that they perform better than the bf16 versions. If you need other precision versions, please visit [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models) to download them.
</Note>

## Wan2.1 Text-to-Video Workflow

Before starting the workflow, please download [wan2.1\_t2v\_1.3B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

> If you need other t2v precision versions, please visit [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models) to download them.

### 1. Workflow File Download

Download the file below and drag it into ComfyUI to load the corresponding workflow:

![Wan2.1 Text-to-Video Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_t2v_1.3b.webp)

### 2. Complete the Workflow Step by Step

![ComfyUI Wan2.1 Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg)

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_t2v_1.3B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. (Optional) You can modify the video dimensions in the `EmptyHunyuanLatentVideo` node if needed
5. (Optional) If you need to modify the prompts (positive and negative), make changes in the `CLIP Text Encoder` node at number `5`
6. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

## Wan2.1 Image-to-Video Workflow

**Since Wan Video separates the 480P and 720P models**, we'll need to provide examples for both resolutions in this guide. In addition to using different models, they also have slight parameter differences.

### 480P Version

#### 1. Workflow and Input Image

Download the image below and drag it into ComfyUI to load the corresponding workflow:
![Wan2.1 Image-to-Video Workflow 14B 480P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_480P.webp)

We'll use the following image as input:

![Wan2.1 Image-to-Video Workflow 14B 480P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/flux_dev_example.png)

#### 2. Model Download

Please download [wan2.1\_i2v\_480p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

#### 3. Complete the Workflow Step by Step

![ComfyUI Wan2.1 Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg)

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_480p_14B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model
5. Upload the provided input image in the `Load Image` node
6. (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node
7. (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed
8. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

### 720P Version

#### 1. Workflow and Input Image

Download the image below and drag it into ComfyUI to load the corresponding workflow:
![Wan2.1 Image-to-Video Workflow 14B 720P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_720P.webp)

We'll use the following image as input:

![Wan2.1 Image-to-Video Workflow 14B 720P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/magician.png)

#### 2. Model Download

Please download [wan2.1\_i2v\_720p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_720p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

#### 3. Complete the Workflow Step by Step

![ComfyUI Wan2.1 Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg)

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_720p_14B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model
5. Upload the provided input image in the `Load Image` node
6. (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node
7. (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed
8. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation


#  ComfyUI  AI 
Source: https://docs.comfy.org/zh-CN/get_started/first_generation

 ComfyUI  ComfyUI 

 ComfyUI  ComfyUI 

1. 
   *  ComfyUI `Workflows template``Text to Image`
   * `metadata` 
2. 
   * 
   * 
   *  **ComfyUI Manager** 
3. 

## 

**Text to Image** AI  AI **()****()** ComfyUI [](/zh-CN/tutorials/basic/image-to-image)

## ComfyUI 

### 1.  ComfyUI

 ComfyUI  ComfyUI 

![ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/comfyui-boot-screen.jpg)

 ComfyUI 

<AccordionGroup>
  <Accordion title="ComfyUI ()">
    ComfyUI  **Windows  MacOS(Apple Silicon)**  Beta 

    *  [Github](https://github.com/Comfy-Org/desktop)

     ComfyUI

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI(Windows)" icon="link" href="/zh-CN/installation/desktop/windows">
           **Nvdia**  **Windows**  ComfyUI 
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI(MacOS)" icon="link" href="/zh-CN/installation/desktop/macos">
           **Apple Silicon**  MacOS ComfyUI 
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI** Linux **[](/zh-CN/installation/manual_install) ComfyUI </Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI (Windows)">
    <Card title="ComfyUI(Windows)" icon="link" href="/zh-CN/installation/comfyui_portable_windows">
       **Navida **  **CPU**  **Windows** ComfyUI  commit 
    </Card>
  </Accordion>

  <Accordion title=" ComfyUI">
    <Card title="ComfyUI " icon="link" href="/zh-CN/installation/manual_install">
       GPU NvidiaAMDIntelApple SiliconAscend NPU MLU ComfyUI
    </Card>
  </Accordion>
</AccordionGroup>

### 2. 

 ComfyUI ,  ComfyUI 

<Tabs>
  <Tab title=" Workflow Template ">
    ![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1.jpg)
    

    1.  ComfyUI **Fit View**
    2. **workflows**
    3.  Workflows**Browse example workflows** 

    

    ![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg)

    4.  **Image Generation** 

    `workflow`**Browse workflow templates** 
    ![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg)
  </Tab>

  <Tab title=" metadata ">
     ComfyUI  metadata  workflow  workflow

     ComfyUI  **Workflows** -> **Open**  workflow

    ![ComfyUI-](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/text-to-image-workflow.png)
  </Tab>

  <Tab title=" workflow.json ">
    ComfyUI  json  **Workflows** -> **Export**  json 

     Github  text-to-image.json 

    <a className="prose" href="https://github.com/Comfy-Org/docs/blob/main/public/text-to-image.json" download style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
      <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}> text-to-image.json </p>
    </a>

     **Workflows** -> **Open**  json  workflow
  </Tab>
</Tabs>

### 3. 

 ComfyUI 

[v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) 

![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg)

 `Download`  ComfyUI 

 `< ComfyUI >/ComfyUI/models/`  `checkpoints``embeddings``vae``lora``upscale_model` ComfyUI `extra_models_config.yaml` 

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg)

 ComfyUI  **** 

<Tabs>
  <Tab title="">
     **Download** ComfyUI ,

    <Tabs>
      <Tab title="ComfyUI ">
         `< ComfyUI >/ComfyUI/models/checkpoints` 
        

        ![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg)

        
      </Tab>

      <Tab title="ComfyUI ">
         `< ComfyUI >/ComfyUI_windows_portable/ComfyUI/models/checkpoints` 
      </Tab>
    </Tabs>
  </Tab>

  <Tab title=" ComfyUI Manager ">
    [ComfyUI Manager](https://github.com/ltdrdata/ComfyUI-Manager)  [ltdrdata](https://github.com/ltdrdata)  ComfyUI  ComfyUI Manager  ComfyUI Manager 

    <Steps>
      <Step title=" ComfyUI Manager">
        ![ComfyUI Manager ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager.jpg)

         `Manager`  ComfyUI Manager 
      </Step>

      <Step title="Model Manager">
        ![ComfyUI Manager ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-model-manager.jpg)

         `Model Manager` 
      </Step>

      <Step title="">
        ![ComfyUI Manager ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-download-model.jpg)

        1. `v1-5-pruned-emaonly.ckpt`
        2.  `install` 

         ComfyUI Manager 
      </Step>
    </Steps>
  </Tab>

  <Tab title="">
    [ v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)
    

    ![Hugging Face ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-5-hugging-face.jpg)

    **v1-5-pruned-emaonly-fp16.safetensors** 

    <Tabs>
      <Tab title="ComfyUI ">
         ComfyUI  `< ComfyUI >/ComfyUI/models/checkpoints`

        ![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-6-2-desktop.jpg)
      </Tab>

      <Tab title="ComfyUI">
        **ComfyUI\_windows\_portable/ComfyUI/models/checkpoints** 
        ![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-6-1-portable.jpg)
      </Tab>

      <Tab title="">
          **ComfyUI/models/checkpoints**
      </Tab>
    </Tabs>

     ComfyUI  ComfyUI 
  </Tab>
</Tabs>

### 4. 



![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg)


1.  **Load Checkpoint**  **v1-5-pruned-emaonly-fp16.safetensors**  **null** 
2.  `Queue`  `Ctrl + enter()` 

 **(Save Image)** 

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg)



<Card title="ComfyUI " icon="link" href="/zh-CN/tutorials/basic/text-to-image">
  
</Card>

## 

### 

 `Load Checkpoint`  **null** ****  ** ComfyUI** 


#  ComfyUI
Source: https://docs.comfy.org/zh-CN/get_started/introduction

ComfyUI [](https://github.com/Comfy-Org/docs)

<img className="block dark:hidden" src="https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui_screenshot.png" alt="Hero Light" />

<img className="hidden dark:block" src="https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui_screenshot.png" alt="Hero Dark" />

## [ComfyUI](https://github.com/comfyanonymous/ComfyUI)

 GUI  [comfyanonymous](https://github.com/comfyanonymous) [](https://github.com/comfyanonymous/ComfyUI/graphs/contributors)

* **ComfyUI** ****
* AI
* ComfyUI 

##  ComfyUI

### ComfyUI 

ComfyUI  WindowsMacOS  Linux :

<AccordionGroup>
  <Accordion title="ComfyUI ()">
    ComfyUI  **Windows  MacOS(Apple Silicon)**  Beta 

    *  [Github](https://github.com/Comfy-Org/desktop)

     ComfyUI

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI(Windows)" icon="link" href="/zh-CN/installation/desktop/windows">
           **Nvdia**  **Windows**  ComfyUI 
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI(MacOS)" icon="link" href="/zh-CN/installation/desktop/macos">
           **Apple Silicon**  MacOS ComfyUI 
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI** Linux **[](/zh-CN/installation/manual_install) ComfyUI </Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI (Windows)">
    <Card title="ComfyUI(Windows)" icon="link" href="/zh-CN/installation/comfyui_portable_windows">
       **Navida **  **CPU**  **Windows** ComfyUI  commit 
    </Card>
  </Accordion>

  <Accordion title=" ComfyUI">
    <Card title="ComfyUI " icon="link" href="/zh-CN/installation/manual_install">
       GPU NvidiaAMDIntelApple SiliconAscend NPU MLU ComfyUI
    </Card>
  </Accordion>
</AccordionGroup>

##  ComfyUI 

 ComfyUI 
<Note></Note>

<Card title="" icon="link" href="/custom-nodes/overview">
   ComfyUI ()
</Card>

## 

 Github  Fork [repo](https://github.com/comfyanonymous/ComfyUI),  PR 


# ComfyUI Windows
Source: https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows

 ComfyUI Portable() 

**ComfyUI Portable()**  ComfyUI Windows  ComfyUI  **Python(python\_embeded)**,, **Nvidia**  **CPU** 



<Tip>
   Nvidia 50 (Blackwell) [](/zh-CN/installation/nvidia-50-series), ComfyUI 
</Tip>

##  ComfyUI Portable()

 **ComfyUI Portable()** 

<a className="prose" href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}> ComfyUI Portable()</p>
</a>

 [7-ZIP](https://7-zip.org/) 



```
ComfyUI_windows_portable
 ComfyUI                   // ComfyUI 
 python_embeded            //  Python 
 update                    // 
 README_VERY_IMPORTANT.txt   //  ComfyUI 
 run_cpu.bat                 //  ComfyUI CPU
 run_nvidia_gpu.bat          //  ComfyUI Nvidia 
```

##  ComfyUI

 `run_nvidia_gpu.bat `  `run_cpu.bat`  ComfyUI

![ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui-portable-cmd.png)



```
To see the GUI go to: http://127.0.0.1:8188
```

 ComfyUI  ComfyUI  `http://127.0.0.1:8188` 

<Note> ComfyUI </Note>

## 

 ComfyUI 

<Card title="" icon="link" href="/zh-CN/get_started/first_generation">
  
</Card>

##  ComfyUI 

### 1. ComfyUI 

 **update**  ComfyUI 

```
ComfyUI_windows_portable
 update
    update.py
    update_comfyui.bat            //  ComfyUI  Commit 
    update_comfyui_and_python_dependencies.bat  // 
    update_comfyui_stable.bat       //  ComfyUI  stable 
```

### 2. ComfyUI 

 [A1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) 

```
ComfyUI_windows_portable
 ComfyUI
    extra_model_paths.yaml.example  // 
```

 `extra_model_paths.yaml.example`  `extra_model_paths.yaml`



```yaml
#Rename this to extra_model_paths.yaml and ComfyUI will load it


#config for a1111 ui
#all you have to do is change the base_path to where yours is installed
a111:
    base_path: path/to/stable-diffusion-webui/

    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet

#config for comfyui
#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.

#comfyui:
#     base_path: path/to/comfyui/
#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads
#     #is_default: true
#     checkpoints: models/checkpoints/
#     clip: models/clip/
#     clip_vision: models/clip_vision/
#     configs: models/configs/
#     controlnet: models/controlnet/
#     diffusion_models: |
#                  models/diffusion_models
#                  models/unet
#     embeddings: models/embeddings/
#     loras: models/loras/
#     upscale_models: models/upscale_models/
#     vae: models/vae/

#other_ui:
#    base_path: path/to/ui
#    checkpoints: models/checkpoints
#    gligen: models/gligen
#    custom_nodes: path/custom_nodes

```

 WebUI  `D:\stable-diffusion-webui\ ` 

```yaml
a111:
    base_path: D:\stable-diffusion-webui\
    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet
```

 `D:\stable-diffusion-webui\models\Stable-diffusion\`  ComfyUI 

### 3. ComfyUI 

 ComfyUI  ComfyUI `run_nvidia_gpu.bat`  `run_cpu.bat` `--listen`
 `--listen`  `run_nvidia_gpu.bat` 

```bat
.\python_embeded\python.exe -s ComfyUI\main.py --listen --windows-standalone-build
pause
```

 ComfyUI 

```
Starting server

To see the GUI go to: http://0.0.0.0:8188
To see the GUI go to: http://[::]:8188
```

 `WIN + R` `cmd`  `ipconfig`  IP  `http://IP:8188`  ComfyUI


# Linux
Source: https://docs.comfy.org/zh-CN/installation/desktop/linux

 ComfyUI Desktop MacOS 

<Warning>Linux[](/zh-CN/installation/manual_install) </Warning>


# MacOs
Source: https://docs.comfy.org/zh-CN/installation/desktop/macos

 ComfyUI Desktop MacOS 

export const log_path_0 = "~/Library/Logs/ComfyUI"

export const config_path_0 = "~/Library/Application Support/ComfyUI"

**ComfyUI Desktop**  **Python**  ComfyUI 

ComfyUI  [](https://github.com/Comfy-Org/desktop)

<Note>ComfyUI MacOS  Apple Silicon</Note>



<Warning> **ComfyUI **  **Beta** </Warning>

## ComfyUI MacOS

 MacOS  **ComfyUI ** 

<a className="prose" href="https://download.comfy.org/mac/dmg/arm64" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download for MacOS</p>
</a>

## ComfyUI 

**ComfyUI****Applications**

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0.png)

 **ComfyUI **
![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0-1.png)

 **(Lanchpad)**  **ComfyUI **  ComfyUI 
![ComfyUI Lanchpad](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-1.jpg)

## ComfyUI 

<Steps>
  <Step title="">
    <Tabs>
      <Tab title="">
        ![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-2.png)
         **Get Started** 
      </Tab>

      <Tab title="">
         ComfyUI  pytorch15 GB git

        

        *  Python 
        *  Python 
        *  gitVC redis
        * 

        

        ![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-1.jpg)

         `All` 

        ![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-2.jpg)
      </Tab>
    </Tabs>
  </Step>

  <Step title="Select GPU(GPU)">
    ![ComfyUI  - GPU ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-3.png)
    

    1. **MPS:**  Metal Performance Shaders (MPS)  GPU  PyTorch  GPU 
    2. **Manual Configuration :**  python 
    3. **Enable CPU Mode  CPU :** 

     **MPS** **Next** 
  </Step>

  <Step title="Install location">
    ![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-4.png)
     ComfyUI 

    * **Python **
    * **Models **
    * **Custom Nodes **
      
    *  ComfyUI 
    *  **5G**  **ComfyUI** 

    <Note>ComfyUI  MacOS  ComfyUI </Note>
     **Next** 
  </Step>

  <Step title="Migrate from Existing Installation - ">
    ![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-5.png)
     ComfyUI  ComfyUI  **ComfyUI** 

    * **User Files **
    * **Models :** 
    * **Custom Nodes :** 

     **Next** 
  </Step>

  <Step title="Desktop Setting()">
    ![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-6.png)
    

    1. **Automatic Updates :**  ComfyUI 
    2. **Usage Metrics :** ****  ComfyUI
    3. **Mirror Settings :**  Python 
       ![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-7.png)
        **Python Install Mirror** 

    

    

    #### Python 

    

    ```
    https://python-standalone.org/mirror/astral-sh/python-build-standalone
    ```

     GitHub  `python-build-standalone` releases

    ```
    https://github.com/astral-sh/python-build-standalone/releases/download
    ```

    

    ```
    https://xxx/astral-sh/python-build-standalone/releases/download
    ```

    <info> Github </info>

    #### PyPI 

    * [https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)
    * [https://mirrors.cloud.tencent.com/pypi/simple/](https://mirrors.cloud.tencent.com/pypi/simple/)
    * [https://pypi.mirrors.ustc.edu.cn/simple/](https://pypi.mirrors.ustc.edu.cn/simple/)
    * [https://pypi.sjtu.edu.cn/simple/](https://pypi.sjtu.edu.cn/simple/)

    #### Torch 

    * : [https://mirrors.aliyun.com/pytorch-wheels/cu121/](https://mirrors.aliyun.com/pytorch-wheels/cu121/)
  </Step>

  <Step title="">
     ComfyUI , 
    ![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-interface.jpg)
  </Step>
</Steps>

## 

 ComfyUI 

<Card title="" icon="link" href="/zh-CN/get_started/first_generation">
  
</Card>

##  ComfyUI 

 ComfyUI 

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg)

##  ComfyUI 

 **ComfyUI **  **Application**  **ComfyUI**

 **ComfyUI ** 

* /Users/Library/Application Support/ComfyUI



* models 
* custom nodes 
* input/output directories. /

## 

### 



![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg)



1.  `Show Teriminal` 
2.  `Open Logs` 
3. 
4. `Reinstall`

**** **log ** **GPT**

![ComfyUI -](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg)
![ComfyUI -GPT ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg)

 ComfyUI 

### 



* Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)
* Comfy : [https://forum.comfy.org/](https://forum.comfy.org/)



1. 

|          |                                           |            |
| ----------- | ------------------------------------------- | ------------ |
| main.log    |  Electron        | {log_path_0} |
| comfyui.log |  ComfyUI  ComfyUI  | {log_path_0} |

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)

2. 

|                         |                            |               |
| -------------------------- | ---------------------------- | --------------- |
| extra\_models\_config.yaml |  ComfyUI  | {config_path_0} |
| config.json                |           | {config_path_0} |

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)


# Windows
Source: https://docs.comfy.org/zh-CN/installation/desktop/windows

 ComfyUI Desktop Windows 

export const log_path_0 = "C:\Users\ <> \AppData\Roaming\ComfyUI\logs"

export const config_path_0 = "C:\Users\ <> \AppData\Roaming\ComfyUI"

**ComfyUI Desktop**  **Python**  ComfyUI [ComfyUI ](/zh-CN/installation/comfyui_portable_windows)

ComfyUI  [](https://github.com/Comfy-Org/desktop)

ComfyUI (Windows)

* NVIDIA 



<Warning> **ComfyUI **  **Beta** </Warning>

## ComfyUI Windows

 Windows  **ComfyUI ** 

<a className="prose" href="https://download.comfy.org/windows/nsis/x64" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download for Windows (NVIDIA)</p>
</a>

## ComfyUI 

 **ComfyUI ** 

![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-shortcut.jpg)

 ComfyUI 

### ComfyUI 

<Steps>
  <Step title="">
    <Tabs>
      <Tab title="">
        ![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-1.jpg)
         **Get Started** 
      </Tab>

      <Tab title="">
         ComfyUI  pytorch15 GB git

        

        *  Python 
        *  Python 
        *  gitVC redis
        * 

        

        ![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-1.jpg)

         `All` 

        ![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-2.jpg)
      </Tab>
    </Tabs>
  </Step>

  <Step title="Select GPU(GPU)">
    ![ComfyUI  - GPU ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-2.jpg)
    

    1. **Nvidia :**  pytorch  CUDA
    2. **Manual Configuration :**  python 
    3. **Enable CPU Mode  CPU :** 

    **NVIDIA** **Next** 
  </Step>

  <Step title="Install location">
    ![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-3.jpg)

     ComfyUI 

    * **Python **
    * **Models **
    * **Custom Nodes **

    

    * **** ComfyUI 
    *  ComfyUI 
    *  **15G**  ComfyUI Desktop 

    <Note>ComfyUI  C  ComfyUI </Note>

     **Next** 
  </Step>

  <Step title="Migrate from Existing Installation - ">
    ![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-4.jpg)

     ComfyUI  ComfyUI  **D:\ComfyUI\_windows\_portable\ComfyUI** 

    * **User Files **
    * **Models :** 
    * **Custom Nodes :** 

     **Next** 
  </Step>

  <Step title="Desktop Setting()">
    ![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-5.jpg)

    

    1. **Automatic Updates :**  ComfyUI 
    2. **Usage Metrics :** ****  ComfyUI
    3. **Mirror Settings :**  Python 

    ![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-6.jpg)
     **Python Install Mirror** 

    

    

    #### Python 

    

    ```
    https://python-standalone.org/mirror/astral-sh/python-build-standalone
    ```

     GitHub  `python-build-standalone` releases

    ```
    https://github.com/astral-sh/python-build-standalone/releases/download
    ```

    

    ```
    https://xxx/astral-sh/python-build-standalone/releases/download
    ```

    <info> Github </info>

    #### PyPI 

    * [https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)
    * [https://mirrors.cloud.tencent.com/pypi/simple/](https://mirrors.cloud.tencent.com/pypi/simple/)
    * [https://pypi.mirrors.ustc.edu.cn/simple/](https://pypi.mirrors.ustc.edu.cn/simple/)
    * [https://pypi.sjtu.edu.cn/simple/](https://pypi.sjtu.edu.cn/simple/)

    #### Torch 

    * : [https://mirrors.aliyun.com/pytorch-wheels/cu121/](https://mirrors.aliyun.com/pytorch-wheels/cu121/)
  </Step>

  <Step title="">
     ComfyUI , 
    ![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-interface.jpg)
  </Step>
</Steps>

## 

 ComfyUI 

<Card title="" icon="link" href="/zh-CN/get_started/first_generation">
  
</Card>

##  ComfyUI 

 ComfyUI 

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg)

##  ComfyUI 

 **ComfyUI **  Windows 

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-uninstall-comfyui.jpg)

 **ComfyUI ** 

* C:\Users\<>\AppData\Local\@comfyorgcomfyui-electron-updater
* C:\Users\<>\AppData\Local\Programs\@comfyorgcomfyui-electron
* C:\Users\<>\AppData\Roaming\ComfyUI



* models 
* custom nodes 
* input/output directories. /

## 

### 

![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-0.jpg)

 ComfyUI Windows **CUDA  Nvdia ** 

* 
*  [ComfyUI](/zh-CN/installation/comfyui_portable_windows) [](/zh-CN/installation/manual_install) ComfyUI

### 



![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg)



1.  `Show Teriminal` 
2.  `Open Logs` 
3. 
4. `Reinstall`

**** **log ** **GPT**

![ComfyUI -](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg)
![ComfyUI -GPT ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg)

 ComfyUI 

### 



* Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)
* Comfy : [https://forum.comfy.org/](https://forum.comfy.org/)



1. 

|          |                                           |            |
| ----------- | ------------------------------------------- | ------------ |
| main.log    |  Electron        | {log_path_0} |
| comfyui.log |  ComfyUI  ComfyUI  | {log_path_0} |

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)

2. 

|                         |                            |               |
| -------------------------- | ---------------------------- | --------------- |
| extra\_models\_config.yaml |  ComfyUI  | {config_path_0} |
| config.json                |           | {config_path_0} |

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)


# 
Source: https://docs.comfy.org/zh-CN/installation/manual_install

 WindowsMacOS  Linux 

<Tip>
   Nvidia 50 (Blackwell) [](/zh-CN/installation/nvidia-50-series), ComfyUI 
</Tip>

<Tabs>
  <Tab title="Windows">
    ### 

    ```bash
    git clone git@github.com:comfyanonymous/ComfyUI.git
    ```

    More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

    <Warning> Microsoft Visual C++ Redistributable[](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170)</Warning>

    ### 

    1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.

       Create an environment with Conda.

       ```
       conda create -n comfyenv
       conda activate comfyenv
       ```
    2. Install GPU Dependencies

       <Accordion title="Nvidia">
         ```
         conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
           ```
         </Accordion>
       </Accordion>

       <Accordion title="AMD">
         ```
         pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
           ```
         </Accordion>
       </Accordion>

       <Accordion title="Mac ARM Silicon">
         ```bash
         conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
         ```
       </Accordion>
    3. ```bash
       cd ComfyUI
       pip install -r requirements.txt
       ```
    4. Start the application
       ```
       cd ComfyUI
       python main.py
       ```
  </Tab>

  <Tab title="Linux">
    ### 

    ```bash
    git clone git@github.com:comfyanonymous/ComfyUI.git
    ```

    More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

    ### 

    1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.

       Create an environment with Conda.

       ```
       conda create -n comfyenv
       conda activate comfyenv
       ```
    2. Install GPU Dependencies

       <Accordion title="Nvidia">
         ```
         conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
           ```
         </Accordion>
       </Accordion>

       <Accordion title="AMD">
         ```
         pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
           ```
         </Accordion>
       </Accordion>

       <Accordion title="Mac ARM Silicon">
         ```bash
         conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
         ```
       </Accordion>
    3. ```bash
       cd ComfyUI
       pip install -r requirements.txt
       ```
    4. Start the application
       ```
       cd ComfyUI
       python main.py
       ```
  </Tab>

  <Tab title="MacOS">
    ### 

    [](https://support.apple.com/guide/terminal/open-or-quit-terminal-apd5265185d-f365-44cb-8b09-71a064a42125/mac)

    ```bash
    git clone git@github.com:comfyanonymous/ComfyUI.git
    ```

    More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

    ### 

    1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.

       Create an environment with Conda.

       ```
       conda create -n comfyenv
       conda activate comfyenv
       ```
    2. Install GPU Dependencies

       <Accordion title="Nvidia">
         ```
         conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
           ```
         </Accordion>
       </Accordion>

       <Accordion title="AMD">
         ```
         pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
           ```
         </Accordion>
       </Accordion>

       <Accordion title="Mac ARM Silicon">
         ```bash
         conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
         ```
       </Accordion>
    3. ```bash
       cd ComfyUI
       pip install -r requirements.txt
       ```
    4. Start the application
       ```
       cd ComfyUI
       python main.py
       ```
  </Tab>
</Tabs>


# 
Source: https://docs.comfy.org/zh-CN/installation/system_requirements

 ComfyUI 

 ComfyUI ,  ComfyUI [ComfyUI](https://github.com/comfyanonymous/ComfyUI)

 ComfyUI Python 

 ComfyUI 

<AccordionGroup>
  <Accordion title="ComfyUI ()">
    ComfyUI  **Windows  MacOS(Apple Silicon)**  Beta 

    *  [Github](https://github.com/Comfy-Org/desktop)

     ComfyUI

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI(Windows)" icon="link" href="/zh-CN/installation/desktop/windows">
           **Nvdia**  **Windows**  ComfyUI 
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI(MacOS)" icon="link" href="/zh-CN/installation/desktop/macos">
           **Apple Silicon**  MacOS ComfyUI 
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI** Linux **[](/zh-CN/installation/manual_install) ComfyUI </Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI (Windows)">
    <Card title="ComfyUI(Windows)" icon="link" href="/zh-CN/installation/comfyui_portable_windows">
       **Navida **  **CPU**  **Windows** ComfyUI  commit 
    </Card>
  </Accordion>

  <Accordion title=" ComfyUI">
    <Card title="ComfyUI " icon="link" href="/zh-CN/installation/manual_install">
       GPU NvidiaAMDIntelApple SiliconAscend NPU MLU ComfyUI
    </Card>
  </Accordion>
</AccordionGroup>

## Nvdia 50 

 Nvdia 50  GPUBlackwell ComfyUI CUDA 12.8  PyTorch, 20253 PyTorch  Blackwell 

[](https://github.com/comfyanonymous/ComfyUI/discussions/6643)

###  Windows 

****
 nightly pytorch 2.7 cu128  ComfyUI Portable 

* [](https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_windows_portable_nvidia_or_cpu_nightly_pytorch.7z)

****
 torch 2.6 Windows 

* [ cuda 12.8 torch  ComfyUI ](https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_cu128_50XX.7z)

### 

Windows  Linux  PyTorch nightly 

```bash
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
```

### Docker 

 Nvidia  PyTorch 

: [https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch)



```bash
docker run -p 8188:8188 --gpus all -it --rm nvcr.io/nvidia/pytorch:25.01-py3
```

Docker

```bash
git clone https://github.com/comfyanonymous/ComfyUI
cd ComfyUI
grep -v 'torchaudio\|torchvision' requirements.txt > temp_requirements.txt
pip install -r temp_requirements.txt
python main.py --listen
```


# ComfyUI Hunyuan3D-2 
Source: https://docs.comfy.org/zh-CN/tutorials/3d/hunyuan3D-2

 Hunyuan3D-2  ComfyUI  3D 

# 3D 2.0 

![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-1.gif)
![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-2.gif)

[3D 2.0](https://github.com/Tencent/Hunyuan3D-2)  3D  3D 

3D 2.03D 2.0:

1. **Hunyuan3D-DiT**Transformer
2. **Hunyuan3D-Paint**PBR

****

* ****PBR
* ****BlenderGradio
* ****Hunyuan3D-2mini5GB6GB+12GB

2025  3  18 3D 2.0 Hunyuan3D-2mv



*  Hunyuan3D-2mv 3D
*  Hunyuan3D-2mv-turbo 3D
*  Hunyuan3D-2 3D

<Tip>
   ComfyUI  Hunyuan3D-2mv [ComfyUI](https://github.com/comfyanonymous/ComfyUI)

   png  Metadata  json 

  *  ComfyUI
  *  `Workflows` -> `Openctrl+o`

   `.glb`  `ComfyUI/output/mesh` 
</Tip>

## ComfyUI Hunyuan3D-2mv 

Hunyuan3D-2mv 3D `front` 3D

### 1. 

 ComfyUI ,

![Hunyuan3D-2mv workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/hunyuan-3d-multiview-elf.webp)



<div class="flex space-x-4">
  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/front.png" alt="input image" class="w-1/3" />

  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/left.png" alt="input image" class="w-1/3" />

  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/back.png" alt="input image" class="w-1/3" />
</div>

<Tip>
  [ComfyUI\_essentials](https://github.com/cubiq/ComfyUI_essentials) 
</Tip>

### 2. 

 ComfyUI 

* hunyuan3d-dit-v2-mv: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv/model.fp16.safetensors?download=true)   `hunyuan3d-dit-v2-mv.safetensors`

```
ComfyUI/
 models/
    checkpoints/
       hunyuan3d-dit-v2-mv.safetensors  // 
```

### 3. 

![ComfyUI hunyuan3d\_2mv](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg)

1.  Image Only Checkpoint Loader(img2vid model)  `hunyuan3d-dit-v2-mv.safetensors` 
2.  `Load Image` 
3.  `Queue`  `Ctrl(cmd) + Enter()` 

 `Hunyuan3Dv2ConditioningMultiView`  `Load Image` 

##  Hunyuan3D-2mv-turbo 

Hunyuan3D-2mv-turbo  Hunyuan3D-2mv-turbo 3D Hunyuan3D-2mv Step Distillation3D `cfg`  1.0  `flux guidance`  `distilled cfg` 

### 1. 

 ComfyUI ,

![Hunyuan3D-2mv-turbo workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/hunyuan-3d-turbo.webp)



<div class="flex space-x-4">
  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/front.png" alt="input image" class="w-1/2" />

  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/right.png" alt="input image" class="w-1/2" />
</div>

### 2. 

 ComfyUI 

* hunyuan3d-dit-v2-mv-turbo: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv-turbo/model.fp16.safetensors?download=true)   `hunyuan3d-dit-v2-mv-turbo.safetensors`

```
ComfyUI/
 models/
    checkpoints/
       hunyuan3d-dit-v2-mv-turbo.safetensors  // 
```

### 3. 

![ComfyUI hunyuan3d\_2mv\_turbo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg)

1.  `Image Only Checkpoint Loader(img2vid model)`  `hunyuan3d-dit-v2-mv-turbo.safetensors` 
2.  `Load Image` 
3.  `Queue`  `Ctrl(cmd) + Enter()` 

##  Hunyuan3D-2 

Hunyuan3D-2  Hunyuan3D-2 3D`Hunyuan3Dv2Conditioning`  `Hunyuan3Dv2ConditioningMultiView` 

### 1. 

 ComfyUI 

![Hunyuan3D-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d-non-multiview-train.webp)



![ComfyUI Hunyuan 3D 2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan_3d_v2_non_multiview_train.png)

### 2. 

 ComfyUI 

* hunyuan3d-dit-v2-0: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2/resolve/main/hunyuan3d-dit-v2-0/model.fp16.safetensors?download=true)   `hunyuan3d-dit-v2.safetensors`

```
ComfyUI/
 models/
    checkpoints/
       hunyuan3d-dit-v2.safetensors  // 
```

### 3. 

![ComfyUI hunyuan3d\_2](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg)

1.  `Image Only Checkpoint Loader(img2vid model)`  `hunyuan3d-dit-v2.safetensors` 
2.  `Load Image` 
3.  `Queue`  `Ctrl(cmd) + Enter()` 

## 

 Hunyuan3D-2  ComfyUI 

* [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)
* [Kijai/Hunyuan3D-2\_safetensors](https://huggingface.co/Kijai/Hunyuan3D-2_safetensors/tree/main)
* [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)

## 3D 2.0 

3D 2.0 3D [Hunyuan3D-2](https://github.com/Tencent/Hunyuan3D-2) 

**Hunyuan3D-2mini **

|                     |            |          |    | Huggingface                                                                          |
| --------------------- | ------------ | ---------- | ---- | ------------------------------------------------------------------------------------ |
| Hunyuan3D-DiT-v2-mini | Mini  | 2025-03-18 | 0.6B | [](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini) |

**Hunyuan3D-2mv **

|                        |                               |          |    | Huggingface                                                                           |
| ------------------------ | ------------------------------- | ---------- | ---- | ------------------------------------------------------------------------------------- |
| Hunyuan3D-DiT-v2-mv-Fast |  DIT            | 2025-03-18 | 1.1B | [](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast) |
| Hunyuan3D-DiT-v2-mv      |  3D  | 2025-03-18 | 1.1B | [](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv)      |

**Hunyuan3D-2 **

|                       |       |          |    | Huggingface                                                                        |
| ----------------------- | ------- | ---------- | ---- | ---------------------------------------------------------------------------------- |
| Hunyuan3D-DiT-v2-0-Fast |   | 2025-02-03 | 1.1B | [](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast) |
| Hunyuan3D-DiT-v2-0      |  | 2025-01-21 | 1.1B | [](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0)      |
| Hunyuan3D-Paint-v2-0    |   | 2025-01-21 | 1.3B | [](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0)    |
| Hunyuan3D-Delight-v2-0  |  | 2025-01-21 | 1.3B | [](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0)  |


# ComfyUI  HiDream-I1 
Source: https://docs.comfy.org/zh-CN/tutorials/advanced/hidream

 ComfyUI  HiDream-I1 

![HiDream-I1 ](https://raw.githubusercontent.com/HiDream-ai/HiDream-I1/main/assets/demo.jpg)

HiDream-I1 (HiDream-ai)20254717B [MIT ](https://github.com/HiDream-ai/HiDream-I1/blob/main/LICENSE) 

## 

****
DiTMoE

* Diffusion TransformerDiTMMDiTDiT
* 

****


* OpenCLIP ViT-bigGOpenAI CLIP ViT-L
* T5-XXL
* Llama-3.1-8B-Instruct
  SOTA

****

(HiDream-ai) HiDream-I1 

* [ HiDream-I1-Full](https://huggingface.co/HiDream-ai/HiDream-I1-Full)  50
* [ HiDream-I1-Dev](https://huggingface.co/HiDream-ai/HiDream-I1-Dev)  28
* [ HiDream-I1-Fast](https://huggingface.co/HiDream-ai/HiDream-I1-Fast)  16

## 

 ComfyOrg  repackaged  [HiDream-I1\_ComfyUI](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/) 

<Tip>
   ComfyUI [](https://github.com/comfyanonymous/ComfyUI/commit/9ad792f92706e2179c58b2e5348164acafa69288)  ComfyUI  HiDream 
</Tip>

## HiDream-I1 

 ComfyUI  HiDream-I1  [diffusion models](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files/diffusion_models) 



* **HiDream-I1-Full** 
* **HiDream-I1-Dev** 
* **HiDream-I1-Fast**  16 

 **dev**  **fast** `cfg`  `1.0`

<Tip>
   27GB  **fp8** 
</Tip>

### 

 **diffusion models** 

**text\_encoders**

* [clip\_l\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)
* [clip\_g\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)
* [t5xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) 
* [llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)

**VAE**

* [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors)  Flux  VAE  Flux 

**diffusion models**




```
 ComfyUI/
  models/
     text_encoders/
       clip_l_hidream.safetensors
       clip_g_hidream.safetensors
       t5xxl_fp8_e4m3fn_scaled.safetensors
       llama_3.1_8b_instruct_fp8_scaled.safetensors
     vae/
       ae.safetensors
     diffusion_models/
        ...               #             
```

### HiDream-I1 full 

#### 1. 

 `ComfyUI/models/diffusion_models/` 

* FP8 [hidream\_i1\_full\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp8.safetensors?download=true)  16GB 
* [hidream\_i1\_full\_f16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp16.safetensors?download=true)  27GB 

#### 2. 

 ComfyUI 
![HiDream-I1 full ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_full.png)

#### 3. 

![HiDream-I1 full ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg)



1. `Load Diffusion Model`  `hidream_i1_full_fp8.safetensors` 
2. `QuadrupleCLIPLoader`  text encoder 
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. `Load VAE`  `ae.safetensors` 
4.  **full**  `ModelSamplingSD3`  `shift`  `3.0`
5.  `Ksampler` 
   * `steps`  `50`
   * `cfg`  `5.0`
   * () `sampler`  `lcm`
   * () `scheduler`  `normal`
6.  `Run`  `Ctrl(cmd) + Enter()` 

### HiDream-I1 dev 

#### 1. 

 `ComfyUI/models/diffusion_models/` 

* FP8 [hidream\_i1\_dev\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_fp8.safetensors?download=true)  16GB 
* [hidream\_i1\_dev\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_bf16.safetensors?download=true)  27GB 

#### 2. 

 ComfyUI 

![HiDream-I1 dev ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_dev.png)

#### 3. 

![HiDream-I1 dev ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg)


1. `Load Diffusion Model`  `hidream_i1_dev_fp8.safetensors` 
2. `QuadrupleCLIPLoader`  text encoder 
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. `Load VAE`  `ae.safetensors` 
4.  **dev**  `ModelSamplingSD3`  `shift`  `6.0`
5.  `Ksampler` 
   * `steps`  `28`
   * () `cfg`  `1.0`
   * () `sampler`  `lcm`
   * () `scheduler`  `normal`
6.  `Run`  `Ctrl(cmd) + Enter()` 

### HiDream-I1 fast 

#### 1. 

 `ComfyUI/models/diffusion_models/` 

* FP8 [hidream\_i1\_fast\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true)  16GB 
* [hidream\_i1\_fast\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true)  27GB 

#### 2. 

 ComfyUI 

![HiDream-I1 fast ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_fast.png)

#### 3. 

![HiDream-I1 fast ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg)



1. `Load Diffusion Model`  `hidream_i1_fast_fp8.safetensors` 
2. `QuadrupleCLIPLoader`  text encoder 
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. `Load VAE`  `ae.safetensors` 
4.  **fast**  `ModelSamplingSD3`  `shift`  `3.0`
5.  `Ksampler` 
   * `steps`  `16`
   * () `cfg`  `1.0`
   * () `sampler`  `lcm`
   * () `scheduler`  `normal`
6.  `Run`  `Ctrl(cmd) + Enter()` 

## 

*  HiDream-I1 
*  fast 

## 

### GGUF 

* [HiDream-I1-Full-gguf](https://huggingface.co/city96/HiDream-I1-Full-gguf)
* [HiDream-I1-Dev-gguf](https://huggingface.co/city96/HiDream-I1-Dev-gguf)

 City96  [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF)   `Unet Loader (GGUF)` `Load Diffusion Model`  GGUF 

* [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF)

### NF4 

* [HiDream-I1-nf4](https://github.com/hykilpikonna/HiDream-I1-nf4)
*  [ComfyUI-HiDream-Sampler](https://github.com/SanDiegoDude/ComfyUI-HiDream-Sampler)  NF4 


# ComfyUI 
Source: https://docs.comfy.org/zh-CN/tutorials/basic/image-to-image



## 

Image to Image ComfyUI 



* 
* 
* 
* 
* ... 




[](/zh-CN/tutorials/basic/text-to-image)

## ComfyUI 

### 1. 

 `ComfyUI/models/checkpoints`  SD1.5 [ ComfyUI  AI ](/zh-CN/get_started/first_generation#3-)



* [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)
* [Dreamshaper 8](https://civitai.com/models/4384?modelVersionId=128713)
* [Anything V5](https://civitai.com/models/9409?modelVersionId=30163)

### 2. 

 ** ComfyUI ** 

![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image_to_image.png)

 ComfyUI  **workflow template**  **image to image** 
![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/image-to-image-01-template.jpg)


![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/input.jpeg)

### 3. 

![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image-to-image-02-guide.jpg)



1.  **Load Checkpoint** 
2.  **Load Image**  `upload` 
3.  `Queue`  `Ctrl + Enter()` 

## 

1.  **KSampler**  `denoise`  1  0 
2. 

## 

 `KSampler`  `denoise`  ** 1**

 `denoise` 

* `denoise` 
* `denoise` 

 `denoise`  `denoise`  1`empty latent image`

[](/zh-CN/tutorials/basic/text-to-image)


# ComfyUI 
Source: https://docs.comfy.org/zh-CN/tutorials/basic/inpaint

 ComfyUI 

 AI  ComfyUI 

* 
*  ComfyUI 
*  VAE Encoder (for Inpainting)

## 

 AI 

 **(AI )**  ******()** **()**



* **** AI
* **** 
* 

## ComfyUI 

### 

#### 1. 

`ComfyUI/models/checkpoints`

* [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)

#### 2. 



![ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/input.png)

<Note> alpha </Note>

#### 3. 

 metadata json **** ComfyUI  **(Workflow)** --> **(Open, `Ctrl + O`)** 

![ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png)

### ComfyUI 

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_workflow.png)



1. 
2.  `Load Image` 
3.  `Queue`  `Ctrl + Enter()` 

![ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png)

[v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)  inpainting 

![ComfyUI  - SD1.5](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png)

 [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) 
 inpainting 



:

1.  
2.  `KSampler` 
3. 

 **(Mask Editor)** `alpha` **(Mask Editor)**  (Mask)

### (Mask Editor) 

`Save Image` `(Clipspace)` 

![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png)

 **(Load Image)** `Paste(Clipspace)` 

![ComfyUI  - ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png)

 **(Load Image)** `(Open in MaskEditor)` 

![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg)

![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint-maskeditor.gif)

1. 
2. 
3.  `Save` 

 (Mask)  VAE Encoder (for Inpainting) 



## 

[](/zh-CN/tutorials/basic/text-to-image)[](/zh-CN/tutorials/basic/image-to-image)  VAE ,
 **VAE ** 

![VAE Encoder (for Inpainting) ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/lantent/inpaint/vae_encode_for_inpainting.jpg)

****

|            |                                                      |
| -------------- | ------------------------------------------------------ |
| `pixels`       |                                          |
| `vae`          |  VAE                              |
| `mask`         |                                  |
| `grow_mask_by` |  |

****

|      |                 |
| -------- | ----------------- |
| `latent` |  VAE  |


# ComfyUI LoRA 
Source: https://docs.comfy.org/zh-CN/tutorials/basic/lora

 LoRA 

**LoRA Low-Rank Adaptation**  Stable Diffusion
 SD1.5 LoRA 

![LoRA ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/compare.png)

 [dreamshaper\_8](https://civitai.com/models/4384?modelVersionId=128713)  [blindbox\_V1Mix](https://civitai.com/models/25995/blindbox) LoRA  LoRA 

 LoRA  LoRA Lycoris, loha, lokr, locon, ... 

[ComfyUI](https://github.com/comfyanonymous/ComfyUI)  LoRA 

1.  LoRA 
2.  LoRA 
3. `Load LoRA` 

## 

 [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16)  `ComfyUI/models/checkpoints` 

 [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model\&format=SafeTensor\&size=full\&fp=fp16)  `ComfyUI/models/loras` 

## LoRA 

, ComfyUI 
![ComfyUI  - LoRA](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/lora.png)

<Tip>
  Metadata  json  ComfyUI  `Workflows` -> `Openctrl+o` 
</Tip>

## 



![ComfyUI  - LoRA ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/flow_diagram.png)

1. `Load Checkpoint`  `dreamshaper_8.safetensors`
2. `Load LoRA`  `blindbox_V1Mix.safetensors`
3.  `Queue`  `Ctrl(cmd) + Enter()` 

## Load LoRA 

![Load LoRA ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_lora.jpg)

`ComfyUI\models\loras`  ComfyUI 

### 

|              |                                         |
| ---------------- | ----------------------------------------- |
| `model`          |                                     |
| `clip`           |  CLIP                                 |
| `lora_name`      |  LoRA                           |
| `strength_model` |  LoRA  model LoRA  |
| `strength_clip`  |  LoRA  CLIP clip             |

### 

|     |                      |
| ------- | ---------------------- |
| `model` |  LoRA        |
| `clip`  |  LoRA  CLIP  |

`Load LoRA`  LoRA [ComfyUI  LoRA ](/zh-CN/tutorials/basic/multiple-loras)

![LoRA ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png)

## 

1.  `Load LoRA`   `strength_model` 
2.  [CivitAI](https://civitai.com/models)  LoRA 


# ComfyUI  LoRA 
Source: https://docs.comfy.org/zh-CN/tutorials/basic/multiple-loras

 ComfyUI  LoRA 

 [ComfyUI LoRA ](/zh-CN/tutorials/basic/lora)  ComfyUI  LoRA 

![LoRA ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png)

`Load LoRA` LoRA  [blindbox\_V1Mix](https://civitai.com/models/25995?modelVersionId=32988)  [MoXinV1](https://civitai.com/models/12597?modelVersionId=14856)  LoRA 

 LoRA 

![ComfyUI  LoRA ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/compare.png)

 LoRA 

![ComfyUI  LoRA ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png)

## 

 [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16)  `ComfyUI/models/checkpoints` 

 [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model\&format=SafeTensor\&size=full\&fp=fp16)  `ComfyUI/models/loras` 

 [MoXinV1.safetensors](https://civitai.com/api/download/models/14856?type=Model\&format=SafeTensor\&size=full\&fp=fp16)  `ComfyUI/models/loras` 

##  LoRA 

, ComfyUI 
![ComfyUI  -  LoRA ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png)

<Tip>
  Metadata  json  ComfyUI  `Workflows` -> `Openctrl+o` 
</Tip>

## 



![ComfyUI  -  LoRA ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/flow_diagram.png)

1. `Load Checkpoint` **dreamshaper\_8.safetensors**
2. `Load LoRA` **blindbox\_V1Mix.safetensors**
3. `Load LoRA` **MoXinV1.safetensors**
4.  `Queue`  `Ctrl(cmd) + Enter()` 

## 

1.  `Load LoRA`  `strength_model`  LoRA 
2.  [CivitAI](https://civitai.com/models)  LoRA 


# ComfyUI Outpaint
Source: https://docs.comfy.org/zh-CN/tutorials/basic/outpaint

 ComfyUI 

 AI  ComfyUI 

* 
*  ComfyUI 
* 

## 

 AI 

 **(AI )**  ****

[](/zh-CN/tutorials/basic/inpaint)**Mask**



* **** 
* **** 
* **** 

## ComfyUI 

### 

#### 1. 

 `ComfyUI/models/checkpoints`  SD1.5 [ ComfyUI  AI ](/zh-CN/get_started/first_generation#3-)



* [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)
* [Dreamshaper 8](https://civitai.com/models/4384?modelVersionId=128713)
* [Anything V5](https://civitai.com/models/9409?modelVersionId=30163)

- [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)

#### 2. 



![ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/input.png)

#### 3. 

 **** ComfyUI  **(Workflow)** --> **(Open, `Ctrl + O`)** 

![ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpaint.png)

### 

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpainting_workflow.jpg)



1.  `(Load Checkpoint)` 
2.  `(Load Image)`  `Upload` 
3.  `Queue`  `Ctrl + Enter()` 

 `Pad Image for outpainting`  [(Inpaint)](/zh-CN/tutorials/basic/inpaint.mdx) Mask

### Pad Image for outpainting 

![Pad Image for outpainting ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/pad_image_for_outpainting.jpg)

Mask

#### 

|          |                            |
| ------------ | ---------------------------- |
| `image`      |                          |
| `left`       |                         |
| `top`        |                         |
| `right`      |                         |
| `bottom`     |                         |
| `feathering` |  |

#### 

|     |                      |
| ------- | ---------------------- |
| `image` | `image`      |
| `mask`  | `mask` |

#### 

 `Pad Image for outpainting` 

![Pad Image for outpainting ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg)



* `Image` 
* `Mask` 


# ComfyUI 
Source: https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image

 AI  ComfyUI 

 ComfyUI  ComfyUI 



* 
* 
* 
*  SD1.5 



## 

**(Text to Image)**  AI  ****



* **** 
* **** 
* \*\*\*\*

**()****()**

## ComfyUI 

### 1. 

 `ComfyUI/models/checkpoints`  SD1.5 [ ComfyUI  AI ](/zh-CN/get_started/first_generation#3-)



* [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)
* [Dreamshaper 8](https://civitai.com/models/4384?modelVersionId=128713)
* [Anything V5](https://civitai.com/models/9409?modelVersionId=30163)

### 2. 

 ComfyUI  **Workflows** -> **Open**  workflow

![ComfyUI-](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/text-to-image-workflow.png)

 **Workflows** -> **Browse example workflows**  **Text to Image** 

### 3. 



![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg)



1. **Load Checkpoint**  **v1-5-pruned-emaonly-fp16.safetensors** **null** 
2.  `Queue`  `Ctrl + Enter()` 

\*\*Save Image\*\*

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg)

<Tip>**KSampler**  `seed` </Tip>

### 4. 

**CLIP Text Encoder**

![CLIP Text Encoder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg)

 KSampler `Positive` KSampler `Negative`

 SD1.5 

* 
*  `,` 
* 
* 
*  `(golden hour:1.2)` `1.2` `golden hour` 
*  `masterpiece, best quality, 4k` 

 prompt  prompt  prompt 

**1. **



```
anime style, 1girl with long pink hair, cherry blossom background, studio ghibli aesthetic, soft lighting, intricate details

masterpiece, best quality, 4k
```



```
low quality, blurry, deformed hands, extra fingers
```

**2. **



```
(ultra realistic portrait:1.3), (elegant woman in crimson silk dress:1.2), 
full body, soft cinematic lighting, (golden hour:1.2), 
(fujifilm XT4:1.1), shallow depth of field, 
(skin texture details:1.3), (film grain:1.1), 
gentle wind flow, warm color grading, (perfect facial symmetry:1.3)
```



```
(deformed, cartoon, anime, doll, plastic skin, overexposed, blurry, extra fingers)
```

**3. **



```
fantasy elf, detailed character, glowing magic, vibrant colors, long flowing hair, elegant armor, ethereal beauty, mystical forest, magical aura, high detail, soft lighting, fantasy portrait, Artgerm style
```



```
blurry, low detail, cartoonish, unrealistic anatomy, out of focus, cluttered, flat lighting
```

## 

**** [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)  ****

```mermaid
graph LR
A[] --> B[]
B --> C[]
C --> D[]
E[] --> F[CLIP]
F --> G[]
G --> B
```



1. **** Latent Space
2. **** Pixel Space



* [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/pdf/2006.11239)
* [Denoising Diffusion Implicit Models (DDIM)](https://arxiv.org/pdf/2010.02502)
* [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752)

## ComfyUI 

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/text-image-workflow.jpg)

### A. Load Checkpoint

![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_checkpoint.jpg)

,  `checkpoint`  `MODELUNet``CLIP`  `VAE` 

* `MODELUNet` UNet , ,
* `CLIP`,prompt,prompt,
* `VAE`,,,,,

### B. LatentEmpty Latent Image

![Latent](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/lantent/empty_latent_image.jpg)

Latent Space, KSampler Latent ****



### C. CLIPCLIP Text Encoder

![CLIP](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg)



*  KSampler  `Positive` 
*  KSampler  `Negative` 

 `Load Checkpoint`  `CLIP`  KSampler 

### D. K KSampler

![K ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/sampling/k_sampler.jpg)

**K ** 

```mermaid
graph LR
A[<br>] --> B{KSampler}
C[] --> B
D[CLIP] --> B
B --> E[Latent]
```

KSampler 

|                          |          |                                    |
| ---------------------------- | ---------- | ------------------------------------ |
| **model**                    |   |                          |
| **positive**                 |   |                         |
| **negative**                 |   |                            |
| **latent\_image**            |  |                          |
| **seed**                     |   |                            |
| **control\_after\_generate** |   |                       |
| **steps**                    |      |                        |
| **cfg**                      |   |                    |
| **sampler\_name**            |      |                           |
| **scheduler**                |       |                         |
| **denoise**                  |      | 0.01.0 |

 KSampler  `seed` , `Positive`  `Negative` 

 `steps`  `denoise` 

### E. VAE VAE Decode

![VAE ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/lantent/vae_decode.jpg)

 **K (KSampler)** 

### F. Save Image

![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/save_image.jpg)

`ComfyUI/output`

## SD1.5 

**SD1.5(Stable Diffusion 1.5)** [Stability AI](https://stability.ai/)AIStable Diffusion **512512**  **512512** 4GB\*\*6GB\*\* SD1.5 ControlNetLoRA
AISD1.5SDXL/SD3

### 

* ****202210
* ****Latent Diffusion Model (LDM)
* ****LAION-Aesthetics v2.55.9
* ****//

### 



*  4GB 
* 
* 
* 



* /
* 1024x1024
* 


# ComfyUI 
Source: https://docs.comfy.org/zh-CN/tutorials/basic/upscale

 AI  ComfyUI 

## 

Image UpscalingAI  ESRGAN
 SD1.5 

 ComfyUI 

* 
* 
* 

## 

 ESRGAN 

<Steps>
  <Step title=" OpenModelDB">
     [OpenModelDB](https://openmodeldb.info/)  RealESRGAN

    ![openmodeldb](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_OpenModelDB.jpg)

    

    1. 
    2. 2

     [4x-ESRGAN](https://openmodeldb.info/models/4x-ESRGAN)  `Download` 

    ![OpenModelDB\_download](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_OpenModelDB_download.jpg)
  </Step>

  <Step title="">
    .pth `ComfyUI\models\upscale_models` 
  </Step>
</Steps>

## 

### 1. 

 ComfyUI 
![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_workflow.png)


![Upscale-input](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale-input.jpg)

### 2. 

![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_simple_workflow.jpg)

1. `(Load Upscale Model)`
2. `(Load Image)`
3.  `Queue`  `Ctrl + Enter()` 

 `Load Upscale Model`  `Upscale Image(Using Model)` 

## 

[](/zh-CN/tutorials/basic/text-to-image)[](/zh-CN/tutorials/basic/text-to-image)

 ComfyUI 
![](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/esrgan_example.png)



## 

<Tip>
  

  * **RealESRGAN**: 
  * **BSRGAN**: 
  * **SwinIR**: 
</Tip>

1. ****2x4x
2. ****"+"
3. ****


# ComfyUI ControlNet 
Source: https://docs.comfy.org/zh-CN/tutorials/controlnet/controlnet

 ControlNet  ComfyUI 

 AI  **ControlNet** 

ControlNet  Stable Diffusion[Lvmin Zhang](https://lllyasviel.github.io/) Maneesh Agrawala  2023 [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543)

ControlNet 
 ControlNet 

 ControlNet 

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/generated_with_random_seed.jpg)

 ControlNet 

![ComfyUI ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/scribble_example.jpg)

 [ComfyUI](https://github.com/comfyanonymous/ComfyUI)  ControlNet , 

![ComfyUI ControlNet ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)

<Tip>
  ControlNet V1.1  ControlNet 
</Tip>

## ControlNet 

 ControlNet 

![](https://github.com/Fannovel16/comfyui_controlnet_aux/blob/main/examples/CNAuxBanner.jpg?raw=true)

> [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

 **Comfy Core**  **** 
 ControlNet 

* [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
* [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## ComfyUI ControlNet 

### 1. ControlNet 

, ComfyUI 

![ComfyUI  - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)

<Tip>
  Metadata  json  ComfyUI  `Workflows` -> `Openctrl+o` 
   ComfyUI 
</Tip>



![ComfyUI ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_input.png)

### 2.  

<Note>
  
</Note>

* [dreamCreationVirtual3DECommerce\_v10.safetensors](https://civitai.com/api/download/models/731340?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)
* [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)

```
ComfyUI/
 models/
    checkpoints/
       dreamCreationVirtual3DECommerce_v10.safetensors
    vae/
       vae-ft-mse-840000-ema-pruned.safetensors
    controlnet/
        control_v11p_sd15_scribble_fp16.safetensors
```

<Note>
   vae  dreamCreationVirtual3DECommerce\_v10.safetensors  vae  vae 
</Note>

### 3. 

![ComfyUI  - ControlNet ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_scribble.png)

1. `Load Checkpoint` **dreamCreationVirtual3DECommerce\_v10.safetensors**
2. `Load VAE` **vae-ft-mse-840000-ema-pruned.safetensors**
3. `Load Image``Upload` 
4. `Load ControlNet` **control\_v11p\_sd15\_scribble\_fp16.safetensors**
5.  `Queue`  `Ctrl(cmd) + Enter()` 

## 

### Load ControlNet 

![load controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_controlnet_model.jpg)

`ComfyUI\models\controlnet`  ComfyUI 

### Apply ControlNet 

![apply controlnet ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg)

 `load controlnet`  ControlNet 

****

|             |                                                                 |
| --------------- | ----------------------------------------------------------------- |
| `positive`      |                                                               |
| `negative`      |                                                               |
| `control_net`   | controlNet                                                  |
| `image`         |  controlNet                                        |
| `vae`           | Vae                                                           |
| `strength`      |  ControlNet  ControlNet                        |
| `start_percent` | controlNet0.2ControlNet20% |
| `end_percent`   | controlNet0.8ControlNet80% |

****

|        |                         |
| ---------- | ------------------------- |
| `positive` |  ControlNet  |
| `negative` |  ControlNet  |

 ControlNet  [ ControlNet ](/zh-CN/tutorials/controlnet/mixing-controlnets.mdx)  ControlNet 
![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg)

<Note>
  `Apply ControlNet(Old)`  ControlNet 
  ![apply controlnet old](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/controlnet/apply_controlnet_old.jpg)
  ****--> **comfy** --> **Node** `Show deprecated nodes in search` 
</Note>

## 

1.  ControlNet  ControlNet 
2.  Apply ControlNet  `Control Strength`  ControlNet 
3.  [ControlNet-v1-1\_fp16\_safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/tree/main)   ControlNet 


# ComfyUI Depth ControlNet 
Source: https://docs.comfy.org/zh-CN/tutorials/controlnet/depth-controlnet

 Depth ControlNet  ComfyUI 

##  Depth ControlNet 

(Depth Map)

![Depth ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png)

Depth ControlNet  ControlNet  AI 

###  ControlNet 



1. ****
2. ****
3. ****
4. ****



## ComfyUI ControlNet 

### 1. ControlNet 

, ComfyUI 

![Depth ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_controlnet.png)

<Tip>
  Metadata  json  ComfyUI  `Workflows` -> `Openctrl+o` 
   ComfyUI 
</Tip>



![Depth ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png)

### 2. 

<Note>
  
</Note>

* [architecturerealmix\_v11.safetensors](https://civitai.com/api/download/models/431755?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [control\_v11f1p\_sd15\_depth\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors?download=true)

```
ComfyUI/
 models/
    checkpoints/
       architecturerealmix_v11.safetensors
    controlnet/
        control_v11f1p_sd15_depth_fp16.safetensors
```

### 3. 

![ComfyUI  - Depth ControlNet ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth.jpg)

1. `Load Checkpoint` **architecturerealmix\_v11.safetensors**
2. `Load ControlNet` **control\_v11f1p\_sd15\_depth\_fp16.safetensors**
3. `Load Image``Upload`  Depth 
4.  `Queue`  `Ctrl(cmd) + Enter()` 

## 

 ControlNet  ControlNet 

1. **Depth + Lineart**
2. **Depth + Pose**

 ControlNet  [ ControlNet](/zh-CN/tutorials/controlnet/mixing-controlnets.mdx) 


# ComfyUI Depth T2I Adapter 
Source: https://docs.comfy.org/zh-CN/tutorials/controlnet/depth-t2i-adapter

 Depth T2I Adapter  ComfyUI 

## T2I Adapter 

[T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter)  [ARC](https://github.com/TencentARC) Stable Diffusion
77M300MB [ControlNet](https://github.com/lllyasviel/ControlNet-v1-1-nightly) 3+

### T2I Adapter  ControlNet 



1. ****T2I Adapter 
2. ****T2I Adapter  ControlNet 3
3. ****ControlNet  T2I Adapter 
4. ****T2I Adapter 

### T2I Adapter 

T2I Adapter 

* ** (Depth)**
* ** (Canny/Sketch)**
* ** (Keypose)**
* ** (Seg)**
* ** (Color)**

 ComfyUI  T2I Adapter  [ControlNet](/zh-CN/tutorials/controlnet/controlnet.mdx)  T2I Adapter 

![ComfyUI Depth T2I Adapter ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)

##  T2I Adapter 

Depth Map

1. ****
2. ****
3. ****
4. ****

 T2I Adapter 

## ComfyUI Depth T2I Adapter

### 1. Depth T2I Adapter 

, ComfyUI 

![ComfyUI  - Depth T2I Adapter](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)

<Tip>
  Metadata  json  ComfyUI  `Workflows` -> `Openctrl+o` 
   ComfyUI 
</Tip>



![ComfyUI ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png)

### 2. 

<Note>
  
</Note>

* [interiordesignsuperm\_v2.safetensors](https://civitai.com/api/download/models/93152?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [t2iadapter\_depth\_sd15v2.pth](https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd15v2.pth?download=true)

```
ComfyUI/
 models/
    checkpoints/
       interiordesignsuperm_v2.safetensors
    controlnet/
        t2iadapter_depth_sd15v2.pth
```

### 3. 

![ComfyUI  - Depth T2I Adapter ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg)

1. `Load Checkpoint` **interiordesignsuperm\_v2.safetensors**
2. `Load ControlNet` **t2iadapter\_depth\_sd15v2.pth**
3. `Load Image``Upload` 
4.  `Queue`  `Ctrl(cmd) + Enter()` 

## T2I Adapter 

### 

 T2I Adapter 

1. ****
2. ****
3. ****
4. ****

## T2I Adapter 

T2I Adapter 

1. ** + **
2. ** + **
3. ** + **

T2I Adapter ControlNet [ ControlNet](/zh-CN/tutorials/controlnet/mixing-controlnets.mdx)  `Apply ControlNet` 


# ComfyUI ControlNet 
Source: https://docs.comfy.org/zh-CN/tutorials/controlnet/mixing-controlnets

 ControlNet  ControlNet 

 AI  ControlNet 

 ControlNet 

1. ****
2. **** ControlNet 
3. **** ControlNet 
4. ****

###  ControlNet 

 ControlNet  ControlNet ComfyUI  `Apply ControlNet`  ControlNet 

![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg)

## ComfyUI ControlNet 

 **Pose ControlNet**  **Scribble ControlNet**  Pose ControlNet  Scribble ControlNet 

### 1. ControlNet 

, ComfyUI 
![ComfyUI  - Mixing ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets.png)

<Tip>
   Metadata  ComfyUI  `Workflows` -> `Openctrl+o` 
</Tip>

 pose :

![ComfyUI  - Mixing ControlNet ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input.png)

 scribble :

![ComfyUI  - Mixing ControlNet ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input_scribble.png)

### 2. 

<Note>
  
</Note>

* [awpainting\_v14.safetensors](https://civitai.com/api/download/models/624939?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)
* [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```
ComfyUI/
 models/
    checkpoints/
       awpainting_v14.safetensors
    controlnet/
       control_v11p_sd15_scribble_fp16.safetensors
       control_v11p_sd15_openpose_fp16.safetensors
    vae/
       vae-ft-mse-840000-ema-pruned.safetensors
```

### 3. 

![ComfyUI  - Mixing ControlNet ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg)



1. `Load Checkpoint` **awpainting\_v14.safetensors**
2. `Load VAE` **vae-ft-mse-840000-ema-pruned.safetensors**

 ControlNet  Openpose :
3\. `Load ControlNet Model` **control\_v11p\_sd15\_openpose\_fp16.safetensors**
4\. `Load Image``Upload`  pose 

 ControlNet  Scribble :
5\. `Load ControlNet Model` **control\_v11p\_sd15\_scribble\_fp16.safetensors**
6\. `Load Image``Upload`  scribble 
7\.  `Queue`  `Ctrl(cmd) + Enter()` 

## 

#### 



*  ControlNet 
*  ControlNet  1.0

#### 



```
"A woman in red dress, a cat riding a scooter, detailed background, high quality"
```



## 



* **Pose + Depth**
* **Pose + Canny**
* **Pose + Reference**

 ControlNet 

 ControlNet 


# ComfyUI Pose ControlNet 
Source: https://docs.comfy.org/zh-CN/tutorials/controlnet/pose-controlnet-2-pass

 Pose ControlNet ComfyUI 

## OpenPose 

[OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) CMU

* ****18
* ****70
* ****21
* ****6

![OpenPose ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/openpose_example.jpg)

 AI OpenPose  ControlNet  AI 
 Stable diffusion 1.5  OpenPose 

## ComfyUI 2 Pass Pose ControlNet 

### 1. Pose ControlNet 

, ComfyUI 

![ComfyUI  - Pose ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass.png)

<Tip>
  Metadata  json  ComfyUI  `Workflows` -> `Openctrl+o` 
   ComfyUI 
</Tip>



![ComfyUI Pose ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass_input.png)

### 2. 

<Note>
  
</Note>

* [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
* [majicmixRealistic\_v7.safetensors](https://civitai.com/api/download/models/176425?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16)
* [japaneseStyleRealistic\_v20.safetensors](https://civitai.com/api/download/models/85426?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```
ComfyUI/
 models/
    checkpoints/
       majicmixRealistic_v7.safetensors
       japaneseStyleRealistic_v20.safetensors
    vae/
       vae-ft-mse-840000-ema-pruned.safetensors
    controlnet/
        control_v11p_sd15_openpose_fp16.safetensors
```

### 3. 

![ComfyUI  - Pose ControlNet ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg)



1. `Load Checkpoint` **majicmixRealistic\_v7.safetensors**
2. `Load VAE` **vae-ft-mse-840000-ema-pruned.safetensors**
3. `Load ControlNet Model` **control\_v11p\_sd15\_openpose\_fp16.safetensors**
4. `Load Image`OpenPose
5. `Load Checkpoint` **japaneseStyleRealistic\_v20.safetensors**
6. `Queue``Ctrl(cmd) + Enter()`

## Pose ControlNet 

2-pass

### 

**majicmixRealistic\_v7**Pose ControlNet

1. `Load Checkpoint`majicmixRealistic\_v7
2. `Load ControlNet Model`
3. `Apply ControlNet`
4. `KSampler`20-30
5. `VAE Decode`



### 

**japaneseStyleRealistic\_v20**

1. `Upscale latent`
2. `Load Checkpoint`japaneseStyleRealistic\_v20
3. `KSampler``denoise`0.4-0.6
4. `VAE Decode``Save Image`



## 



1. ****
2. ****
3. ****
4. ****
5. **GPU**GPU

<Tip>
  ControlNet[ControlNet](/zh-CN/tutorials/controlnet/mixing-controlnets.mdx)
</Tip>


# ComfyUI Flux.1 ControlNet 
Source: https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-controlnet

 Flux.1 ControlNet  ControlNet 

![Flux.1 Canny Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-canny-controlnet.png)
![Flux.1 Depth Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-depth-controlnet.png)

## FLUX.1 ControlNet 

FLUX.1 Canny  Depth  [Black Forest Labs](https://blackforestlabs.ai/)  [FLUX.1 Tools ](https://blackforestlabs.ai/flux-1-tools/)  FLUX.1 

**FLUX.1-Depth-dev**  **FLUX.1-Canny-dev**  12B  Rectified Flow Transformer  Depth  Canny 



* 
* 
* 
* 
*  API pro dev 

Black Forest Labs  **FLUX.1-Depth-dev-lora**  **FLUX.1-Canny-dev-lora**  FLUX.1 \[dev] 

 **FLUX.1-Canny-dev**   **FLUX.1-Depth-dev-lora** ComfyUI  Flux  ControlNet 

<Tip>
  Metadata  json  ComfyUI  `Workflows` -> `Openctrl+o` 
   ComfyUI 

  

  * [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
  * [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)
</Tip>

## FLUX.1-Canny-dev 

### 1. 

, ComfyUI 

![ComfyUI  - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev.png)



![ComfyUI Flux.1 Canny Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev-input.png)

### 2. 

<Note>
  [ Flux ](/zh-CN/tutorials/flux/flux-1-text-to-image) **flux1-canny-dev.safetensors** 
   [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev)  [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev) 
  ![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux1_canny_dev_agreement.jpg)
</Note>



* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-canny-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/resolve/main/flux1-canny-dev.safetensors?download=true)  repo 



```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       t5xxl_fp16.safetensors
    vae/
       ae.safetensors
    diffusion_models/
        flux1-canny-dev.safetensors
```

### 3. 

![ComfyUI Flux.1 Canny Controlnet ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg)

1. `Load VAE``ae.safetensors`
2. `Load Diffusion Model``flux1-canny-dev.safetensors`
3. `DualCLIPLoader`
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
4. `Load Image`
5.  `Queue`  `Ctrl(cmd) + Enter()` 

### 4. 

[FLUX.1-Depth-dev](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev)  Depth 


![ComfyUI ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png)

:

* [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
* [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## FLUX.1-Depth-dev-lora 

LoRA  LoRA ,[ Flux ](/zh-CN/tutorials/flux/flux-1-text-to-image), LoRA 

### 1. 

, ComfyUI 

![ComfyUI  - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora.png)



![ComfyUI Flux.1 Depth Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora-input.png)

### 2. 

<Tip>
  [ Flux ](/zh-CN/tutorials/flux/flux-1-text-to-image) **flux1-depth-dev-lora.safetensors** 
</Tip>



* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors?download=true)
* [flux1-depth-dev-lora.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora/resolve/main/flux1-depth-dev-lora.safetensors?download=true)



```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       t5xxl_fp16.safetensors
    vae/
       ae.safetensors
    diffusion_models/
       flux1-dev.safetensors
    loras/
        flux1-depth-dev-lora.safetensors
```

### 3. 

![ComfyUI Flux.1 Depth Controlnet ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg)

1. `Load Diffusion Model``flux1-dev.safetensors`
2. `LoraLoaderModelOnly``flux1-depth-dev-lora.safetensors`
3. `DualCLIPLoader`
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
4. `Load Image`
5. `Load VAE``ae.safetensors`
6.  `Queue`  `Ctrl(cmd) + Enter()` 

### 4. 

[FLUX.1-Canny-dev-lora](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora)  Canny 

 [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)  [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux) 

##  Flux Controlnets

XLab  InstantX + Shakker Labs  Flux  Controlnet

**InstantX:**

* [FLUX.1-dev-Controlnet-Canny](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Canny/blob/main/diffusion_pytorch_model.safetensors)
* [FLUX.1-dev-ControlNet-Depth](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Depth/blob/main/diffusion_pytorch_model.safetensors)
* [FLUX.1-dev-ControlNet-Union-Pro](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/blob/main/diffusion_pytorch_model.safetensors)

**XLab**: [flux-controlnet-collections](https://huggingface.co/XLabs-AI/flux-controlnet-collections)

 `ComfyUI/models/controlnet` 

[Flux Controlnet ](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/flux_controlnet_example.png)[](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/girl_in_field.png)


# ComfyUI Flux.1 fill dev 
Source: https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-fill-dev

Flux.1 fill dev  Inpainting  Outpainting 

![Flux.1 fill dev](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-fill-dev-demo.jpeg)

## Flux.1 fill dev 

Flux.1 fill dev  [Black Forest Labs](https://blackforestlabs.ai/)  [FLUX.1 Tools ](https://blackforestlabs.ai/flux-1-tools/) 

Flux.1 fill dev 

* (Inpainting)(Outpainting) FLUX.1 Fill \[pro]
* 
* 
*  [FLUX.1 \[dev\] ](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md)

[FLUX.1 \[dev\]](https://huggingface.co/black-forest-labs/FLUX.1-dev)

 Flux.1 fill dev  Inpainting  Outpainting 
 Inpainting  Outpainting  [ComfyUI ](/zh-CN/tutorials/basic/inpaint)  [ComfyUI ](/zh-CN/tutorials/basic/outpaint)

## Flux.1 Fill dev 

 Flux.1 Fill dev  inpainting  outpainting  [Flux.1 ](/zh-CN/tutorials/flux/flux-1-text-to-image) **flux1-fill-dev.safetensors** 

 [black-forest-labs/FLUX.1-Fill-dev](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev)
![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux1_fill_dev_agreement.jpg)



* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-fill-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/resolve/main/flux1-fill-dev.safetensors?download=true)



```
ComfyUI/
 models/
    text_encoders/
        clip_l.safetensors
        t5xxl_fp16.safetensors
    vae/
        ae.safetensors
    diffusion_models/
         flux1-fill-dev.safetensors
```

## Flux.1 Fill dev inpainting 

### 1. Inpainting 

 ComfyUI 
![ComfyUI Flux.1 inpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint.png)


![ComfyUI Flux.1 inpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input.png)

<Note>
   alpha , [](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input_original.png) [ComfyUI ](/zh-CN/tutorials/basic/inpaint)  MaskEditor `Load Image`
</Note>

### 2. 

![ComfyUI Flux.1 Fill dev Inpainting ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_inpaint.jpg)

1. `Load Diffusion Model``flux1-fill-dev.safetensors`
2. `DualCLIPLoader`
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
3. `Load VAE``ae.safetensors`
4. `Load Image`
5.  `Queue`  `Ctrl(cmd) + Enter()` 

## Flux.1 Fill dev Outpainting 

### 1. Outpainting 

 ComfyUI 
![ComfyUI Flux.1 outpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint.png)


![ComfyUI Flux.1 outpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint_input.png)

### 2. 

![ComfyUI Flux.1 Fill dev Outpainting ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_outpaint.jpg)

1. `Load Diffusion Model``flux1-fill-dev.safetensors`
2. `DualCLIPLoader`
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
3. `Load VAE``ae.safetensors`
4. `Load Image`
5.  `Queue`  `Ctrl(cmd) + Enter()` 


# ComfyUI Flux 
Source: https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-text-to-image

 Flux  Flux  FP8 Checkpoint 

![Flux](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_example.png)
Flux AI 12B 23GB [Black Forest Labs](https://blackforestlabs.ai/)  Stable Diffusion 
Flux 

 Flux.1  

* **Flux.1 Pro**  API 
* **[Flux.1 \[dev\]](https://huggingface.co/black-forest-labs/FLUX.1-dev)**  Pro Pro
* \*\*[Flux.1 \[schnell\]](https://huggingface.co/black-forest-labs/FLUX.1-schnell)\*\* Apache2.0 4

**Flux.1 **

* ****  Transformer 
* **** Flux  12B 
* **** 

 Flux.1 Dev  Flux.1 Schnell  FP8 Checkpoint 

* **Flux ** 16GB
* **Flux FP8 Checkpoint**  fp8 

<Tip>
   Metadata 

  *  ComfyUI
  *  `Workflows` -> `Openctrl+o`

   Desktop 
   ComfyUI 
</Tip>

## Flux.1 

<Note>
   [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev)  Huggingface  Repo 
  ![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_agreement.jpg)
</Note>

### Flux.1 Dev 

#### 1. 

 ComfyUI 
![Flux Dev ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_t5fp16.png)

#### 2. 

<Note>
  * `flux1-dev.safetensors`  [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) 
  *  [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true)  `t5xxl_fp16.safetensors` 
</Note>



* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)  32GB 
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors)



```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       t5xxl_fp16.safetensors
    vae/
       ae.safetensors
    diffusion_models/
        flux1-dev.safetensors
```

#### 3. 



![ComfyUI Flux Dev](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg)

1. `DualCLIPLoader`
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
2. `Load Diffusion Model``flux1-dev.safetensors`
3. `Load VAE``ae.safetensors`
4.  `Queue`  `Ctrl(cmd) + Enter()` 

<Tip>
   Flux 
</Tip>

### Flux.1 Schnell 

#### 1. 

 ComfyUI 

![Flux Schnell ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_t5fp8.png)

#### 2. 

<Note>
   Flux1 Dev , t5xxl  fp16 

  * **t5xxl\_fp16.safetensors** -> **t5xxl\_fp8.safetensors**
  * **flux1-dev.safetensors** -> **flux1-schnell.safetensors**
</Note>



* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-schnell.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors)



```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       t5xxl_fp8_e4m3fn.safetensors
    vae/
       ae.safetensors
    diffusion_models/
        flux1-schnell.safetensors
```

#### 3. 

![Flux Schnell ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg)

1. `DualCLIPLoader`
   * clip\_name1: t5xxl\_fp8\_e4m3fn.safetensors
   * clip\_name2: clip\_l.safetensors
2. `Load Diffusion Model``flux1-schnell.safetensors`
3. `Load VAE``ae.safetensors`
4.  `Queue`  `Ctrl(cmd) + Enter()` 

## Fp8 Checkpoint 

fp8  flux1  fp16  fp16 

### Flux.1 Dev fp8 Checkpoint 

 ComfyUI 

![Flux Dev fp8 Checkpoint ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_fp8.png)

 [flux1-dev-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors?download=true) `ComfyUI/models/Checkpoints/` 

 `Load Checkpoint`  `flux1-dev-fp8.safetensors`

### Flux.1 Schnell fp8 Checkpoint 

 ComfyUI 

![Flux Schnell fp8 Checkpoint ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_fp8.png)

[flux1-schnell-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-schnell/resolve/main/flux1-schnell-fp8.safetensors?download=true) `ComfyUI/models/Checkpoints/` 

 `Load Checkpoint`  `flux1-schnell-fp8.safetensors`


# ComfyUI 
Source: https://docs.comfy.org/zh-CN/tutorials/video/hunyuan-video

 ComfyUI 

<video controls className="w-full aspect-video" src="https://github.com/user-attachments/assets/442afb73-3092-454f-bc46-02361c285930" />

Hunyuan Video[](https://huggingface.co/tencent)[](https://github.com/Tencent/HunyuanVideo)
[](https://github.com/Tencent/HunyuanVideo-I2V) 13B



* **** SoraDiTDiffusion Transformer
* **3D VAE**  3D VAE 
* **--**  MLLM 

[](https://github.com/Tencent/HunyuanVideo) [-I2V](https://github.com/Tencent/HunyuanVideo-I2V) 

 ComfyUI  ****  **** 

<Tip>
   Metadata  ComfyUI  `Workflows` -> `Openctrl+o` 

    Desktop 

  [](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/tree/main/split_files)
</Tip>

## 



* [clip\_l.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors?download=true)
* [llava\_llama3\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors?download=true)
* [hunyuan\_video\_vae\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors?download=true)



```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       llava_llama3_fp8_scaled.safetensors
    vae/
       hunyuan_video_vae_bf16.safetensors
```

## 

 2024  12  5 

### 1. 

 ComfyUI 
![ComfyUI  - ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/t2v/kitchen.webp)

### 2. 

 [hunyuan\_video\_t2v\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_t2v_720p_bf16.safetensors?download=true)  `ComfyUI/models/diffusion_models` 



```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors                       // 
       llava_llama3_fp8_scaled.safetensors      // 
    vae/
       hunyuan_video_vae_bf16.safetensors       // 
    diffusion_models/
        hunyuan_video_t2v_720p_bf16.safetensors  // T2V 
```

### 3. 

![ComfyUI  T2V ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg)

1. `DualCLIPLoader`
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. `Load Diffusion Model``hunyuan_video_t2v_720p_bf16.safetensors`
3. `Load VAE``hunyuan_video_vae_bf16.safetensors`
4.  `Queue`  `Ctrl(cmd) + Enter()` 

<Tip>
  `EmptyHunyuanLatentVideo`  `length`  1 
</Tip>

## 

202536 HunyuanVideo  LoRA 



* v1 concat : 
* v2 replace: v1  V1 

<div class="flex justify-between">
  <div class="text-center">
    <p>v1 concat</p>

    <img src="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video.webp" alt="HunyuanVideo v1" />
  </div>

  <div class="text-center">
    <p>v2 replace</p>

    <img src="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video_v2.webp" alt="HunyuanVideo v2" />
  </div>
</div>

### v1  v2 

 `ComfyUI/models/clip_vision` 

* [llava\_llama3\_vision.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors?download=true)

### v1 concat 

#### 1. 

, ComfyUI 
![ComfyUI  - v1](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v1_robot.webp)


![](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/hunyuan-video/i2v/robot-ballet.png)

#### 2. v1 

* [hunyuan\_video\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_image_to_video_720p_bf16.safetensors?download=true)



```
ComfyUI/
 models/
    clip_vision/
       llava_llama3_vision.safetensors                     // I2V 
    text_encoders/
       clip_l.safetensors                                  //  
       llava_llama3_fp8_scaled.safetensors                 //  
    vae/
       hunyuan_video_vae_bf16.safetensors                  // 
    diffusion_models/
        hunyuan_video_image_to_video_720p_bf16.safetensors  // I2V v1 "concat" 
```

#### 3. 

![ComfyUI I2V v1 ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg)

1.  `DualCLIPLoader` 
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2.   `Load CLIP Vision`  `llava_llama3_vision.safetensors`
3.  `Load Image Model`  `hunyuan_video_image_to_video_720p_bf16.safetensors`
4.  `Load VAE`  `hunyuan_video_vae_bf16.safetensors`
5.  `Load Diffusion Model`  `hunyuan_video_image_to_video_720p_bf16.safetensors`
6.  `Queue`  `Ctrl(cmd) + Enter()` 

### v2 replace 

v2  v1  **replace**  `Load Diffusion Model` 

#### 1. 

, ComfyUI 

![ComfyUI  - v1](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v2_fennec_gril.webp)


![](https://comfyanonymous.github.io/ComfyUI_examples/flux/flux_dev_example.png)

#### 2. v2 

* [hunyuan\_video\_v2\_replace\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors?download=true)



```
ComfyUI/
 models/
    clip_vision/
       llava_llama3_vision.safetensors                                // I2V 
    text_encoders/
       clip_l.safetensors                                             //  
       llava_llama3_fp8_scaled.safetensors                            //  
    vae/
       hunyuan_video_vae_bf16.safetensors                             //  
    diffusion_models/
        hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors  // V2 "replace" 
```

#### 3. 

![ComfyUI I2V v2 ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg)

1.  `DualCLIPLoader` 
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2.  `Load CLIP Vision`  `llava_llama3_vision.safetensors`
3.  `Load Image Model`  `hunyuan_video_image_to_video_720p_bf16.safetensors`
4.  `Load VAE`  `hunyuan_video_vae_bf16.safetensors`
5.  `Load Diffusion Model`  `hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors`
6.  `Queue`  `Ctrl(cmd) + Enter()` 

## 



![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png)

```
Futuristic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene
```

***

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/samurai.png)

```
Samurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic
```

***

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/a_flying_car.png)

```
flying car fastly moving and flying through the city
```

***

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png)

```
cyberpunk car race in night city, dynamic, super fast, fast shot
```


# LTX-Video
Source: https://docs.comfy.org/zh-CN/tutorials/video/ltxv



<Tip>
   ComfyUI 
</Tip>

## 

[LTX-Video](https://huggingface.co/Lightricks/LTX-Video)  Lightricks 



 [ltx-video-2b-v0.9.5.safetensors](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors?download=true)  `ComfyUI/models/checkpoints` 

 [t5xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/mochi_preview_repackaged/resolve/main/split_files/text_encoders/t5xxl_fp16.safetensors?download=true)  `ComfyUI/models/text_encoders` 

## 

[](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house1.png)  [](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house2.png)

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/workflow.webp" alt="LTX-Video " />

## 

[](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/girl1.png)

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/workflow.webp" alt="LTX-Video " />

## 

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/t2v.webp" alt="LTX-Video " />


# ComfyUI Wan2.1 Fun Control 
Source: https://docs.comfy.org/zh-CN/tutorials/video/wan/fun-control

 ComfyUI  Wan2.1 Fun Control 

##  Wan2.1-Fun-Control

**Wan2.1-Fun-Control** Control Codes

 Fun Control  **Canny****Depth****OpenPose****MLSD**  ****
 512768  1024 16  81  5 



* **1.3B** ****
* **14B**  32GB+ ****



* [Wan2.1-Fun-1.3B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control)
* [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control)
* [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

** ComfyUI  Wan2.1 Fun Control **  ComfyUI [](https://github.com/comfyanonymous/ComfyUI/commit/3661c833bcc41b788a7c9f0e7bc48524f8ee5f82)



*  Comfy Core 
* 

<Tip>
  , 
</Tip>

## 



 [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged)  [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334) 

 Wan  **Diffusino models**

**Diffusion models**  1.3B  14B, 14B 32GB

* [wan2.1\_fun\_control\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_control_1.3B_bf16.safetensors?download=true)
* [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/blob/main/diffusion_pytorch_model.safetensors?download=true)  `Wan2.1-Fun-14B-Control.safetensors`

**Text encoders** fp16 

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)



```
 ComfyUI/
  models/
     diffusion_models/
       wan2.1_fun_control_1.3B_bf16.safetensors
     text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors
     vae/
       wan_2.1_vae.safetensors
     clip_vision/
         clip_vision_h.safetensors                 
```

## ComfyUI 

 WebP `Load Image`  mp4  Canny Edge , 

 ComfyUI [](#)

### 1. 

#### 1.1 

 ComfyUI 

![Wan2.1 Fun Control ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_native.webp)

#### 1.2 



![](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_remix.png)

![](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_video.webp)

### 2. 

![Wan2.1 Fun Control ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_native_flow_diagram.png)

1.  `Load Diffusion Model`  `wan2.1_fun_control_1.3B_bf16.safetensors`
2.  `Load CLIP`  `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3.  `Load VAE`  `wan_2.1_vae.safetensors`
4.  `Load CLIP Vision`  `clip_vision_h.safetensors `
5.  `Load Image` `Start_image` 
6.  `Load Image`   mp4  Webp 
7.  Prompt 
8.  `WanFunControlToVideo` 
9.  `Run`  `Ctrl(cmd) + Enter()` 

### 3. 

*  `WanFunControlToVideo`  [](#)
*  [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) 

## 



* [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
* [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

 [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) 

### 1. 

#### 1.1 

 ComfyUI 

![](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.webp)

<Note>
  [](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.json) Json 
</Note>

#### 1.2 


![](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/02-robot's_eye.png)

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/02-man's_eye.mp4" />

### 2. 

![Wan2.1 Fun Control ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png)

> 

1.  `Load Diffusion Model`  `wan2.1_fun_control_1.3B_bf16.safetensors`
2.  `Load CLIP`  `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3.  `Load VAE`  `wan_2.1_vae.safetensors`
4.  `Load CLIP Vision`  `clip_vision_h.safetensors `
5.  `Load Image` 
6.  `Load Video(Upload)`  mp4  `frame_load_cap`
7. `DWPose Estimator`  `detect_face` 
8.  Prompt 
9.  `WanFunControlToVideo` 
10.  `Run`  `Ctrl(cmd) + Enter()` 

### 3. 

 ComfyUI 

*  `Load Video(Upload)`  mp4 
* `Load Video(Upload)`  `video_info`  `fps`
*  `DWPose Estimator`  `ComfyUI-comfyui_controlnet_aux` 

## 

![Apply Multi Control Videos](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/apply_multi_control_videos.jpg)

*  `Image Blend` 
*  `ComfyUI-VideoHelperSuite`  `Video Combine`  mp4 
*  `SaveAnimatedWEBP`  **mp4** ,  `SaveAnimatedWEBP` 
* 
* 
*  `WanFunControlToVideo` `control_video`  320x320

##  Wan2.1 Fun Control 

* [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)
* [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)


# ComfyUI Wan2.1 Fun InP 
Source: https://docs.comfy.org/zh-CN/tutorials/video/wan/fun-inp

 ComfyUI  Wan2.1 Fun InP 

##  Wan2.1-Fun-InP

Wan-Fun InP  Wan2.1-Fun 

****

* 
* 51251276876810241024

****

* 1.3B 
* 14B  32GB+



* [Wan2.1-Fun-1.3B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Input)
* [Wan2.1-Fun-14B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Input)
* [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

<Tip>
   ComfyUI  Wan2.1 Fun InP  ComfyUI [](https://github.com/comfyanonymous/ComfyUI/commit/0a1f8869c9998bbfcfeb2e97aa96a6d3e0a2b5df)
</Tip>

## Wan2.1 Fun Control 

 ComfyUI 

![](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_inp/wan2.1_fun_inp.webp)

### 1. 

### 2. 



 [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged)  [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334) 

**Diffusion models**  1.3B  14B, 14B 32GB

* [wan2.1\_fun\_inp\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_inp_1.3B_bf16.safetensors?download=true)
* [Wan2.1-Fun-14B-InP](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP/resolve/main/diffusion_pytorch_model.safetensors?download=true)  `Wan2.1-Fun-14B-InP.safetensors`

**Text encoders** fp16 

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)



```
 ComfyUI/
  models/
     diffusion_models/
       wan2.1_fun_inp_1.3B_bf16.safetensors
     text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors
     vae/
       wan_2.1_vae.safetensors
     clip_vision/
         clip_vision_h.safetensors                 
```

### 3. 

![ComfyUI Wan2.1 Fun Control ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_inp_flow_diagram.png)

1.  `Load Diffusion Model`  `wan2.1_fun_inp_1.3B_bf16.safetensors`
2.  `Load CLIP`  `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3.  `Load VAE`  `wan_2.1_vae.safetensors`
4.  `Load CLIP Vision`  `clip_vision_h.safetensors `
5.  `Load Image` `Start_image` 
6.  `Load Image`   mp4  Webp 
7.  Prompt 
8.  `WanFunInpaintToVideo` 
9.  `Run`  `Ctrl(cmd) + Enter()` 

### 4. 

<Tip>
   `wan2.1_fun_inp_1.3B_bf16.safetensors`   `wan2.1_fun_control_1.3B_bf16.safetensors` 
</Tip>

*  Wan Fun InP 

##  Wan2.1 Fun Inp 

* [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
* [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)
* [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)


# ComfyUI Wan2.1 Video 
Source: https://docs.comfy.org/zh-CN/tutorials/video/wan/wan-video

 ComfyUI  Wan2.1 Video 

Wan2.1 Video  20252 [Apache 2.0](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file) 14B140 1.3B13T2VI2V
 8GB 

<video controls>
  <source src="https://github.com/user-attachments/assets/4aca6063-60bf-4953-bfb7-e265053f49ef" type="video/mp4" />
</video>

* [Wan2.1 ](https://github.com/Wan-Video/Wan2.1)
* [Wan2.1 ](https://huggingface.co/Wan-AI)

## Wan2.1 ComfyUI native

## 

[](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files), 

**Text encoders** 

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)



```
ComfyUI/
 models/
    diffusion_models/
    ...                  # 
    text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors
    vae/
        wan_2.1_vae.safetensors
    clip_vision/
         clip_vision_h.safetensors   
```

<Note>
   diffusion  fp16  bf16  fp16 [](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models)
</Note>

## Wan2.1 

 [wan2.1\_t2v\_1.3B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors?download=true) `ComfyUI/models/diffusion_models/` 

>  t2v [](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models)

### 1. 

 ComfyUI 

![Wan2.1 ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_t2v_1.3b.webp)

### 2. 

![ComfyUI Wan2.1 ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg)

1. `Load Diffusion Model` `wan2.1_t2v_1.3B_fp16.safetensors` 
2. `Load CLIP` `umt5_xxl_fp8_e4m3fn_scaled.safetensors` 
3. `Load VAE` `wan_2.1_vae.safetensors` 
4. `EmptyHunyuanLatentVideo` 
5. `5`  `CLIP Text Encoder` 
6.  `Run`  `Ctrl(cmd) + Enter()` 

## Wan2.1 

** Wan Video  480P  720P ** 

### 480P 

#### 1. 

 ComfyUI 
![Wan2.1  14B 480P Workflow ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_480P.webp)



![Wan2.1  14B 480P Workflow ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/flux_dev_example.png)

#### 2. 

[wan2.1\_i2v\_480p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp16.safetensors?download=true) `ComfyUI/models/diffusion_models/` 

#### 3. 

![ComfyUI Wan2.1 ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg)

1. `Load Diffusion Model` `wan2.1_i2v_480p_14B_fp16.safetensors` 
2. `Load CLIP` `umt5_xxl_fp8_e4m3fn_scaled.safetensors` 
3. `Load VAE` `wan_2.1_vae.safetensors` 
4. `Load CLIP Vision` `clip_vision_h.safetensors` 
5. `Load Image`
6. `CLIP Text Encoder`
7. `WanImageToVideo` 
8.  `Run`  `Ctrl(cmd) + Enter()` 

### 720P 

#### 1. 

 ComfyUI 
![Wan2.1  14B 720P Workflow ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_720P.webp)



![Wan2.1  14B 720P Workflow ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/magician.png)

#### 2. 

[wan2.1\_i2v\_720p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_720p_14B_fp16.safetensors?download=true) `ComfyUI/models/diffusion_models/` 

#### 3. 

![ComfyUI Wan2.1 ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg)

1. `Load Diffusion Model` `wan2.1_i2v_720p_14B_fp16.safetensors` 
2. `Load CLIP` `umt5_xxl_fp8_e4m3fn_scaled.safetensors` 
3. `Load VAE` `wan_2.1_vae.safetensors` 
4. `Load CLIP Vision` `clip_vision_h.safetensors` 
5. `Load Image`
6. `CLIP Text Encoder`
7. `WanImageToVideo` 
8.  `Run`  `Ctrl(cmd) + Enter()` 


