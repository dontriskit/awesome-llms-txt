# Get Agent Stats
Source: https://docs.play.ai/api-reference/agents/endpoints/v1/agent-stats/get

GET /api/v1/agent-stats/{agentId}
Retrieve the usage statistics of an agent.



# Get Agent
Source: https://docs.play.ai/api-reference/agents/endpoints/v1/agents/get

GET /api/v1/agents/{agentId}
Retrieve all information about an agent.



# Update Agent
Source: https://docs.play.ai/api-reference/agents/endpoints/v1/agents/patch

PATCH /api/v1/agents/{agentId}
Updates the properties of the agent with the specified ID.



# Create Agent
Source: https://docs.play.ai/api-reference/agents/endpoints/v1/agents/post

POST /api/v1/agents
Create a new PlayAI Agent.

Required parameters include the agent's name and the agent's prompt.

After you create your agent, you can proceed to start a conversation using our [Websocket API](/api-reference/agents/websocket), or you can try it
out through our web interface at `https://play.ai/agent/<your-agent-id>`.

To update the agents see the [Update Agent](/api-reference/agents/endpoints/v1/agents/patch) endpoint.


# Get Agent Conversations
Source: https://docs.play.ai/api-reference/agents/endpoints/v1/conversations/get

GET /api/v1/agents/{agentId}/conversations
Retrieve all information about an agent's conversations.

### Response Headers for Pagination

| Header Name          | Type    | Description                                   |
| -------------------- | ------- | --------------------------------------------- |
| `X-Page-Size`        | integer | The number of items per page.                 |
| `X-Start-After`      | string  | The ID of the last item on the previous page. |
| `X-Next-Start-After` | string  | The ID of the last item on the current page.  |
| `X-Total-Count`      | integer | The total number of items.                    |

These headers are included in the response to help manage pagination when retrieving conversations for a specific agent.


# Get Agent Conversation
Source: https://docs.play.ai/api-reference/agents/endpoints/v1/conversations/get-one

GET /api/v1/agents/{agentId}/conversations/{conversationId}
Retrieve all information about an agent conversation.



# Get Conversation Transcript
Source: https://docs.play.ai/api-reference/agents/endpoints/v1/conversations/get-transcript

GET /api/v1/agents/{agentId}/conversations/{conversationId}/transcript
Retrieve the transcript of a specific agent conversation.

### Response Headers for Pagination

| Header Name          | Type    | Description                                   |
| -------------------- | ------- | --------------------------------------------- |
| `X-Page-Size`        | integer | The number of items per page.                 |
| `X-Start-After`      | string  | The ID of the last item on the previous page. |
| `X-Next-Start-After` | string  | The ID of the last item on the current page.  |
| `X-Total-Count`      | integer | The total number of items.                    |

These headers are included in the response to help manage pagination when retrieving conversation transcript for a specific agent conversation.


# Delete External Function
Source: https://docs.play.ai/api-reference/agents/endpoints/v1/external-functions/delete

DELETE /api/v1/external-functions/{functionId}
Deletes the external function with the specified ID.



# Get All External Functions
Source: https://docs.play.ai/api-reference/agents/endpoints/v1/external-functions/get

GET /api/v1/external-functions
Retrieve all information about all external functions that you have created.



# Get External Function
Source: https://docs.play.ai/api-reference/agents/endpoints/v1/external-functions/get-one

GET /api/v1/external-functions/{functionId}
Retrieve all information about the external function with the specified ID.



# Update External Function
Source: https://docs.play.ai/api-reference/agents/endpoints/v1/external-functions/patch

PATCH /api/v1/external-functions/{functionId}
Updates the properties of the external function with the specified ID.



# Create External Function
Source: https://docs.play.ai/api-reference/agents/endpoints/v1/external-functions/post

POST /api/v1/external-functions
Use this endpoint to create new external functions.

Required parameters include the external function's name and the external function's description.

After you create your agent, you can attach the external function to an agent.

To update the external functions see the [Update External Function](/api-reference/agents/endpoints/v1/external-functions/patch) endpoint.


# Introduction
Source: https://docs.play.ai/api-reference/agents/introduction

Create and manage PlayAI agents via the API

PlayAI provides a simple and easy to use HTTP API to create and manage AI Agents.

After you create your agent, you can proceed to start a conversation using our [Websocket API](/api-reference/agents/websocket), or you can try it
out through our web interface at `https://play.ai/agent/<your-agent-id>`.

## Authentication

All API endpoints are authenticated using a User ID and API Key. After you have created an account and logged in, you
can get your API Key from the [For Developers](https://play.ai/api/keys) page.


# Websocket API
Source: https://docs.play.ai/api-reference/agents/websocket

Enhance your app with our audio-in, audio-out API, enabling seamless, natural conversations with your PlayAI agent. Transform your user experience with the power of voice.

<Tip>
  To use our WebSocket, you will need beforehand:

  * A [PlayAI account](https://play.ai/pricing)
  * An [API key to authenticate](https://play.ai/api/keys) with the PlayAI API
  * An agent ID of a PlayAI Agent (created via our [Web UI](https://play.ai/my-agents) or our [Create Agent endpoint](/api-reference/agents/endpoints/v1/agents/post))

  To fully leverage our WebSocket API, the steps are:

  * Connect to our `wss://api.play.ai/v1/talk/<your_agent_id>` URL
  * Send a `{"type":"setup","apiKey":"yourKey"}` message as first message
  * Send audio input as base64 encoded string in `{"type":"audioIn","data":"base64Data"}` messages
  * Receive audio output in `{"type":"audioStream","data":"base64Data"}` messages
</Tip>

# Establishing a Connection

To initiate a conversation, establish a websocket connection to our `talk` URL, including the `agentId` as a path parameter:

```text
wss://api.play.ai/v1/talk/<your_agent_id>
```

For example, assuming `Agent-XP5tVPa8GDWym6j` is the ID of an agent
you have created via our [Web UI](https://play.ai/my-agents) or through our [Create Agent endpoint](/api-reference/agents/endpoints/v1/agents/post),
the WebSocket URL should look like:

```js
const myWs = new WebSocket('wss://api.play.ai/v1/talk/Agent-XP5tVPa8GDWym6j');
```

# Initial Setup Message

Before you can start sending and receiving audio data, you must first send a `setup` message to authenticate
and configure your session.

<Frame caption="WebSocket basic connection, setup and message flow" type="glass">
  ```mermaid
  graph TB
  subgraph "conversation"
  C --> D[Send 'audioIn' messages containing your user's audio data]
  D --> C
  end
  B --> C[Receive 'audioStream' messages containing Agent's audio data]
  subgraph setup
  A[Establish WebSocket Connection] --> B[Send 'setup' message]
  end

  ```
</Frame>

The only required field in the setup message is the `apiKey`. This assumes you are comfortable with the default
values for audio input and audio output formats. In this scenario, your first setup message could be as simple as:

```json
{ "type": "setup", "apiKey": "yourKey" }
```

<Note>Get your API Key at our [Developers](https://play.ai/api/keys) page</Note>

Code example:

```js
const myWs = new WebSocket('wss://api.play.ai/v1/talk/Agent-XP5tVPa8GDWym6j');
myWs.onopen = () => {
  console.log('connected!');
  myWs.send(JSON.stringify({ type: 'setup', apiKey: 'yourApiKey' }));
};
```

## Setup Options

The setup message configures important details of the session,
including the format/encoding of the audio that you intend to send us and the format that you expect to receive.

```json Example setup messages with various options:
// mulaw 16KHz as input
{ "type": "setup", "apiKey": "...", "inputEncoding": "mulaw", "inputSampleRate": 16000 }
// 24Khz mp3 output
{ "type": "setup", "apiKey": "...", "outputFormat": "mp3", "outputSampleRate": 24000 }
// mulaw 8KHz in and out
{ "type": "setup", "apiKey": "...", "inputEncoding": "mulaw", "inputSampleRate": 8000, "outputFormat": "mulaw", "outputSampleRate": 8000 }
```

The following fields are available for configuration:

<table>
  <thead>
    <tr>
      <th>Property</th>
      <th>Accepted values</th>
      <th>Description</th>
      <th>Default value</th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td>`type`<br />(required)</td>
      <td>`"setup"`</td>
      <td>Specifies that the message is a setup command.</td>
      <td>-</td>
    </tr>

    <tr>
      <td>`apiKey`<br />(required)</td>
      <td>`string`</td>
      <td>[Your API Key](https://play.ai/api/keys).</td>
      <td>-</td>
    </tr>

    <tr>
      <td>`outputFormat`<br />(optional)</td>

      <td>
        * `"mp3"`
        * `"raw"`
        * `"wav"`
        * `"ogg"`
        * `"flac"`
        * `"mulaw"`
      </td>

      <td>
        The format of audio you want our agent to output in the `audioStream` messages.

        * `mp3` = 128kbps MP3
        * `raw` = PCM\_FP32
        * `wav` = 16-bit (uint16) PCM
        * `ogg` = 80kbps OGG Vorbis
        * `flac` = 16-bit (int16) FLAC
        * `mulaw` = 8-bit (uint8) PCM headerless
      </td>

      <td>`"mp3"`</td>
    </tr>

    <tr>
      <td>`outputSampleRate`<br />(optional)</td>
      <td>`number`</td>
      <td>The sample rate of the audio you want our agent to output in the `audioStream` messages</td>
      <td>`44100`</td>
    </tr>

    <tr>
      <td>`inputEncoding`<br />(optional)</td>

      <td>
        For non-headerless formats:

        `"media-container"`

        For headerless formats:

        * `"mulaw"`
        * `"linear16"`
        * `"flac"`
        * `"amr-nb"`
        * `"amr-wb"`
        * `"opus"`
        * `"speex"`
        * `"g729"`
      </td>

      <td>
        The encoding of the audio you intend to send in the `audioIn` messages.

        If your are sending audio formats that use media containers (that is, audio that contain headers, such as `mp4`,
        `m4a`, `mp3`, `ogg`, `flac`, `wav`, `mkv`, `webm`, `aiff`), just use `"media-container"` as value for
        `inputEncoding` (or don't pass any value at all since `"media-container"` is the default).
        This will instruct our servers to process the audio based on the data headers.

        If, on the other hand, you will send us audio in headerless formats, you have to specify the format you will
        be sending. In this case, specify it by, e.g., setting `inputEncoding` to `"mulaw"`, `"flac"`, etc.
      </td>

      <td>`"media-container"`</td>
    </tr>

    <tr>
      <td>`inputSampleRate`<br />(optional)</td>
      <td>`number`</td>
      <td>The sample rate of the audio you intend to send. Required if you are specifying an `inputEncoding` different
      than `"media-container"`. Optional, otherwise</td>
      <td>-</td>
    </tr>

    <tr>
      <td>`customGreeting`<br />(optional)</td>
      <td>`string`</td>

      <td>
        Your agent will say this message to start every conversation.
        This overrides the agent's greeting.
      </td>

      <td>-</td>
    </tr>

    <tr>
      <td>`prompt`<br />(optional)</td>
      <td>`string`</td>

      <td>
        Give instructions to your AI about how it should behave and interact with others in conversation.
        This is appended to the agent's prompt.
      </td>

      <td>`""`</td>
    </tr>

    <tr>
      <td>`continueConversation`<br />(optional)</td>
      <td>`string`</td>

      <td>
        If you want to continue a conversation from a previous session, pass the `conversationId` here.
        The agent will continue the conversation from where it left off.
      </td>

      <td>-</td>
    </tr>
  </tbody>
</table>

<br />

<br />

# `audioIn`: Sending Audio Input

After the setup, you can send audio input in the form of an `audioIn` message.

The audio must be sent as a base64 encoded string in the `data` field. The message format is:

```json
{ "type": "audioIn", "data": "<base64Data>" }
```

<Tip>The audio you send must match the `inputEncoding` and `inputSampleRate` you configured in the setup options.</Tip>

## Sample Code for Sending Audio

Assuming `myWs` is a WebSocket connected to our `/v1/talk` endpoint, the sample code below would
send audio directly from the browser:

```javascript
const stream = await navigator.mediaDevices.getUserMedia({
  audio: {
    channelCount: 1,
    echoCancellation: true,
    autoGainControl: true,
    noiseSuppression: true,
  },
});
const mediaRecorder = new MediaRecorder(stream);
mediaRecorder.ondataavailable = async (event) => {
  const base64Data = await blobToBase64(event.data);

  // Relevant:
  myWs.send(JSON.stringify({ type: 'audioIn', data: base64Data }));
};

async function blobToBase64(blob) {
  const reader = new FileReader();
  reader.readAsDataURL(blob);
  return new Promise((resolve) => {
    reader.onloadend = () => resolve(reader.result.split(',')[1]);
  });
}
```

# `audioStream`: Receiving Audio Output

Audio output from the server will be received in an `audioStream` message. The message format is:

```json
{ "type": "audioStream", "data": "<base64Data>" }
```

<Tip>
  The audio you receive will match the `outputFormat` and `outputSampleRate` you configured in the setup options.
</Tip>

## Sample Code for Receiving Audio

```javascript
myWs.on('message', (message) => {
  const event = JSON.parse(message);
  if (event.type === 'audioStream') {
    // deserialize event.data from a base64 string to binary
    // enqueue/play the binary data at your player
    return;
  }
});
```

# Voice Activity Detection: `voiceActivityStart` and `voiceActivityEnd`

During the conversation, you will receive `voiceActivityStart` and `voiceActivityEnd` messages indicating the detection
of speech activity in the audio input. These messages help in understanding when the user starts and stops speaking.

When our service detects that the user started to speak, it will emit a `voiceActivityStart` event.
Such a message will have the format:

```json
{ "type": "voiceActivityStart" }
```

It is up to you to decide how to react to this event.
We highly recommend you stop playing whatever audio is being played, since the `voiceActivityStart` generally indicates
the user wanted to interrupt the agent.

Similarly, when our service detects that the user stopped speaking, it emits a `voiceActivityEnd` event:

```json
{ "type": "voiceActivityEnd" }
```

# `newAudioStream`: Handling New Audio Streams

A `newAudioStream` message indicates the start the audio of a new response.
It is recommended to clear your player buffer and start playing the new stream content upon receiving this message.
This message contains no additional fields.

# Error Handling

Errors from the server are sent as `error` message type, a numeric code and a message in the following format:

```json
{ "type": "error", "code": <errorCode>, "message": "<errorMessage>" }
```

The table below provides a quick reference to the various error codes and their corresponding messages for the Agent Websocket API.

| Error Code | Error Message                                                                                                                                                                                                                                                       |
| ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1001       | Invalid authorization token.                                                                                                                                                                                                                                        |
| 1002       | Invalid agent id.                                                                                                                                                                                                                                                   |
| 1003       | Invalid authorization credentials.                                                                                                                                                                                                                                  |
| 1005       | Not enough credits.                                                                                                                                                                                                                                                 |
| 4400       | Invalid parameters. Indicates the message sent to the server failed to match the expected format. Double check the logic and try again.                                                                                                                             |
| 4401       | Unauthorized. Invalid authorization token or invalid authorization credentials for specified agent.                                                                                                                                                                 |
| 4429       | You have reached the maximum number of concurrent connections allowed by your current plan. Please consider upgrading your plan or reducing the number of active connections to continue.                                                                           |
| 4500       | Generic error code for internal errors, such as failures to generate responses.<br />Generally, the user is not at fault when these happen. An appropriate reaction is to wait a few moments and try again. If the problem persists, contacting support is advised. |

***

This documentation covers the essential aspects of interacting with the PlayAI Websocket API for agent conversations. Ensure that your implementation handles the specified message types and follows the outlined protocols for a seamless integration.


# Get All PlayNotes
Source: https://docs.play.ai/api-reference/playnote/get-all

GET /api/v1/playnotes
Retrieve all PlayNotes.



# Get PlayNote
Source: https://docs.play.ai/api-reference/playnote/get-id

GET /api/v1/playnotes/{playNoteId}
Retrieve all information about a PlayNote.



# Create PlayNote
Source: https://docs.play.ai/api-reference/playnote/post

POST /api/v1/playnotes
Create a new PlayNote by providing a file URL.

<Tip>
  Check out the [Generate Conversation from PDF with PlayNote API](/documentation/playnote/playnote-quickstart) guide
  for a step-by-step approach to using the PlayNote API to create a podcast-style conversation (and more!) from a PDF.
</Tip>

After you create your PlayNotes, you can proceed to poll its status via the [Get PlayNote](/api-reference/playnote/get-id) endpoint.

Note: You can have only **one active generation**. If you face this error code `403` with the message `{"errorMessage":"User already has an active generation","errorId":"UNAUTHORIZED"}` then please wait for some time and try again later.


# Create Speech
Source: https://docs.play.ai/api-reference/text-to-speech/endpoints/v1/create-speech

POST /api/v1/tts
Convert text to speech with our top-of-the-line PlayAI models.

Convert text to speech with our top-of-the-line PlayAI models.

This endpoint supports two models:

* **Play 3.0 Mini**: Our fast and efficient model for single-voice text-to-speech.
* **Dialog 1.0**: Our flagship model with best quality and multi-turn dialogue capabilities.

We also offer **Dialog 1.0 Turbo** which is a faster version of Dialog 1.0 from [a separate endpoint](/api-reference/text-to-speech/endpoints/v1/stream-speech-turbo).

For more information, see [Models](/documentation/text-to-speech/tts-models).

<Tip>
  Check out the [How to use Dialog 1.0 Text-to-Speech API](/documentation/tutorials/tts/dialogs/how-to-use-tts-api) guide
  for a step-by-step approach to using the Dialog 1.0 API to convert text into natural human-like sounding audio.

  Make sure to see the [Create a Multi-Turn Scripted Conversation with the Dialog 1.0 API](/documentation/tutorials/tts/dialogs/create-ai-podcast)
  guide for examples on how to create a multi-turn scripted conversation between two distinct speakers.
</Tip>


# Get an Async TTS Job
Source: https://docs.play.ai/api-reference/text-to-speech/endpoints/v1/get-async

GET /api/v1/tts/{asyncTtsJobId}
Gets the current status of an async TTS job.




# List Voices
Source: https://docs.play.ai/api-reference/text-to-speech/endpoints/v1/list-voices

GET /api/v1/voices
Get a list of all pre-built voices.



# Stream Speech
Source: https://docs.play.ai/api-reference/text-to-speech/endpoints/v1/stream-speech

openapi POST /api/v1/tts/stream
Streams the audio bytes with our ultra-fast text-in, audio-out API.

Convert text to speech and receive audio bytes in real-time.

This endpoint supports two models:

* **Play 3.0 Mini**: Our fast and efficient model for single-voice text-to-speech.
* **Dialog 1.0**: Our flagship model with best quality and multi-turn dialogue capabilities.

We also offer **Dialog 1.0 Turbo** which is a faster version of Dialog 1.0 from [a separate endpoint](/api-reference/text-to-speech/endpoints/v1/stream-speech-turbo).

For more information, see [Models](/documentation/text-to-speech/tts-models).

<Tip>
  Check out the [How to use Dialog 1.0 Text-to-Speech API](/documentation/tutorials/tts/dialogs/how-to-use-tts-api) guide
  for a step-by-step approach to using the Dialog 1.0 API to convert text into natural human-like sounding audio.

  Make sure to see the [Create a Multi-Turn Scripted Conversation with the Dialog 1.0 API](/documentation/tutorials/tts/dialogs/create-ai-podcast)
  guide for examples on how to create a multi-turn scripted conversation between two distinct speakers.
</Tip>


# Stream Speech in Turbo Mode
Source: https://docs.play.ai/api-reference/text-to-speech/endpoints/v1/stream-speech-turbo

openapi-tts-dialog-turbo POST /api/v1/tts/stream
Stream speech with our fastest and best-quality model, Dialog 1.0 Turbo.

Convert text to speech and receive audio bytes in real-time in Turbo mode.

This endpoint only supports **Dialog 1.0 Turbo**: Our fastest model with best quality and multi-turn dialogue capabilities, but with a narrower feature set than **Dialog 1.0** and **Play 3.0 Mini**.

For more information, see [Models](/documentation/text-to-speech/tts-models).

<Tip>
  Check out the [How to use Dialog 1.0 Text-to-Speech API](/documentation/tutorials/tts/dialogs/how-to-use-tts-api) guide
  for a step-by-step approach to using the PlayAI API to convert text into natural human-like sounding audio.

  Or click "Try it" above to see the API in action!
</Tip>


# Introduction
Source: https://docs.play.ai/api-reference/text-to-speech/introduction

Create lifelike speech via the API

The PlayAI Text-to-Speech API enables you to convert written text into natural-sounding speech. Our API provides high-quality voice synthesis with multiple voices, languages, and customization options to suit your needs.

## Models

We offer three models:

* [**Dialog 1.0**](/documentation/text-to-speech/tts-models#dialog-1-0): Our flagship model with best quality and multi-turn dialogue capabilities.
* [**Dialog 1.0 Turbo**](/documentation/text-to-speech/tts-models#dialog-1-0-turbo): A faster version of Dialog 1.0, available exclusively via the [Dialog 1.0 Turbo endpoint](/api-reference/text-to-speech/endpoints/v1/stream-speech-turbo).
* [**Play 3.0 Mini**](/documentation/text-to-speech/tts-models#play-3-0-mini): Our fast and efficient model for single-voice text-to-speech.

## Features

* Multiple voice options with different accents and styles
* Support for various languages and dialects
* Adjustable speech parameters (speed, pitch, volume)
* Real-time streaming capabilities
* High-quality audio output in multiple formats


# Websocket API
Source: https://docs.play.ai/api-reference/text-to-speech/websocket

Enhance your app with our audio-in, audio-out API, enabling seamless, natural conversations with your PlayAI agent. Transform your user experience with the power of voice.

<Tip>
  To fully leverage our WebSocket API, the steps are:

  * Send a POST request to `https://api.play.ai/api/v1/websocket-auth` with `Authorization: Bearer <your_api_key>` and `X-User-Id: <your_user_id>` headers
  * Receive a JSON response with a `webSocketUrls` field containing the WebSocket URL according to the desired model
  * Connect to the provided websocket URL
  * Send TTS commands with the same options as our [TTS streaming API](/api-reference/text-to-speech/endpoints/v1/stream-speech), but in `snake_case`, e.g., `{"text":"Hello World","voice":"...","output_format":"mp3"}`
  * Receive audio output as binary messages
</Tip>

## Prerequisites

* [Access credentials](https://play.ai/api/keys) to get your API key and User ID.

# Quickstart - Runnable Demo

If you want to get started quickly, you can clone the [`play-showcase`](https://github.com/playht/playai-showcase) repository
and run the [`tts-websocket`](https://github.com/playht/playai-showcase/tree/main/tts-websocket) app locally.

```shell
# Clone this repository
git clone https://github.com/playht/playai-showcase.git
# Navigate to the tts-websocket demo app
cd tts-websocket
# NPM install
npm install
# Run the server and follow the instructions
npm start
```

<br />

# Establishing a WebSocket Connection

To establish a WebSocket connection, you will need to send a POST request to the `https://api.play.ai/api/v1/tts/websocket-auth` endpoint with the following headers:

```Text HTTP
Authorization: Bearer <your_api_key>
X-User-Id: <your_user_id>
Content-Type: application/json
```

You can obtain your `api_key` and `user_id` from your [PlayAI account](https://play.ai/api/keys).

The response will contain a JSON object with a `webSocketUrls` field that you can use to connect to the WebSocket server according to the desired model.

```json
{
  "webSocketUrls": {
    "Play3.0-mini": "wss://ws.fal.run/playht-fal/playht-tts/stream?fal_jwt_token=<your_session_token>",
    "PlayDialog": "wss://ws.fal.run/playht-fal/playht-tts-ldm/stream?fal_jwt_token=<your_session_token>",
    "PlayDialogMultilingual": "wss://ws.fal.run/playht-fal/playht-tts-multilingual-ldm/stream?fal_jwt_token=<your_session_token>"
  },
  "expiresAt": "2025-01-06T05:13:04.650Z"
}
```

After this point, you can forward the `webSocketUrls[<desired model>]` to your WebSocket client to establish a connection, such as in the following example:

```javascript
const ws = new WebSocket('wss://ws.fal.run/playht-fal/playht-tts/stream?fal_jwt_token=<your_session_token>');
```

<Warning>
  The WebSocket connection duration is **1 hour**.
  After this period, you will need to re-authenticate and establish a new connection.
</Warning>

<br />

# Sending TTS Commands

Once connected to the WebSocket, you can send TTS commands as JSON messages.
The structure of these commands is similar to our [TTS streaming API](/api-reference/text-to-speech/endpoints/v1/stream-speech), but in `snake_case`.

Here's an example:

```javascript
const ttsCommand = {
  text: 'Hello, world! This is a test of the PlayAI TTS WebSocket API.',
  voice: 's3://voice-cloning-zero-shot/775ae416-49bb-4fb6-bd45-740f205d20a1/jennifersaad/manifest.json',
  output_format: 'mp3',
  temperature: 0.7,
};

ws.send(JSON.stringify(ttsCommand));
```

Examples of the [available options for the TTS command](/api-reference/text-to-speech/endpoints/v1/stream-speech) are:

* `request_id` (optional): A unique identifier for the request, useful for correlating responses (see more details below).
* `text` (required): The text to be converted to speech.
* `voice` (required): The voice ID or URL to use for synthesis.
* `output_format` (optional): The desired audio format (default is "mp3").
* `temperature` (optional): Controls the randomness of the generated speech (0.0 to 1.0).
* `speed` (optional): The speed of the generated speech (0.5 to 2.0).

For the complete list of parameters, refer to the [TTS API documentation](/api-reference/text-to-speech/endpoints/v1/stream-speech).

<br />

# Receiving Audio Output

<Tip>
  If you send a sequence of TTS commands, the audio output will be in the same order as the requests.
</Tip>

After sending a TTS command, you'll receive two kinds of messages:

* One initial text message with the format `{"type":"start","request_id":<request_id>}` to acknowledge the request.
* The audio output as a series of binary messages.
* One final text message with the format `{"type":"end","request_id":<request_id>}` to indicate the end of the audio stream.
* In this response message, `request_id` is the unique identifier you provided in the TTS command, or `null` if you didn't provide one.

To handle these messages and play the audio, you can use the following approach:

```javascript
let audioChunks = [];

ws.onmessage = (event) => {
  if (event.data instanceof Blob) {
    // Received binary audio data
    audioChunks.push(event.data);
  } else {
    // Received a text message (e.g., request_id )
    const message = JSON.parse(event.data);
    if (message.type === 'end') {
      // If you provided a request_id, you can use it to correlate responses
      // End of audio stream, play the audio
      // If you specified a different output_format, you may need to adjust the audio player logic accordingly
      const audioBlob = new Blob(audioChunks, { type: 'audio/mpeg' });
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);
      audio.play();

      // Clear the audio chunks for the next request
      audioChunks = [];
    }
  }
};
```

This code collects the binary audio chunks as they arrive and combines them into a single audio blob when the
*End or Request* message (`{"type":"end","request_id":<request id>}`) is received.
It then creates an audio URL and plays the audio using the Web Audio API.

<br />

# Error Handling

It's important to implement error handling in your WebSocket client. Here's an example of how to handle errors and connection closures:

```javascript
ws.onerror = (error) => {
  console.error('WebSocket Error:', error);
};

ws.onclose = (event) => {
  console.log('WebSocket connection closed:', event.code, event.reason);
  // Implement reconnection logic if needed
};
```

<br />

# Best Practices

1. **Authentication**: Always keep your API key secure. While the WebSocket URL can be shared with client-side code, the API Key and User ID should be kept private.

2. **Error Handling**: Implement robust error handling and reconnection logic in your WebSocket client.

3. **Resource Management**: Close the WebSocket connection when it's no longer needed to free up server resources.

4. **Rate Limiting**: Be aware of [rate limits](/documentation/resources/rate-limits) on the API and implement appropriate throttling in your application.

5. **Testing**: Thoroughly test your implementation with various inputs and network conditions to ensure reliability.

By following these guidelines and using the provided examples, you can effectively integrate the PlayAI TTS WebSocket API into your application, enabling real-time text-to-speech functionality with low latency and high performance.


# Agent Flutter SDK
Source: https://docs.play.ai/documentation/agent-sdks/flutter-sdk

Integrate AI agents into your Flutter applications

## üöÄ Features

* üéôÔ∏è **Two-way voice conversations** with AI agents
* üîä **Voice Activity Detection (VAD)** for natural conversations
* üß† **Custom actions** that allow agents to trigger code in your app
* üì± **Cross-platform** - works on iOS, Android, and Web
* üîå **Audio session management** for handling interruptions and device changes
* üìù **Real-time transcripts** of both user and agent speech
* üö¶ **Rich state management** with ValueNotifiers for UI integration

## Installation

Add the package to your `pubspec.yaml`:

```yaml
dependencies:
  agents:
    git:
      url: https://github.com/playht/agents-client-sdk-flutter.git
      ref: main
```

Then save, or run:

```bash
flutter pub get
```

### Platform Configuration

#### iOS

1. Add the following to your `Info.plist`:

```xml
<key>NSMicrophoneUsageDescription</key>
<string>We need access to your microphone to enable voice conversations with the AI agent.</string>
```

2. Add the following to your `Podfile`, since we depend on `permission_handler` to manage permissions and `audio_session` to manage audio sessions.

```
post_install do |installer|
  installer.pods_project.targets.each do |target|
    target.build_configurations.each do |config|
      config.build_settings['GCC_PREPROCESSOR_DEFINITIONS'] ||= [
        '$(inherited)',
        # audio_session settings
        'AUDIO_SESSION_MICROPHONE=0',
        # For microphone access
        'PERMISSION_MICROPHONE=1'
    end
  end
end
```

3. Due to an [issue](https://github.com/gtbluesky/onnxruntime_flutter/issues/24) of the Onnx Runtime getting stripped by XCode when archived, you need to follow these steps in XCode for the voice activity detector (VAD) to work on iOS builds:
   * Under "Targets", choose "Runner" (or your project's name)
   * Go to "Build Settings" tab
   * Filter for "Deployment"
   * Set "Stripped Linked Product" to "No"
   * Set "Strip Style" to "Non-Global-Symbols"

#### Android

1. Add the following permissions to your `AndroidManifest.xml`:

```xml
<uses-permission android:name="android.permission.RECORD_AUDIO" />
<uses-permission android:name="android.permission.INTERNET" />
<uses-permission android:name="android.permission.MODIFY_AUDIO_SETTINGS" />
<uses-permission android:name="android.permission.BLUETOOTH" />
<uses-permission android:name="android.permission.BLUETOOTH_CONNECT" />
```

2. Add the following to `android/gradle.properties` (unless they're already there):

```
android.useAndroidX=true
android.enableJetifier=true
```

3. Add the following settings to `android/app/build.gradle`:

```
android {
    compileSdkVersion 34
    ...
}
```

#### Web

For VAD to work on web platforms, please following the instructions [here](https://pub.dev/packages/vad#web).

## Getting Started

### 1. Create an Agent on PlayAI

Follow the instructions [here](/documentation/agent/agent-quickstart) to create an agent on PlayAI.

### 2. Implement the Agent in Your Flutter App

```dart [expandable]
final agent = Agent(
  // Replace with your agent ID from PlayAI
  agentId: 'your-agent-id-here',
  // Customize your agent's behavior
  prompt: 'You are a helpful assistant who speaks in a friendly, casual tone.',
  // Define actions the agent can take in your app
  actions: [
    AgentAction(
      name: 'show_weather',
      triggerInstructions: 'Trigger this when the user asks about weather.',
      argumentSchema: {
        'city': AgentActionParameter(
          type: 'string',
          description: 'The city to show weather for',
        ),
      },
      callback: (data) async {
        final city = data['city'] as String;
        // In a real app, you would fetch weather data here
        return 'Weather data fetched for $city!';
      },
    ),
  ],
  // Configure callbacks to respond to agent events
  callbackConfig: AgentCallbackConfig(
    // Get user speech transcript
    onUserTranscript: (text) {
      setState(() => _messages.add(ChatMessage(text, isUser: true)));
    },
    // Get agent speech transcript
    onAgentTranscript: (text) {
      setState(() => _messages.add(ChatMessage(text, isUser: false)));
    },
    // Handle any errors
    onError: (error, isFatal) {
      ScaffoldMessenger.of(context).showSnackBar(
        SnackBar(content: Text('Error: $error')),
      );
    },
  ),
);
```

### 3. Connect the Agent to Start a Conversation

```dart
await agent.connect();
```

### 4. Mute and Unmute the User during a Conversation

```dart
await agent.muteUser();
await agent.unmuteUser();
```

### 5. Disconnect the Agent

```dart
await agent.disconnect();
```

## Key Features

### Monitor the Agent's State

1. `AgentState`: The agent can be in one of four states:

* `idle`: Not connected to a conversation
* `connecting`: In the process of establishing a connection
* `connected`: Connected and ready to converse
* `disconnecting`: In the process of ending a conversation

2. `Agent` also exposes `ValueListenable`s which you can listen to for changes in the agent's state.

```dart
ValueListenableBuilder<AgentState>(
  valueListenable: agent.isUserSpeakingNotifier,
  builder: (context, isUserSpeaking, _) => Text('User is speaking: $isUserSpeaking'),
)
```

3. Pass callbacks as `AgentCallbackConfig` to the `Agent` constructor to handle events from the agent.

```dart
final config = AgentCallbackConfig(
  onUserTranscript: (text) => print('User just said: $text'),
  onAgentTranscript: (text) => print('Agent just said: $text'),
)

final agent = Agent(
  // ...
  callbackConfig: config,
);
```

### Agent Actions

One of the most exciting features of the PlayAI Agents SDK is the ability to define custom actions that allow the agent to interact with your app.

```dart
AgentAction(
  name: 'open_settings',
  triggerInstructions: 'Trigger this when the user asks to open settings',
  argumentSchema: {
    'section': AgentActionParameter(
      type: 'string',
      description: 'The settings section to open',
    ),
  },
  callback: (data) async {
    final section = data['section'] as String;
    // Navigate to settings section in your app
    return 'Opened $section settings';
  },
)
```

### Developer Messages

Send contextual information to the agent during a conversation to inform it of changes in your app.

```dart
// When user navigates to a new screen
void _onNavigate(String routeName) {
  agent.sendDeveloperMessage(
    'User navigated to $routeName screen. You can now discuss the content on this page.',
  );
}

// When relevant data changes
void _onCartUpdated(List<Product> products) {
  agent.sendDeveloperMessage(
    'User\'s cart has been updated, now containing: ${products.map((p) => p.name).join(", ")}.',
  );
}
```

## Error Handling

The package uses a robust error handling system with specific exception types:

```dart
try {
  await agent.connect();
} on MicrophonePermissionDenied {
  // Handle microphone permission issues
} on WebSocketConnectionError catch (e) {
  // Handle connection issues
} on ServerError catch (e) {
  // Handle server-side errors
  if (e.isFatal) {
    // Handle fatal errors
  }
} on AgentException catch (e) {
  // Handle all other agent exceptions
  print('Error code: ${e.code}, Message: ${e.readableMessage}');
}
```

## Lifecycle Management

Don't forget to dispose of the agent when it's no longer needed to free up resources.

```dart
@override
void dispose() {
  // Clean up resources
  agent.dispose();
  super.dispose();
}
```

## UI Integration Examples

### Mute Button

```dart
ValueListenableBuilder<bool>(
  valueListenable: agent.isMutedNotifier,
  builder: (context, isMuted, _) => IconButton(
    icon: Icon(isMuted ? Icons.mic_off : Icons.mic),
    onPressed: () => isMuted ? agent.unmuteUser() : agent.muteUser(),
    tooltip: isMuted ? 'Unmute' : 'Mute',
  ),
)
```

### Speaking Indicator

```dart
ValueListenableBuilder<bool>(
  valueListenable: agent.isAgentSpeakingNotifier,
  builder: (context, isSpeaking, _) => AnimatedContainer(
    duration: const Duration(milliseconds: 300),
    width: 40,
    height: 40,
    decoration: BoxDecoration(
      shape: BoxShape.circle,
      color: isSpeaking ? Colors.blue : Colors.grey.shade300,
    ),
    child: Center(
      child: Icon(
        Icons.record_voice_over,
        size: 24,
        color: Colors.white,
      ),
    ),
  ),
)
```

## Tips for Effective Usage

1. **Prompt Engineering**: Craft clear, specific prompts to guide agent behavior
2. **Action Design**: Design actions with clear trigger instructions and parameter descriptions
3. **Context Management**: Use `sendDeveloperMessage` to keep the agent updated on app state
4. **Error Handling**: Implement comprehensive error handling for a smooth user experience
5. **UI Feedback**: Use the provided `ValueListenable`s to give clear feedback on conversation state

## Acknowledgments

* Voice Activity Detection powered by [vad](https://pub.dev/packages/vad)
* Audio session management by [audio\_session](https://pub.dev/packages/audio_session)


# Agent Web SDK
Source: https://docs.play.ai/documentation/agent-sdks/web-sdk

Integrate AI agents into your web applications

## Overview

The **Agent Web SDK** is a TypeScript SDK that facilitates real-time, bi-directional audio conversations with your PlayAI Agent via [WebSocket API](/api-reference/agents/websocket). It takes care of the following:

* WebSocket connection management
* Microphone capture and voice activity detection (VAD)
* Sending user audio to the Agent
* Receiving Agent audio and playing it back in the browser
* Managing event listeners such as user or agent transcripts
* Muting/unmuting the user's microphone
* Hanging up (ending) the agent conversation
* Error handling

<Tip>
  This SDK is designed for modern web browsers that support the [Web Audio
  API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API) and
  [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSocket). If you want to integrate PlayAI Agent into a
  Flutter app, check out our [Flutter SDK](/documentation/agent-sdks/flutter-sdk). We plan to support other platforms in
  the future.
</Tip>

## Installation

<CodeGroup>
  ```bash npm
  npm install @play-ai/agent-web-sdk
  ```

  ```bash yarn
  yarn add @play-ai/agent-web-sdk
  ```

  ```bash pnpm
  pnpm add @play-ai/agent-web-sdk
  ```
</CodeGroup>

## Create agent

To start a conversation with your agent, first create an agent in [PlayAI app](https://play.ai/my-agents).
Once you have an agent, you can find the agent ID in the agent "Deploy ¬∑ Web" section, which is required to connect to the agent.

## Basic usage

Below is a simple example illustrating how to initiate a conversation with your agent using the `connectAgent` function:

```ts
import { connectAgent } from '@play-ai/agent-web-sdk';

async function startConversation() {
  try {
    const agentController = await connectAgent('YOUR_AGENT_ID');
    console.log('Connected to agent. Conversation ID:', agentController.conversationId);
    // Use agentController to control the conversation...
  } catch (error) {
    console.error('Failed to start conversation:', error);
  }
}

startConversation();
```

<Note>
  The function `connectAgent` returns a Promise

  * If any error occurs during the connection process, the Promise is rejected.
  * When the conversation is successfully established, the Promise resolves to `AgentConnectionController` object.
</Note>

## Config

You can customize the agent's configuration by passing an optional `ConnectAgentConfig` object as the second parameter to `connectAgent`.

```ts
const agentController = await connectAgent('YOUR_AGENT_ID', {
  debug: true, // Enable debug logging in the console
  customGreeting: 'Hello, and welcome to my custom agent!', // Override the default greeting
  prompt: 'You are an AI that helps with scheduling tasks.', // Append additional instructions to the agent's prompt
  continueConversation: 'PREVIOUS_CONVERSATION_ID', // Continue a previous conversation
});
```

**Config Options**:

* **`debug`:** Enables debug logging for troubleshooting.
* **`customGreeting`:** Overrides the default greeting used by the agent.
* **`prompt`:** Appends additional instructions to the agent's core prompt.
* **`continueConversation`:** An optional conversation ID to continue a previous conversation.
* **`listeners`:** Attach various listener callbacks (see [Event listeners](#event-listeners) section).

## Event listeners

Event listeners enable you to handle specific moments during the conversation:

```ts
const agentController = await connectAgent('YOUR_AGENT_ID', {
  listeners: {
    onUserTranscript: (transcript) => console.log(`USER said: "${transcript}".`),
    onAgentTranscript: (transcript) => console.log(`AGENT will say: "${transcript}".`),
    onUserStartedSpeaking: () => console.log(`USER started speaking...`),
    onUserStoppedSpeaking: () => console.log(`USER stopped speaking.`),
    onAgentDecidedToSpeak: () => console.log(`AGENT decided to speak... (not speaking yet, just thinking)`),
    onAgentStartedSpeaking: () => console.log(`AGENT started speaking...`),
    onAgentStoppedSpeaking: () => console.log(`AGENT stopped speaking.`),
    onHangup: (endedBy) => console.log(`Conversation has ended by ${endedBy}`),
    onError: (err) => console.error(err),
  },
});
```

## Mute/unmute

Once you have an active `AgentConnectionController` from `connectAgent`, you can mute or unmute the user's microphone:

```ts
const agentController = await connectAgent('YOUR_AGENT_ID');
agentController.mute(); // The agent won't hear any mic data
agentController.unmute(); // The agent hears the mic data again
```

## Hangup

Use `agentController.hangup()` to end the conversation from the user side.

```ts
const agentController = await connectAgent('YOUR_AGENT_ID');
setTimeout(() => {
  // End the conversation after 60 seconds
  agentController.hangup();
}, 60000);
```

When the conversation ends (either by user or agent), the `onHangup` callback (if provided) is triggered.

## Error handling

Errors can occur at different stages of the conversation:

* Starting the conversation. For example:
  * Microphone permissions denied
  * WebSocket fails to connect or closes unexpectedly
  * Invalid agent ID
* During the conversation. For example:
  * Agent fails to generate a response
  * Internal Agent errors
  * Network issues

Errors that occur before the conversation starts are caught by the `connectAgent` Promise. You can handle these errors in the `catch` block.
Errors that occur during the conversation are caught by the `onError` listener.

```ts
import { connectAgent } from '@play-ai/agent-web-sdk';

async function startConversation() {
  try {
    const agentController = await connectAgent('YOUR_AGENT_ID', {
      listeners: {
        onError: (error) => {
          console.error('Error occurred:', error.description);
          if (error.isFatal) {
            // Possibly reconnection logic or UI error message
          }
        },
      },
    });
  } catch (err) {
    console.error('Failed to start the conversation:', err);
  }
}
```

**Error object**:

```ts
interface ErrorDuringConversation {
  description: string; // Human-readable message
  isFatal: boolean; // Whether the error ended the conversation
  serverCode?: number; // If the server gave a specific error code
  wsEvent?: Event; // Low-level WebSocket event
  cause?: Error; // JS error cause
}
```

## Code example

<iframe height="600px" width="100%" src="https://codesandbox.io/embed/gpdxch?view=preview&module=%2Fsrc%2FApp.tsx&hidenavigation=1" title="play-ai-web-sdk-demo" allow="accelerometer; ambient-light-sensor; camera; encrypted-media; geolocation; gyroscope; hid; microphone; midi; payment; usb; vr; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts" />


# Actions and Integrations
Source: https://docs.play.ai/documentation/agent/actions-integrations

Learn how to empower your agents to perform actions on your or your users' behalf

## Prerequisites

* Basic understanding of REST APIs. If you're new to APIs, check out [this video](https://www.youtube.com/watch?v=Yzx7ihtCGBs).

## What is an Action?

An action is a task that your agent can execute that involves interfacing with the outside world. For example:

* Calling the Google Calendar API to book an appointment for a user.
* Fetching a funny quote from a public API and reciting it to a user.

Fundamentally, actions are flexible and discrete pieces of functionality that you can build into any agent ‚Äî and unlock limitless utility!

## Creating an Action

<Steps>
  <Step title="Navigate to Actions Page">
    Go to our [Actions page](https://play.ai/integrations) where you'll find the action creation form

    <img height="200" src="https://framerusercontent.com/images/mammpETxoaWagG2qV5gaYkvtOM.png" />
  </Step>

  <Step title="Describe Your Action">
    Create a clear description that agents will use to determine when to invoke the action. For example:

    * "Get a funny quote" will work when users ask for funny quotes
    * "fry me a hot dog" won't work as the agent won't understand the action

    <img height="150" src="https://framerusercontent.com/images/lHD3aeZFVVaQSUMOKWtJKuReapk.png" />
  </Step>

  <Step title="Configure Endpoint and Method">
    Enter the URL for your API endpoint and choose the appropriate request method. This can be:

    * A public API
    * An API you built yourself
    * Any other accessible API endpoint

    <img height="150" src="https://framerusercontent.com/images/kaPvyWwOC691hQBVZJ3XQ9XGOXc.png" />
  </Step>

  <Step title="Set Up Parameters">
    Configure your parameters in two ways:

    **Static Parameters:**

    * Parameters that remain the same for every action invocation
    * Useful for API keys and constant values
    * Uncheck "Conversation time parameter" checkbox

    <img height="150" src="https://framerusercontent.com/images/azmsZBUCkJDXe0MR3UuR1G50.png" />

    **Dynamic Parameters:**

    * Parameters determined from the conversation
    * Examples: user's name or email address
    * Check "Conversation time parameter" checkbox

    <img height="150" src="https://framerusercontent.com/images/2SQgef6hWYYHxwnpR3znxVIAkY.png" />
  </Step>

  <Step title="Enable Your Action">
    Enable your action by either:

    * [Editing an existing agent](https://play.ai/my-agents)
    * [Creating a new agent](https://play.ai/create-agent)

    Check the box for your action at the bottom of the composition form

    <img height="150" src="https://framerusercontent.com/images/7RhIhSK0V4zh827BYtbQi5pWB2g.png" />
  </Step>

  <Step title="Test Your Action">
    1. Navigate to [your agents](https://play.ai/my-agents)
    2. Start a conversation
    3. Ask the agent to perform your action
    4. If it fails, debug using the [conversations page](https://play.ai/conversations)

    <img height="300" src="https://framerusercontent.com/images/eNawEi6tLf7eaE8Dgkmk8FS10rQ.png" />

    Note: A conversation ID will be included in the request body to help identify requests from the same conversation.
  </Step>
</Steps>


# Create Your First Agent
Source: https://docs.play.ai/documentation/agent/agent-quickstart

Build your first AI voice agent in minutes

<Info>
  This guide will walk you through the process of creating an agent on [PlayAI Dashboard](https://play.ai/my-agents). To
  create agents programmatically, you can use our [Agent API](/api-reference/agents/introduction).
</Info>

## Prerequisites

* A PlayAI account. If you don't have one, you can create one at [PlayAI](play.ai).

## Creating Your Agent

<Steps>
  <Step title="Agent Creation">
    * Navigate to [PlayAI](https://play.ai/) and log in
    * Click "Create an Agent" to open the agent creation portal
  </Step>

  <Step title="Configure Agent Identity">
    * **Name:** Give your agent a unique and descriptive name. This name will be visible to users
    * **Voice:** Choose a pre-built voice from the library. Voices are categorized by gender, accent, and style (conversational, storytelling, etc.). You can audition each voice before selection. Alternatively, create a clone of your own voice for a personalized touch
    * **Speed:** Adjust the speaking speed of the chosen voice. The default is 1.0x, with options to increase or decrease the speed
    * **Avatar:** Select a visual representation for your agent. Choose from a library of pre-designed avatars or upload a custom image
    * **Privacy:** Configure the agent's accessibility:

      * **Private:** Only you can access and interact with the agent
      * **Unlisted:** Accessible to anyone with the unique link to the agent. Ideal for sharing with a select group
      * **Public:** Anyone can interact with and clone the agent
  </Step>

  <Step title="Define Agent Behavior">
    * **Agent Greeting:** Craft the initial message your agent will use to greet users. This sets the tone for the
      conversation. Example: "Hello! I'm Jarvis. How can I help you today?"
    * **Agent Prompt:** This is the core instruction
      set that defines your agent's purpose and behavior. Be clear and specific about the agent's role and the types of
      interactions it should handle. This acts similarly to a system prompt in large language models. Example: "You are a
      helpful assistant. You will provide information and answer questions about the services our company offers."
  </Step>

  <Step title="Set Up Agent Knowledge">
    * **LLM Selection:** Choose the underlying language model that powers your agent's intelligence. Several options may be available, each with varying capabilities and costs
    * **Custom Knowledge:** Enhance your agent's knowledge with specific information relevant to its purpose. Upload files (PDFs, FAQs, Epub, .txt) containing information like product details, company policies, or specialized knowledge
    * **Guardrails (Optional):** Restrict the agent's responses to only the uploaded custom knowledge base, preventing it from relying on its general knowledge training. This is important for ensuring accuracy and control in specific domains
    * **Dynamic Context (Optional):** Provide the agent with contextually relevant information such as:

      * Current Date & Time
      * Caller Information (phone number, email) - if applicable to the deployment method
  </Step>

  <Step title="Configure Agent Actions">
    Select specific actions that your agent can perform. These actions might integrate with external services like Zapier and Make or tools. Examples:

    * Get a funny quote
    * Get Weather information
    * Schedule a meeting (integration with Google Calendar)
    * Send an email/SMS message
    * Upload an order (integration with backend systems)
    * Get Financial Data
  </Step>

  <Step title="Deploy Your Agent">
    Choose your deployment method:

    * **Phone:** Deploy the agent to a phone number. This allows users to call and interact with the agent through voice calls. Pricing details for call minutes may apply
    * **Web:** Embed the agent on your website by copying and pasting provided code into the `<head>` section of your HTML. This enables text-based or voice interaction directly on your site. Customize styling and content as needed
  </Step>

  <Step title="Test and Iterate">
    Once deployed, thoroughly test your agent to refine its behavior, prompts, and knowledge base for optimal performance
  </Step>
</Steps>

## Additional Notes

* The "Actions" functionality is a powerful way to extend your agent's capabilities and integrate it with your existing workflows.
* Carefully consider the privacy setting for your agent to control its accessibility.
* The Agent Prompt is critical for shaping your agent's responses. Experiment with different prompts to achieve the desired behavior.


# Knowledge Base
Source: https://docs.play.ai/documentation/agent/crawl-website

Learn how to give agents access to your documents and websites

## Upload a document

You can upload a document to your agent's knowledge base by clicking the "Upload Knowledge Files" button in the create/edit agent flow.

Documents are automatically chunked and indexed as necessary.

## Crawl a website

PlayAI Agents can also crawl a website and answer questions about the content and navigate the user to the relevant pages.

<Steps>
  <Step title="Enable crawling">
    On the last page of the create/edit agent flow, check the "Crawl website" checkbox and enter the URL of the website you want to crawl.

    <Frame>
      <img src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/crawl-a-website/enable-crawling.png" alt="Enable crawling" />
    </Frame>
  </Step>

  <Step title="Click the 'Crawl' button to start the crawling process">
    Stay on the page until the crawling process is complete. This can take a while if the website has many pages.

    <Frame>
      <img src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/crawl-a-website/click-crawl.png" alt="Click 'Crawl'" />
    </Frame>
  </Step>

  <Step title="Wait for crawling to complete">
    Your embed should now be able to answer questions about the crawled website and navigate the user to relevant pages on the site.

    <Frame>
      <img src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/crawl-a-website/crawling-completed.png" alt="Click 'Crawl'" />
    </Frame>
  </Step>
</Steps>

## Delete a crawled website

Go into the "Knowledge" tab of the create/edit agent flow and click the trash icon next to the website you want to delete under the "Custom Knowledge" section.

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/crawl-a-website/delete-website.png" alt="Delete website" />
</Frame>


# PlayAI Agent
Source: https://docs.play.ai/documentation/agent/introduction

Deploy voice-powered agents to web, apps, and telephony in minutes

PlayAI's AI Agent platform enables you to create intelligent, voice-enabled conversational agents that can understand and respond to user input naturally. Our agents combine advanced language models with high-quality text-to-speech to provide engaging, human-like interactions.

<iframe width="560" height="315" src="https://www.youtube.com/embed/HFXJwCenmLA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen />

## Key Features

<CardGroup cols={3}>
  <Card title="Natural Conversations" icon="comments">
    Engage in human-like dialogue with context awareness
  </Card>

  <Card title="Voice Interaction" icon="microphone">
    Bi-directional voice communication
  </Card>

  <Card title="Custom Actions" icon="bolt">
    Define custom actions and integrations
  </Card>

  <Card title="Web Embedding" icon="code">
    Easy integration into websites and applications
  </Card>

  <Card title="Knowledge Base" icon="book">
    Let your agents learn from your documents and websites
  </Card>

  <Card title="Works everywhere" icon="mobile">
    Support for web and mobile platforms
  </Card>
</CardGroup>

## Getting Started

1. **Quick Start**: Follow our [Agent Quickstart](/documentation/agent/agent-quickstart) guide
2. **Add Actions & Integrations**: Learn [how to add actions and integrations](/documentation/agent/actions-integrations)
3. **Knowledge Base**: Learn [give agents access to your documents and websites](/documentation/agent/crawl-website)

## SDKs

We provide official SDKs for easy integration:

<CardGroup cols={2}>
  <Card title="Web SDK" icon="js" iconType="duotone" href="/documentation/agent-sdks/web-sdk">
    * JavaScript/TypeScript support
    * React components
    * Easy website integration
  </Card>

  <Card title="Flutter SDK (Alpha)" icon="flutter" iconType="duotone" href="/documentation/agent-sdks/flutter-sdk">
    * Cross-platform mobile support
    * Native performance
    * Easy integration
  </Card>
</CardGroup>

## Web Examples

The following examples show how to use the web SDK to embed an agent on a website using [Web Embed](/documentation/agent/web-embed).

### Form filling

Assists a user in filling out a form. Showcases the ability to pass a custom prompt to the web embed from javascript.

[Live example](https://play.ai/embed/demo/form-filling)

[Code](https://github.com/playht/web-embed-examples/blob/main/form-filling/nextjs/app/page.tsx)

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/web-embed-examples/form-filling.png" alt="Form Filling Demo" />
</Frame>

### Minimize web embed

Showcases the ability to minimize the web embed from javascript.

[Live example](https://play.ai/embed/demo/minimize)

[Code](https://github.com/playht/web-embed-examples/blob/main/minimize-embed/nextjs/app/page.tsx)

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/web-embed-examples/minimize-embed.png" alt="Minimize Web Embed Demo" />
</Frame>

### Image generation

Generates an image based on the user's description.

[Live example](https://play.ai/embed/demo/image-gen)

[Code](https://github.com/playht/web-embed-examples/blob/main/image-gen/nextjs/app/page.tsx)

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/web-embed-examples/image-gen.png" alt="Image Generation Demo" />
</Frame>

### Music mood

Plays music based on the user's mood.

[Live example](https://play.ai/embed/demo/music-mood)

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/web-embed-examples/music-mood.jpeg" alt="Music Mood Demo" />
</Frame>

### Color math

Game where the user has to guess the color that is created by mixing two other colors.

[Live example](https://play.ai/embed/demo/color-math)

### Color painter

Changes the background color of the webpage based on the user's description.

[Live example](https://play.ai/embed/demo/color-painter)

## API Reference

Explore our comprehensive API documentation:

* [Create Agent](/api-reference/agents/endpoints/v1/agents/post)
* [Manage Agent](/api-reference/agents/endpoints/v1/agents/patch)
* [Agent Actions](/api-reference/agents/endpoints/v1/external-functions/post)
* [Agent Integrations](/api-reference/agents/endpoints/v1/external-functions/post)
* [Web Embed](/api-reference/agents/websocket)

## Best Practices

1. **Agent Design**

   * Define clear personality and behavior
   * Set appropriate capabilities
   * Test thoroughly before deployment

2. **Performance**

   * Optimize response times
   * Handle errors gracefully
   * Monitor usage and limits

3. **Security**
   * Secure API credentials
   * Validate user input
   * Implement proper access controls

## Resources

* [Rate Limits](/documentation/resources/rate-limits)
* [Error Messages](/documentation/resources/error-messages)
* [Troubleshooting Guide](/documentation/resources/troubleshooting)


# Web Embed
Source: https://docs.play.ai/documentation/agent/web-embed

Learn how to embed a PlayAI Agent on your website

This document provides detailed information about the key components of the PlayAI web embed API: the events array, the onEvent handler, and the openEmbed function.

<Tip>
  Looking for a tutorial? Check out our [web embed tutorial](/documentation/tutorials/agent/web-embed-tutorial).
</Tip>

## Installation

```bash
npm install @play-ai/agent-web-sdk
```

## `openEmbed` Function

The `openEmbed` function initializes and opens the PlayAI web embed on the webpage. It is imported from the `@play-ai/agent-web-sdk` package as follows

```typescript
import { open as openEmbed } from '@play-ai/agent-web-sdk';
```

It has the following signature:

```typescript
function openEmbed(
  webEmbedId: string,
  options: {
    events?: ReadonlyArray<Event>;
    onEvent?: OnEventHandler;
    prompt?: string;
    customGreeting?: string;
  },
): { setMinimized: (minimize?: boolean) => void };
```

### Parameters

* `webEmbedId`: A string representing your unique web embed ID provided by PlayAI.
* `options`: An object containing:
  * `events?`: The array of custom events your application can handle.
  * `onEvent?`: The event handler function.
  * `customGreeting?`: A custom greeting that replaces the default greeting.
  * `prompt?`: A prompt to give the agent in addition to the default prompt. Use this to give context that is page-specific or user-specific, e.g. "The form fields on the current page are X, Y, and Z".

### Return type

* `setMinimized(minimize?: boolean)`: A function that allows you to toggle the minimized state of the web embed. Pass in `true` to minimize and `false` to maximize the web embed. Toggle the minimize state by passing in `undefined`.

### Example

```javascript
import { open as openEmbed } from '@play-ai/agent-web-sdk';

// ... (events and onEvent definitions) ...

useEffect(() => {
  const { setMinimized } = openEmbed(webEmbedId, {
    events,
    onEvent,
    customGreeting: "Let's fill out this form together!",
    prompt: 'The form fields on the current page are name, email, and shipping address',
  });
}, []);
```

In this example, the openEmbed function is called inside a useEffect hook to initialize the web embed when the component mounts.

## Events Array

The events array defines the custom events that your application can handle. Each event in the array is an object with the following structure:

```typescript
type Event = {
  name: string;
  when: string;
  data: {
    [key: string]: {
      type: string;
      description?: string;
      values?: string[];
    };
  };
};
```

### Properties

* `name`: A string that uniquely identifies the event.
* `when`: A string describing the condition that triggers this event.
* `data`: An object describing the data that should be passed to the event handler. Each key in this object represents the name of the data field, and its value is an object with:
  * `type`: The data type of the field (currently supports `string`, `number`, `boolean`, and `enum`).
  * `description?`: A brief description of what this data field represents.
  * `values?`: An array of strings representing the possible values for the field if the type is `enum`.

### Example

```javascript
const events = [
  {
    name: "change-text",
    when: "The user says what they want to change the text on the screen to",
    data: {
      text: { type: "string", description: "The text to change to" },
    },
  },
] as const;
```

## onEvent Handler

The onEvent handler is a function that processes events triggered by the PlayAI web embed. It has the following signature:

```typescript
type OnEventHandler = (event: { name: string; data: Record<string, any> }) => void;
```

### Parameters

* `event`: An object containing:
  * `name`: The name of the triggered event (matching an event name from the events array).
  * `data`: An object containing the data associated with the event (matching the data structure from the events array).

### Example

```javascript
const onEvent = (event) => {
  console.log('onEvent: ', event);
  if (event.name === 'change-text') {
    setText(event.data.text);
  }
};
```

In this example, the handler logs all events and updates the text state when a "change-text" event is received.

## Putting It All Together

Here's how these components work together:

1. You define your custom events in the events array.
2. You implement the onEvent handler to process these events.
3. You call the openEmbed function, passing your web embed ID, the events array, the onEvent handler, and optionally customGreeting and prompt.
4. When a user interacts with the AI agent, it may trigger one of your defined events.
5. The onEvent handler receives the triggered event and processes it according to your implementation.

This structure allows for flexible, event-driven interactions between your web application and the PlayAI web embed.


# Welcome to PlayAI
Source: https://docs.play.ai/documentation/get-started/overview

Comprehensive platform for building world-class voice AI solutions

<Frame>
  <img className="block dark:hidden" src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/header.png" alt="Hero Light" />

  <img className="hidden dark:block" src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/header.png" alt="Hero Dark" />
</Frame>

Explore our platform to build, test, and monitor world-class voice AI solutions.

1. [**Text-to-Speech (TTS)**](/documentation/text-to-speech/introduction): Create natural, human-like speech from text with industry-leading latency and quality.
2. [**AI Voice Agents**](/documentation/agent/introduction): Build conversational AI agents that can understand and respond to voice input.
3. [**PlayNote**](/documentation/playnote/introduction): Transform documents, PDFs, and other content into engaging multi-speaker podcasts.

## Quickstart (\~5 minutes)

<CardGroup cols={2}>
  <Card title="Text to Speech" icon="waveform-lines" iconType="duotone" color="#32ADE6" href="/documentation/text-to-speech/tts-quickstart">
    Create natural, human-like speech from text using our advanced PlayDialog model
  </Card>

  <Card title="AI Voice Agents" icon="robot" iconType="duotone" color="#FF9500" href="/documentation/agent/agent-quickstart">
    Build conversational AI agents that can understand and respond to voice input
  </Card>
</CardGroup>

<Tip>
  ### Coding with LLMs?

  * List of docs pages is available at [/llms.txt](https://docs.play.ai/llms.txt).
  * Full docs content is available at [/llms-full.txt](https://docs.play.ai/llms-full.txt).
  * On any individual page, copy Markdown content with Ctrl/Cmd + C.
</Tip>

## Platform Features

<CardGroup cols={3}>
  <Card title="Best-in-class Voice Generation" icon="circle-play">
    Create human-like speech with our advanced PlayDialog model
  </Card>

  <Card title="Voice Agents" icon="robot">
    Build voice-powered AI agents that integrate with your tools
  </Card>

  <Card title="Document to Podcast" icon="microphone-lines">
    Transform any document into an engaging multi-speaker podcast
  </Card>

  <Card title="Easy Integration" icon="plug">
    Simple APIs and comprehensive SDKs for quick implementation
  </Card>

  <Card title="200+ Prebuilt Voices" icon="users">
    Hundreds of studio-quality voices for your projects across a wide range of languages and accents
  </Card>

  <Card title="Industry-leading Latency" icon="bolt">
    Real-time processing with sub-second latency
  </Card>
</CardGroup>


# Use Cases
Source: https://docs.play.ai/documentation/get-started/use-cases

Explore real-world applications and use cases for PlayAI products

Discover how PlayAI's products can transform your business and applications through these real-world use cases.

## Text-to-Speech Use Cases

<CardGroup cols={2}>
  <Card title="Voice Cloning" icon="microphone">
    Create custom AI voices that match your brand or specific requirements.
  </Card>

  <Card title="Multilingual Content" icon="language">
    Generate content in multiple languages with natural-sounding voices.
  </Card>

  <Card title="Audiobooks" icon="book">
    Convert written content into engaging audiobooks with professional narration.
  </Card>

  <Card title="Voice Assistants" icon="robot">
    Build intelligent voice assistants for various applications.
  </Card>
</CardGroup>

## Agent Use Cases

<CardGroup cols={2}>
  <Card title="Customer Support" icon="headset">
    Deploy AI agents to handle customer inquiries and support tickets.
  </Card>

  <Card title="Sales Representatives" icon="chart-line">
    Create AI sales agents to qualify leads and handle initial sales conversations.
  </Card>

  <Card title="Virtual Assistants" icon="user">
    Build personal AI assistants for task management and information retrieval.
  </Card>

  <Card title="Educational Tutors" icon="graduation-cap">
    Develop AI tutors for personalized learning experiences.
  </Card>
</CardGroup>


# Introduction to PlayNote
Source: https://docs.play.ai/documentation/playnote/introduction

Transform documents into engaging podcasts

PlayNote is PlayAI's powerful tool for transforming documents, PDFs, and other content into engaging multi-speaker podcasts. Using advanced AI, PlayNote analyzes your content and creates natural, conversational audio presentations with multiple voices.

## Key Features

<CardGroup cols={3}>
  <Card title="Document Processing" icon="file-pdf">
    Support for PDFs, documents, and more
  </Card>

  <Card title="Multi-speaker" icon="users">
    Natural conversations with multiple voices
  </Card>

  <Card title="Smart Analysis" icon="brain">
    AI-powered content analysis and structuring
  </Card>

  <Card title="Custom Voices" icon="microphone">
    Choose from multiple voice options
  </Card>

  <Card title="High Quality" icon="star">
    Studio-quality audio output
  </Card>

  <Card title="Easy Integration" icon="plug">
    Simple API for quick implementation
  </Card>
</CardGroup>

## Use Cases

PlayNote is perfect for:

1. **Content Creation**

   * Transform blog posts into podcasts
   * Create audio versions of documentation
   * Generate educational content

2. **Business Applications**

   * Convert reports into presentations
   * Create training materials
   * Generate product demonstrations

3. **Educational Content**
   * Create audio textbooks
   * Generate study materials
   * Produce educational podcasts

## Getting Started

1. **Quick Start**: Follow our [PlayNote tutorial](/documentation/playnote/playnote-quickstart)
2. **API Reference**: Explore our [PlayNote APIs](/api-reference/playnote)

## Best Practices

1. **Content Preparation**

   * Structure your content clearly
   * Use appropriate formatting
   * Include clear section breaks

2. **Voice Selection**

   * Choose voices that match your content
   * Consider using different voices for different speakers
   * Test voice combinations

3. **Output Quality**
   * Monitor audio quality
   * Check for proper pacing
   * Verify content accuracy

## Resources

* [Rate Limits](/documentation/resources/rate-limits)
* [Error Messages](/documentation/resources/error-messages)
* [Troubleshooting Guide](/documentation/resources/troubleshooting)


# Generate Conversation from PDF
Source: https://docs.play.ai/documentation/playnote/playnote-quickstart

Learn how to transform PDF documents into engaging multi-speaker conversations

This guide will walk you through generating a conversational-style podcast from a PDF using the PlayNote API. We'll use the `PlayNote API` to take a PDF source file, synthesize it into an audio conversation between two voices, and retrieve the generated podcast URL.

## Prerequisites

Before you start, ensure you have the following:

* Access your [credentials](https://play.ai/api/keys) (API key and user ID)
* Development environment for your chosen programming language
* Python's `requests` library installed (`pip install requests`)

## Steps

<Steps>
  <Step title="Set Up Environment Variables">
    Add your API key and user ID to your environment variables.

    <CodeGroup>
      ```bash macOS (zsh)
      echo 'export PLAYAI_API_KEY="your_api_key_here"' >> ~/.zshrc
      echo 'export PLAYAI_USER_ID="your_user_id_here"' >> ~/.zshrc
      source ~/.zshrc
      ```

      ```bash bash
      echo 'export PLAYAI_API_KEY="your_api_key_here"' >> ~/.bashrc
      echo 'export PLAYAI_USER_ID="your_user_id_here"' >> ~/.bashrc
      source ~/.bashrc
      ```

      ```cmd Windows
      setx PLAYAI_API_KEY "your_api_key_here"
      setx PLAYAI_USER_ID "your_user_id_here"
      ```
    </CodeGroup>
  </Step>

  <Step title="Configure API Access">
    Create a script with the following authentication setup:

    <CodeGroup>
      ```python Python
      import requests
      import os

      # Define the URL of your PDF file
      SOURCE_FILE_URL = "https://godinton.kent.sch.uk/media/2601/goldilocks-story.pdf"

      # PlayNote API URL
      url = "https://api.play.ai/api/v1/playnotes"

      # Retrieve API key and User ID from environment variables
      api_key = os.getenv("PLAYAI_API_KEY")
      user_id = os.getenv("PLAYAI_USER_ID")

      # Set up headers with authorization details
      headers = {
          'AUTHORIZATION': api_key,
          'X-USER-ID': user_id,
          'accept': 'application/json'
      }
      ```

      ```javascript JavaScript
      const SOURCE_FILE_URL = "https://godinton.kent.sch.uk/media/2601/goldilocks-story.pdf";
      const url = "https://api.play.ai/api/v1/playnotes";

      const apiKey = process.env.PLAYAI_API_KEY;
      const userId = process.env.PLAYAI_USER_ID;

      const headers = {
        'AUTHORIZATION': apiKey,
        'X-USER-ID': userId,
        'accept': 'application/json'
      };
      ```

      ```go Go
      const SOURCE_FILE_URL = "https://godinton.kent.sch.uk/media/2601/goldilocks-story.pdf"
      const url = "https://api.play.ai/api/v1/playnotes"

      apiKey := os.Getenv("PLAYAI_API_KEY")
      userId := os.Getenv("PLAYAI_USER_ID")

      headers := map[string]string{
        "AUTHORIZATION": apiKey,
        "X-USER-ID": userId,
        "accept": "application/json",
      }
      ```

      ```dart Dart
      import 'dart:io';
      import 'package:http/http.dart' as http;

      const SOURCE_FILE_URL = "https://godinton.kent.sch.uk/media/2601/goldilocks-story.pdf";
      final url = Uri.parse("https://api.play.ai/api/v1/playnotes");

      final apiKey = Platform.environment['PLAYAI_API_KEY'];
      final userId = Platform.environment['PLAYAI_USER_ID'];

      final headers = {
        'AUTHORIZATION': apiKey!,
        'X-USER-ID': userId!,
        'accept': 'application/json'
      };
      ```

      ```swift Swift
      import Foundation

      let SOURCE_FILE_URL = "https://godinton.kent.sch.uk/media/2601/goldilocks-story.pdf"
      let url = URL(string: "https://api.play.ai/api/v1/playnotes")!

      guard let apiKey = ProcessInfo.processInfo.environment["PLAYAI_API_KEY"],
            let userId = ProcessInfo.processInfo.environment["PLAYAI_USER_ID"] else {
        fatalError("Please set PLAYAI_API_KEY and PLAYAI_USER_ID environment variables")
      }

      let headers = [
        "AUTHORIZATION": apiKey,
        "X-USER-ID": userId,
        "accept": "application/json"
      ]
      ```
    </CodeGroup>
  </Step>

  <Step title="Send Request to Generate PlayNote">
    Configure and send the request to create your podcast:

    <CodeGroup>
      ```python Python
      # Configure the request parameters
      files = {
          'sourceFileUrl': (None, SOURCE_FILE_URL),
          'synthesisStyle': (None, 'podcast'),
          'voice1': (None, 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json'),
          'voice1Name': (None, 'Angelo'),
          'voice2': (None, 's3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json'),
          'voice2Name': (None, 'Deedee'),
      }

      # Send the POST request
      response = requests.post(url, headers=headers, files=files)

      if response.status_code == 201:
          print("Request sent successfully!")
          playNoteId = response.json().get('id')
          print(f"Generated PlayNote ID: {playNoteId}")
      else:
          print(f"Failed to generate PlayNote: {response.text}")
      ```

      ```javascript JavaScript
      const files = {
        sourceFileUrl: SOURCE_FILE_URL,
        synthesisStyle: 'podcast',
        voice1: 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
        voice1Name: 'Angelo',
        voice2: 's3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json',
        voice2Name: 'Deedee'
      };

      fetch(url, {
        method: 'POST',
        headers: headers,
        body: JSON.stringify(files)
      })
      .then(response => response.json())
      .then(data => {
        if (response.status === 201) {
          console.log("Request sent successfully!");
          console.log(`Generated PlayNote ID: ${data.id}`);
        } else {
          console.log(`Failed to generate PlayNote: ${response.text}`);
        }
      });
      ```

      ```go Go
      files := map[string]string{
        "sourceFileUrl": SOURCE_FILE_URL,
        "synthesisStyle": "podcast",
        "voice1": "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
        "voice1Name": "Angelo",
        "voice2": "s3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json",
        "voice2Name": "Deedee",
      }

      jsonData, _ := json.Marshal(files)
      req, _ := http.NewRequest("POST", url, bytes.NewBuffer(jsonData))

      for key, value := range headers {
        req.Header.Add(key, value)
      }

      client := &http.Client{}
      resp, err := client.Do(req)
      if err != nil {
        log.Fatal(err)
      }
      defer resp.Body.Close()

      if resp.StatusCode == 201 {
        var result map[string]interface{}
        json.NewDecoder(resp.Body).Decode(&result)
        fmt.Println("Request sent successfully!")
        fmt.Printf("Generated PlayNote ID: %v\n", result["id"])
      } else {
        body, _ := io.ReadAll(resp.Body)
        fmt.Printf("Failed to generate PlayNote: %s\n", string(body))
      }
      ```

      ```dart Dart
      final files = {
        'sourceFileUrl': SOURCE_FILE_URL,
        'synthesisStyle': 'podcast',
        'voice1': 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
        'voice1Name': 'Angelo',
        'voice2': 's3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json',
        'voice2Name': 'Deedee'
      };

      try {
        final response = await http.post(
          url,
          headers: headers,
          body: jsonEncode(files),
        );

        if (response.statusCode == 201) {
          final data = jsonDecode(response.body);
          print("Request sent successfully!");
          print("Generated PlayNote ID: ${data['id']}");
        } else {
          print("Failed to generate PlayNote: ${response.body}");
        }
      } catch (e) {
        print("Error: $e");
      }
      ```

      ```swift Swift
      let files: [String: Any] = [
        "sourceFileUrl": SOURCE_FILE_URL,
        "synthesisStyle": "podcast",
        "voice1": "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
        "voice1Name": "Angelo",
        "voice2": "s3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json",
        "voice2Name": "Deedee"
      ]

      func createPlayNote() async throws {
        var request = URLRequest(url: url)
        request.httpMethod = "POST"
        request.allHTTPHeaderFields = headers
        request.httpBody = try JSONSerialization.data(withJSONObject: files)

        let (data, response) = try await URLSession.shared.data(for: request)
        
        guard let httpResponse = response as? HTTPURLResponse else {
          throw NSError(domain: "", code: -1, userInfo: [NSLocalizedDescriptionKey: "Invalid response"])
        }

        if httpResponse.statusCode == 201 {
          let result = try JSONDecoder().decode([String: Any].self, from: data)
          print("Request sent successfully!")
          if let playNoteId = result["id"] as? String {
            print("Generated PlayNote ID: \(playNoteId)")
          }
        } else {
          print("Failed to generate PlayNote: \(String(data: data, encoding: .utf8) ?? "")")
        }
      }
      ```
    </CodeGroup>
  </Step>

  <Step title="Poll for Completion">
    Check the status of your PlayNote generation:

    <CodeGroup>
      ```python Python
      import urllib.parse
      import time

      # Double-encode the PlayNote ID for the URL
      double_encoded_id = urllib.parse.quote(playNoteId, safe='')

      # Construct the final URL to check the status
      status_url = f"https://api.play.ai/api/v1/playnotes/{double_encoded_id}"

      # Poll for completion
      while True:
          response = requests.get(status_url, headers=headers)
          if response.status_code == 200:
              playnote_data = response.json()
              status = playnote_data['status']
              if status == 'completed':
                  print("PlayNote generation complete!")
                  print("Audio URL:", playnote_data['audioUrl'])
                  break
              elif status == 'generating':
                  print("Please wait, your PlayNote is still generating...")
                  time.sleep(120)  # Wait for 2 minutes before polling again
              else:
                  print("PlayNote creation failed, please try again.")
                  break
          else:
              print(f"Error polling for PlayNote status: {response.text}")
              break
      ```

      ```javascript JavaScript
      const doubleEncodedId = encodeURIComponent(playNoteId);
      const statusUrl = `https://api.play.ai/api/v1/playnotes/${doubleEncodedId}`;

      const pollStatus = async () => {
        const response = await fetch(statusUrl, { headers });
        if (response.ok) {
          const data = await response.json();
          if (data.status === 'completed') {
            console.log("PlayNote generation complete!");
            console.log("Audio URL:", data.audioUrl);
            return;
          } else if (data.status === 'generating') {
            console.log("Please wait, your PlayNote is still generating...");
            setTimeout(pollStatus, 120000); // Poll every 2 minutes
          } else {
            console.log("PlayNote creation failed, please try again.");
          }
        } else {
          console.log(`Error polling for PlayNote status: ${response.text}`);
        }
      };

      pollStatus();
      ```

      ```go Go
      doubleEncodedId := url.QueryEscape(playNoteId)
      statusUrl := fmt.Sprintf("https://api.play.ai/api/v1/playnotes/%s", doubleEncodedId)

      for {
        req, _ := http.NewRequest("GET", statusUrl, nil)
        for key, value := range headers {
          req.Header.Add(key, value)
        }

        resp, err := client.Do(req)
        if err != nil {
          log.Fatal(err)
        }

        var data map[string]interface{}
        json.NewDecoder(resp.Body).Decode(&data)
        resp.Body.Close()

        if resp.StatusCode == 200 {
          status := data["status"].(string)
          if status == "completed" {
            fmt.Println("PlayNote generation complete!")
            fmt.Printf("Audio URL: %v\n", data["audioUrl"])
            break
          } else if status == "generating" {
            fmt.Println("Please wait, your PlayNote is still generating...")
            time.Sleep(120 * time.Second)
          } else {
            fmt.Println("PlayNote creation failed, please try again.")
            break
          }
        } else {
          fmt.Printf("Error polling for PlayNote status: %s\n", resp.Status)
          break
        }
      }
      ```

      ```dart Dart
      Future<void> pollStatus(String playNoteId) async {
        final doubleEncodedId = Uri.encodeComponent(playNoteId);
        final statusUrl = Uri.parse("https://api.play.ai/api/v1/playnotes/$doubleEncodedId");

        while (true) {
          try {
            final response = await http.get(statusUrl, headers: headers);
            if (response.statusCode == 200) {
              final data = jsonDecode(response.body);
              final status = data['status'];
              
              if (status == 'completed') {
                print("PlayNote generation complete!");
                print("Audio URL: ${data['audioUrl']}");
                break;
              } else if (status == 'generating') {
                print("Please wait, your PlayNote is still generating...");
                await Future.delayed(Duration(seconds: 120));
              } else {
                print("PlayNote creation failed, please try again.");
                break;
              }
            } else {
              print("Error polling for PlayNote status: ${response.body}");
              break;
            }
          } catch (e) {
            print("Error polling status: $e");
            break;
          }
        }
      }
      ```

      ```swift Swift
      func pollStatus(playNoteId: String) async throws {
        let doubleEncodedId = playNoteId.addingPercentEncoding(withAllowedCharacters: .urlPathAllowed) ?? playNoteId
        let statusUrl = URL(string: "https://api.play.ai/api/v1/playnotes/\(doubleEncodedId)")!

        while true {
          var request = URLRequest(url: statusUrl)
          request.allHTTPHeaderFields = headers

          let (data, response) = try await URLSession.shared.data(for: request)
          
          guard let httpResponse = response as? HTTPURLResponse else {
            throw NSError(domain: "", code: -1, userInfo: [NSLocalizedDescriptionKey: "Invalid response"])
          }

          if httpResponse.statusCode == 200 {
            let result = try JSONDecoder().decode([String: Any].self, from: data)
            if let status = result["status"] as? String {
              switch status {
              case "completed":
                print("PlayNote generation complete!")
                if let audioUrl = result["audioUrl"] as? String {
                  print("Audio URL: \(audioUrl)")
                }
                return
              case "generating":
                print("Please wait, your PlayNote is still generating...")
                try await Task.sleep(nanoseconds: 120_000_000_000) // 120 seconds
              default:
                print("PlayNote creation failed, please try again.")
                return
              }
            }
          } else {
            print("Error polling for PlayNote status: \(String(data: data, encoding: .utf8) ?? "")")
            return
          }
        }
      }
      ```
    </CodeGroup>
  </Step>

  <Step title="Run and Test">
    Follow these steps to run your code:

    <Tabs>
      <Tab title="Python">
        1. Save your code as `playnote_generator.py`
        2. Open terminal in your code directory
        3. Run: `python3 playnote_generator.py`
        4. Wait for the generation process to complete
        5. Access your generated audio using the provided URL
      </Tab>

      <Tab title="JavaScript">
        1. Save your code as `playnote_generator.js`
        2. Install dependencies: `npm install node-fetch`
        3. Run: `node playnote_generator.js`
        4. Wait for the generation process to complete
        5. Access your generated audio using the provided URL
      </Tab>

      <Tab title="Go">
        1. Save your code as `playnote_generator.go`
        2. Run: `go run playnote_generator.go`
        3. Wait for the generation process to complete
        4. Access your generated audio using the provided URL
      </Tab>

      <Tab title="Dart">
        1. Save your code as `playnote_generator.dart`
        2. Install dependencies: `dart pub add http`
        3. Run: `dart run playnote_generator.dart`
        4. Wait for the generation process to complete
        5. Access your generated audio using the provided URL
      </Tab>

      <Tab title="Swift">
        1. Save your code as `playnote_generator.swift`
        2. Run: `swift playnote_generator.swift`
        3. Wait for the generation process to complete
        4. Access your generated audio using the provided URL
      </Tab>
    </Tabs>
  </Step>
</Steps>

## Complete Code

<CodeGroup>
  ```python Python [expandable]
  import requests
  import os
  import urllib.parse
  import time

  # Define the URL of your PDF file
  SOURCE_FILE_URL = "https://godinton.kent.sch.uk/media/2601/goldilocks-story.pdf"

  # PlayNote API URL
  url = "https://api.play.ai/api/v1/playnotes"

  # Retrieve API key and User ID from environment variables
  api_key = os.getenv("PLAYAI_API_KEY")
  user_id = os.getenv("PLAYAI_USER_ID")

  # Set up headers with authorization details
  headers = {
      'AUTHORIZATION': api_key,
      'X-USER-ID': user_id,
      'accept': 'application/json'
  }

  # Configure the request parameters
  files = {
      'sourceFileUrl': (None, SOURCE_FILE_URL),
      'synthesisStyle': (None, 'podcast'),
      'voice1': (None, 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json'),
      'voice1Name': (None, 'Angelo'),
      'voice2': (None, 's3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json'),
      'voice2Name': (None, 'Deedee'),
  }

  # Send the POST request
  response = requests.post(url, headers=headers, files=files)

  if response.status_code == 201:
      print("Request sent successfully!")
      playNoteId = response.json().get('id')
      print(f"Generated PlayNote ID: {playNoteId}")

      # Double-encode the PlayNote ID for the URL
      double_encoded_id = urllib.parse.quote(playNoteId, safe='')

      # Construct the final URL to check the status
      status_url = f"https://api.play.ai/api/v1/playnotes/{double_encoded_id}"

      # Poll for completion
      while True:
          response = requests.get(status_url, headers=headers)
          if response.status_code == 200:
              playnote_data = response.json()
              status = playnote_data['status']
              if status == 'completed':
                  print("PlayNote generation complete!")
                  print("Audio URL:", playnote_data['audioUrl'])
                  break
              elif status == 'generating':
                  print("Please wait, your PlayNote is still generating...")
                  time.sleep(120)  # Wait for 2 minutes before polling again
              else:
                  print("PlayNote creation failed, please try again.")
                  break
          else:
              print(f"Error polling for PlayNote status: {response.text}")
              break
  else:
      print(f"Failed to generate PlayNote: {response.text}")
  ```

  ```javascript JavaScript [expandable]
  const fetch = require('node-fetch');

  const SOURCE_FILE_URL = "https://godinton.kent.sch.uk/media/2601/goldilocks-story.pdf";
  const url = "https://api.play.ai/api/v1/playnotes";

  const apiKey = process.env.PLAYAI_API_KEY;
  const userId = process.env.PLAYAI_USER_ID;

  const headers = {
    'AUTHORIZATION': apiKey,
    'X-USER-ID': userId,
    'accept': 'application/json'
  };

  const files = {
    sourceFileUrl: SOURCE_FILE_URL,
    synthesisStyle: 'podcast',
    voice1: 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
    voice1Name: 'Angelo',
    voice2: 's3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json',
    voice2Name: 'Deedee'
  };

  fetch(url, {
    method: 'POST',
    headers: headers,
    body: JSON.stringify(files)
  })
  .then(response => response.json())
  .then(data => {
    if (response.status === 201) {
      console.log("Request sent successfully!");
      console.log(`Generated PlayNote ID: ${data.id}`);
    } else {
      console.log(`Failed to generate PlayNote: ${response.text}`);
    }
  });
  ```

  ```go Go [expandable]
  package main

  import (
    "bytes"
    "encoding/json"
    "fmt"
    "io"
    "log"
    "net/http"
    "net/url"
    "os"
    "time"
  )

  const SOURCE_FILE_URL = "https://godinton.kent.sch.uk/media/2601/goldilocks-story.pdf"
  const apiUrl = "https://api.play.ai/api/v1/playnotes"

  func main() {
    apiKey := os.Getenv("PLAYAI_API_KEY")
    userId := os.Getenv("PLAYAI_USER_ID")

    headers := map[string]string{
      "AUTHORIZATION": apiKey,
      "X-USER-ID": userId,
      "accept": "application/json",
    }

    files := map[string]string{
      "sourceFileUrl": SOURCE_FILE_URL,
      "synthesisStyle": "podcast",
      "voice1": "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
      "voice1Name": "Angelo",
      "voice2": "s3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json",
      "voice2Name": "Deedee",
    }

    jsonData, _ := json.Marshal(files)
    req, _ := http.NewRequest("POST", apiUrl, bytes.NewBuffer(jsonData))
    
    for key, value := range headers {
      req.Header.Add(key, value)
    }

    client := &http.Client{}
    resp, err := client.Do(req)
    if err != nil {
      log.Fatal(err)
    }
    defer resp.Body.Close()

    if resp.StatusCode == 201 {
      var result map[string]interface{}
      json.NewDecoder(resp.Body).Decode(&result)
      fmt.Println("Request sent successfully!")
      playNoteId := result["id"].(string)
      fmt.Printf("Generated PlayNote ID: %s\n", playNoteId)

      doubleEncodedId := url.QueryEscape(playNoteId)
      statusUrl := fmt.Sprintf("https://api.play.ai/api/v1/playnotes/%s", doubleEncodedId)

      for {
        req, _ := http.NewRequest("GET", statusUrl, nil)
        for key, value := range headers {
          req.Header.Add(key, value)
        }

        resp, err := client.Do(req)
        if err != nil {
          log.Fatal(err)
        }

        var data map[string]interface{}
        json.NewDecoder(resp.Body).Decode(&data)
        resp.Body.Close()

        if resp.StatusCode == 200 {
          status := data["status"].(string)
          if status == "completed" {
            fmt.Println("PlayNote generation complete!")
            fmt.Printf("Audio URL: %v\n", data["audioUrl"])
            break
          } else if status == "generating" {
            fmt.Println("Please wait, your PlayNote is still generating...")
            time.Sleep(120 * time.Second)
          } else {
            fmt.Println("PlayNote creation failed, please try again.")
            break
          }
        } else {
          fmt.Printf("Error polling for PlayNote status: %s\n", resp.Status)
          break
        }
      }
    } else {
      body, _ := io.ReadAll(resp.Body)
      fmt.Printf("Failed to generate PlayNote: %s\n", string(body))
    }
  }
  ```

  ```dart Dart [expandable]
  import 'dart:io';
  import 'package:http/http.dart' as http;
  import 'dart:convert';

  const SOURCE_FILE_URL = "https://godinton.kent.sch.uk/media/2601/goldilocks-story.pdf";
  final url = Uri.parse("https://api.play.ai/api/v1/playnotes");

  final apiKey = Platform.environment['PLAYAI_API_KEY'];
  final userId = Platform.environment['PLAYAI_USER_ID'];

  final headers = {
    'AUTHORIZATION': apiKey!,
    'X-USER-ID': userId!,
    'accept': 'application/json'
  };

  final files = {
    'sourceFileUrl': SOURCE_FILE_URL,
    'synthesisStyle': 'podcast',
    'voice1': 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
    'voice1Name': 'Angelo',
    'voice2': 's3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json',
    'voice2Name': 'Deedee'
  };

  void main() async {
    try {
      final response = await http.post(
        url,
        headers: headers,
        body: jsonEncode(files),
      );

      if (response.statusCode == 201) {
        final data = jsonDecode(response.body);
        print("Request sent successfully!");
        final playNoteId = data['id'];
        print("Generated PlayNote ID: $playNoteId");

        await pollStatus(playNoteId);
      } else {
        print("Failed to generate PlayNote: ${response.body}");
      }
    } catch (e) {
      print("Error: $e");
    }
  }

  Future<void> pollStatus(String playNoteId) async {
    final doubleEncodedId = Uri.encodeComponent(playNoteId);
    final statusUrl = Uri.parse("https://api.play.ai/api/v1/playnotes/$doubleEncodedId");

    while (true) {
      try {
        final response = await http.get(statusUrl, headers: headers);
        if (response.statusCode == 200) {
          final data = jsonDecode(response.body);
          final status = data['status'];
          
          if (status == 'completed') {
            print("PlayNote generation complete!");
            print("Audio URL: ${data['audioUrl']}");
            break;
          } else if (status == 'generating') {
            print("Please wait, your PlayNote is still generating...");
            await Future.delayed(Duration(seconds: 120));
          } else {
            print("PlayNote creation failed, please try again.");
            break;
          }
        } else {
          print("Error polling for PlayNote status: ${response.body}");
          break;
        }
      } catch (e) {
        print("Error polling status: $e");
        break;
      }
    }
  }
  ```

  ```swift Swift [expandable]
  import Foundation

  let SOURCE_FILE_URL = "https://godinton.kent.sch.uk/media/2601/goldilocks-story.pdf"
  let url = URL(string: "https://api.play.ai/api/v1/playnotes")!

  guard let apiKey = ProcessInfo.processInfo.environment["PLAYAI_API_KEY"],
        let userId = ProcessInfo.processInfo.environment["PLAYAI_USER_ID"] else {
    fatalError("Please set PLAYAI_API_KEY and PLAYAI_USER_ID environment variables")
  }

  let headers = [
    "AUTHORIZATION": apiKey,
    "X-USER-ID": userId,
    "accept": "application/json"
  ]

  let files: [String: Any] = [
    "sourceFileUrl": SOURCE_FILE_URL,
    "synthesisStyle": "podcast",
    "voice1": "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
    "voice1Name": "Angelo",
    "voice2": "s3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json",
    "voice2Name": "Deedee"
  ]

  func createPlayNote() async throws {
    var request = URLRequest(url: url)
    request.httpMethod = "POST"
    request.allHTTPHeaderFields = headers
    request.httpBody = try JSONSerialization.data(withJSONObject: files)

    let (data, response) = try await URLSession.shared.data(for: request)
    
    guard let httpResponse = response as? HTTPURLResponse else {
      throw NSError(domain: "", code: -1, userInfo: [NSLocalizedDescriptionKey: "Invalid response"])
    }

    if httpResponse.statusCode == 201 {
        let result = try JSONDecoder().decode([String: Any].self, from: data)
        print("Request sent successfully!")
        if let playNoteId = result["id"] as? String {
            print("Generated PlayNote ID: \(playNoteId)")
            try await pollStatus(playNoteId: playNoteId)
        }
    } else {
        print("Failed to generate PlayNote: \(String(data: data, encoding: .utf8) ?? "")")
    }
  }

  func pollStatus(playNoteId: String) async throws {
    let doubleEncodedId = playNoteId.addingPercentEncoding(withAllowedCharacters: .urlPathAllowed) ?? playNoteId
    let statusUrl = URL(string: "https://api.play.ai/api/v1/playnotes/\(doubleEncodedId)")!

    while true {
        var request = URLRequest(url: statusUrl)
        request.allHTTPHeaderFields = headers

        let (data, response) = try await URLSession.shared.data(for: request)
        
        guard let httpResponse = response as? HTTPURLResponse else {
            throw NSError(domain: "", code: -1, userInfo: [NSLocalizedDescriptionKey: "Invalid response"])
        }

        if httpResponse.statusCode == 200 {
            let result = try JSONDecoder().decode([String: Any].self, from: data)
            if let status = result["status"] as? String {
                switch status {
                case "completed":
                    print("PlayNote generation complete!")
                    if let audioUrl = result["audioUrl"] as? String {
                        print("Audio URL: \(audioUrl)")
                    }
                    return
                case "generating":
                    print("Please wait, your PlayNote is still generating...")
                    try await Task.sleep(nanoseconds: 120_000_000_000) // 120 seconds
                default:
                    print("PlayNote creation failed, please try again.")
                    return
                }
            }
        } else {
            print("Error polling for PlayNote status: \(String(data: data, encoding: .utf8) ?? "")")
            return
        }
    }
  }

  // Run the async function
  Task {
    do {
        try await createPlayNote()
    } catch {
        print("Error: \(error)")
    }
  }
  ```
</CodeGroup>

## Troubleshooting

If you encounter issues, check these common problems:

* **Authentication Issues:**
  * Verify your API key and user ID are correctly set in your environment
  * Confirm the `AUTHORIZATION` header is properly formatted

* **Source File Issues:**
  * Ensure your PDF URL is publicly accessible
  * Verify the PDF file is not corrupted or password-protected

* **Generation Time:**
  * The process typically takes 5-10 minutes depending on the PDF size
  * If the status remains "generating" for an extended period, try creating a new request

* **API Endpoint Errors:**
  * Verify you're using the correct PlayNote API endpoint
  * Check that your request payload matches the expected format

This guide provides a simple yet powerful way to turn text content from a PDF into a rich, conversational podcast format using PlayNote API. Modify the voice parameters to customize the conversation to match your desired style.


# Changelog
Source: https://docs.play.ai/documentation/resources/changelog

Latest updates and improvements to PlayAI

<Update label="2025-04-09" description="v1">
  * Add [MCP](/documentation/resources/mcp) to the documentation.
  * PlayAI Docs now has a voice agent that can help you navigate the docs.
</Update>

<Update label="2025-03-29" description="v1">
  Introducing [Dialog 1.0 Turbo](/documentation/text-to-speech/tts-models#dialog-1-0-turbo), a latency-optimized version
  of our flagship Dialog 1.0 model, offering high-quality text-to-speech at a blazing speed.
</Update>

<Update label="2025-03-27" description="v1">
  New "List Voices" endpoint: Added a new [endpoint](/api-reference/text-to-speech/endpoints/v1/list-voices) to list all
  available pre-built voices in the PlayAI API.
</Update>


# Error Messages
Source: https://docs.play.ai/documentation/resources/error-messages

Understanding and handling PlayAI API errors

<Info>This page is under construction. Please check back soon for more information.</Info>


# Model Context Protocol (MCP)
Source: https://docs.play.ai/documentation/resources/mcp

Set up MCP to enable AI tools to access your PlayAI services

## What is MCP?

Model Context Protocol (MCP) is a standardized way for AI models to interact with external tools and APIs. It allows AI assistants like Claude, Cursor, and Windsurf to use your PlayAI APIs directly.

MCP acts as a universal adapter between large language models (LLMs) and various data sources and tools. With MCP integration, AI tools can:

* Search and understand your PlayAI documentation
* Make direct API calls to PlayAI services on your behalf
* Provide accurate, real-time information about PlayAI capabilities

<Tip>
  MCP transforms AI assistants from simple chatbots to powerful tools that can help you use PlayAI services more
  effectively.
</Tip>

## Setting Up PlayAI MCP

<Steps>
  <Step title="Install the MCP CLI">
    First, install the Mintlify MCP CLI tool:

    ```bash
    npm install -g @mintlify/mcp
    ```
  </Step>

  <Step title="Add the PlayAI MCP Server">
    Run the following command to install the PlayAI MCP server:

    ```bash
    npx @mintlify/mcp add playhtinc
    ```

    <Warning>
      During installation, the CLI will prompt you for authentication information. When it asks for "Bearer Token", provide your Secret Key. When it asks for "API Key", provide your PlayAI **User ID**.
    </Warning>
  </Step>

  <Step title="Start the MCP Server">
    After installation, you can start the MCP server with the command provided at the end of the installation process. It will look similar to:

    ```bash
    node /Users/[username]/.mcp/playhtinc/src/index.js
    ```

    This server runs locally on your machine and acts as a bridge between AI tools and PlayAI services.
  </Step>

  <Step title="Configure Your AI Tools">
    Depending on which AI assistant you're using, you'll need to configure it to use the PlayAI MCP server.

    ### For Cursor

    The installation process should automatically update your Cursor configuration. You can verify this by checking `~/.cursor/mcp.json`.

    ### For Claude Desktop

    1. Navigate to Settings > Developer
    2. Click on Edit Config
    3. Add the following to your `claude_desktop_config.json`:

    ```json
    {
      "mcpServers": {
        "playhtinc": {
          "command": "node",
          "args": [
            "/Users/[username]/.mcp/playhtinc/src/index.js"
          ]
        }
      }
    }
    ```

    Replace `[username]` with your actual username.
  </Step>
</Steps>

## Using MCP with AI Tools

Once your MCP server is running, supported AI tools like Claude Desktop or Cursor will be able to:

1. Access PlayAI documentation to answer your questions
2. Generate text-to-speech content using PlayAI's models
3. Manage voices and other PlayAI resources

Simply ask the AI tool about PlayAI services, and it will use the MCP server to provide accurate information and perform actions.

<Note>
  Your MCP server runs locally and securely on your machine. Your credentials are never sent to third parties.
</Note>

## Troubleshooting

If you encounter issues with your MCP setup:

1. **Authentication Errors**: Double-check that you provided your User ID when prompted during installation, not your API key
2. **Connection Issues**: Ensure the MCP server is running in a terminal window
3. **AI Tool Not Detecting MCP**: Verify your AI tool's configuration has been updated correctly

Need further assistance? Contact our [support team](mailto:support@play.ai) for help with MCP configuration.


# Rate Limits
Source: https://docs.play.ai/documentation/resources/rate-limits

Understanding PlayAI API rate limits and quotas

To prevent abuse, our APIs are rate-limited. The specific limits depend on the API you are using and the plan you are on.

The limits applied to our API are shown in the table below. They are applied per API key and are reset every minute.

| Endpoint                           | Rate Limit      |
| ---------------------------------- | --------------- |
| `POST /api/v1/agents`              | 20 requests/min |
| `PATCH /api/v1/agents/:agentId`    | 20 requests/min |
| `GET /api/v1/agents/:agentId`      | 60 requests/min |
| `GET /api/v1/agent-stats/:agentId` | 60 requests/min |

## How are rate limits enforced?

Our API enforces rate limits to ensure fair usage and maintain system stability. The rate limits are calculated per
minute, based on the total number of requests made within a 60-second window.

## What happens when the rate limits are exceeded?

If you exceed your allotted rate limit, your subsequent requests will receive a `429 - Too Many Requests` HTTP status
code. This response indicates that the request quota for the current minute has been exceeded. You can resume sending
requests after the minute has elapsed, at which point the rate limit counter resets.

## Can I request higher rate limits?

Need higher call volumes? We can accommodate that! Rate limits are configurable on a per-client basis to accommodate
varying needs. If you require higher limits, please [contact us](https://play.ht/contact-us/) with details about your
specific use case requirements., and we'll work together to find the most suitable rate limit strategy.


# Troubleshooting
Source: https://docs.play.ai/documentation/resources/troubleshooting

Common issues and solutions for PlayAI APIs

<Info>This page is under construction. Please check back soon for more information.</Info>


# Text to Speech
Source: https://docs.play.ai/documentation/text-to-speech/introduction

Turn text into lifelike speech with PlayAI's API

PlayAI's Text-to-Speech (TTS) service provides advanced capabilities for generating natural, human-like speech from text. Our PlayDialog model offers state-of-the-art voice synthesis with support for multiple speakers, pacing control, and real-time streaming.

## Key Features

<CardGroup cols={3}>
  <Card title="Realistic Speech" icon="waveform-lines" iconType="duotone">
    Generate lifelike speech with natural intonation and prosody
  </Card>

  <Card title="200+ Prebuilt Voices" icon="microphone-lines" iconType="duotone">
    Choose from a wide range of studio-quality voices
  </Card>

  <Card title="Multi-Speaker" icon="users" iconType="duotone">
    Support for multi-speaker dialogs
  </Card>

  <Card title="Industry-leading Voice Cloning" icon="clone" iconType="duotone">
    Create high-quality custom voices from 30-second audio samples
  </Card>

  <Card title="Real-time Streaming" icon="bolt" iconType="duotone">
    Stream audio in real-time to reduce latency
  </Card>

  <Card title="Style Control and Pacing" icon="face-smile" iconType="duotone">
    Control speech style, pacing, and emotion natively
  </Card>
</CardGroup>

## API Options

PlayAI provides multiple ways to use our TTS service:

1. **Real-time HTTP Streaming**

   * Stream audio as it's generated
   * Perfect for interactive applications
   * Low latency response

2. **Async HTTP API**

   * Generate audio files asynchronously
   * Better for longer texts
   * Background processing

3. **WebSocket API**
   * Bi-directional communication
   * Real-time streaming with control
   * Ideal for chat applications

## Getting Started

1. **Quick Start**: Follow our [TTS Quickstart](/documentation/text-to-speech/tts-quickstart) guide
2. **Create an AI Podcast**: Explore [dialog creation](/documentation/tutorials/tts/dialogs/create-ai-podcast)

## Best Practices

1. **Voice Selection**

   * Choose appropriate voices for your use case
   * Consider using voice cloning for custom voices
   * Test different voices for optimal results

2. **Performance**

   * Use streaming for real-time applications
   * Consider async API for longer texts
   * Cache frequently used audio

3. **Error Handling**
   * Implement proper error handling
   * Monitor API rate limits
   * Handle network issues gracefully

## Resources

* [Rate Limits](/documentation/resources/rate-limits)
* [Error Messages](/documentation/resources/error-messages)
* [Troubleshooting Guide](/documentation/resources/troubleshooting)


# Models
Source: https://docs.play.ai/documentation/text-to-speech/tts-models

Explore our state-of-the-art text-to-speech models

<CardGroup cols={2}>
  <Card title="Dialog 1.0" href="/documentation/text-to-speech/tts-models#dialog-1-0">
    Our flagship, feature-rich model for realistic, multi-speaker speech.
  </Card>

  <Card title="Play 3.0 Mini" href="/documentation/text-to-speech/tts-models#play-3-0-mini">
    Our fastest model for ultra-low latency applications.
  </Card>
</CardGroup>

<Card title="Dialog 1.0 Turbo" href="/documentation/text-to-speech/tts-models#dialog-1-0-turbo">
  Our fastest and best-quality model, optimized for speed while maintaining high quality, but with a more focused
  feature set.
</Card>

## Overview

The PlayAI API offers a range of text-to-speech models optimized for different use cases, quality levels, and performance requirements.

| Model             | Model ID          | Description                                                                           | Best For                                                                     |
| ----------------- | ----------------- | ------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| Dialog 1.0        | `PlayDialog`      | Our flagship model for realistic, multi-speaker speech with rich emotional expression | High-quality voice synthesis, multi-speaker conversations, emotional content |
| Dialog 1.0 Turbo  | `PlayDialogTurbo` | A faster version of Dialog 1.0 with a more focused feature set                        | Low-latency applications, real-time interactions, gaming                     |
| Play 3.0 Mini     | `Play3.0-mini`    | Ultra-fast model optimized for real-time applications                                 | Low-latency applications, real-time interactions, gaming                     |
| Play 2.0 (Legacy) | `Play2.0`         | Previous generation model with high performance                                       | General purpose text-to-speech applications                                  |

**Language Support**

All non-Turbo models support the following languages: Afrikaans, Albanian, Amharic, Arabic, Bengali, Bulgarian, Catalan, Croatian, Czech, Danish, Dutch, English, French, Galician, German, Greek, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Korean, Malay, Mandarin, Polish, Portuguese, Russian, Serbian, Spanish, Swedish, Tagalog, Thai, Turkish, Ukrainian, Urdu, and Xhosa.
(Performance may vary across languages.)

At this time, Dialog 1.0 Turbo only supports English and Arabic.

## Dialog 1.0

Dialog 1.0 is our most advanced speech synthesis model, designed for high-quality, emotionally-aware speech with native multi-speaker support. It produces natural, lifelike speech with rich emotional expression and contextual understanding across multiple languages.

The model delivers consistent voice quality and personality across all supported languages while maintaining the speaker's unique characteristics and accent. It offers the most comprehensive emotion control among our models, allowing for precise emotional expression in the synthesized speech.

This model excels in scenarios requiring high-quality, emotionally nuanced speech:

* **Multi-Speaker Conversations**: Native support for multiple speakers in a single conversation.
* **Character Voiceovers**: Ideal for gaming and animation due to its emotional range.
* **Professional Content**: Well-suited for corporate videos and e-learning materials.
* **Multilingual Projects**: Maintains consistent voice quality across language switches.

Dialog 1.0 supports native pace adjustment, allowing you to control the speed of speech while maintaining natural prosody. While it has a higher latency than Play 3.0 Mini, it delivers superior quality for projects where lifelike speech and emotional expression are important.

## Dialog 1.0 Turbo

Dialog 1.0 Turbo is a latency-optimized version of Dialog 1.0, while maintaining similar quality.

Dialog 1.0 Turbo has a more focused feature set compared to Dialog 1.0, with support for English and Arabic languages only. It currently works with a curated set of pre-built voices and does not support custom voice cloning.

Key characteristics:

* **Fast and High Quality**: Optimized for minimal latency while maintaining Dialog 1.0's quality.
* **Limited Language Support**: Currently supports English and Arabic only.
* **Pre-built Voices**: Works with a select set of high-quality pre-built voices.
* **Streamlined Features**: Focused feature set for maximum performance.

This model is ideal for applications that:

* Need Dialog 1.0's quality with faster processing.
* Work primarily with English or Arabic content.
* Require consistent, high-quality pre-built voices.
* Need reliable, production-ready speech synthesis.

For detailed API information and available voices, see the [Dialog 1.0 Turbo endpoint documentation](/api-reference/text-to-speech/endpoints/v1/stream-speech-turbo).

## Play 3.0 Mini

Play 3.0 Mini is our fastest speech synthesis model, designed for real-time applications and ultra-low latency scenarios. It delivers high-quality speech with minimal delay (\~50ms) while maintaining natural-sounding output across all supported languages.

The model balances speed and quality, making it ideal for interactive applications while maintaining consistent voice characteristics. Like Dialog 1.0, it supports native pace adjustment, allowing for flexible speech rate control.

This model is particularly well-suited for:

* **Real-time Applications**: Perfect for live voice interactions and chatbots.
* **Interactive Gaming**: Ideal for games requiring immediate voice feedback.
* **Low-latency Systems**: Efficient for applications where speed is critical.
* **Large-Scale Processing**: Cost-effective for bulk text-to-speech conversion.

With its lower latency and competitive pricing, Play 3.0 Mini is the optimal choice for applications requiring fast, reliable speech synthesis without compromising on quality.

## Play 2.0 (Legacy)

Play 2.0 is our fastest speech synthesis model, delivering blazing-fast performance for real-time applications. While it may not have the advanced features of Dialog 1.0, it provides ultra-low latency speech synthesis that's perfect for applications requiring immediate response times.

This model is well-suited for:

* **Ultra-Low Latency Applications**: Perfect for real-time voice interactions and immediate feedback.
* **High-Performance Systems**: Ideal for applications where speed is the top priority.
* **Basic Text-to-Speech Applications**: Reliable for standard use cases with maximum performance.
* **Simple Voice Applications**: Effective for straightforward voice synthesis needs with minimal delay.

While Play 2.0 continues to be supported, we recommend using Dialog 1.0 or Play 3.0 Mini for new applications to take advantage of their enhanced features and capabilities.


# Text-to-Speech Quickstart
Source: https://docs.play.ai/documentation/text-to-speech/tts-quickstart

Get started with PlayAI Text-to-Speech in minutes

This quickstart guide will help you programmatically convert text into audio using PlayAI's models.

<Tip>
  Check out our [Playground](https://play.ai/home) for a quick way to try out our voices without writing any code.
</Tip>

## Prerequisites

* [Access credentials](https://play.ai/api/keys) (your secret key and user ID)

## Converting Text to Speech

<Steps>
  <Step title="Set Up Your Environment">
    First, let's set up your API credentials securely:

    <CodeGroup>
      ```bash macOS (zsh)
        echo 'export PLAYAI_KEY="your_api_key_here"' >> ~/.zshrc
        echo 'export PLAYAI_USER_ID="your_user_id_here"' >> ~/.zshrc
        source ~/.zshrc
      ```

      ```bash bash
      echo 'export PLAYAI_KEY="your_api_key_here"' >> ~/.bashrc
      echo 'export PLAYAI_USER_ID="your_user_id_here"' >> ~/.bashrc
      source ~/.bashrc
      ```

      ```cmd Windows
      setx PLAYAI_KEY "your_api_key_here"
      setx PLAYAI_USER_ID "your_user_id_here"
      ```
    </CodeGroup>
  </Step>

  <Step title="Create Your First Audio">
    Run the following script to create your first audio.

    <CodeGroup>
      ```bash bash
      curl -X POST 'https://api.play.ai/api/v1/tts/stream' \
        -H "Authorization: Bearer $PLAYAI_KEY" \
        -H "Content-Type: application/json" \
        -H "X-USER-ID: $PLAYAI_USER_ID" \
        -d '{
          "model": "PlayDialog",
          "text": "Hello! This is my first text-to-speech audio using PlayAI!",
          "voice": "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
          "outputFormat": "wav"
        }' \
        --output hello.wav
      ```

      ```python Python
      import requests
      import os

      # Get credentials from environment variables
      api_key = os.getenv("PLAYAI_KEY")
      user_id = os.getenv("PLAYAI_USER_ID")

      # Set up headers
      headers = {
          'Authorization': f'Bearer {api_key}',
          'Content-Type': 'application/json',
          'X-USER-ID': user_id
      }

      # Prepare the request
      json_data = {
          'model': 'PlayDialog',
          'text': "Hello! This is my first text-to-speech audio using PlayAI!",
          'voice': 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
          'outputFormat': 'wav'
      }

      # Make the API call
      response = requests.post(
          'https://api.play.ai/api/v1/tts/stream',
          headers=headers,
          json=json_data
      )

      # Save the audio file
      if response.status_code == 200:
          with open('hello.wav', 'wb') as f:
              f.write(response.content)
          print("Audio saved as hello.wav")
      else:
          print(f"Error: {response.status_code} - {response.text}")
      ```

      ```javascript JavaScript
      const fs = require('fs');

      async function generateSpeech() {
        const apiKey = process.env.PLAYAI_KEY;
        const userId = process.env.PLAYAI_USER_ID;

        const headers = {
          'Authorization': `Bearer ${apiKey}`,
          'Content-Type': 'application/json',
          'X-USER-ID': userId
        };

        const jsonData = {
          model: 'PlayDialog',
          text: 'Hello! This is my first text-to-speech audio using PlayAI!',
          voice: 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
          outputFormat: 'wav'
        };

        try {
          const response = await fetch('https://api.play.ai/api/v1/tts/stream', {
            method: 'POST',
            headers: headers,
            body: JSON.stringify(jsonData)
          });

          if (!response.ok) {
            throw new Error(`Error: ${response.status} - ${await response.text()}`);
          }

          const buffer = await response.arrayBuffer();
          fs.writeFileSync('hello.wav', Buffer.from(buffer));
          console.log('Audio saved as hello.wav');
        } catch (error) {
          console.error('Error:', error.message);
        }
      }

      generateSpeech();
      ```

      ```go Go
      package main

      import (
          "bytes"
          "encoding/json"
          "fmt"
          "io"
          "net/http"
          "os"
      )

      func main() {
          apiKey := os.Getenv("PLAYAI_KEY")
          userID := os.Getenv("PLAYAI_USER_ID")

          requestBody := map[string]interface{}{
              "model":        "PlayDialog",
              "text":        "Hello! This is my first text-to-speech audio using PlayAI!",
              "voice":       "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
              "outputFormat": "wav",
          }

          jsonData, err := json.Marshal(requestBody)
          if err != nil {
              fmt.Printf("Error marshaling JSON: %v\n", err)
              return
          }

          req, err := http.NewRequest("POST", "https://api.play.ai/api/v1/tts/stream", bytes.NewBuffer(jsonData))
          if err != nil {
              fmt.Printf("Error creating request: %v\n", err)
              return
          }

          req.Header.Set("Authorization", "Bearer "+apiKey)
          req.Header.Set("Content-Type", "application/json")
          req.Header.Set("X-USER-ID", userID)

          client := &http.Client{}
          resp, err := client.Do(req)
          if err != nil {
              fmt.Printf("Error making request: %v\n", err)
              return
          }
          defer resp.Body.Close()

          if resp.StatusCode != http.StatusOK {
              body, _ := io.ReadAll(resp.Body)
              fmt.Printf("Error: %d - %s\n", resp.StatusCode, string(body))
              return
          }

          out, err := os.Create("hello.wav")
          if err != nil {
              fmt.Printf("Error creating file: %v\n", err)
              return
          }
          defer out.Close()

          _, err = io.Copy(out, resp.Body)
          if err != nil {
              fmt.Printf("Error saving file: %v\n", err)
              return
          }

          fmt.Println("Audio saved as hello.wav")
      }
      ```

      ```dart Dart
      import 'dart:io';
      import 'dart:convert';
      import 'package:http/http.dart' as http;

      Future<void> generateSpeech() async {
        final apiKey = Platform.environment['PLAYAI_KEY'];
        final userId = Platform.environment['PLAYAI_USER_ID'];

        final headers = {
          'Authorization': 'Bearer $apiKey',
          'Content-Type': 'application/json',
          'X-USER-ID': userId,
        };

        final jsonData = {
          'model': 'PlayDialog',
          'text': 'Hello! This is my first text-to-speech audio using PlayAI!',
          'voice': 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
          'outputFormat': 'wav'
        };

        try {
          final response = await http.post(
            Uri.parse('https://api.play.ai/api/v1/tts/stream'),
            headers: headers,
            body: jsonEncode(jsonData),
          );

          if (response.statusCode == 200) {
            await File('hello.wav').writeAsBytes(response.bodyBytes);
            print('Audio saved as hello.wav');
          } else {
            print('Error: ${response.statusCode} - ${response.body}');
          }
        } catch (e) {
          print('Error: $e');
        }
      }

      void main() {
        generateSpeech();
      }
      ```

      ```swift Swift
      import Foundation

      func generateSpeech() {
          guard let apiKey = ProcessInfo.processInfo.environment["PLAYAI_KEY"],
                let userId = ProcessInfo.processInfo.environment["PLAYAI_USER_ID"] else {
              print("Missing environment variables")
              return
          }

          let jsonData: [String: Any] = [
              "model": "PlayDialog",
              "text": "Hello! This is my first text-to-speech audio using PlayAI!",
              "voice": "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
              "outputFormat": "wav"
          ]

          var request = URLRequest(url: URL(string: "https://api.play.ai/api/v1/tts/stream")!)
          request.httpMethod = "POST"
          request.setValue("Bearer \(apiKey)", forHTTPHeaderField: "Authorization")
          request.setValue("application/json", forHTTPHeaderField: "Content-Type")
          request.setValue(userId, forHTTPHeaderField: "X-USER-ID")
          request.httpBody = try? JSONSerialization.data(withJSONObject: jsonData)

          let semaphore = DispatchSemaphore(value: 0)

          let task = URLSession.shared.dataTask(with: request) { (data, response, error) in
              if let error = error {
                  print("Error: \(error)")
                  semaphore.signal()
                  return
              }

              guard let httpResponse = response as? HTTPURLResponse else {
                  print("Invalid response")
                  semaphore.signal()
                  return
              }

              if httpResponse.statusCode == 200, let data = data {
                  do {
                      try data.write(to: URL(fileURLWithPath: "hello.wav"))
                      print("Audio saved as hello.wav")
                  } catch {
                      print("Error saving file: \(error)")
                  }
              } else {
                  print("Error: \(httpResponse.statusCode) - \(String(data: data ?? Data(), encoding: .utf8) ?? "")")
              }

              semaphore.signal()
          }

          task.resume()
          semaphore.wait()
      }

      generateSpeech()
      ```
    </CodeGroup>
  </Step>
</Steps>

## Understanding the Code

Let's break down the key components:

1. **Authentication**: We use environment variables for secure credential management
2. **Request Headers**: Include your API key and user ID for authentication
3. **Request Body**:
   * `model`: Set to `PlayDialog` for our advanced TTS model
   * `text`: The text you want to convert to speech
   * `voice`: URL to the voice manifest (using a default voice for this example)
   * `outputFormat`: The audio format (wav or mp3)

## Next Steps

1. **Try Different Voices**: Explore our [voice library](/api-reference/text-to-speech/endpoints/v1/list-voices)
2. **Create Multi-speaker Dialogues**: Learn how to [create conversations](/documentation/tutorials/tts/dialogs/create-ai-podcast)
3. **Stream Audio**: Check out our [WebSocket API](/api-reference/text-to-speech/websocket) for real-time streaming

## Troubleshooting

Common issues and solutions:

1. **Authentication Errors**

   * Verify your API key and user ID are correct
   * Ensure the 'Bearer ' prefix is included in the Authorization header

2. **API Errors**

   * Check the API endpoint URL is correct
   * Verify the model name is `PlayDialog`
   * Ensure your text input is valid

3. **File Saving Issues**
   * Check write permissions in your working directory
   * Verify sufficient disk space

Need more help? Check out our [troubleshooting guide](/documentation/resources/troubleshooting) or [error messages](/documentation/resources/error-messages).


# Voices
Source: https://docs.play.ai/documentation/text-to-speech/tts-voices

How to use our pre-built voices or create your own

## Overview

PlayAI provides powerful voice creation and customization capabilities. Our platform supports a wide range of voice options, including pre-built voices optimized for different use cases and custom voice cloning.

All our pre-built voices are available in multiple languages and are regularly updated and improved.

## Voice Types

<CardGroup cols={2}>
  <Card title="Pre-built Voices" icon="gear" iconType="duotone">
    High-quality voices optimized for various use cases and languages
  </Card>

  <Card title="Cloned Voices" icon="copy" iconType="duotone" href="/documentation/text-to-speech/voice-cloning">
    Create custom voices by cloning from audio samples
  </Card>
</CardGroup>

## List All Voices

You can see a list of all available pre-built voices with the [List Voices](/api-reference/text-to-speech/endpoints/v1/list-voices) endpoint.

## Voice Cloning

Create custom voices by cloning from audio samples. Learn more about voice cloning in our [Voice Cloning](/documentation/text-to-speech/voice-cloning) documentation.

## Pre-built Voice Library

Our curated collection of high-quality voices is designed to meet diverse content needs across multiple industries and use cases.

<Tip>
  ### Voice Selection Tips

  * Choose voices that match your content's tone and style
  * Test different voices with sample content
  * Consider your target audience and use case
  * Use consistent voices across related content
</Tip>

The following are some common categories of voices to help you get started.

### Narration & Storytelling

Perfect for audiobooks, documentaries, and long-form content that requires clear articulation, natural pacing, and subtle emotional expression.

<div className="grid grid-cols-1 gap-4">
  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/frederick-13&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/eleanor-12&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/charles-6&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/charlotte-7&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/arthur-1&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />
</div>

### Business & Professional

Ideal for corporate videos, presentations, e-learning modules, and training materials that demand authority, clarity, and a professional tone.

<div className="grid grid-cols-1 gap-4">
  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/william-training-24&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/susan-votraining-23&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/jordan-16&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/aurora-2&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/oliver-18&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />
</div>

### Conversational & Casual

Suited for podcasts, interviews, and interactive content that benefits from authentic, engaging, and naturally flowing speech patterns.

<div className="grid grid-cols-1 gap-4">
  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/delilah-10&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/mason-17&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/autumn-3&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/chris-8&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/jennifer-15&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />
</div>

### Advertising & Marketing

Optimized for commercials, promotions, and brand messaging requiring captivating delivery, energetic presentation, and persuasive tones.

<div className="grid grid-cols-1 gap-4">
  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/susan-advertising-22&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/arthur-1&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/olivia-19&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/darrell-9&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/ayla-4&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />
</div>

### Character & Creative

Specialized voices for gaming, animation, and creative media featuring distinctive personality traits and wide emotional range.

<div className="grid grid-cols-1 gap-4">
  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/billy-5&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/hook-14&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/sarge-21&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  {' '}

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/dylan-11&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />

  <iframe width="100%" height="90" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//soundcloud.com/user-346843825/phoebe-20&hide_related=true&show_user=false&show_artwork=false&color=%23cccccc" />
</div>


# Voice Cloning
Source: https://docs.play.ai/documentation/text-to-speech/voice-cloning

Learn how to create custom voices from your own audio samples

<Info>This guide is in development. Please check again soon for updates.</Info>


# Web Embed Examples
Source: https://docs.play.ai/documentation/tutorials/agent/web-embed-examples

Examples of AI agent web embeddings and implementations

## Form filling

Assists a user in filling out a form. Showcases the ability to pass a custom prompt to the web embed from javascript.

[Live example](https://play.ai/embed/demo/form-filling)

[Code](https://github.com/playht/web-embed-examples/blob/main/form-filling/nextjs/app/page.tsx)

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/web-embed-examples/form-filling.png" alt="Form Filling Demo" />
</Frame>

## Minimize web embed

Showcases the ability to minimize the web embed from javascript.

[Live example](https://play.ai/embed/demo/minimize)

[Code](https://github.com/playht/web-embed-examples/blob/main/minimize-embed/nextjs/app/page.tsx)

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/web-embed-examples/minimize-embed.png" alt="Minimize Web Embed Demo" />
</Frame>

## Image generation

Generates an image based on the user's description.

[Live example](https://play.ai/embed/demo/image-gen)

[Code](https://github.com/playht/web-embed-examples/blob/main/image-gen/nextjs/app/page.tsx)

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/web-embed-examples/image-gen.png" alt="Image Generation Demo" />
</Frame>

## Music mood

Plays music based on the user's mood.

[Live example](https://play.ai/embed/demo/music-mood)

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/web-embed-examples/music-mood.jpeg" alt="Music Mood Demo" />
</Frame>

## Color math

Game where the user has to guess the color that is created by mixing two other colors.

[Live example](https://play.ai/embed/demo/color-math)

## Color painter

Changes the background color of the webpage based on the user's description.

[Live example](https://play.ai/embed/demo/color-painter)


# Web Embed Tutorial
Source: https://docs.play.ai/documentation/tutorials/agent/web-embed-tutorial

Learn how to embed AI agents in your web applications

## Introduction

The PlayAI web embed allows you to integrate AI-powered interactions into your web application. This guide will walk you through the process of setting up and using the PlayAI web embed in your React application.

## Installation

First, install the `@play-ai/agent-web-sdk` package:

```bash
npm install @play-ai/agent-web-sdk
```

## Basic Usage

Here's a step-by-step guide to implement the play.ai web embed in your React application:

<Steps>
  <Step title="Create an agent">
    If you haven't already, create an agent on [https://play.ai](https://play.ai).
  </Step>

  <Step title="Import the necessary dependencies">
    ```javascript
    import { useEffect, useState } from 'react';
    import { open as openEmbed } from '@play-ai/agent-web-sdk';
    ```
  </Step>

  <Step title="Define your web embed ID">
    ```javascript
    const webEmbedId = 'YOUR_WEB_EMBED_ID';
    ```

    Replace `YOUR_WEB_EMBED_ID` with the actual ID provided by play.ai, which can be found in the Agent editor.

    <Frame caption="Find your web embed ID on the last page of the Agent editor under 'Deploy Web'">
      <img src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/embedding-an-agent-on-your-website/web-embed-id.png" alt="Web Embed ID" />
    </Frame>
  </Step>

  <Step title="Define custom events (optional)">
    ```javascript
    const events = [
      {
        name: "change-text",
        when: "The user says what they want to change the text on the screen to",
        data: {
          text: { type: "string", description: "The text to change to" },
        },
      },
    ] as const;
    ```

    Custom events are optional, but they allow you to define custom behavior for your agent, to allow it to execute javascript and interact with the page. Learn more about custom events <a target="_blank" href="/documentation/agent/web-embed#events-array">here</a>.

    This example defines a single event called "change-text" that will be triggered when the user specifies the text they want to display.
  </Step>

  <Step title="Implement the custom event handler (optional)">
    ```javascript
    const onEvent = (event: any) => {
      console.log("onEvent: ", event);
      if (event.name === "change-text") {
        setText(event.data.text);
      }
    };
    ```

    If you define custom events, you must implement an event handler to handle the events. Learn more about the custom event handler <a target="_blank" href="/documentation/agent/web-embed#onevent-handler">here</a>.

    This handler logs the event and updates the text state when a "change-text" event is received.
  </Step>

  <Step title="Initialize the web embed">
    ```javascript
    useEffect(() => {
      openEmbed(webEmbedId, { events, onEvent });
    }, []);
    ```

    This `useEffect` hook initializes the web embed when the component mounts. See all the parameters that can be passed to `openEmbed()` <a target="_blank" href="/documentation/agent/web-embed#openembed-function">here</a>.
  </Step>

  <Step title="Render the component">
    ```jsx
    return (
      <>
        <div className="flex justify-center items-center h-[70vh]">
          <div className="font-medium text-2xl">{text}</div>
        </div>
      </>
    );
    ```

    This example renders the current text in the center of the page.
  </Step>
</Steps>

## Full Example

Here's the complete example of a React component using the play.ai web embed.

```jsx
"use client";
import { useEffect, useState } from "react";
import { open as openEmbed } from "@play-ai/agent-web-sdk";

const webEmbedId = "YOUR_WEB_EMBED_ID";

export default function Home() {
  const [text, setText] = useState("Change this text");

  const events = [
    {
      name: "change-text",
      when: "The user says what they want to change the text on the screen to",
      data: {
        text: { type: "string", description: "The text to change to" },
      },
    },
  ] as const;

  const onEvent = (event: any) => {
    console.log("onEvent: ", event);
    if (event.name === "change-text") {
      setText(event.data.text);
    }
  };

  useEffect(() => {
    openEmbed(webEmbedId, { events, onEvent });
  }, []);

  return (
    <>
      <div className="flex justify-center items-center h-[70vh]">
        <div className="font-medium text-2xl">{text}</div>
      </div>
    </>
  );
}
```

View the live example [here](https://play.ai/embed/demo/change-text).

View the code [here](https://github.com/playht/web-embed-examples/blob/main/change-text/nextjs/app/page.tsx).

## Customization

You can customize the behavior of your AI agent by modifying the agent greeting and prompt. In this example, the agent is instructed to change the text on the page and end the call immediately after doing so.

<Frame caption="The greeting and prompt for the Change Text demo agent">
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/playhtinc/media/embedding-an-agent-on-your-website/greeting-and-prompt.png" alt="Web Embed ID" />
</Frame>

## Other examples

* [View other examples](/documentation/tutorials/agent/web-embed-examples)


# Create an AI Podcast
Source: https://docs.play.ai/documentation/tutorials/tts/dialogs/create-ai-podcast

Learn how to create engaging AI-powered podcasts using PlayAI

This guide provides a step-by-step approach to using the PlayAI's Dialog 1.0 model to create a multi-turn scripted conversation between two distinct speakers.

## Overview&#x20;

* This is an async API endpoint.&#x20;

* You will make a request to trigger podcast generation.

* You will then request another endpoint to see if the podcast is ready.

## Prerequisites

* Access your [credentials](https://play.ai/api/keys) (secret key and user ID).
* Development environment for your chosen programming language.

## Steps

<Steps>
  <Step title="Set up environment variables">
    Choose your operating system and set up the environment variables:

    <CodeGroup>
      ```bash macOS (zsh)
      echo 'export PLAYAI_API_KEY="your_api_key_here"' >> ~/.zshrc
      echo 'export PLAYAI_USER_ID="your_user_id_here"' >> ~/.zshrc
      source ~/.zshrc
      ```

      ```bash bash
      echo 'export PLAYAI_API_KEY="your_api_key_here"' >> ~/.bashrc
      echo 'export PLAYAI_USER_ID="your_user_id_here"' >> ~/.bashrc
      source ~/.bashrc
      ```

      ```cmd Windows
      setx PLAYAI_API_KEY "your_api_key_here"
      setx PLAYAI_USER_ID "your_user_id_here"
      ```
    </CodeGroup>
  </Step>

  <Step title="Create a new script">
    Create a new file and add the following code:

    <CodeGroup>
      ```python Python
      import requests
      import os
      import time

      # Set up headers with your API secrety key and user ID
      user_id = os.getenv("PLAYAI_USER_ID")
      secret_key = os.getenv("PLAYAI_API_KEY")

      headers = {
          'X-USER-ID': user_id,
          'Authorization': secret_key,
          'Content-Type': 'application/json',
      }
      ```

      ```javascript JavaScript
      const axios = require('axios');
      require('dotenv').config();

      // Set up headers with your API secret key and user ID
      const headers = {
          'X-USER-ID': process.env.PLAYAI_USER_ID,
          'Authorization': process.env.PLAYAI_API_KEY,
          'Content-Type': 'application/json',
      };
      ```

      ```go Go
      package main

      import (
          "bytes"
          "encoding/json"
          "fmt"
          "io"
          "net/http"
          "os"
          "time"
      )
      ```

      ```dart Dart
      import 'dart:convert';
      import 'package:http/http.dart' as http;
      import 'package:dotenv/dotenv.dart';
      ```

      ```swift Swift
      import Foundation
      ```

      ```rust Rust
      use reqwest::Client;
      use serde::{Deserialize, Serialize};
      use std::env;
      use tokio;
      ```
    </CodeGroup>
  </Step>

  <Step title="Configure the model and voices">
    Define the model and select voices for your hosts:

    <CodeGroup>
      ```python Python
      # define the model
      model = 'PlayDialog'

      # define voices for the 2 hosts
      # find all voices here https://docs.play.ai/tts-api-reference/voices
      voice_1 = 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json'
      voice_2 = 's3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json'
      ```

      ```javascript JavaScript
      // Define the model
      const model = 'PlayDialog';

      // Define voices for the 2 hosts
      const voice1 = 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json';
      const voice2 = 's3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json';
      ```

      ```go Go
      // Define the model and voices
      model := "PlayDialog"
      voice1 := "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json"
      voice2 := "s3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json"
      ```

      ```dart Dart
      // Define the model and voices
      const model = 'PlayDialog';
      const voice1 = 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json';
      const voice2 = 's3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json';
      ```

      ```swift Swift
      // Define the model and voices
      let model = "PlayDialog"
      let voice1 = "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json"
      let voice2 = "s3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json"
      ```

      ```rust Rust
      // Define the model and voices
      let model = "PlayDialog";
      let voice1 = "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json";
      let voice2 = "s3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json";
      ```
    </CodeGroup>
  </Step>

  <Step title="Add your podcast transcript">
    Add your scripted conversation in the correct format:

    <CodeGroup>
      ```python Python
      # podcast transcript should be in the format of Host 1: ... Host 2:
      transcript = """
      Host 1: Welcome to The Tech Tomorrow Podcast! Today we're diving into the fascinating world of voice AI and what the future holds.
      Host 2: And what a topic this is. The technology has come so far from those early days of basic voice commands.
      Host 1: Remember when we thought it was revolutionary just to ask our phones to set a timer?
      Host 2: Now we're having full conversations with AI that can understand context, emotion, and even cultural nuances. It's incredible.
      Host 1: Though it does raise some interesting questions about privacy and ethics. Where do we draw the line?
      Host 2: Exactly. The potential benefits for accessibility and education are huge, but we need to be thoughtful about implementation.
      Host 1: Well, we'll be exploring all of these aspects today. Stay with us as we break down the future of voice AI.
      """
      ```

      ```javascript JavaScript
      // Podcast transcript
      const transcript = `
      Host 1: Welcome to The Tech Tomorrow Podcast! Today we're diving into the fascinating world of voice AI and what the future holds.
      Host 2: And what a topic this is. The technology has come so far from those early days of basic voice commands.
      Host 1: Remember when we thought it was revolutionary just to ask our phones to set a timer?
      Host 2: Now we're having full conversations with AI that can understand context, emotion, and even cultural nuances. It's incredible.
      Host 1: Though it does raise some interesting questions about privacy and ethics. Where do we draw the line?
      Host 2: Exactly. The potential benefits for accessibility and education are huge, but we need to be thoughtful about implementation.
      Host 1: Well, we'll be exploring all of these aspects today. Stay with us as we break down the future of voice AI.
      `;
      ```

      ```go Go
      // Podcast transcript
      transcript := `
      Host 1: Welcome to The Tech Tomorrow Podcast! Today we're diving into the fascinating world of voice AI and what the future holds.
      Host 2: And what a topic this is. The technology has come so far from those early days of basic voice commands.
      Host 1: Remember when we thought it was revolutionary just to ask our phones to set a timer?
      Host 2: Now we're having full conversations with AI that can understand context, emotion, and even cultural nuances. It's incredible.
      Host 1: Though it does raise some interesting questions about privacy and ethics. Where do we draw the line?
      Host 2: Exactly. The potential benefits for accessibility and education are huge, but we need to be thoughtful about implementation.
      Host 1: Well, we'll be exploring all of these aspects today. Stay with us as we break down the future of voice AI.
      `
      ```

      ```dart Dart
      // Podcast transcript
      const transcript = '''
      Host 1: Welcome to The Tech Tomorrow Podcast! Today we're diving into the fascinating world of voice AI and what the future holds.
      Host 2: And what a topic this is. The technology has come so far from those early days of basic voice commands.
      Host 1: Remember when we thought it was revolutionary just to ask our phones to set a timer?
      Host 2: Now we're having full conversations with AI that can understand context, emotion, and even cultural nuances. It's incredible.
      Host 1: Though it does raise some interesting questions about privacy and ethics. Where do we draw the line?
      Host 2: Exactly. The potential benefits for accessibility and education are huge, but we need to be thoughtful about implementation.
      Host 1: Well, we'll be exploring all of these aspects today. Stay with us as we break down the future of voice AI.
      ''';
      ```

      ```swift Swift
      // Podcast transcript
      let transcript = """
      Host 1: Welcome to The Tech Tomorrow Podcast! Today we're diving into the fascinating world of voice AI and what the future holds.
      Host 2: And what a topic this is. The technology has come so far from those early days of basic voice commands.
      Host 1: Remember when we thought it was revolutionary just to ask our phones to set a timer?
      Host 2: Now we're having full conversations with AI that can understand context, emotion, and even cultural nuances. It's incredible.
      Host 1: Though it does raise some interesting questions about privacy and ethics. Where do we draw the line?
      Host 2: Exactly. The potential benefits for accessibility and education are huge, but we need to be thoughtful about implementation.
      Host 1: Well, we'll be exploring all of these aspects today. Stay with us as we break down the future of voice AI.
      """
      ```

      ```rust Rust
      // Podcast transcript
      let transcript = r#"
      Host 1: Welcome to The Tech Tomorrow Podcast! Today we're diving into the fascinating world of voice AI and what the future holds.
      Host 2: And what a topic this is. The technology has come so far from those early days of basic voice commands.
      Host 1: Remember when we thought it was revolutionary just to ask our phones to set a timer?
      Host 2: Now we're having full conversations with AI that can understand context, emotion, and even cultural nuances. It's incredible.
      Host 1: Though it does raise some interesting questions about privacy and ethics. Where do we draw the line?
      Host 2: Exactly. The potential benefits for accessibility and education are huge, but we need to be thoughtful about implementation.
      Host 1: Well, we'll be exploring all of these aspects today. Stay with us as we break down the future of voice AI.
      "#;
      ```
    </CodeGroup>
  </Step>

  <Step title="Configure the API payload">
    Set up the payload with your configuration:

    <CodeGroup>
      ```python Python
      payload = {
          'model': model,
          'text': transcript,
          'voice': voice_1,
          'voice2': voice_2,
          'turnPrefix': 'Host 1:',
          'turnPrefix2': 'Host 2:',
          'outputFormat': 'mp3',
      }
      ```

      ```javascript JavaScript
      const payload = {
          model,
          text: transcript,
          voice: voice1,
          voice2,
          turnPrefix: 'Host 1:',
          turnPrefix2: 'Host 2:',
          outputFormat: 'mp3',
      };
      ```

      ```go Go
      payload := Payload{
          Model:        model,
          Text:         transcript,
          Voice:        voice1,
          Voice2:       voice2,
          TurnPrefix:   "Host 1:",
          TurnPrefix2:  "Host 2:",
          OutputFormat: "mp3",
      }
      ```

      ```dart Dart
      final payload = {
          'model': model,
          'text': transcript,
          'voice': voice1,
          'voice2': voice2,
          'turnPrefix': 'Host 1:',
          'turnPrefix2': 'Host 2:',
          'outputFormat': 'mp3',
      };
      ```

      ```swift Swift
      let payload = Payload(
          model: model,
          text: transcript,
          voice: voice1,
          voice2: voice2,
          turnPrefix: "Host 1:",
          turnPrefix2: "Host 2:",
          outputFormat: "mp3"
      )
      ```

      ```rust Rust
      let payload = serde_json::json!({
          "model": model,
          "text": transcript,
          "voice": voice1,
          "voice2": voice2,
          "turnPrefix": "Host 1:",
          "turnPrefix2": "Host 2:",
          "outputFormat": "mp3"
      });
      ```
    </CodeGroup>
  </Step>

  <Step title="Send the request and monitor progress">
    Add the code to send the request and check the status:

    <CodeGroup>
      ```python Python [expandable]
      # Send the POST request to trigger podcast generation
      response = requests.post('https://api.play.ai/api/v1/tts/', headers=headers, json=payload)

      # get the job id to check the status
      job_id = response.json().get('id')

      # use the job id to check completion status
      url = f'https://api.play.ai/api/v1/tts/{job_id}'
      delay_seconds = 2

      # keep checking until status is COMPLETED.
      # longer transcripts take more time to complete.
      while True:
          response = requests.get(url, headers=headers)

          if response.ok:
              status = response.json().get('output', {}).get('status')
              print(status)
              if status == 'COMPLETED':
                  # once completed audio url will be avaialable
                  podcast_audio = response.json().get('output', {}).get('url')
                  break

          time.sleep(delay_seconds)

      print(podcast_audio)
      ```

      ```javascript JavaScript [expandable]
      async function generatePodcast() {
          try {
              // Send the POST request to trigger podcast generation
              const response = await axios.post('https://api.play.ai/api/v1/tts/', payload, { headers });
              const jobId = response.data.id;

              // Check status until completed
              while (true) {
                  const statusResponse = await axios.get(`https://api.play.ai/api/v1/tts/${jobId}`, { headers });

                  if (statusResponse.data.output?.status === 'COMPLETED') {
                      const podcastAudio = statusResponse.data.output.url;
                      console.log(podcastAudio);
                      break;
                  }

                  console.log(statusResponse.data.output?.status);
                  await new Promise(resolve => setTimeout(resolve, 2000));
              }
          } catch (error) {
              console.error('Error:', error.message);
          }
      }

      generatePodcast();
      ```

      ```go Go [expandable]
      // Send request
      client := &http.Client{}
      resp, err := client.Do(req)
      if err != nil {
          fmt.Println("Error sending request:", err)
          return
      }
      defer resp.Body.Close()

      // Read response
      body, err := io.ReadAll(resp.Body)
      if err != nil {
          fmt.Println("Error reading response:", err)
          return
      }

      var response Response
      if err := json.Unmarshal(body, &response); err != nil {
          fmt.Println("Error unmarshaling response:", err)
          return
      }

      jobID := response.ID

      // Check status until completed
      for {
          req, err := http.NewRequest("GET", fmt.Sprintf("https://api.play.ai/api/v1/tts/%s", jobID), nil)
          if err != nil {
              fmt.Println("Error creating status request:", err)
              return
          }

          for key, value := range headers {
              req.Header.Add(key, value)
          }

          resp, err := client.Do(req)
          if err != nil {
              fmt.Println("Error checking status:", err)
              return
          }

          body, err := io.ReadAll(resp.Body)
          resp.Body.Close()

          if err != nil {
              fmt.Println("Error reading status response:", err)
              return
          }

          var statusResponse Response
          if err := json.Unmarshal(body, &statusResponse); err != nil {
              fmt.Println("Error unmarshaling status response:", err)
              return
          }

          fmt.Println(statusResponse.Output.Status)

          if statusResponse.Output.Status == "COMPLETED" {
              fmt.Println(statusResponse.Output.URL)
              break
          }

          time.Sleep(2 * time.Second)
      }
      ```

      ```dart Dart [expandable]
      try {
          // Send the POST request to trigger podcast generation
          final response = await http.post(
              Uri.parse('https://api.play.ai/api/v1/tts/'),
              headers: headers,
              body: jsonEncode(payload),
          );

          if (response.statusCode == 200) {
              final jobId = jsonDecode(response.body)['id'];

              // Check status until completed
              while (true) {
                  final statusResponse = await http.get(
                      Uri.parse('https://api.play.ai/api/v1/tts/$jobId'),
                      headers: headers,
                  );

                  if (statusResponse.statusCode == 200) {
                      final statusData = jsonDecode(statusResponse.body);
                      final status = statusData['output']['status'];
                      print(status);

                      if (status == 'COMPLETED') {
                          final podcastAudio = statusData['output']['url'];
                          print(podcastAudio);
                          break;
                      }
                  }

                  await Future.delayed(const Duration(seconds: 2));
              }
          } else {
              print('Error: ${response.statusCode}');
              print(response.body);
          }
      } catch (e) {
          print('Error: $e');
      }
      ```

      ```swift Swift [expandable]
      // Send request
      let (data, _) = try await URLSession.shared.data(for: request)
      let response = try JSONDecoder().decode(Response.self, from: data)
      let jobId = response.id

      // Check status until completed
      while true {
          var statusRequest = URLRequest(url: URL(string: "https://api.play.ai/api/v1/tts/\(jobId)")!)
          statusRequest.allHTTPHeaderFields = headers

          let (statusData, _) = try await URLSession.shared.data(for: statusRequest)
          let statusResponse = try JSONDecoder().decode(Response.self, from: statusData)

          print(statusResponse.output.status)

          if statusResponse.output.status == "COMPLETED" {
              if let podcastAudio = statusResponse.output.url {
                  print(podcastAudio)
              }
              break
          }

          try await Task.sleep(nanoseconds: 2_000_000_000) // 2 seconds
      }
      ```

      ```rust Rust [expandable]
      // Create HTTP client
      let client = Client::new();

      // Send request
      let response = client
          .post("https://api.play.ai/api/v1/tts/")
          .headers(headers.clone())
          .json(&payload)
          .send()
          .await?;

      let response_data: Response = response.json().await?;
      let job_id = response_data.id;

      // Check status until completed
      loop {
          let status_response = client
              .get(&format!("https://api.play.ai/api/v1/tts/{}", job_id))
              .headers(headers.clone())
              .send()
              .await?;

          let status_data: Response = status_response.json().await?;
          println!("{}", status_data.output.status);

          if status_data.output.status == "COMPLETED" {
              if let Some(url) = status_data.output.url {
                  println!("{}", url);
              }
              break;
          }

          tokio::time::sleep(tokio::time::Duration::from_secs(2)).await;
      }

      Ok(())
      ```
    </CodeGroup>
  </Step>
</Steps>

## Complete Code

<CodeGroup>
  ```python Python [expandable]
  import requests
  import os
  import time

  # Set up headers with your API secrety key and user ID
  user_id = os.getenv("PLAYAI_USER_ID")
  secret_key = os.getenv("PLAYAI_API_KEY")

  headers = {
      'X-USER-ID': user_id,
      'Authorization': secret_key,
      'Content-Type': 'application/json',
  }

  # define the model
  model = 'PlayDialog'

  # define voices for the 2 hosts
  # find all voices here https://docs.play.ai/tts-api-reference/voices
  voice_1 = 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json'
  voice_2 = 's3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json'

  # podcast transcript should be in the format of Host 1: ... Host 2:
  transcript = """
  Host 1: Welcome to The Tech Tomorrow Podcast! Today we're diving into the fascinating world of voice AI and what the future holds.
  Host 2: And what a topic this is. The technology has come so far from those early days of basic voice commands.
  Host 1: Remember when we thought it was revolutionary just to ask our phones to set a timer?
  Host 2: Now we're having full conversations with AI that can understand context, emotion, and even cultural nuances. It's incredible.
  Host 1: Though it does raise some interesting questions about privacy and ethics. Where do we draw the line?
  Host 2: Exactly. The potential benefits for accessibility and education are huge, but we need to be thoughtful about implementation.
  Host 1: Well, we'll be exploring all of these aspects today. Stay with us as we break down the future of voice AI.
  """

  payload = {
      'model': model,
      'text': transcript,
      'voice': voice_1,
      'voice2': voice_2,
      'turnPrefix': 'Host 1:',
      'turnPrefix2': 'Host 2:',
      'outputFormat': 'mp3',
  }

  # Send the POST request to trigger podcast generation
  response = requests.post('https://api.play.ai/api/v1/tts/', headers=headers, json=payload)

  # get the job id to check the status
  job_id = response.json().get('id')

  # use the job id to check completion status
  url = f'https://api.play.ai/api/v1/tts/{job_id}'
  delay_seconds = 2

  # keep checking until status is COMPLETED.
  # longer transcripts take more time to complete.
  while True:
      response = requests.get(url, headers=headers)

      if response.ok:
          status = response.json().get('output', {}).get('status')
          print(status)
          if status == 'COMPLETED':
              # once completed audio url will be avaialable
              podcast_audio = response.json().get('output', {}).get('url')
              break

      time.sleep(delay_seconds)

  print(podcast_audio)
  ```

  ```javascript JavaScript [expandable]
  const axios = require('axios');
  require('dotenv').config();

  // Set up headers with your API secret key and user ID
  const headers = {
    'X-USER-ID': process.env.PLAYAI_USER_ID,
    Authorization: process.env.PLAYAI_API_KEY,
    'Content-Type': 'application/json',
  };

  // Define the model
  const model = 'PlayDialog';

  // Define voices for the 2 hosts
  const voice1 = 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json';
  const voice2 = 's3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json';

  // Podcast transcript
  const transcript = `
  Host 1: Welcome to The Tech Tomorrow Podcast! Today we're diving into the fascinating world of voice AI and what the future holds.
  Host 2: And what a topic this is. The technology has come so far from those early days of basic voice commands.
  Host 1: Remember when we thought it was revolutionary just to ask our phones to set a timer?
  Host 2: Now we're having full conversations with AI that can understand context, emotion, and even cultural nuances. It's incredible.
  Host 1: Though it does raise some interesting questions about privacy and ethics. Where do we draw the line?
  Host 2: Exactly. The potential benefits for accessibility and education are huge, but we need to be thoughtful about implementation.
  Host 1: Well, we'll be exploring all of these aspects today. Stay with us as we break down the future of voice AI.
  `;

  const payload = {
    model,
    text: transcript,
    voice: voice1,
    voice2,
    turnPrefix: 'Host 1:',
    turnPrefix2: 'Host 2:',
    outputFormat: 'mp3',
  };

  async function generatePodcast() {
    try {
      // Send the POST request to trigger podcast generation
      const response = await axios.post('https://api.play.ai/api/v1/tts/', payload, { headers });
      const jobId = response.data.id;

      // Check status until completed
      while (true) {
        const statusResponse = await axios.get(`https://api.play.ai/api/v1/tts/${jobId}`, { headers });

        if (statusResponse.data.output?.status === 'COMPLETED') {
          const podcastAudio = statusResponse.data.output.url;
          console.log(podcastAudio);
          break;
        }

        console.log(statusResponse.data.output?.status);
        await new Promise((resolve) => setTimeout(resolve, 2000));
      }
    } catch (error) {
      console.error('Error:', error.message);
    }
  }

  generatePodcast();
  ```

  ```go Go [expandable]
  package main

  import (
      "bytes"
      "encoding/json"
      "fmt"
      "io"
      "net/http"
      "os"
      "time"
  )

  type Payload struct {
      Model        string `json:"model"`
      Text         string `json:"text"`
      Voice        string `json:"voice"`
      Voice2       string `json:"voice2"`
      TurnPrefix   string `json:"turnPrefix"`
      TurnPrefix2  string `json:"turnPrefix2"`
      OutputFormat string `json:"outputFormat"`
  }

  type Response struct {
      ID     string `json:"id"`
      Output struct {
          Status string `json:"status"`
          URL    string `json:"url"`
      } `json:"output"`
  }

  func main() {
      // Set up headers
      headers := map[string]string{
          "X-USER-ID":     os.Getenv("PLAYAI_USER_ID"),
          "Authorization": os.Getenv("PLAYAI_API_KEY"),
          "Content-Type":  "application/json",
      }

      // Define the model and voices
      model := "PlayDialog"
      voice1 := "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json"
      voice2 := "s3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json"

      // Podcast transcript
      transcript := `
      Host 1: Welcome to The Tech Tomorrow Podcast! Today we're diving into the fascinating world of voice AI and what the future holds.
      Host 2: And what a topic this is. The technology has come so far from those early days of basic voice commands.
      Host 1: Remember when we thought it was revolutionary just to ask our phones to set a timer?
      Host 2: Now we're having full conversations with AI that can understand context, emotion, and even cultural nuances. It's incredible.
      Host 1: Though it does raise some interesting questions about privacy and ethics. Where do we draw the line?
      Host 2: Exactly. The potential benefits for accessibility and education are huge, but we need to be thoughtful about implementation.
      Host 1: Well, we'll be exploring all of these aspects today. Stay with us as we break down the future of voice AI.
      `

      payload := Payload{
          Model:        model,
          Text:         transcript,
          Voice:        voice1,
          Voice2:       voice2,
          TurnPrefix:   "Host 1:",
          TurnPrefix2:  "Host 2:",
          OutputFormat: "mp3",
      }

      jsonData, err := json.Marshal(payload)
      if err != nil {
          fmt.Println("Error marshaling payload:", err)
          return
      }

      // Create request
      req, err := http.NewRequest("POST", "https://api.play.ai/api/v1/tts/", bytes.NewBuffer(jsonData))
      if err != nil {
          fmt.Println("Error creating request:", err)
          return
      }

      // Add headers
      for key, value := range headers {
          req.Header.Add(key, value)
      }

      // Send request
      client := &http.Client{}
      resp, err := client.Do(req)
      if err != nil {
          fmt.Println("Error sending request:", err)
          return
      }
      defer resp.Body.Close()

      // Read response
      body, err := io.ReadAll(resp.Body)
      if err != nil {
          fmt.Println("Error reading response:", err)
          return
      }

      var response Response
      if err := json.Unmarshal(body, &response); err != nil {
          fmt.Println("Error unmarshaling response:", err)
          return
      }

      jobID := response.ID

      // Check status until completed
      for {
          req, err := http.NewRequest("GET", fmt.Sprintf("https://api.play.ai/api/v1/tts/%s", jobID), nil)
          if err != nil {
              fmt.Println("Error creating status request:", err)
              return
          }

          for key, value := range headers {
              req.Header.Add(key, value)
          }

          resp, err := client.Do(req)
          if err != nil {
              fmt.Println("Error checking status:", err)
              return
          }

          body, err := io.ReadAll(resp.Body)
          resp.Body.Close()

          if err != nil {
              fmt.Println("Error reading status response:", err)
              return
          }

          var statusResponse Response
          if err := json.Unmarshal(body, &statusResponse); err != nil {
              fmt.Println("Error unmarshaling status response:", err)
              return
          }

          fmt.Println(statusResponse.Output.Status)

          if statusResponse.Output.Status == "COMPLETED" {
              fmt.Println(statusResponse.Output.URL)
              break
          }

          time.Sleep(2 * time.Second)
      }
  }
  ```

  ```dart Dart [expandable]
  import 'dart:convert';
  import 'package:http/http.dart' as http;
  import 'package:dotenv/dotenv.dart';

  void main() async {
      // Load environment variables
      var env = DotEnv(includePlatformEnvironment: true)..load();

      // Set up headers
      final headers = {
          'X-USER-ID': env['PLAYAI_USER_ID'] ?? '',
          'Authorization': env['PLAYAI_API_KEY'] ?? '',
          'Content-Type': 'application/json',
      };

      // Define the model and voices
      const model = 'PlayDialog';
      const voice1 = 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json';
      const voice2 = 's3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json';

      // Podcast transcript
      const transcript = '''
      Host 1: Welcome to The Tech Tomorrow Podcast! Today we're diving into the fascinating world of voice AI and what the future holds.
      Host 2: And what a topic this is. The technology has come so far from those early days of basic voice commands.
      Host 1: Remember when we thought it was revolutionary just to ask our phones to set a timer?
      Host 2: Now we're having full conversations with AI that can understand context, emotion, and even cultural nuances. It's incredible.
      Host 1: Though it does raise some interesting questions about privacy and ethics. Where do we draw the line?
      Host 2: Exactly. The potential benefits for accessibility and education are huge, but we need to be thoughtful about implementation.
      Host 1: Well, we'll be exploring all of these aspects today. Stay with us as we break down the future of voice AI.
      ''';

      final payload = {
          'model': model,
          'text': transcript,
          'voice': voice1,
          'voice2': voice2,
          'turnPrefix': 'Host 1:',
          'turnPrefix2': 'Host 2:',
          'outputFormat': 'mp3',
      };

      try {
          // Send the POST request to trigger podcast generation
          final response = await http.post(
              Uri.parse('https://api.play.ai/api/v1/tts/'),
              headers: headers,
              body: jsonEncode(payload),
          );

          if (response.statusCode == 200) {
              final jobId = jsonDecode(response.body)['id'];

              // Check status until completed
              while (true) {
                  final statusResponse = await http.get(
                      Uri.parse('https://api.play.ai/api/v1/tts/$jobId'),
                      headers: headers,
                  );

                  if (statusResponse.statusCode == 200) {
                      final statusData = jsonDecode(statusResponse.body);
                      final status = statusData['output']['status'];
                      print(status);

                      if (status == 'COMPLETED') {
                          final podcastAudio = statusData['output']['url'];
                          print(podcastAudio);
                          break;
                      }
                  }

                  await Future.delayed(const Duration(seconds: 2));
              }
          } else {
              print('Error: ${response.statusCode}');
              print(response.body);
          }
      } catch (e) {
          print('Error: $e');
      }
  }
  ```

  ```swift Swift [expandable]
  import Foundation

  struct Payload: Codable {
      let model: String
      let text: String
      let voice: String
      let voice2: String
      let turnPrefix: String
      let turnPrefix2: String
      let outputFormat: String
  }

  struct Response: Codable {
      let id: String
      let output: Output
  }

  struct Output: Codable {
      let status: String
      let url: String?
  }

  func generatePodcast() async throws {
      // Set up headers
      let headers = [
          "X-USER-ID": ProcessInfo.processInfo.environment["PLAYAI_USER_ID"] ?? "",
          "Authorization": ProcessInfo.processInfo.environment["PLAYAI_API_KEY"] ?? "",
          "Content-Type": "application/json"
      ]

      // Define the model and voices
      let model = "PlayDialog"
      let voice1 = "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json"
      let voice2 = "s3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json"

      // Podcast transcript
      let transcript = """
      Host 1: Welcome to The Tech Tomorrow Podcast! Today we're diving into the fascinating world of voice AI and what the future holds.
      Host 2: And what a topic this is. The technology has come so far from those early days of basic voice commands.
      Host 1: Remember when we thought it was revolutionary just to ask our phones to set a timer?
      Host 2: Now we're having full conversations with AI that can understand context, emotion, and even cultural nuances. It's incredible.
      Host 1: Though it does raise some interesting questions about privacy and ethics. Where do we draw the line?
      Host 2: Exactly. The potential benefits for accessibility and education are huge, but we need to be thoughtful about implementation.
      Host 1: Well, we'll be exploring all of these aspects today. Stay with us as we break down the future of voice AI.
      """

      let payload = Payload(
          model: model,
          text: transcript,
          voice: voice1,
          voice2: voice2,
          turnPrefix: "Host 1:",
          turnPrefix2: "Host 2:",
          outputFormat: "mp3"
      )

      // Create URL request
      var request = URLRequest(url: URL(string: "https://api.play.ai/api/v1/tts/")!)
      request.httpMethod = "POST"
      request.allHTTPHeaderFields = headers
      request.httpBody = try JSONEncoder().encode(payload)

      // Send request
      let (data, _) = try await URLSession.shared.data(for: request)
      let response = try JSONDecoder().decode(Response.self, from: data)
      let jobId = response.id

      // Check status until completed
      while true {
          var statusRequest = URLRequest(url: URL(string: "https://api.play.ai/api/v1/tts/\(jobId)")!)
          statusRequest.allHTTPHeaderFields = headers

          let (statusData, _) = try await URLSession.shared.data(for: statusRequest)
          let statusResponse = try JSONDecoder().decode(Response.self, from: statusData)

          print(statusResponse.output.status)

          if statusResponse.output.status == "COMPLETED" {
              if let podcastAudio = statusResponse.output.url {
                  print(podcastAudio)
              }
              break
          }

          try await Task.sleep(nanoseconds: 2_000_000_000) // 2 seconds
      }
  }

  // Run the async function
  Task {
      do {
          try await generatePodcast()
      } catch {
          print("Error: \(error)")
      }
  }
  ```

  ```rust Rust [expandable]
  use reqwest::Client;
  use serde::{Deserialize, Serialize};
  use std::env;
  use tokio;

  #[derive(Serialize)]
  struct Payload {
      model: String,
      text: String,
      voice: String,
      voice2: String,
      turn_prefix: String,
      turn_prefix2: String,
      output_format: String,
  }

  #[derive(Deserialize)]
  struct Response {
      id: String,
      output: Output,
  }

  #[derive(Deserialize)]
  struct Output {
      status: String,
      url: Option<String>,
  }

  #[tokio::main]
  async fn main() -> Result<(), Box<dyn std::error::Error>> {
      // Set up headers
      let headers = {
          let mut headers = reqwest::header::HeaderMap::new();
          headers.insert(
              "X-USER-ID",
              env::var("PLAYAI_USER_ID")?.parse()?,
          );
          headers.insert(
              "Authorization",
              env::var("PLAYAI_API_KEY")?.parse()?,
          );
          headers.insert(
              "Content-Type",
              "application/json".parse()?,
          );
          headers
      };

      // Define the model and voices
      let model = "PlayDialog";
      let voice1 = "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json";
      let voice2 = "s3://voice-cloning-zero-shot/e040bd1b-f190-4bdb-83f0-75ef85b18f84/original/manifest.json";

      // Podcast transcript
      let transcript = r#"
      Host 1: Welcome to The Tech Tomorrow Podcast! Today we're diving into the fascinating world of voice AI and what the future holds.
      Host 2: And what a topic this is. The technology has come so far from those early days of basic voice commands.
      Host 1: Remember when we thought it was revolutionary just to ask our phones to set a timer?
      Host 2: Now we're having full conversations with AI that can understand context, emotion, and even cultural nuances. It's incredible.
      Host 1: Though it does raise some interesting questions about privacy and ethics. Where do we draw the line?
      Host 2: Exactly. The potential benefits for accessibility and education are huge, but we need to be thoughtful about implementation.
      Host 1: Well, we'll be exploring all of these aspects today. Stay with us as we break down the future of voice AI.
      "#;

      let payload = Payload {
          model: model.to_string(),
          text: transcript.to_string(),
          voice: voice1.to_string(),
          voice2: voice2.to_string(),
          turn_prefix: "Host 1:".to_string(),
          turn_prefix2: "Host 2:".to_string(),
          output_format: "mp3".to_string(),
      };

      // Create HTTP client
      let client = Client::new();

      // Send request
      let response = client
          .post("https://api.play.ai/api/v1/tts/")
          .headers(headers.clone())
          .json(&payload)
          .send()
          .await?;

      let response_data: Response = response.json().await?;
      let job_id = response_data.id;

      // Check status until completed
      loop {
          let status_response = client
              .get(&format!("https://api.play.ai/api/v1/tts/{}", job_id))
              .headers(headers.clone())
              .send()
              .await?;

          let status_data: Response = status_response.json().await?;
          println!("{}", status_data.output.status);

          if status_data.output.status == "COMPLETED" {
              if let Some(url) = status_data.output.url {
                  println!("{}", url);
              }
              break;
          }

          tokio::time::sleep(tokio::time::Duration::from_secs(2)).await;
      }

      Ok(())
  }
  ```
</CodeGroup>

## Key API Parameters

The following API payload define the conversation, speaker details, and audio generation options:

* **`model`**: Specifies the PlayAI's Dialog 1.0 model to be used. Here, `PlayDialog` supports multi-turn conversation generation.

* **`text`**: Contains the scripted conversation, with each turn prefixed by the speaker's name (e.g., `"Country Mouse"` & `"Town Mouse"`).

* **`voice`**: URL path to the voice manifest for the first speaker.

* **`voice_2`**: URL path to the voice manifest for the second speaker.

* **`turn_prefix` / `turn_prefix_2`**: Used to specify each speaker's dialogue turns within the `text` field. For example: `turn_prefix` says `Country Mouse` to indicate the position where Speaker 1's dialogue and `turn_prefix_2` says `Town Mouse` that indicates the position where Speaker 2's dialogue parts are.

* **`output_format`**: Format for the generated audio file, typically `wav` or `mp3`.

If you happen to save the code as `country_mouse.py` then Run the code using `python3 country_mouse.py` pointing your terminal to the directory where the `country_mouse.py` file is stored. This will save the `dialogue.wav` in the same working directory.

# Code Explanation

This script uses the Dialog 1.0 model to generate a multi-turn conversation between two characters. The `AUTHORIZATION token` and `X-USER-ID` are required for authentication, which you'll need to replace with your own credentials.

Each line of dialogue is labeled by character name (e.g., "`Country Mouse`" or "`Town Mouse`") to simulate a natural conversation. The script assigns a unique voice to each character using `voice` and `voice2`. On a successful API call, the generated audio is saved as `dialogue.wav`. Any errors are reported with status details.

**To run the script:**

* Replace placeholders in the headers with your API key and user ID.

* Update the `text` with your scripted conversation

* Update the Speaker Details and their respective voices

* Run the script. If successful, an audio file, `dialogue.wav`, will be saved in the current directory, capturing the dialogue as configured.

* This setup can easily adapt to more complex dialogues or different speakers.

# Troubleshooting

* Authentication Issues: Verify your `API key` and `user ID`. Ensure the `AUTHORIZATION` header includes "Bearer " followed by your token.

* API Endpoint Errors: Confirm you're using the correct PlayAI's Dialog 1.0 API endpoint URL and the `model` name is `PlayDialog`


# Async TTS API Quickstart
Source: https://docs.play.ai/documentation/tutorials/tts/dialogs/how-to-use-async-tts-api

Learn how to use PlayAI's asynchronous Text-to-Speech API

This guide provides a step-by-step approach to using the PlayAI Text-to-Speech API to convert text into natural human-like sounding audio using [the Async (non-streaming) API Endpoint](/api-reference/text-to-speech/endpoints/v1/create-speech).

In this example, we'll have [Dialog 1.0](/documentation/text-to-speech/tts-models#dialog-10) create a simple audio from the given input text.

## Prerequisites

* [Access credentials](https://play.ai/api/keys) (Secret key and User ID)
* Development environment for your chosen programming language

## Steps

<Steps>
  <Step title="Set Up Environment Variables">
    Add your API key and user ID to your environment variables.

    <CodeGroup>
      ```bash macOS (zsh)
      echo 'export PLAYAI_API_KEY="your_api_key_here"' >> ~/.zshrc
      echo 'export PLAYAI_USER_ID="your_user_id_here"' >> ~/.zshrc
      source ~/.zshrc
      ```

      ```bash bash
      echo 'export PLAYAI_API_KEY="your_api_key_here"' >> ~/.bashrc
      echo 'export PLAYAI_USER_ID="your_user_id_here"' >> ~/.bashrc
      source ~/.bashrc
      ```

      ```cmd Windows
      setx PLAYAI_API_KEY "your_api_key_here"
      setx PLAYAI_USER_ID "your_user_id_here"
      ```
    </CodeGroup>
  </Step>

  <Step title="Configure API Access">
    Create a script with the following authentication setup for your chosen language:

    <CodeGroup>
      ```python Python
      import os

      api_key = os.getenv("PLAYAI_API_KEY")
      user_id = os.getenv("PLAYAI_USER_ID")

      headers = {
          'Authorization': f'Bearer {api_key}',
          'Content-Type': 'application/json',
          'X-USER-ID': user_id
      }
      ```

      ```javascript JavaScript
      const apiKey = process.env.PLAYAI_API_KEY;
      const userId = process.env.PLAYAI_USER_ID;

      const headers = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
        'X-USER-ID': userId
      };
      ```

      ```go Go
      apiKey := os.Getenv("PLAYAI_API_KEY")
      userId := os.Getenv("PLAYAI_USER_ID")

      headers := map[string]string{
        "Authorization": fmt.Sprintf("Bearer %s", apiKey),
        "Content-Type": "application/json",
        "X-USER-ID": userId,
      }
      ```

      ```dart Dart
      final apiKey = Platform.environment['PLAYAI_API_KEY'];
      final userId = Platform.environment['PLAYAI_USER_ID'];

      final headers = {
        'Authorization': 'Bearer $apiKey',
        'Content-Type': 'application/json',
        'X-USER-ID': userId,
      };
      ```

      ```swift Swift
      let apiKey = ProcessInfo.processInfo.environment["PLAYAI_API_KEY"] ?? ""
      let userId = ProcessInfo.processInfo.environment["PLAYAI_USER_ID"] ?? ""

      let headers = [
        "Authorization": "Bearer \(apiKey)",
        "Content-Type": "application/json",
        "X-USER-ID": userId
      ]
      ```
    </CodeGroup>
  </Step>

  <Step title="Submit TTS Job">
    Create and submit your TTS job in your chosen language:

    <CodeGroup>
      ```python Python
      json_data = {
          'model': 'PlayDialog',
          'text': "This is the greatest moment to be alive",
          'voice': 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
          'outputFormat': 'mp3',
          'speed': 1,
          'language': 'english',
      }

      response = requests.post('https://api.play.ai/api/v1/tts',
                             headers=headers,
                             json=json_data)

      if response.status_code == 201:
          job_id = response.json().get('id')
          print(f"Job submitted successfully. Job ID: {job_id}")
      else:
          print(f"Job submission failed with status code {response.status_code}: {response.text}")
      ```

      ```javascript JavaScript
      const jsonData = {
        model: 'PlayDialog',
        text: "This is the greatest moment to be alive",
        voice: 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
        outputFormat: 'mp3',
        speed: 1,
        language: 'english'
      };

      fetch('https://api.play.ai/api/v1/tts', {
        method: 'POST',
        headers: headers,
        body: JSON.stringify(jsonData)
      })
      .then(response => {
        if (response.status === 201) {
          return response.json();
        }
        throw new Error(`Request failed with status ${response.status}`);
      })
      .then(data => {
        const jobId = data.id;
        console.log(`Job submitted successfully. Job ID: ${jobId}`);
      })
      .catch(error => console.error('Error:', error));
      ```

      ```go Go
      jsonData := map[string]interface{}{
        "model": "PlayDialog",
        "text": "This is the greatest moment to be alive",
        "voice": "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
        "outputFormat": "mp3",
        "speed": 1,
        "language": "english",
      }

      jsonBytes, _ := json.Marshal(jsonData)
      req, _ := http.NewRequest("POST", "https://api.play.ai/api/v1/tts", bytes.NewBuffer(jsonBytes))

      for key, value := range headers {
        req.Header.Add(key, value)
      }

      client := &http.Client{}
      resp, err := client.Do(req)
      if err != nil {
        log.Fatal(err)
      }
      defer resp.Body.Close()

      if resp.StatusCode == 201 {
        var result map[string]interface{}
        json.NewDecoder(resp.Body).Decode(&result)
        jobId := result["id"].(string)
        fmt.Printf("Job submitted successfully. Job ID: %s\n", jobId)
      } else {
        body, _ := io.ReadAll(resp.Body)
        fmt.Printf("Job submission failed with status code %d: %s\n", resp.StatusCode, string(body))
      }
      ```

      ```dart Dart
      final jsonData = {
        'model': 'PlayDialog',
        'text': 'This is the greatest moment to be alive',
        'voice': 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
        'outputFormat': 'mp3',
        'speed': 1,
        'language': 'english',
      };

      final response = await http.post(
        Uri.parse('https://api.play.ai/api/v1/tts'),
        headers: headers,
        body: jsonEncode(jsonData),
      );

      if (response.statusCode == 201) {
        final data = jsonDecode(response.body);
        final jobId = data['id'];
        print('Job submitted successfully. Job ID: $jobId');
      } else {
        print('Job submission failed with status code ${response.statusCode}: ${response.body}');
      }
      ```

      ```swift Swift
      let jsonData: [String: Any] = [
        "model": "PlayDialog",
        "text": "This is the greatest moment to be alive",
        "voice": "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
        "outputFormat": "mp3",
        "speed": 1,
        "language": "english"
      ]

      guard let jsonData = try? JSONSerialization.data(withJSONObject: jsonData) else {
        print("Failed to serialize JSON data")
        exit(1)
      }

      var request = URLRequest(url: URL(string: "https://api.play.ai/api/v1/tts")!)
      request.httpMethod = "POST"
      request.allHTTPHeaderFields = headers
      request.httpBody = jsonData

      let task = URLSession.shared.dataTask(with: request) { data, response, error in
        if let error = error {
            print("Error: \(error)")
            return
        }

        guard let httpResponse = response as? HTTPURLResponse else {
            print("Invalid response")
            return
        }

        if httpResponse.statusCode == 201 {
            if let data = data,
               let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
               let jobId = json["id"] as? String {
                print("Job submitted successfully. Job ID: \(jobId)")
            }
        } else {
            print("Job submission failed with status code \(httpResponse.statusCode)")
            if let data = data, let responseString = String(data: data, encoding: .utf8) {
                print("Response: \(responseString)")
            }
        }
      }
      task.resume()
      ```
    </CodeGroup>
  </Step>

  <Step title="Poll Job Status">
    Monitor your job's progress in your chosen language:

    <CodeGroup>
      ```python Python
      polling_url = f'https://api.play.ai/api/v1/tts/{job_id}'

      while True:
          response = requests.get(polling_url, headers=headers)
          status = response.json()['output']['status']

          if status == 'COMPLETED':
              audio_url = response.json()['output']['url']
              print(f"Job completed. Audio URL: {audio_url}")
              break
          elif status == 'IN_PROGRESS':
              print("Job is still in progress. Retrying in 5 seconds...")
              time.sleep(5)
          else:
              print(f"Job failed or encountered an unknown status: {status}")
              break
      ```

      ```javascript JavaScript
      const pollJobStatus = async (jobId) => {
        while (true) {
          const response = await fetch(`https://api.play.ai/api/v1/tts/${jobId}`, {
            headers: headers
          });
          const data = await response.json();
          const status = data.output.status;

          if (status === 'COMPLETED') {
            const audioUrl = data.output.url;
            console.log(`Job completed. Audio URL: ${audioUrl}`);
            break;
          } else if (status === 'IN_PROGRESS') {
            console.log("Job is still in progress. Retrying in 5 seconds...");
            await new Promise(resolve => setTimeout(resolve, 5000));
          } else {
            console.log(`Job failed or encountered an unknown status: ${status}`);
            break;
          }
        }
      };
      ```

      ```go Go
      func pollJobStatus(jobId string) {
        for {
          req, _ := http.NewRequest("GET", fmt.Sprintf("https://api.play.ai/api/v1/tts/%s", jobId), nil)
          for key, value := range headers {
            req.Header.Add(key, value)
          }

          client := &http.Client{}
          resp, err := client.Do(req)
          if err != nil {
            log.Fatal(err)
          }
          defer resp.Body.Close()

          var result map[string]interface{}
          json.NewDecoder(resp.Body).Decode(&result)
          status := result["output"].(map[string]interface{})["status"].(string)

          if status == "COMPLETED" {
            audioUrl := result["output"].(map[string]interface{})["url"].(string)
            fmt.Printf("Job completed. Audio URL: %s\n", audioUrl)
            break
          } else if status == "IN_PROGRESS" {
            fmt.Println("Job is still in progress. Retrying in 5 seconds...")
            time.Sleep(5 * time.Second)
          } else {
            fmt.Printf("Job failed or encountered an unknown status: %s\n", status)
            break
          }
        }
      }
      ```

      ```dart Dart
      Future<void> pollJobStatus(String jobId) async {
        while (true) {
          final response = await http.get(
            Uri.parse('https://api.play.ai/api/v1/tts/$jobId'),
            headers: headers,
          );

          final data = jsonDecode(response.body);
          final status = data['output']['status'];

          if (status == 'COMPLETED') {
            final audioUrl = data['output']['url'];
            print('Job completed. Audio URL: $audioUrl');
            break;
          } else if (status == 'IN_PROGRESS') {
            print('Job is still in progress. Retrying in 5 seconds...');
            await Future.delayed(Duration(seconds: 5));
          } else {
            print('Job failed or encountered an unknown status: $status');
            break;
          }
        }
      }
      ```

      ```swift Swift
      func pollJobStatus(jobId: String) {
        while true {
          var request = URLRequest(url: URL(string: "https://api.play.ai/api/v1/tts/\(jobId)")!)
          request.allHTTPHeaderFields = headers

          let task = URLSession.shared.dataTask(with: request) { data, response, error in
            if let error = error {
                print("Error: \(error)")
                return
            }

            guard let httpResponse = response as? HTTPURLResponse,
                  let data = data,
                  let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
                  let output = json["output"] as? [String: Any],
                  let status = output["status"] as? String else {
                return
            }

            if status == "COMPLETED" {
                if let audioUrl = output["url"] as? String {
                    print("Job completed. Audio URL: \(audioUrl)")
                }
                exit(0)
            } else if status == "IN_PROGRESS" {
                print("Job is still in progress. Retrying in 5 seconds...")
                Thread.sleep(forTimeInterval: 5)
            } else {
                print("Job failed or encountered an unknown status: \(status)")
                exit(1)
            }
          }
          task.resume()
          RunLoop.main.run(until: Date(timeIntervalSinceNow: 5))
        }
      }
      ```
    </CodeGroup>
  </Step>

  <Step title="Download and Save Audio">
    Save the generated audio file in your chosen language:

    <CodeGroup>
      ```python Python
      audio_url = response.json()['output']['url']
      audio_response = requests.get(audio_url)

      if audio_response.status_code == 200:
          with open('output.mp3', 'wb') as f:
              f.write(audio_response.content)
          print("Audio file saved as output.mp3")
      else:
          print(f"Failed to download audio. Status code: {audio_response.status_code}")
      ```

      ```javascript JavaScript
      const downloadAudio = async (audioUrl) => {
        const response = await fetch(audioUrl);
        if (response.ok) {
          const blob = await response.blob();
          const url = window.URL.createObjectURL(blob);
          const a = document.createElement('a');
          a.href = url;
          a.download = 'output.mp3';
          a.click();
          window.URL.revokeObjectURL(url);
          console.log("Audio file saved as output.mp3");
        } else {
          console.log(`Failed to download audio. Status: ${response.status}`);
        }
      };
      ```

      ```go Go
      func downloadAudio(audioUrl string) {
        resp, err := http.Get(audioUrl)
        if err != nil {
          log.Fatal(err)
        }
        defer resp.Body.Close()

        if resp.StatusCode == 200 {
          out, _ := os.Create("output.mp3")
          defer out.Close()
          io.Copy(out, resp.Body)
          fmt.Println("Audio file saved as output.mp3")
        } else {
          fmt.Printf("Failed to download audio. Status code: %d\n", resp.StatusCode)
        }
      }
      ```

      ```dart Dart
      Future<void> downloadAudio(String audioUrl) async {
        final response = await http.get(Uri.parse(audioUrl));
        if (response.statusCode == 200) {
          final file = File('output.mp3');
          await file.writeAsBytes(response.bodyBytes);
          print('Audio file saved as output.mp3');
        } else {
          print('Failed to download audio. Status code: ${response.statusCode}');
        }
      }
      ```

      ```swift Swift
      func downloadAudio(audioUrl: String) {
        guard let url = URL(string: audioUrl) else { return }
        
        let task = URLSession.shared.dataTask(with: url) { data, response, error in
          if let error = error {
            print("Error: \(error)")
            return
          }

          guard let httpResponse = response as? HTTPURLResponse else {
            print("Invalid response")
            return
          }

          if httpResponse.statusCode == 200 {
            if let data = data {
              do {
                try data.write(to: URL(fileURLWithPath: "output.mp3"))
                print("Audio file saved as output.mp3")
              } catch {
                print("Error saving file: \(error)")
              }
            }
          } else {
            print("Failed to download audio. Status code: \(httpResponse.statusCode)")
          }
        }
        task.resume()
      }
      ```
    </CodeGroup>
  </Step>
</Steps>

## Complete Code

<CodeGroup>
  ```python Python [expandable]
  import os
  import requests
  import time

  api_key = os.getenv("PLAYAI_API_KEY")
  user_id = os.getenv("PLAYAI_USER_ID")

  headers = {
      'Authorization': f'Bearer {api_key}',
      'Content-Type': 'application/json',
      'X-USER-ID': user_id
  }

  # Submit TTS Job
  json_data = {
      'model': 'PlayDialog',
      'text': "This is the greatest moment to be alive",
      'voice': 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
      'outputFormat': 'mp3',
      'speed': 1,
      'language': 'english',
  }

  response = requests.post('https://api.play.ai/api/v1/tts',
                         headers=headers,
                         json=json_data)

  if response.status_code == 201:
      job_id = response.json().get('id')
      print(f"Job submitted successfully. Job ID: {job_id}")

      # Poll Job Status
      polling_url = f'https://api.play.ai/api/v1/tts/{job_id}'
      while True:
          response = requests.get(polling_url, headers=headers)
          status = response.json()['output']['status']

          if status == 'COMPLETED':
              audio_url = response.json()['output']['url']
              print(f"Job completed. Audio URL: {audio_url}")

              # Download Audio
              audio_response = requests.get(audio_url)
              if audio_response.status_code == 200:
                  with open('output.mp3', 'wb') as f:
                      f.write(audio_response.content)
                  print("Audio file saved as output.mp3")
              else:
                  print(f"Failed to download audio. Status code: {audio_response.status_code}")
              break
          elif status == 'IN_PROGRESS':
              print("Job is still in progress. Retrying in 5 seconds...")
              time.sleep(5)
          else:
              print(f"Job failed or encountered an unknown status: {status}")
              break
  else:
      print(f"Job submission failed with status code {response.status_code}: {response.text}")
  ```

  ```javascript JavaScript [expandable]
  const fetch = require('node-fetch');

  const apiKey = process.env.PLAYAI_API_KEY;
  const userId = process.env.PLAYAI_USER_ID;

  const headers = {
    'Authorization': `Bearer ${apiKey}`,
    'Content-Type': 'application/json',
    'X-USER-ID': userId
  };

  // Submit TTS Job
  const jsonData = {
    model: 'PlayDialog',
    text: "This is the greatest moment to be alive",
    voice: 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
    outputFormat: 'mp3',
    speed: 1,
    language: 'english'
  };

  fetch('https://api.play.ai/api/v1/tts', {
    method: 'POST',
    headers: headers,
    body: JSON.stringify(jsonData)
  })
  .then(response => {
    if (response.status === 201) {
      return response.json();
    }
    throw new Error(`Request failed with status ${response.status}`);
  })
  .then(data => {
    const jobId = data.id;
    console.log(`Job submitted successfully. Job ID: ${jobId}`);

    // Poll Job Status
    const pollJobStatus = async (jobId) => {
      while (true) {
        const response = await fetch(`https://api.play.ai/api/v1/tts/${jobId}`, {
          headers: headers
        });
        const data = await response.json();
        const status = data.output.status;

        if (status === 'COMPLETED') {
          const audioUrl = data.output.url;
          console.log(`Job completed. Audio URL: ${audioUrl}`);

          // Download Audio
          const audioResponse = await fetch(audioUrl);
          if (audioResponse.ok) {
            const blob = await audioResponse.blob();
            const url = window.URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'output.mp3';
            a.click();
            window.URL.revokeObjectURL(url);
            console.log("Audio file saved as output.mp3");
          } else {
            console.log(`Failed to download audio. Status: ${audioResponse.status}`);
          }
          break;
        } else if (status === 'IN_PROGRESS') {
          console.log("Job is still in progress. Retrying in 5 seconds...");
          await new Promise(resolve => setTimeout(resolve, 5000));
        } else {
          console.log(`Job failed or encountered an unknown status: ${status}`);
          break;
        }
      }
    };

    return pollJobStatus(jobId);
  })
  .catch(error => console.error('Error:', error));
  ```

  ```go Go [expandable]
  package main

  import (
    "bytes"
    "encoding/json"
    "fmt"
    "io"
    "log"
    "net/http"
    "os"
    "time"
  )

  func main() {
    apiKey := os.Getenv("PLAYAI_API_KEY")
    userId := os.Getenv("PLAYAI_USER_ID")

    headers := map[string]string{
      "Authorization": fmt.Sprintf("Bearer %s", apiKey),
      "Content-Type": "application/json",
      "X-USER-ID": userId,
    }

    // Submit TTS Job
    jsonData := map[string]interface{}{
      "model": "PlayDialog",
      "text": "This is the greatest moment to be alive",
      "voice": "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
      "outputFormat": "mp3",
      "speed": 1,
      "language": "english",
    }

    jsonBytes, _ := json.Marshal(jsonData)
    req, _ := http.NewRequest("POST", "https://api.play.ai/api/v1/tts", bytes.NewBuffer(jsonBytes))
    
    for key, value := range headers {
      req.Header.Add(key, value)
    }

    client := &http.Client{}
    resp, err := client.Do(req)
    if err != nil {
      log.Fatal(err)
    }
    defer resp.Body.Close()

    if resp.StatusCode == 201 {
      var result map[string]interface{}
      json.NewDecoder(resp.Body).Decode(&result)
      jobId := result["id"].(string)
      fmt.Printf("Job submitted successfully. Job ID: %s\n", jobId)

      // Poll Job Status
      for {
        req, _ := http.NewRequest("GET", fmt.Sprintf("https://api.play.ai/api/v1/tts/%s", jobId), nil)
        for key, value := range headers {
          req.Header.Add(key, value)
        }

        resp, err := client.Do(req)
        if err != nil {
          log.Fatal(err)
        }
        defer resp.Body.Close()

        var result map[string]interface{}
        json.NewDecoder(resp.Body).Decode(&result)
        status := result["output"].(map[string]interface{})["status"].(string)

        if status == "COMPLETED" {
          audioUrl := result["output"].(map[string]interface{})["url"].(string)
          fmt.Printf("Job completed. Audio URL: %s\n", audioUrl)

          // Download Audio
          resp, err := http.Get(audioUrl)
          if err != nil {
            log.Fatal(err)
          }
          defer resp.Body.Close()

          if resp.StatusCode == 200 {
            out, _ := os.Create("output.mp3")
            defer out.Close()
            io.Copy(out, resp.Body)
            fmt.Println("Audio file saved as output.mp3")
          } else {
            fmt.Printf("Failed to download audio. Status code: %d\n", resp.StatusCode)
          }
          break
        } else if status == "IN_PROGRESS" {
          fmt.Println("Job is still in progress. Retrying in 5 seconds...")
          time.Sleep(5 * time.Second)
        } else {
          fmt.Printf("Job failed or encountered an unknown status: %s\n", status)
          break
        }
      }
    } else {
      body, _ := io.ReadAll(resp.Body)
      fmt.Printf("Job submission failed with status code %d: %s\n", resp.StatusCode, string(body))
    }
  }
  ```

  ```dart Dart [expandable]
  import 'dart:io';
  import 'package:http/http.dart' as http;
  import 'dart:convert';

  void main() async {
    final apiKey = Platform.environment['PLAYAI_API_KEY'];
    final userId = Platform.environment['PLAYAI_USER_ID'];

    final headers = {
      'Authorization': 'Bearer $apiKey',
      'Content-Type': 'application/json',
      'X-USER-ID': userId,
    };

    // Submit TTS Job
    final jsonData = {
      'model': 'PlayDialog',
      'text': 'This is the greatest moment to be alive',
      'voice': 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
      'outputFormat': 'mp3',
      'speed': 1,
      'language': 'english',
    };

    final response = await http.post(
      Uri.parse('https://api.play.ai/api/v1/tts'),
      headers: headers,
      body: jsonEncode(jsonData),
    );

    if (response.statusCode == 201) {
      final data = jsonDecode(response.body);
      final jobId = data['id'];
      print('Job submitted successfully. Job ID: $jobId');

      // Poll Job Status
      while (true) {
        final response = await http.get(
          Uri.parse('https://api.play.ai/api/v1/tts/$jobId'),
          headers: headers,
        );

        final data = jsonDecode(response.body);
        final status = data['output']['status'];

        if (status == 'COMPLETED') {
          final audioUrl = data['output']['url'];
          print('Job completed. Audio URL: $audioUrl');

          // Download Audio
          final audioResponse = await http.get(Uri.parse(audioUrl));
          if (audioResponse.statusCode == 200) {
            final file = File('output.mp3');
            await file.writeAsBytes(audioResponse.bodyBytes);
            print('Audio file saved as output.mp3');
          } else {
            print('Failed to download audio. Status code: ${audioResponse.statusCode}');
          }
          break;
        } else if (status == 'IN_PROGRESS') {
          print('Job is still in progress. Retrying in 5 seconds...');
          await Future.delayed(Duration(seconds: 5));
        } else {
          print('Job failed or encountered an unknown status: $status');
          break;
        }
      }
    } else {
      print('Job submission failed with status code ${response.statusCode}: ${response.body}');
    }
  }
  ```

  ```swift Swift [expandable]
  import Foundation

  let apiKey = ProcessInfo.processInfo.environment["PLAYAI_API_KEY"] ?? ""
  let userId = ProcessInfo.processInfo.environment["PLAYAI_USER_ID"] ?? ""

  let headers = [
    "Authorization": "Bearer \(apiKey)",
    "Content-Type": "application/json",
    "X-USER-ID": userId
  ]

  // Submit TTS Job
  let jsonData: [String: Any] = [
    "model": "PlayDialog",
    "text": "This is the greatest moment to be alive",
    "voice": "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
    "outputFormat": "mp3",
    "speed": 1,
    "language": "english"
  ]

  guard let jsonData = try? JSONSerialization.data(withJSONObject: jsonData) else {
    print("Failed to serialize JSON data")
    exit(1)
  }

  var request = URLRequest(url: URL(string: "https://api.play.ai/api/v1/tts")!)
  request.httpMethod = "POST"
  request.allHTTPHeaderFields = headers
  request.httpBody = jsonData

  let task = URLSession.shared.dataTask(with: request) { data, response, error in
    if let error = error {
        print("Error: \(error)")
        return
    }

    guard let httpResponse = response as? HTTPURLResponse else {
        print("Invalid response")
        return
    }

    if httpResponse.statusCode == 201 {
        if let data = data,
           let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
           let jobId = json["id"] as? String {
            print("Job submitted successfully. Job ID: \(jobId)")

            // Poll Job Status
            func pollJobStatus(jobId: String) {
                while true {
                    var request = URLRequest(url: URL(string: "https://api.play.ai/api/v1/tts/\(jobId)")!)
                    request.allHTTPHeaderFields = headers

                    let task = URLSession.shared.dataTask(with: request) { data, response, error in
                        if let error = error {
                            print("Error: \(error)")
                            return
                        }

                        guard let httpResponse = response as? HTTPURLResponse,
                              let data = data,
                              let json = try? JSONSerialization.jsonObject(with: data) as? [String: Any],
                              let output = json["output"] as? [String: Any],
                              let status = output["status"] as? String else {
                            return
                        }

                        if status == "COMPLETED" {
                            if let audioUrl = output["url"] as? String {
                                print("Job completed. Audio URL: \(audioUrl)")

                                // Download Audio
                                guard let url = URL(string: audioUrl) else { return }
                                let downloadTask = URLSession.shared.dataTask(with: url) { data, response, error in
                                    if let error = error {
                                        print("Error: \(error)")
                                        return
                                    }

                                    guard let httpResponse = response as? HTTPURLResponse else {
                                        print("Invalid response")
                                        return
                                    }

                                    if httpResponse.statusCode == 200 {
                                        if let data = data {
                                            do {
                                                try data.write(to: URL(fileURLWithPath: "output.mp3"))
                                                print("Audio file saved as output.mp3")
                                            } catch {
                                                print("Error saving file: \(error)")
                                            }
                                        }
                                    } else {
                                        print("Failed to download audio. Status code: \(httpResponse.statusCode)")
                                    }
                                }
                                downloadTask.resume()
                            }
                            exit(0)
                        } else if status == "IN_PROGRESS" {
                            print("Job is still in progress. Retrying in 5 seconds...")
                            Thread.sleep(forTimeInterval: 5)
                        } else {
                            print("Job failed or encountered an unknown status: \(status)")
                            exit(1)
                        }
                    }
                    task.resume()
                    RunLoop.main.run(until: Date(timeIntervalSinceNow: 5))
                }
            }

            pollJobStatus(jobId: jobId)
        }
    } else {
        print("Job submission failed with status code \(httpResponse.statusCode)")
        if let data = data, let responseString = String(data: data, encoding: .utf8) {
            print("Response: \(responseString)")
        }
    }
  }
  task.resume()
  RunLoop.main.run()
  ```
</CodeGroup>

## Troubleshooting

If you encounter issues, check these common problems:

* **Authentication Issues:**
  * Verify your API key and user ID
  * Confirm the `AUTHORIZATION` header includes "Bearer " prefix

* **Job Status Polling:**
  * Ensure you're using the correct job ID
  * Check that the polling URL is properly formatted
  * Verify the polling interval is appropriate for your use case

* **API Endpoint Errors:**
  * Verify the correct PlayAI's Dialog 1.0 API endpoint URL
  * Confirm the model name is correct
  * Check that all required parameters are included in the request

* **Language-Specific Issues:**
  * JavaScript: Ensure `node-fetch` is installed for Node.js environments
  * Go: Check for proper error handling and response body closing
  * Dart: Verify the `http` package is added to your `pubspec.yaml`
  * Swift: Make sure you're running on macOS for file system access


# TTS API Quickstart
Source: https://docs.play.ai/documentation/tutorials/tts/dialogs/how-to-use-tts-api

Learn how to use PlayAI's Text-to-Speech API for natural conversations

This guide provides a step-by-step approach to using the PlayAI's Text-to-Speech API to convert text into natural human-like sounding audio.

In this example, we'll have [Dialog 1.0](/documentation/text-to-speech/tts-models#dialog-10) create a simple audio from the given input text.

## Prerequisites

* Access your [credentials](https://play.ai/api/keys) (API key and user ID)
* Development environment for your chosen programming language

## Steps

<Steps>
  <Step title="Set Up Environment Variables">
    Add your API key and user ID to your environment variables.

    <CodeGroup>
      ```bash macOS (zsh)
      echo 'export PLAYAI_API_KEY="your_api_key_here"' >> ~/.zshrc
      echo 'export PLAYAI_USER_ID="your_user_id_here"' >> ~/.zshrc
      source ~/.zshrc
      ```

      ```bash bash
      echo 'export PLAYAI_API_KEY="your_api_key_here"' >> ~/.bashrc
      echo 'export PLAYAI_USER_ID="your_user_id_here"' >> ~/.bashrc
      source ~/.bashrc
      ```

      ```cmd Windows
      setx PLAYAI_API_KEY "your_api_key_here"
      setx PLAYAI_USER_ID "your_user_id_here"
      ```
    </CodeGroup>
  </Step>

  <Step title="Configure API Access">
    Create a script with the following authentication setup for your chosen language:

    <CodeGroup>
      ```python Python
      import os

      api_key = os.getenv("PLAYAI_API_KEY")
      user_id = os.getenv("PLAYAI_USER_ID")

      headers = {
          'Authorization': f'Bearer {api_key}',
          'Content-Type': 'application/json',
          'X-USER-ID': user_id
      }
      ```

      ```javascript JavaScript
      const apiKey = process.env.PLAYAI_API_KEY;
      const userId = process.env.PLAYAI_USER_ID;

      const headers = {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
        'X-USER-ID': userId
      };
      ```

      ```go Go
      apiKey := os.Getenv("PLAYAI_API_KEY")
      userId := os.Getenv("PLAYAI_USER_ID")

      headers := map[string]string{
        "Authorization": fmt.Sprintf("Bearer %s", apiKey),
        "Content-Type": "application/json",
        "X-USER-ID": userId,
      }
      ```

      ```dart Dart
      final apiKey = Platform.environment['PLAYAI_API_KEY'];
      final userId = Platform.environment['PLAYAI_USER_ID'];

      final headers = {
        'Authorization': 'Bearer $apiKey',
        'Content-Type': 'application/json',
        'X-USER-ID': userId,
      };
      ```

      ```swift Swift
      let apiKey = ProcessInfo.processInfo.environment["PLAYAI_API_KEY"] ?? ""
      let userId = ProcessInfo.processInfo.environment["PLAYAI_USER_ID"] ?? ""

      let headers = [
        "Authorization": "Bearer \(apiKey)",
        "Content-Type": "application/json",
        "X-USER-ID": userId
      ]
      ```
    </CodeGroup>
  </Step>

  <Step title="Prepare API Parameters">
    Define your API payload with these key parameters:

    * **`model`**: Use `PlayDialog` for multi-turn conversation
      generation
    * **`text`**: Your input text for speech generation
    * **`voice`**: URL path to the voice manifest
    * **`output_format`**: Choose `wav` or `mp3`

    <CodeGroup>
      ```python Python
      json_data = {
          'model': 'PlayDialog',
          'text': "All human wisdom is summed up in these two words: Wait and hope.",
          'voice': 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
          'outputFormat': 'wav'
      }
      ```

      ```javascript JavaScript
      const jsonData = {
        model: 'PlayDialog',
        text: "All human wisdom is summed up in these two words: Wait and hope.",
        voice: 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
        outputFormat: 'wav'
      };
      ```

      ```go Go
      jsonData := map[string]interface{}{
        "model": "PlayDialog",
        "text": "All human wisdom is summed up in these two words: Wait and hope.",
        "voice": "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
        "outputFormat": "wav",
      }
      ```

      ```dart Dart
      final jsonData = {
        'model': 'PlayDialog',
        'text': 'All human wisdom is summed up in these two words: Wait and hope.',
        'voice': 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
        'outputFormat': 'wav',
      };
      ```

      ```swift Swift
      let jsonData: [String: Any] = [
        "model": "PlayDialog",
        "text": "All human wisdom is summed up in these two words: Wait and hope.",
        "voice": "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
        "outputFormat": "wav"
      ]
      ```
    </CodeGroup>
  </Step>

  <Step title="Implement API Call">
    Create the complete API implementation in your chosen language:

    <CodeGroup>
      ```python Python
      response = requests.post('https://api.play.ai/api/v1/tts/stream',
                             headers=headers,
                             json=json_data)

      if response.status_code == 200:
          with open('dialogue.wav', 'wb') as f:
              f.write(response.content)
          print("Audio file saved as dialogue.wav")
      else:
          print(f"Request failed with status code {response.status_code}: {response.text}")
      ```

      ```javascript JavaScript
      fetch('https://api.play.ai/api/v1/tts/stream', {
        method: 'POST',
        headers: headers,
        body: JSON.stringify(jsonData)
      })
      .then(response => {
        if (response.ok) {
          return response.blob();
        }
        throw new Error(`Request failed with status ${response.status}`);
      })
      .then(blob => {
        const url = window.URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = 'dialogue.wav';
        a.click();
        window.URL.revokeObjectURL(url);
        console.log("Audio file saved as dialogue.wav");
      })
      .catch(error => console.error('Error:', error));
      ```

      ```go Go
      jsonBytes, _ := json.Marshal(jsonData)
      req, _ := http.NewRequest("POST", "https://api.play.ai/api/v1/tts/stream", bytes.NewBuffer(jsonBytes))

      for key, value := range headers {
        req.Header.Add(key, value)
      }

      client := &http.Client{}
      resp, err := client.Do(req)
      if err != nil {
        log.Fatal(err)
      }
      defer resp.Body.Close()

      if resp.StatusCode == 200 {
        out, _ := os.Create("dialogue.wav")
        defer out.Close()
        io.Copy(out, resp.Body)
        fmt.Println("Audio file saved as dialogue.wav")
      } else {
        body, _ := io.ReadAll(resp.Body)
        fmt.Printf("Request failed with status code %d: %s\n", resp.StatusCode, string(body))
      }
      ```

      ```dart Dart
      final response = await http.post(
        Uri.parse('https://api.play.ai/api/v1/tts/stream'),
        headers: headers,
        body: jsonEncode(jsonData),
      );

      if (response.statusCode == 200) {
        final file = File('dialogue.wav');
        await file.writeAsBytes(response.bodyBytes);
        print('Audio file saved as dialogue.wav');
      } else {
        print('Request failed with status code ${response.statusCode}: ${response.body}');
      }
      ```

      ```swift Swift
      guard let jsonData = try? JSONSerialization.data(withJSONObject: jsonData) else {
        print("Failed to serialize JSON data")
        exit(1)
      }

      var request = URLRequest(url: URL(string: "https://api.play.ai/api/v1/tts/stream")!)
      request.httpMethod = "POST"
      request.allHTTPHeaderFields = headers
      request.httpBody = jsonData

      let task = URLSession.shared.dataTask(with: request) { data, response, error in
        if let error = error {
            print("Error: \(error)")
            return
        }

        guard let httpResponse = response as? HTTPURLResponse else {
            print("Invalid response")
            return
        }

        if httpResponse.statusCode == 200 {
            if let data = data {
                do {
                    try data.write(to: URL(fileURLWithPath: "dialogue.wav"))
                    print("Audio file saved as dialogue.wav")
                } catch {
                    print("Error saving file: \(error)")
                }
            }
        } else {
            print("Request failed with status code \(httpResponse.statusCode)")
            if let data = data, let responseString = String(data: data, encoding: .utf8) {
                print("Response: \(responseString)")
            }
        }
      }
      task.resume()
      ```
    </CodeGroup>
  </Step>

  <Step title="Run and Test">
    Follow these steps to run your code:

    <Tabs>
      <Tab title="Python">
        1. Save your code as `playdialog_tts.py`
        2. Open terminal in your code directory
        3. Run: `python3 playdialog_tts.py`
        4. Check for the generated `dialogue.wav` file
      </Tab>

      <Tab title="JavaScript">
        1. Save your code as `playdialog_tts.js`
        2. Install dependencies: `npm install node-fetch`
        3. Run: `node playdialog_tts.js`
        4. Check for the generated `dialogue.wav` file
      </Tab>

      <Tab title="Go">
        1. Save your code as `playdialog_tts.go`
        2. Run: `go run playdialog_tts.go`
        3. Check for the generated `dialogue.wav` file
      </Tab>

      <Tab title="Dart">
        1. Save your code as `playdialog_tts.dart`
        2. Install dependencies: `dart pub add http`
        3. Run: `dart run playdialog_tts.dart`
        4. Check for the generated `dialogue.wav` file
      </Tab>

      <Tab title="Swift">
        1. Save your code as `playdialog_tts.swift`
        2. Run: `swift playdialog_tts.swift`
        3. Check for the generated `dialogue.wav` file
      </Tab>
    </Tabs>
  </Step>

  <Step title="Customize and Adapt">
    Modify your implementation by:

    * Updating the input text
    * Changing speaker details and voices
    * Adjusting output format as needed
    * Adding multiple speakers or complex dialogues
  </Step>
</Steps>

## Complete Code

<CodeGroup>
  ```python Python [expandable]
  import os
  import requests

  api_key = os.getenv("PLAYAI_API_KEY")
  user_id = os.getenv("PLAYAI_USER_ID")

  headers = {
      'Authorization': f'Bearer {api_key}',
      'Content-Type': 'application/json',
      'X-USER-ID': user_id
  }

  json_data = {
      'model': 'PlayDialog',
      'text': "All human wisdom is summed up in these two words: Wait and hope.",
      'voice': 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
      'outputFormat': 'wav'
  }

  response = requests.post('https://api.play.ai/api/v1/tts/stream',
                         headers=headers,
                         json=json_data)

  if response.status_code == 200:
      with open('dialogue.wav', 'wb') as f:
          f.write(response.content)
      print("Audio file saved as dialogue.wav")
  else:
      print(f"Request failed with status code {response.status_code}: {response.text}")
  ```

  ```javascript JavaScript [expandable]
  const fetch = require('node-fetch');

  const apiKey = process.env.PLAYAI_API_KEY;
  const userId = process.env.PLAYAI_USER_ID;

  const headers = {
    'Authorization': `Bearer ${apiKey}`,
    'Content-Type': 'application/json',
    'X-USER-ID': userId
  };

  const jsonData = {
    model: 'PlayDialog',
    text: "All human wisdom is summed up in these two words: Wait and hope.",
    voice: 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
    outputFormat: 'wav'
  };

  fetch('https://api.play.ai/api/v1/tts/stream', {
    method: 'POST',
    headers: headers,
    body: JSON.stringify(jsonData)
  })
  .then(response => {
    if (response.ok) {
      return response.blob();
    }
    throw new Error(`Request failed with status ${response.status}`);
  })
  .then(blob => {
    const url = window.URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = 'dialogue.wav';
    a.click();
    window.URL.revokeObjectURL(url);
    console.log("Audio file saved as dialogue.wav");
  })
  .catch(error => console.error('Error:', error));
  ```

  ```go Go [expandable]
  package main

  import (
    "bytes"
    "encoding/json"
    "fmt"
    "io"
    "log"
    "net/http"
    "os"
  )

  func main() {
    apiKey := os.Getenv("PLAYAI_API_KEY")
    userId := os.Getenv("PLAYAI_USER_ID")

    headers := map[string]string{
      "Authorization": fmt.Sprintf("Bearer %s", apiKey),
      "Content-Type": "application/json",
      "X-USER-ID": userId,
    }

    jsonData := map[string]interface{}{
      "model": "PlayDialog",
      "text": "All human wisdom is summed up in these two words: Wait and hope.",
      "voice": "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
      "outputFormat": "wav",
    }

    jsonBytes, _ := json.Marshal(jsonData)
    req, _ := http.NewRequest("POST", "https://api.play.ai/api/v1/tts/stream", bytes.NewBuffer(jsonBytes))
    
    for key, value := range headers {
      req.Header.Add(key, value)
    }

    client := &http.Client{}
    resp, err := client.Do(req)
    if err != nil {
      log.Fatal(err)
    }
    defer resp.Body.Close()

    if resp.StatusCode == 200 {
      out, _ := os.Create("dialogue.wav")
      defer out.Close()
      io.Copy(out, resp.Body)
      fmt.Println("Audio file saved as dialogue.wav")
    } else {
      body, _ := io.ReadAll(resp.Body)
      fmt.Printf("Request failed with status code %d: %s\n", resp.StatusCode, string(body))
    }
  }
  ```

  ```dart Dart [expandable]
  import 'dart:io';
  import 'package:http/http.dart' as http;
  import 'dart:convert';

  void main() async {
    final apiKey = Platform.environment['PLAYAI_API_KEY'];
    final userId = Platform.environment['PLAYAI_USER_ID'];

    final headers = {
      'Authorization': 'Bearer $apiKey',
      'Content-Type': 'application/json',
      'X-USER-ID': userId,
    };

    final jsonData = {
      'model': 'PlayDialog',
      'text': 'All human wisdom is summed up in these two words: Wait and hope.',
      'voice': 's3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json',
      'outputFormat': 'wav',
    };

    final response = await http.post(
      Uri.parse('https://api.play.ai/api/v1/tts/stream'),
      headers: headers,
      body: jsonEncode(jsonData),
    );

    if (response.statusCode == 200) {
      final file = File('dialogue.wav');
      await file.writeAsBytes(response.bodyBytes);
      print('Audio file saved as dialogue.wav');
    } else {
      print('Request failed with status code ${response.statusCode}: ${response.body}');
    }
  }
  ```

  ```swift Swift [expandable]
  import Foundation

  let apiKey = ProcessInfo.processInfo.environment["PLAYAI_API_KEY"] ?? ""
  let userId = ProcessInfo.processInfo.environment["PLAYAI_USER_ID"] ?? ""

  let headers = [
    "Authorization": "Bearer \(apiKey)",
    "Content-Type": "application/json",
    "X-USER-ID": userId
  ]

  let jsonData: [String: Any] = [
    "model": "PlayDialog",
    "text": "All human wisdom is summed up in these two words: Wait and hope.",
    "voice": "s3://voice-cloning-zero-shot/baf1ef41-36b6-428c-9bdf-50ba54682bd8/original/manifest.json",
    "outputFormat": "wav"
  ]

  guard let jsonData = try? JSONSerialization.data(withJSONObject: jsonData) else {
    print("Failed to serialize JSON data")
    exit(1)
  }

  var request = URLRequest(url: URL(string: "https://api.play.ai/api/v1/tts/stream")!)
  request.httpMethod = "POST"
  request.allHTTPHeaderFields = headers
  request.httpBody = jsonData

  let task = URLSession.shared.dataTask(with: request) { data, response, error in
    if let error = error {
        print("Error: \(error)")
        return
    }

    guard let httpResponse = response as? HTTPURLResponse else {
        print("Invalid response")
        return
    }

    if httpResponse.statusCode == 200 {
        if let data = data {
            do {
                try data.write(to: URL(fileURLWithPath: "dialogue.wav"))
                print("Audio file saved as dialogue.wav")
            } catch {
                print("Error saving file: \(error)")
            }
        }
    } else {
        print("Request failed with status code \(httpResponse.statusCode)")
        if let data = data, let responseString = String(data: data, encoding: .utf8) {
            print("Response: \(responseString)")
        }
    }
  }
  task.resume()
  ```
</CodeGroup>

## Troubleshooting

If you encounter issues, check these common problems:

* **Authentication Issues:**
  * Verify your API key and user ID
  * Confirm the `AUTHORIZATION` header includes "Bearer " prefix

* **API Endpoint Errors:**
  * Verify the correct PlayAI's Dialog 1.0 API endpoint URL
  * Confirm the `model` name is `PlayDialog`

* **Language-Specific Issues:**
  * JavaScript: Ensure `node-fetch` is installed for Node.js environments
  * Go: Check for proper error handling and response body closing
  * Dart: Verify the `http` package is added to your `pubspec.yaml`
  * Swift: Make sure you're running on macOS for file system access


