# Getting Started
Source: https://docs.comfy.org/comfy-cli/getting-started



### Overview

`comfy-cli` is a [command line tool](https://github.com/Comfy-Org/comfy-cli) that makes it easier to install and manage Comfy.

### Install CLI

<CodeGroup>
  ```bash pip
  pip install comfy-cli
  ```

  ```bash homebrew
  brew tap Comfy-Org/comfy-cli
  brew install comfy-org/comfy-cli/comfy-cli
  ```
</CodeGroup>

To get shell completion hints:

```bash
comfy --install-completion
```

### Install ComfyUI

Create a virtual environment with any Python version greater than 3.9.

<CodeGroup>
  ```bash conda
  conda create -n comfy-env python=3.11
  conda activate comfy-env
  ```

  ```bash venv
  python3 -m venv comfy-env
  source comfy-env/bin/activate
  ```
</CodeGroup>

Install ComfyUI

```bash
comfy install
```

<Warning>You still need to install CUDA, or ROCm depending on your GPU.</Warning>

### Run ComfyUI

```bash
comfy launch
```

### Manage Custom Nodes

```bash
comfy node install <NODE_NAME>
```

We use `cm-cli` for installing custom nodes. See the [docs](https://github.com/ltdrdata/ComfyUI-Manager/blob/main/docs/en/cm-cli.md) for more information.

### Manage Models

Downloading models with `comfy-cli` is easy. Just run:

```bash
comfy model download <url> models/checkpoints
```

### Contributing

We encourage contributions to comfy-cli! If you have suggestions, ideas, or bug reports, please open an issue on our [GitHub repository](https://github.com/Comfy-Org/comfy-cli/issues). If you want to contribute code, fork the repository and submit a pull request.

Refer to the [Dev Guide](https://github.com/Comfy-Org/comfy-cli/blob/main/DEV_README.md) for further details.

### Analytics

We track usage of the CLI to improve the user experience. You can disable this by running:

```bash
comfy tracking disable
```

To re-enable tracking, run:

```bash
comfy tracking enable
```


# Reference
Source: https://docs.comfy.org/comfy-cli/reference



# CLI

## Nodes

**Usage**:

```console
$ comfy node [OPTIONS] COMMAND [ARGS]...
```

**Options**:

* `--install-completion`: Install completion for the current shell.
* `--show-completion`: Show completion for the current shell, to copy it or customize the installation.
* `--help`: Show this message and exit.

**Commands**:

* `deps-in-workflow`
* `disable`
* `enable`
* `fix`
* `install`
* `install-deps`
* `reinstall`
* `restore-dependencies`
* `restore-snapshot`
* `save-snapshot`: Save a snapshot of the current ComfyUI...
* `show`
* `simple-show`
* `uninstall`
* `update`

### `deps-in-workflow`

**Usage**:

```console
$ deps-in-workflow [OPTIONS]
```

**Options**:

* `--workflow TEXT`: Workflow file (.json/.png)  \[required]
* `--output TEXT`: Workflow file (.json/.png)  \[required]
* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `disable`

**Usage**:

```console
$ disable [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: disable custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `enable`

**Usage**:

```console
$ enable [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: enable custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `fix`

**Usage**:

```console
$ fix [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: fix dependencies for specified custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `install`

**Usage**:

```console
$ install [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: install custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `install-deps`

**Usage**:

```console
$ install-deps [OPTIONS]
```

**Options**:

* `--deps TEXT`: Dependency spec file (.json)
* `--workflow TEXT`: Workflow file (.json/.png)
* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `reinstall`

**Usage**:

```console
$ reinstall [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: reinstall custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `restore-dependencies`

**Usage**:

```console
$ restore-dependencies [OPTIONS]
```

**Options**:

* `--help`: Show this message and exit.

### `restore-snapshot`

**Usage**:

```console
$ restore-snapshot [OPTIONS] PATH
```

**Arguments**:

* `PATH`: \[required]

**Options**:

* `--help`: Show this message and exit.

### `save-snapshot`

Save a snapshot of the current ComfyUI environment

**Usage**:

```console
$ save-snapshot [OPTIONS]
```

**Options**:

* `--output TEXT`: Specify the output file path. (.json/.yaml)
* `--help`: Show this message and exit.

### `show`

**Usage**:

```console
$ show [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: \[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list]  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `simple-show`

**Usage**:

```console
$ simple-show [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: \[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list]  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `uninstall`

**Usage**:

```console
$ uninstall [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: uninstall custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `update`

**Usage**:

```console
$ update [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: update custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

## Models

**Usage**:

```console
$ comfy model [OPTIONS] COMMAND [ARGS]...
```

**Options**:

* `--install-completion`: Install completion for the current shell.
* `--show-completion`: Show completion for the current shell, to copy it or customize the installation.
* `--help`: Show this message and exit.

**Commands**:

* `download`: Download a model to a specified relative...
* `list`: Display a list of all models currently...
* `remove`: Remove one or more downloaded models,...

### `download`

Download a model to a specified relative path if it is not already downloaded.

**Usage**:

```console
$ download [OPTIONS]
```

**Options**:

* `--url TEXT`: The URL from which to download the model  \[required]
* `--relative-path TEXT`: The relative path from the current workspace to install the model.  \[default: models/checkpoints]
* `--help`: Show this message and exit.

### `list`

Display a list of all models currently downloaded in a table format.

**Usage**:

```console
$ list [OPTIONS]
```

**Options**:

* `--relative-path TEXT`: The relative path from the current workspace where the models are stored.  \[default: models/checkpoints]
* `--help`: Show this message and exit.

### `remove`

Remove one or more downloaded models, either by specifying them directly or through an interactive selection.

**Usage**:

```console
$ remove [OPTIONS]
```

**Options**:

* `--relative-path TEXT`: The relative path from the current workspace where the models are stored.  \[default: models/checkpoints]
* `--model-names TEXT`: List of model filenames to delete, separated by spaces
* `--help`: Show this message and exit.


# Contributing
Source: https://docs.comfy.org/community/contributing



### Create a PR

Fork the [repo](https://github.com/comfyanonymous/ComfyUI), and create a PR.


# Datatypes
Source: https://docs.comfy.org/custom-nodes/backend/datatypes



These are the most important built in datatypes. You can also [define your own](./more_on_inputs#custom-datatypes).

Datatypes are used on the client side to prevent a workflow from passing the wrong form of data into a node - a bit like strong typing.
The JavaScript client side code will generally not allow a node output to be connected to an input of a different datatype,
although a few exceptions are noted below.

## Comfy datatypes

### COMBO

* No additional parameters in `INPUT_TYPES`

* Python datatype: defined as `list[str]`, output value is `str`

Represents a dropdown menu widget.
Unlike other datatypes, `COMBO` it is not specified in `INPUT_TYPES` by a `str`, but by a `list[str]`
corresponding to the options in the dropdown list, with the first option selected by default.

`COMBO` inputs are often dynamically generated at run time. For instance, in the built-in `CheckpointLoaderSimple` node, you find

```
"ckpt_name": (folder_paths.get_filename_list("checkpoints"), )
```

or they might just be a fixed list of options,

```
"play_sound": (["no","yes"], {}),
```

### Primitive and reroute

Primitive and reroute nodes only exist on the client side. They do not have an intrinsic datatype, but when connected they take on
the datatype of the input or output to which they have been connected (which is why they can't connect to a `*` input...)

## Python datatypes

### INT

* Additional parameters in `INPUT_TYPES`:

  * `default` is required

  * `min` and `max` are optional

* Python datatype `int`

### FLOAT

* Additional parameters in `INPUT_TYPES`:

  * `default` is required

  * `min`, `max`, `step` are optional

* Python datatype `float`

### STRING

* Additional parameters in `INPUT_TYPES`:

  * `default` is required

* Python datatype `str`

### BOOLEAN

* Additional parameters in `INPUT_TYPES`:

  * `default` is required

* Python datatype `bool`

## Tensor datatypes

### IMAGE

* No additional parameters in `INPUT_TYPES`

* Python datatype `torch.Tensor` with *shape* \[B,H,W,C]

A batch of `B` images, height `H`, width `W`, with `C` channels (generally `C=3` for `RGB`).

### LATENT

* No additional parameters in `INPUT_TYPES`

* Python datatype `dict`, containing a `torch.Tensor` with *shape* \[B,C,H,W]

The `dict` passed contains the key `samples`, which is a `torch.Tensor` with *shape* \[B,C,H,W] representing
a batch of `B` latents, with `C` channels (generally `C=4` for existing stable diffusion models), height `H`, width `W`.

The height and width are 1/8 of the corresponding image size (which is the value you set in the Empty Latent Image node).

Other entries in the dictionary contain things like latent masks.

{/* TODO need to dig into this */}

{/* TODO new SD models might have different C values? */}

### MASK

* No additional parameters in `INPUT_TYPES`

* Python datatype `torch.Tensor` with *shape* \[H,W] or \[B,C,H,W]

### AUDIO

* No additional parameters in `INPUT_TYPES`

* Python datatype `dict`, containing a `torch.Tensor` with *shape* \[B, C, T] and a sample rate.

The `dict` passed contains the key `waveform`, which is a `torch.Tensor` with *shape* \[B, C, T] representing a batch of `B` audio samples, with `C` channels (`C=2` for stereo and `C=1` for mono), and `T` time steps (i.e., the number of audio samples).

The `dict` contains another key `sample_rate`, which indicates the sampling rate of the audio.

## Custom Sampling datatypes

### Noise

The `NOISE` datatype represents a *source* of noise (not the actual noise itself). It can be represented by any Python object
that provides a method to generate noise, with the signature `generate_noise(self, input_latent:Tensor) -> Tensor`, and a
property, `seed:Optional[int]`.

<Tip>The `seed` is passed into `sample` guider in the `SamplerCustomAdvanced`, but does not appear to be used in any of the standard guiders.
It is Optional, so you can generally set it to None.</Tip>

When noise is to be added, the latent is passed into this method, which should return a `Tensor` of the same shape containing the noise.

See the [noise mixing example](./snippets#creating-noise-variations)

### Sampler

The `SAMPLER` datatype represents a sampler, which is represented as a Python object providing a `sample` method.
Stable diffusion sampling is beyond the scope of this guide; see `comfy/samplers.py` if you want to dig into this part of the code.

### Sigmas

The `SIGMAS` datatypes represents the values of sigma before and after each step in the sampling process, as produced by a scheduler.
This is represented as a one-dimensional tensor, of length `steps+1`, where each element represents the noise expected to be present
before the corresponding step, with the final value representing the noise present after the final step.

A `normal` scheduler, with 20 steps and denoise of 1, for an SDXL model, produces:

```
tensor([14.6146, 10.7468,  8.0815,  6.2049,  4.8557,  
         3.8654,  3.1238,  2.5572,  2.1157,  1.7648,  
         1.4806,  1.2458,  1.0481,  0.8784,  0.7297,  
         0.5964,  0.4736,  0.3555,  0.2322,  0.0292,  0.0000])
```

<Tip>The starting value of sigma depends on the model, which is why a scheduler node requires a `MODEL` input to produce a SIGMAS output</Tip>

### Guider

A `GUIDER` is a generalisation of the denoising process, as 'guided' by a prompt or any other form of conditioning. In Comfy the guider is
represented by a `callable` Python object providing a `__call__(*args, **kwargs)` method which is called by the sample.

The `__call__` method takes (in `args[0]`) a batch of noisy latents (tensor `[B,C,H,W]`), and returns a prediction of the noise (a tensor of the same shape).

## Model datatypes

There are a number of more technical datatypes for stable diffusion models. The most significant ones are `MODEL`, `CLIP`, `VAE` and `CONDITIONING`.
Working with these is (for the time being) beyond the scope of this guide! {/* TODO but maybe not forever */}

## Additional Parameters

Below is a list of officially supported keys that can be used in the 'extra options' portion of an input definition.

<Warning>You can use additional keys for your own custom widgets, but should *not* reuse any of the keys below for other purposes.</Warning>

{/* TODO -- did I actually get everything? */}

| Key              | Description                                                                                                                                                                                      |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `default`        | The default value of the widget                                                                                                                                                                  |
| `min`            | The minimum value of a number (`FLOAT` or `INT`)                                                                                                                                                 |
| `max`            | The maximum value of a number (`FLOAT` or `INT`)                                                                                                                                                 |
| `step`           | The amount to increment or decrement a widget                                                                                                                                                    |
| `label_on`       | The label to use in the UI when the bool is `True` (`BOOL`)                                                                                                                                      |
| `label_off`      | The label to use in the UI when the bool is `False` (`BOOL`)                                                                                                                                     |
| `defaultInput`   | Defaults to an input socket rather than a supported widget                                                                                                                                       |
| `forceInput`     | `defaultInput` and also don't allow converting to a widget                                                                                                                                       |
| `multiline`      | Use a multiline text box (`STRING`)                                                                                                                                                              |
| `placeholder`    | Placeholder text to display in the UI when empty (`STRING`)                                                                                                                                      |
| `dynamicPrompts` | Causes the front-end to evaluate dynamic prompts                                                                                                                                                 |
| `lazy`           | Declares that this input uses [Lazy Evaluation](./lazy_evaluation)                                                                                                                               |
| `rawLink`        | When a link exists, rather than receiving the evaluated value, you will receive the link (i.e. `["nodeId", <outputIndex>]`). Primarily useful when your node uses [Node Expansion](./expansion). |


# Node Expansion
Source: https://docs.comfy.org/custom-nodes/backend/expansion



## Node Expansion

Normally, when a node is executed, that execution function immediately returns the output results of that node. "Node Expansion" is a relatively advanced technique that allows nodes to return a new subgraph of nodes that should take its place in the graph. This technique is what allows custom nodes to implement loops.

### A Simple Example

First, here's a simple example of what node expansion looks like:

<Tip>We highly recommend using the `GraphBuilder` class when creating subgraphs. It isn't mandatory, but it prevents you from making many easy mistakes.</Tip>

```python
def load_and_merge_checkpoints(self, checkpoint_path1, checkpoint_path2, ratio):
    from comfy_execution.graph_utils import GraphBuilder # Usually at the top of the file
    graph = GraphBuilder()
    checkpoint_node1 = graph.node("CheckpointLoaderSimple", checkpoint_path=checkpoint_path1)
    checkpoint_node2 = graph.node("CheckpointLoaderSimple", checkpoint_path=checkpoint_path2)
    merge_model_node = graph.node("ModelMergeSimple", model1=checkpoint_node1.out(0), model2=checkpoint_node2.out(0), ratio=ratio)
    merge_clip_node = graph.node("ClipMergeSimple", clip1=checkpoint_node1.out(1), clip2=checkpoint_node2.out(1), ratio=ratio)
    return {
        # Returning (MODEL, CLIP, VAE) outputs
        "result": (merge_model_node.out(0), merge_clip_node.out(0), checkpoint_node1.out(2)),
        "expand": graph.finalize(),
    }
```

While this same node could previously be implemented by manually calling into ComfyUI internals, using expansion means that each subnode will be cached separately (so if you change `model2`, you don't have to reload `model1`).

### Requirements

In order to perform node expansion, a node must return a dictionary with the following keys:

1. `result`: A tuple of the outputs of the node. This may be a mix of finalized values (like you would return from a normal node) and node outputs.
2. `expand`: The finalized graph to perform expansion on. See below if you are not using the `GraphBuilder`.

#### Additional Requirements if not using GraphBuilder

The format expected from the `expand` key is the same as the ComfyUI API format. The following requirements are handled by the `GraphBuilder`, but must be handled manually if you choose to forego it:

1. Node IDs must be unique across the entire graph. (This includes between multiple executions of the same node due to the use of lists.)
2. Node IDs must be deterministic and consistent between multiple executions of the graph (including partial executions due to caching).

Even if you don't want to use the `GraphBuilder` for actually building the graph (e.g. because you're loading the raw json of the graph from a file), you can use the `GraphBuilder.alloc_prefix()` function to generate a prefix and `comfy.graph_utils.add_graph_prefix` to fix existing graphs to meet these requirements.

### Efficient Subgraph Caching

While you can pass non-literal inputs to nodes within the subgraph (like torch tensors), this can inhibit caching *within* the subgraph. When possible, you should pass links to subgraph objects rather than the node itself. (You can declare an input as a `rawLink` within the input's [Additional Parameters](./datatypes#additional-parameters) to do this easily.)


# Images, Latents, and Masks
Source: https://docs.comfy.org/custom-nodes/backend/images_and_masks



When working with these datatypes, you will need to know about the `torch.Tensor` class.
Complete documentation is [here](https://pytorch.org/docs/stable/tensors.html), or
an introduction to the key concepts required for Comfy [here](./tensors).

<Warning>If your node has a single output which is a tensor, remember to return `(image,)` not `(image)`</Warning>

Most of the concepts below are illustrated in the [example code snippets](./snippets).

## Images

An IMAGE is a `torch.Tensor` with shape `[B,H,W,C]`, `C=3`. If you are going to save or load images, you will
need to convert to and from `PIL.Image` format - see the code snippets below! Note that some `pytorch` operations
offer (or expect) `[B,C,H,W]`, known as 'channel first', for reasons of computational efficiency. Just be careful.

### Working with PIL.Image

If you want to load and save images, you'll want to use PIL:

```python
from PIL import Image, ImageOps
```

## Masks

A MASK is a `torch.Tensor` with shape `[B,H,W]`.
In many contexts, masks have binary values (0 or 1), which are used to indicate which pixels should undergo specific operations.
In some cases values between 0 and 1 are used indicate an extent of masking, (for instance, to alter transparency, adjust filters, or composite layers).

### Masks from the Load Image Node

The `LoadImage` node uses an image's alpha channel (the "A" in "RGBA") to create MASKs.
The values from the alpha channel are normalized to the range \[0,1] (torch.float32) and then inverted.
The `LoadImage` node always produces a MASK output when loading an image. Many images (like JPEGs) don't have an alpha channel.
In these cases, `LoadImage` creates a default mask with the shape `[1, 64, 64]`.

### Understanding Mask Shapes

In libraries like `numpy`, `PIL`, and many others, single-channel images (like masks) are typically represented as 2D arrays, shape `[H,W]`.
This means the `C` (channel) dimension is implicit, and thus unlike IMAGE types, batches of MASKs have only three dimensions: `[B, H, W]`.
It is not uncommon to encounter a mask which has had the `B` dimension implicitly squeezed, giving a tensor `[H,W]`.

To use a MASK, you will often have to match shapes by unsqueezing to produce a shape `[B,H,W,C]` with `C=1`
To unsqueezing the `C` dimension, so you should `unsqueeze(-1)`, to unsqueeze `B`, you `unsqueeze(0)`.
If your node receives a MASK as input, you would be wise to always check `len(mask.shape)`.

## Latents

A LATENT is a `dict`; the latent sample is referenced by the key `samples` and has shape `[B,C,H,W]`, with `C=4`.

<Tip>LATENT is channel first, IMAGE is channel last</Tip>


# Lazy Evaluation
Source: https://docs.comfy.org/custom-nodes/backend/lazy_evaluation



## Lazy Evaluation

By default, all `required` and `optional` inputs are evaluated before a node can be run. Sometimes, however, an
input won't necessarily be used and evaluating it would result in unnecessary processing. Here are some examples
of nodes where lazy evaluation may be beneficial:

1. A `ModelMergeSimple` node where the ratio is either `0.0` (in which case the first model doesn't need to be loaded)
   or `1.0` (in which case the second model doesn't need to be loaded).
2. Interpolation between two images where the ratio (or mask) is either entirely `0.0` or entirely `1.0`.
3. A Switch node where one input determines which of the other inputs will be passed through.

<Tip>There is very little cost in making an input lazy. If it's something you can do, you generally should.</Tip>

### Creating Lazy Inputs

There are two steps to making an input a "lazy" input. They are:

1. Mark the input as lazy in the dictionary returned by `INPUT_TYPES`
2. Define a method named `check_lazy_status` (note: *not* a class method) that will be called prior to evaluation to determine if any more inputs are necessary.

To demonstrate these, we'll make a "MixImages" node that interpolates between two images according to a mask. If the entire mask is `0.0`, we don't need to evaluate any part of the tree leading up to the second image. If the entire mask is `1.0`, we can skip evaluating the first image.

#### Defining `INPUT_TYPES`

Declaring that an input is lazy is as simple as adding a `lazy: True` key-value pair to the input's options dictionary.

```python
@classmethod
def INPUT_TYPES(cls):
    return {
        "required": {
            "image1": ("IMAGE",{"lazy": True}),
            "image2": ("IMAGE",{"lazy": True}),
            "mask": ("MASK",),
        },
    }
```

In this example, `image1` and `image2` are both marked as lazy inputs, but `mask` will always be evaluated.

#### Defining `check_lazy_status`

A `check_lazy_status` method is called if there are one or more lazy inputs that are not yet available. This method receives the same arguments as the standard execution function. All available inputs are passed in with their final values while unavailable lazy inputs have a value of `None`.

The responsibility of the `check_lazy_status` function is to return a list of the names of any lazy inputs that are needed to proceed. If all lazy inputs are available, the function should return an empty list.

Note that `check_lazy_status` may be called multiple times. (For example, you might find after evaluating one lazy input that you need to evaluate another.)

<Tip>Note that because the function uses actual input values, it is *not* a class method.</Tip>

```python
def check_lazy_status(self, mask, image1, image2):
    mask_min = mask.min()
    mask_max = mask.max()
    needed = []
    if image1 is None and (mask_min != 1.0 or mask_max != 1.0):
        needed.append("image1")
    if image2 is None and (mask_min != 0.0 or mask_max != 0.0):
        needed.append("image2")
    return needed
```

### Full Example

```python
class LazyMixImages:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image1": ("IMAGE",{"lazy": True}),
                "image2": ("IMAGE",{"lazy": True}),
                "mask": ("MASK",),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "mix"

    CATEGORY = "Examples"

    def check_lazy_status(self, mask, image1, image2):
        mask_min = mask.min()
        mask_max = mask.max()
        needed = []
        if image1 is None and (mask_min != 1.0 or mask_max != 1.0):
            needed.append("image1")
        if image2 is None and (mask_min != 0.0 or mask_max != 0.0):
            needed.append("image2")
        return needed

    # Not trying to handle different batch sizes here just to keep the demo simple
    def mix(self, mask, image1, image2):
        mask_min = mask.min()
        mask_max = mask.max()
        if mask_min == 0.0 and mask_max == 0.0:
            return (image1,)
        elif mask_min == 1.0 and mask_max == 1.0:
            return (image2,)

        result = image1 * (1. - mask) + image2 * mask,
        return (result[0],)
```

## Execution Blocking

While Lazy Evaluation is the recommended way to "disable" part of a graph, there are times when you want to disable an `OUTPUT` node that doesn't implement lazy evaluation itself. If it's an output node that you developed yourself, you should just add lazy evaluation as follows:

1. Add a required (if this is a new node) or optional (if you care about backward compatibility) input for `enabled` that defaults to `True`
2. Make all other inputs `lazy` inputs
3. Only evaluate the other inputs if `enabled` is `True`

If it's not a node you control, you can make use of a `comfy_execution.graph.ExecutionBlocker`. This special object can be returned as an output from any socket. Any nodes which receive an `ExecutionBlocker` as input will skip execution and return that `ExecutionBlocker` for any outputs.

<Tip>**There is intentionally no way to stop an ExecutionBlocker from propagating forward.** If you think you want this, you should really be using Lazy Evaluation.</Tip>

### Usage

There are two ways to construct and use an `ExecutionBlocker`

1. Pass `None` into the constructor to silently block execution. This is useful for cases where blocking execution is part of a successful run -- like disabling an output.

```python
def silent_passthrough(self, passthrough, blocked):
    if blocked:
        return (ExecutionBlocker(None),)
    else:
        return (passthrough,)
```

2. Pass a string into the constructor to display an error message when a node is blocked due to receiving the object. This can be useful if you want to display a meaningful error message if someone uses a meaningless output -- for example, the `VAE` output when loading a model that doesn't contain VAEs.

```python
def load_checkpoint(self, ckpt_name):
    ckpt_path = folder_paths.get_full_path("checkpoints", ckpt_name)
    model, clip, vae = load_checkpoint(ckpt_path)
    if vae is None:
        # This error is more useful than a "'NoneType' has no attribute" error
        # in a later node
        vae = ExecutionBlocker(f"No VAE contained in the loaded model {ckpt_name}")
    return (model, clip, vae)
```


# Lifecycle
Source: https://docs.comfy.org/custom-nodes/backend/lifecycle



## How Comfy loads custom nodes

When Comfy starts, it scans the directory `custom_nodes` for Python modules, and attempts to load them.
If the module exports `NODE_CLASS_MAPPINGS`, it will be treated as a custom node.
<Tip>A python module is a directory containing an `__init__.py` file.
The module exports whatever is listed in the `__all__` attribute defined in `__init__.py`.</Tip>

### **init**.py

`__init__.py` is executed when Comfy attempts to import the module. For a module to be recognized as containing
custom node definitions, it needs to export `NODE_CLASS_MAPPINGS`. If it does (and if nothing goes wrong in the import),
the nodes defined in the module will be available in Comfy. If there is an error in your code,
Comfy will continue, but will report the module as having failed to load. So check the Python console!

A very simple `__init__.py` file would look like this:

```python
from .python_file import MyCustomNode
NODE_CLASS_MAPPINGS = { "My Custom Node" : MyCustomNode }
__all__ = ["NODE_CLASS_MAPPINGS"]
```

#### NODE\_CLASS\_MAPPINGS

`NODE_CLASS_MAPPINGS` must be a `dict` mapping custom node names (unique across the Comfy install)
to the corresponding node class.

#### NODE\_DISPLAY\_NAME\_MAPPINGS

`__init__.py` may also export `NODE_DISPLAY_NAME_MAPPINGS`, which maps the same unique name to a display name for the node.
If `NODE_DISPLAY_NAME_MAPPINGS` is not provided, Comfy will use the unique name as the display name.

#### WEB\_DIRECTORY

If you are deploying client side code, you will also need to export the path, relative to the module, in which the
JavaScript files are to be found. It is conventional to place these in a subdirectory of your custom node named `js`.
<Tip>*Only* `.js` files will be served; you can't deploy `.css` or other types in this way</Tip>

<Warning>In previous versions of Comfy, `__init__.py` was required to copy the JavaScript files into the main Comfy web
subdirectory. You will still see code that does this. Don't.</Warning>


# Data lists
Source: https://docs.comfy.org/custom-nodes/backend/lists



## Length one processing

Internally, the Comfy server represents data flowing from one node to the next as a Python `list`, normally length 1, of the relevant datatype.
In normal operation, when a node returns an output, each element in the output `tuple` is separately wrapped in a list (length 1); then when the
next node is called, the data is unwrapped and passed to the main function.

<Tip>You generally don't need to worry about this, since Comfy does the wrapping and unwrapping.</Tip>

<Tip>This isn't about batches. A batch (of, for instance, latents, or images) is a *single entry* in the list (see [tensor datatypes](./images_and_masks))</Tip>

## List processing

In some circumstance, multiple data instances are processed in a single workflow, in which case the internal data will be a list containing the data instances.
An example of this might be processing a series of images one at a time to avoid running out of VRAM, or handling images of different sizes.

By default, Comfy will process the values in the list sequentially:

* if the inputs are `list`s of different lengths, the shorter ones are padded by repeating the last value
* the main method is called once for each value in the input lists
* the outputs are `list`s, each of which is the same length as the longest input

The relevant code can be found in the method `map_node_over_list` in `execution.py`.

However, as Comfy wraps node outputs into a `list` of length one, if the `tuple` returned by
a custom node contains a `list`, that `list` will be wrapped, and treated as a single piece of data.
In order to tell Comfy that the list being returned should not be wrapped, but treated as a series of data for sequential processing,
the node should provide a class attribute `OUTPUT_IS_LIST`, which is a `tuple[bool]`, of the same length as `RETURN_TYPES`, specifying
which outputs which should be so treated.

A node can also override the default input behaviour and receive the whole list in a single call. This is done by setting a class attribute
`INPUT_IS_LIST` to `True`.

Here's a (lightly annotated) example from the built in nodes - `ImageRebatch` takes one or more batches of images (received as a list, because `INPUT_IS_LIST - True`)
and rebatches them into batches of the requested size.

<Tip>`INPUT_IS_LIST` is node level - all inputs get the same treatment. So the value of the `batch_size` widget is given by `batch_size[0]`.</Tip>

```Python

class ImageRebatch:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": { "images": ("IMAGE",),
                              "batch_size": ("INT", {"default": 1, "min": 1, "max": 4096}) }}
    RETURN_TYPES = ("IMAGE",)
    INPUT_IS_LIST = True
    OUTPUT_IS_LIST = (True, )
    FUNCTION = "rebatch"
    CATEGORY = "image/batch"

    def rebatch(self, images, batch_size):
        batch_size = batch_size[0]    # everything comes as a list, so batch_size is list[int]

        output_list = []
        all_images = []
        for img in images:                    # each img is a batch of images
            for i in range(img.shape[0]):     # each i is a single image
                all_images.append(img[i:i+1])

        for i in range(0, len(all_images), batch_size): # take batch_size chunks and turn each into a new batch
            output_list.append(torch.cat(all_images[i:i+batch_size], dim=0))  # will die horribly if the image batches had different width or height!

        return (output_list,)
```

#### INPUT\_IS\_LIST


# Publishing to the Manager
Source: https://docs.comfy.org/custom-nodes/backend/manager



{/*
  description: "Understand how to publish a custom node to the ComfyUI Manager database."
  */}

{/*
  ## What is a custom node?

  One of the great powers of Comfy is that its node-based approach allows you to develop new workflows by plugging together the nodes provided in different ways. The built-in nodes provide a wide range of functionality, but you may find that you need a feature not provided by a core node. 

  Custom nodes are nodes developed by the community. It allows you to implement new features and share them with the wider community. If you are interested in developing custom nodes, you can read more about it [here](/custom-nodes/overview).

  ## ComfyUI Manager

  While custom nodes can be installed manually, most people use
  [ComfyUI Manager](https://github.com/ltdrdata/ComfyUI-Manager) to install them. **ComfyUI Manager** takes care of installing, 
  updating, and removing custom nodes, and any dependencies. But it isn't part
  of the Comfy core, so you need to manually install it. 

  ### Installing ComfyUI Manager

  ```bash
  cd ComfyUI/custom_nodes
  git clone https://github.com/ltdrdata/ComfyUI-Manager.git
  ```

  Restart Comfy afterwards. 
  See [ComfyUI Manager Install](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#installation) for details or special cases.

  */}

### Using ComfyUI Manager

To make your custom node available through **ComfyUI Manager** you need to save it as a git repository (generally at `github.com`)
and then submit a Pull Request on the **ComfyUI Manager** git, in which you have edited `custom-node-list.json` to add your node.
[More details](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#how-to-register-your-custom-node-into-comfyui-manager).

When a user installs the node, **ComfyUI Manager** will:

<Steps>
  <Step title="Git Clone">
    git clone the repository,
  </Step>

  <Step title="Install Python Dependencies">
    install the pip dependencies listed in the custom node repository under `requirements.txt` (if present),

    ```
    pip install -r requirements.txt
    ```

    <Tip>As is always the case with `pip`, it is possible that your node requirements will be in conflict with other
    custom nodes. Don't make your `requirements.txt` any more restrictive than they need to be.</Tip>
  </Step>

  <Step title="Run Install Script">
    execute `install.py`, if it is present in the custom node repository.
    <Tip>`install.py` is executed from the root path of the custom node</Tip>
  </Step>
</Steps>

### ComfyUI Manager files

As indicated above, there are a number of files and scripts that **ComfyUI Manager** will use to manage the lifecycle of
a custom node. These are all optional.

* `requirements.txt` - Python dependencies as mentioned above
* `install.py`, `uninstall.py` - executed when the custom node is installed or uninstalled
  <Tip>Users can just delete the directory, so you can't rely on `uninstall.py` being run</Tip>
* `disable.py`, `enable.py` - executed when a custom node is disabled or re-enabled
  <Tip>`enable.py` is only run when a disabled node is re-enabled - it should just reverse anything done in `disable.py`</Tip>
  <Tip>Disabled custom node subdirectory have `.disabled` appended to their names, and Comfy ignores these modules</Tip>
* `node_list.json` - only required if the custom nodes pattern of NODE\_CLASS\_MAPPINGS is not conventional.

See the [ComfyUI Manager guide](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#custom-node-support-guide) for official details.


# Hidden and Flexible inputs
Source: https://docs.comfy.org/custom-nodes/backend/more_on_inputs



## Hidden inputs

Alongside the `required` and `optional` inputs, which create corresponding inputs or widgets on the client-side,
there are three `hidden` input options which allow the custom node to request certain information from the server.

These are accessed by returning a value for `hidden` in the `INPUT_TYPES` `dict`, with the signature `dict[str,str]`,
containing one or more of `PROMPT`, `EXTRA_PNGINFO`, or `UNIQUE_ID`

```python
@classmethod
def INPUT_TYPES(s):
    return {
        "required": {...},
        "optional": {...},
        "hidden": {
            "unique_id": "UNIQUE_ID",
            "prompt": "PROMPT", 
            "extra_pnginfo": "EXTRA_PNGINFO",
        }
    }
```

### UNIQUE\_ID

`UNIQUE_ID` is the unique identifier of the node, and matches the `id` property of the node on the client side.
It is commonly used in client-server communications (see [messages](/essentials/comfyui-server/comms_messages#getting-node-id)).

### PROMPT

`PROMPT` is the complete prompt sent by the client to the server.
See [the prompt object](/custom-nodes/js/javascript_objects_and_hijacking#prompt) for a full description.

### EXTRA\_PNGINFO

`EXTRA_PNGINFO` is a dictionary that will be copied into the metadata of any `.png` files saved. Custom nodes can store additional
information in this dictionary for saving (or as a way to communicate with a downstream node).

<Tip>Note that if Comfy is started with the `disable_metadata` option, this data won't be saved.</Tip>

### DYNPROMPT

`DYNPROMPT` is an instance of `comfy_execution.graph.DynamicPrompt`. It differs from `PROMPT` in that it may mutate during the course of execution in response to [Node Expansion](/custom-nodes/backend/expansion).
<Tip>`DYNPROMPT` should only be used for advanced cases (like implementing loops in custom nodes).</Tip>

## Flexible inputs

### Custom datatypes

If you want to pass data between your own custom nodes, you may find it helpful to define a custom datatype. This is (almost) as simple as
just choosing a name for the datatype, which should be a unique string in upper case, such as `CHEESE`.

You can then use `CHEESE` in your node `INPUT_TYPES` and `RETURN_TYPES`, and the Comfy client will only allow `CHEESE` outputs to connect to a `CHEESE` input.
`CHEESE` can be any python object.

The only point to note is that because the Comfy client doesn't know about `CHEESE` you need (unless you define a custom widget for `CHEESE`,
which is a topic for another day), to force it to be an input rather than a widget. This can be done with the `forceInput` option in the input options dictionary:

```python
@classmethod
def INPUT_TYPES(s):
    return {
        "required": { "my_cheese": ("CHEESE", {"forceInput":True}) }
    }
```

### Wildcard inputs

```python
@classmethod
def INPUT_TYPES(s):
    return {
        "required": { "anything": ("*",{})},
    }

@classmethod
def VALIDATE_INPUTS(s, input_types):
    return True
```

The frontend allows `*` to indicate that an input can be connected to any source. Because this is not officially supported by the backend, you can skip the backend validation of types by accepting a parameter named `input_types` in your `VALIDATE_INPUTS` function. (See [VALIDATE\_INPUTS](./server_overview#validate-inputs) for more information.)
It's up to the node to make sense of the data that is passed.

### Dynamically created inputs

If inputs are dynamically created on the client side, they can't be defined in the Python source code.
In order to access this data we need an `optional` dictionary that allows Comfy to pass data with
arbitrary names. Since the Comfy server

```python
class ContainsAnyDict(dict):
    def __contains__(self, key):
        return True
...

@classmethod
def INPUT_TYPES(s):
    return {
        "required": {},
        "optional": ContainsAnyDict()
    }
...

def main_method(self, **kwargs):
    # the dynamically created input data will be in the dictionary kwargs

```

<Tip>Hat tip to rgthree for this pythonic trick!</Tip>


# Properties
Source: https://docs.comfy.org/custom-nodes/backend/server_overview

Properties of a custom node

### Simple Example

Here's the code for the Invert Image Node, which gives an overview of the key concepts in custom node development.

```python
class InvertImageNode:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": { "image_in" : ("IMAGE", {}) },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image_out",)
    CATEGORY = "examples"
    FUNCTION = "invert"

    def invert(self, image_in):
        image_out = 1 - image_in
        return (image_out,)
```

### Main properties

Every custom node is a Python class, with the following key properties:

#### INPUT\_TYPES

`INPUT_TYPES`, as the name suggests, defines the inputs for the node. The method returns a `dict`
which *must* contain the key `required`, and *may* also include the keys `optional` and/or `hidden`. The only difference
between `required` and `optional` inputs is that `optional` inputs can be left unconnected.
For more information on `hidden` inputs, see [Hidden Inputs](./more_on_inputs#hidden-inputs).

Each key has, as its value, another `dict`, in which key-value pairs specify the names and types of the inputs.
The types are defined by a `tuple`, the first element of which defines the data type,
and the second element of which is a `dict` of additional parameters.

Here we have just one required input, named `image_in`, of type `IMAGE`, with no additional parameters.

Note that unlike the next few attributes, this `INPUT_TYPES` is a `@classmethod`. This is so
that the options in dropdown widgets (like the name of the checkpoint to be loaded) can be
computed by Comfy at run time. We'll go into this more later. {/* TODO link when written */}

#### RETURN\_TYPES

A `tuple` of `str` defining the data types returned by the node.
If the node has no outputs this must still be provided `RETURN_TYPES = ()`
<Warning>If you have exactly one output, remember the trailing comma: `RETURN_TYPES = ("IMAGE",)`.
This is required for Python to make it a `tuple`</Warning>

#### RETURN\_NAMES

The names to be used to label the outputs. This is optional; if omitted, the names are simply the `RETURN_TYPES` in lowercase.

#### CATEGORY

Where the node will be found in the ComfyUI **Add Node** menu. Submenus can be specified as a path, eg. `examples/trivial`.

#### FUNCTION

The name of the Python function in the class that should be called when the node is executed.

The function is called with named arguments. All `required` (and `hidden`) inputs will be included;
`optional` inputs will be included only if they are connected, so you should provide default values for them in the function
definition (or capture them with `**kwargs`).

The function returns a tuple corresponding to the `RETURN_TYPES`. This is required even if nothing is returned (`return ()`).
Again, if you only have one output, remember that trailing comma `return (image_out,)`!

### Execution Control Extras

A great feature of Comfy is that it caches outputs,
and only executes nodes that might produce a different result than the previous run.
This can greatly speed up lots of workflows.

In essence this works by identifying which nodes produce an output (these, notably the Image Preview and Save Image nodes, are always executed), and then working
backwards to identify which nodes provide data that might have changed since the last run.

Two optional features of a custom node assist in this process.

#### OUTPUT\_NODE

By default, a node is not considered an output. Set `OUTPUT_NODE = True` to specify that it is.

#### IS\_CHANGED

By default, Comfy considers that a node has changed if any of its inputs or widgets have changed.
This is normally correct, but you may need to override this if, for instance, the node uses a random
number (and does not specify a seed - it's best practice to have a seed input in this case so that
the user can control reproducability and avoid unecessary execution), or loads an input that may have
changed externally, or sometimes ignores inputs (so doesn't need to execute just because those inputs changed).

<Warning>Despite the name, IS\_CHANGED should not return a `bool`</Warning>

`IS_CHANGED` is passed the same arguments as the main function defined by `FUNCTION`, and can return any
Python object. This object is compared with the one returned in the previous run (if any) and the node
will be considered to have changed if `is_changed != is_changed_old` (this code is in `execution.py` if you need to dig).

Since `True == True`, a node that returns `True` to say it has changed will be considered not to have! I'm sure this would
be changed in the Comfy code if it wasn't for the fact that it might break existing nodes to do so.

To specify that your node should always be considered to have changed (which you should avoid if possible, since it
stops Comfy optimising what gets run), `return float("NaN")`. This returns a `NaN` value, which is not equal
to anything, even another `NaN`.

A good example of actually checking for changes is the code from the built-in LoadImage node, which loads the image and returns a hash

```python
    @classmethod
    def IS_CHANGED(s, image):
        image_path = folder_paths.get_annotated_filepath(image)
        m = hashlib.sha256()
        with open(image_path, 'rb') as f:
            m.update(f.read())
        return m.digest().hex()
```

### Other attributes

There are three other attributes that can be used to modify the default Comfy treatment of a node.

#### INPUT\_IS\_LIST, OUTPUT\_IS\_LIST

These are used to control sequential processing of data, and are described [later](./lists.mdx).

### VALIDATE\_INPUTS

If a class method `VALIDATE_INPUTS` is defined, it will be called before the workflow begins execution.
`VALIDATE_INPUTS` should return `True` if the inputs are valid, or a message (as a `str`) describing the error (which will prevent execution).

#### Validating Constants

<Warning>Note that `VALIDATE_INPUTS` will only receive inputs that are defined as constants within the workflow. Any inputs that are received from other nodes will *not* be available in `VALIDATE_INPUTS`.</Warning>

`VALIDATE_INPUTS` is called with only the inputs that its signature requests (those returned by `inspect.getfullargspec(obj_class.VALIDATE_INPUTS).args`). Any inputs which are received in this way will *not* run through the default validation rules. For example, in the following snippet, the front-end will use the specified `min` and `max` values of the `foo` input, but the back-end will not enforce it.

```python
class CustomNode:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": { "foo" : ("INT", {"min": 0, "max": 10}) },
        }

    @classmethod
    def VALIDATE_INPUTS(cls, foo):
        # YOLO, anything goes!
        return True
```

Additionally, if the function takes a `**kwargs` input, it will receive *all* available inputs and all of them will skip validation as if specified explicitly.

#### Validating Types

If the `VALIDATE_INPUTS` method receives an argument named `input_types`, it will be passed a dictionary in which the key is the name of each input which is connected to an output from another node and the value is the type of that output.

When this argument is present, all default validation of input types is skipped. Here's an example making use of the fact that the front-end allows for the specification of multiple types:

```python
class AddNumbers:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "input1" : ("INT,FLOAT", {"min": 0, "max": 1000})
                "input2" : ("INT,FLOAT", {"min": 0, "max": 1000})
            },
        }

    @classmethod
    def VALIDATE_INPUTS(cls, input_types):
        # The min and max of input1 and input2 are still validated because
        # we didn't take `input1` or `input2` as arguments
        if input_types["input1"] not in ("INT", "FLOAT"):
            return "input1 must be an INT or FLOAT type"
        if input_types["input2"] not in ("INT", "FLOAT"):
            return "input2 must be an INT or FLOAT type"
        return True
```


# Annotated Examples
Source: https://docs.comfy.org/custom-nodes/backend/snippets



A growing collection of fragments of example code...

## Images and Masks

### Load an image

Load an image into a batch of size 1 (based on `LoadImage` source code in `nodes.py`)

```python
i = Image.open(image_path)
i = ImageOps.exif_transpose(i)
if i.mode == 'I':
    i = i.point(lambda i: i * (1 / 255))
image = i.convert("RGB")
image = np.array(image).astype(np.float32) / 255.0
image = torch.from_numpy(image)[None,]
```

### Save an image batch

Save a batch of images (based on `SaveImage` source code in `nodes.py`)

```python
for (batch_number, image) in enumerate(images):
    i = 255. * image.cpu().numpy()
    img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))
    filepath = # some path that takes the batch number into account
    img.save(filepath)
```

### Invert a mask

Inverting a mask is a straightforward process. Since masks are normalised to the range \[0,1]:

```python
mask = 1.0 - mask
```

### Convert a mask to Image shape

```Python
# We want [B,H,W,C] with C = 1
if len(mask.shape)==2: # we have [H,W], so insert B and C as dimension 1
    mask = mask[None,:,:,None]
elif len(mask.shape)==3 and mask.shape[2]==1: # we have [H,W,C]
    mask = mask[None,:,:,:]
elif len(mask.shape)==3:                      # we have [B,H,W]
    mask = mask[:,:,:,None]
```

### Using Masks as Transparency Layers

When used for tasks like inpainting or segmentation, the MASK's values will eventually be rounded to the nearest integer so that they are binary — 0 indicating regions to be ignored and 1 indicating regions to be targeted. However, this doesn't happen until the MASK is passed to those nodes. This flexibility allows you to use MASKs as as you would in digital photography contexts as a transparency layer:

```python
# Invert mask back to original transparency layer
mask = 1.0 - mask

# Unsqueeze the `C` (channels) dimension
mask = mask.unsqueeze(-1)

# Concatenate ("cat") along the `C` dimension
rgba_image = torch.cat((rgb_image, mask), dim=-1)
```

## Noise

### Creating noise variations

Here's an example of creating a noise object which mixes the noise from two sources. This could be used to create slight noise variations by varying `weight2`.

```python
class Noise_MixedNoise:
    def __init__(self, nosie1, noise2, weight2):
        self.noise1  = noise1
        self.noise2  = noise2
        self.weight2 = weight2

    @property
    def seed(self): return self.noise1.seed

    def generate_noise(self, input_latent:torch.Tensor) -> torch.Tensor:
        noise1 = self.noise1.generate_noise(input_latent)
        noise2 = self.noise2.generate_noise(input_latent)
        return noise1 * (1.0-self.weight2) + noise2 * (self.weight2)
```


# Working with torch.Tensor
Source: https://docs.comfy.org/custom-nodes/backend/tensors



## pytorch, tensors, and torch.Tensor

All the core number crunching in Comfy is done by [pytorch](https://pytorch.org/). If your custom nodes are going
to get into the guts of stable diffusion you will need to become familiar with this library, which is way beyond
the scope of this introduction.

However, many custom nodes will need to manipulate images, latents and masks, each of which are represented internally
as `torch.Tensor`, so you'll want to bookmark the
[documentation for torch.Tensor](https://pytorch.org/docs/stable/tensors.html).

### What is a Tensor?

`torch.Tensor` represents a tensor, which is the mathematical generalization of a vector or matrix to any number of dimensions.
A tensor's *rank* is the number of dimensions it has (so a vector has *rank* 1, a matrix *rank* 2); its *shape* describes the
size of each dimension.

So an RGB image (of height H and width W) might be thought of as three arrays (one for each color channel), each measuring H x W,
which could be represented as a tensor with *shape* `[H,W,3]`. In Comfy images almost always come in a batch (even if the batch
only contains a single image). `torch` always places the batch dimension first, so Comfy images have *shape* `[B,H,W,3]`, generally
written as `[B,H,W,C]` where C stands for Channels.

### squeeze, unsqueeze, and reshape

If a tensor has a dimension of size 1 (known as a collapsed dimension), it is equivalent to the same tensor with that dimension removed
(a batch with 1 image is just an image). Removing such a collapsed dimension is referred to as squeezing, and
inserting one is known as unsqueezing.

<Warning>Some torch code, and some custom node authors, will return a squeezed tensor when a dimension is collapsed - such
as when a batch has only one member. This is a common cause of bugs!</Warning>

To represent the same data in a different shape is referred to as reshaping. This often requires you to know
the underlying data structure, so handle with care!

### Important notation

`torch.Tensor` supports most Python slice notation, iteration, and other common list-like operations. A tensor
also has a `.shape` attribute which returns its size as a `torch.Size` (which is a subclass of `tuple` and can
be treated as such).

There are some other important bits of notation you'll often see (several of these are less common
standard Python notation, seen much more frequently when dealing with tensors)

* `torch.Tensor` supports the use of `None` in slice notation
  to indicate the insertion of a dimension of size 1.

* `:` is frequently used when slicing a tensor; this simply means 'keep the whole dimension'.
  It's like using `a[start:end]` in Python, but omitting the start point and end point.

* `...` represents 'the whole of an unspecified number of dimensions'. So `a[0, ...]` would extract the first
  item from a batch regardless of the number of dimensions.

* in methods which require a shape to be passed, it is often passed as a `tuple` of the dimensions, in
  which a single dimension can be given the size `-1`, indicating that the size of this dimension should
  be calculated based on the total size of the data.

```python
>>> a = torch.Tensor((1,2))
>>> a.shape
torch.Size([2])
>>> a[:,None].shape 
torch.Size([2, 1])
>>> a.reshape((1,-1)).shape
torch.Size([1, 2])
```

### Elementwise operations

Many binary on `torch.Tensor` (including '+', '-', '\*', '/' and '==') are applied elementwise (independantly applied to each element).
The operands must be *either* two tensors of the same shape, *or* a tensor and a scalar. So:

```python
>>> import torch
>>> a = torch.Tensor((1,2))
>>> b = torch.Tensor((3,2))
>>> a*b
tensor([3., 4.])
>>> a/b
tensor([0.3333, 1.0000])
>>> a==b
tensor([False,  True])
>>> a==1
tensor([ True, False])
>>> c = torch.Tensor((3,2,1)) 
>>> a==c
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0
```

### Tensor truthiness

<Warning>The 'truthiness' value of a Tensor is not the same as that of Python lists.</Warning>

You may be familiar with the truthy value of a Python list as `True` for any non-empty list, and `False` for `None` or `[]`.
By contrast A `torch.Tensor` (with more than one elements) does not have a defined truthy value. Instead you need to use
`.all()` or `.any()` to combine the elementwise truthiness:

```python
>>> a = torch.Tensor((1,2))
>>> print("yes" if a else "no")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: Boolean value of Tensor with more than one value is ambiguous
>>> a.all()
tensor(False)
>>> a.any()
tensor(True)
```

This also means that you need to use `if a is not None:` not `if a:` to determine if a tensor variable has been set.


# Annotated Examples
Source: https://docs.comfy.org/custom-nodes/js/javascript_examples



A growing collection of fragments of example code...

## Right click menus

### Background menu

The main background menu (right-click on the canvas) is generated by a call to\
`LGraph.getCanvasMenuOptions`. One way to add your own menu options is to hijack this call:

```Javascript
/* in setup() */
    const original_getCanvasMenuOptions = LGraphCanvas.prototype.getCanvasMenuOptions;
    LGraphCanvas.prototype.getCanvasMenuOptions = function () {
        // get the basic options 
        const options = original_getCanvasMenuOptions.apply(this, arguments);
        options.push(null); // inserts a divider
        options.push({
            content: "The text for the menu",
            callback: async () => {
                // do whatever
            }
        })
        return options;
    }
```

### Node menu

When you right click on a node, the menu is similarly generated by `node.getExtraMenuOptions`.
But instead of returning an options object, this one gets it passed in...

```javascript
/* in beforeRegisterNodeDef() */
if (nodeType?.comfyClass=="MyNodeClass") { 
    const original_getExtraMenuOptions = nodeType.prototype.getExtraMenuOptions;
    nodeType.prototype.getExtraMenuOptions = function(_, options) {
        original_getExtraMenuOptions?.apply(this, arguments);
        options.push({
            content: "Do something fun",
            callback: async () => {
                // fun thing
            }
        })
    }   
}
```

### Submenus

If you want a submenu, provide a callback which uses `LiteGraph.ContextMenu` to create it:

```javascript
function make_submenu(value, options, e, menu, node) {
    const submenu = new LiteGraph.ContextMenu(
        ["option 1", "option 2", "option 3"],
        { 
            event: e, 
            callback: function (v) { 
                // do something with v (=="option x")
            }, 
            parentMenu: menu, 
            node:node
        }
    )
}

/* ... */
    options.push(
        {
            content: "Menu with options",
            has_submenu: true,
            callback: make_submenu,
        }
    )
```

## Capture UI events

This works just like you'd expect - find the UI element in the DOM and
add an eventListener. `setup()` is a good place to do this, since the page
has fully loaded. For instance, to detect a click on the 'Queue' button:

```Javascript
function queue_button_pressed() { console.log("Queue button was pressed!") }
document.getElementById("queue-button").addEventListener("click", queue_button_pressed);
```

## Detect when a workflow starts

This is one of many `api` events:

```javascript
import { api } from "../../scripts/api.js";
/* in setup() */
    function on_execution_start() { 
        /* do whatever */
    }
    api.addEventListener("execution_start", on_execution_start);
```

## Detect an interrupted workflow

A simple example of hijacking the api:

```Javascript
import { api } from "../../scripts/api.js";
/* in setup() */
    const original_api_interrupt = api.interrupt;
    api.interrupt = function () {
        /* Do something before the original method is called */
        original_api_interrupt.apply(this, arguments);
        /* Or after */
    }
```

## Catch clicks on your node

`node` has a mouseDown method you can hijack.
This time we're careful to pass on any return value.

```javascript
async nodeCreated(node) {
    if (node?.comfyClass === "My Node Name") {
        const original_onMouseDown = node.onMouseDown;
        node.onMouseDown = function( e, pos, canvas ) {
            alert("ouch!");
            return original_onMouseDown?.apply(this, arguments);
        }        
    }
}
```


# Comfy Hooks
Source: https://docs.comfy.org/custom-nodes/js/javascript_hooks



## Extension hooks

At various points during Comfy execution, the application calls
`#invokeExtensionsAsync` or `#invokeExtensions` with the name of a hook.
These invoke, on all registered extensions, the appropriately named method (if present), such as `setup`
in the example above.

Comfy provides a variety of hooks for custom extension code to use to modify client behavior.

<Tip>These hooks are called during creation and modification of the Comfy client side elements.
<br />Events during workflow execution are handled by
the `apiUpdateHandlers`</Tip> {/* TODO link when written */}

A few of the most significant hooks are described below.
As Comfy is being actively developed, from time to time additional hooks are added, so
search for `#invokeExtensions` in `app.js` to find all available hooks.

See also the [sequence](#call-sequences) in which hooks are invoked.

### Commonly used hooks

Start with `beforeRegisterNodeDef`, which is used by the majority of extensions, and is often the only one needed.

#### beforeRegisterNodeDef()

Called once for each node type (the list of nodes available in the `AddNode` menu), and is used to
modify the bahaviour of the node.

```Javascript
async beforeRegisterNodeDef(nodeType, nodeData, app) 
```

The object passed in the `nodeType` parameter essentially serves as a template
for all nodes that will be created of this type, so modifications made to `nodeType.prototype` will apply
to all nodes of this type. `nodeData` is an encapsulation of aspects of the node defined in the Python code,
such as its category, inputs, and outputs. `app` is a reference to the main Comfy app object (which you
have already imported anyway!)

<Tip>This method is called, on each registered extension, for *every* node type, not just the ones added by that extension.</Tip>

The usual idiom is to check `nodeType.ComfyClass`, which holds the Python class name corresponding to this node,
to see if yuo need to modify the node. Often this means modifying the custom nodes that you have added,
although you may sometimes need to modify the behavior of other nodes (or other custom nodes
might modify yours!), in which case care should be taken to ensure interoperability.

<Tip>Since other extensions may also modify nodes, aim to write code that makes as few assumptions as possible.
And play nicely - isolate your changes wherever possible.</Tip>

A very common idiom in `beforeRegisterNodeDef` is to 'hijack' an existing method:

```Javascript
async beforeRegisterNodeDef(nodeType, nodeData, app) {
	if (nodeType.comfyClass=="MyNodeClass") { 
		const onConnectionsChange = nodeType.prototype.onConnectionsChange;
		nodeType.prototype.onConnectionsChange = function (side,slot,connect,link_info,output) {     
			const r = onConnectionsChange?.apply(this, arguments);   
			console.log("Someone changed my connection!");
			return r;
		}
	}
}
```

In this idiom the existing prototype method is stored, and then replaced. The replacement calls the
original method (the `?.apply` ensures that if there wasn't one this is still safe) and then
performs additional operations. Depending on your code logic, you may need to place the `apply` elsewhere in your replacement code,
or even make calling it conditional.

When hijacking a method in this way, you will want to look at the core comfy code (breakpoints are your friend) to check
and conform with the method signature.

#### nodeCreated()

```Javascript
async nodeCreated(node)
```

Called when a specific instance of a node gets created
(right at the end of the `ComfyNode()` function on `nodeType` which serves as a constructor).
In this hook you can make modifications to individual instances of your node.

<Tip>Changes that apply to all instances are better added to the prototype in `beforeRegisterNodeDef` as described above.</Tip>

#### init()

```Javascript
async init()
```

Called when the Comfy webpage is loaded (or reloaded). The call is made after the graph object has been created, but before any
nodes are registered or created. It can be used to modify core Comfy behavior by hijacking methods of the app, or of the
graph (a `LiteGraph` object). This is discussed further in [Comfy Objects](./javascript_objects_and_hijacking).

<Warning>With great power comes great responsibility. Hijacking core behavior makes it more likely your nodes
will be incompatible with other custom nodes, or future Comfy updates</Warning>

#### setup()

```Javascript
async setup()
```

Called at the end of the startup process. A good place to add event listeners (either for Comfy events, or DOM events),
or adding to the global menus, both of which are discussed elsewhere. {/* TODO link when written */}

<Tip>To do something when a workflow has loaded, use `afterConfigureGraph`, not `setup`</Tip>

### Call sequences

These sequences were obtained by insert logging code into the Comfy `app.js` file. You may find similar code helpful
in understanding the execution flow.

```Javascript
/* approx line 220 at time of writing: */
	#invokeExtensions(method, ...args) {
		console.log(`invokeExtensions      ${method}`) // this line added
		// ...
	}
/* approx line 250 at time of writing: */
	async #invokeExtensionsAsync(method, ...args) {
		console.log(`invokeExtensionsAsync ${method}`) // this line added
		// ...
	}
```

#### Web page load

```
invokeExtensionsAsync init
invokeExtensionsAsync addCustomNodeDefs
invokeExtensionsAsync getCustomWidgets
invokeExtensionsAsync beforeRegisterNodeDef    [repeated multiple times]
invokeExtensionsAsync registerCustomNodes
invokeExtensionsAsync beforeConfigureGraph
invokeExtensionsAsync nodeCreated
invokeExtensions      loadedGraphNode
invokeExtensionsAsync afterConfigureGraph
invokeExtensionsAsync setup
```

#### Loading workflow

```
invokeExtensionsAsync beforeConfigureGraph
invokeExtensionsAsync beforeRegisterNodeDef   [zero, one, or multiple times]
invokeExtensionsAsync nodeCreated             [repeated multiple times]
invokeExtensions      loadedGraphNode         [repeated multiple times]
invokeExtensionsAsync afterConfigureGraph
```

{/* TODO why does beforeRegisterNodeDef get called again? */}

#### Adding new node

```
invokeExtensionsAsync nodeCreated
```


# Comfy Objects
Source: https://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking



## LiteGraph

The Comfy UI is built on top of [LiteGraph](https://github.com/jagenjo/litegraph.js).
Much of the Comfy functionality is provided by LiteGraph, so if developing more complex
nodes you will probably find it helpful to clone that repository and browse the documentation,
which can be found at `doc/index.html`.

## ComfyApp

The `app` object (always accessible by `import { app } from "../../scripts/app.js";`) represents the Comfy application running in the browser,
and contains a number of useful properties and functions, some of which are listed below.

<Warning>Hijacking functions on `app` is not recommended, as Comfy is under constant development, and core behavior may change.</Warning>

### Properties

Important properties of `app` include (this is not an exhaustive list):

| property        | contents                                                                                                                                                        |
| --------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `canvas`        | An LGraphCanvas object, representing the current user interface. It contains some potentially interesting properties, such as `node_over` and `selected_nodes`. |
| `canvasEl`      | The DOM `<canvas>` element                                                                                                                                      |
| `graph`         | A reference to the LGraph object describing the current graph                                                                                                   |
| `runningNodeId` | During execution, the node currently being executed                                                                                                             |
| `ui`            | Provides access to some UI elements, such as the queue, menu, and dialogs                                                                                       |

`canvas` (for graphical elements) and `graph` (for logical connections) are probably the ones you are most likely to want to access.

### Functions

Again, there are many. A few significant ones are:

| function          | notes                                                                 |
| ----------------- | --------------------------------------------------------------------- |
| graphToPrompt     | Convert the graph into a prompt that can be sent to the Python server |
| loadGraphData     | Load a graph                                                          |
| queuePrompt       | Submit a prompt to the queue                                          |
| registerExtension | You've seen this one - used to add an extension                       |

## LGraph

The `LGraph` object is part of the LiteGraph framework, and represents the current logical state of the graph (nodes and links).
If you want to manipulate the graph, the LiteGraph documentation (at `doc/index.html` if you clone `https://github.com/jagenjo/litegraph.js`)
describes the functions you will need.

You can use `graph` to obtain details of nodes and links, for example:

```Javascript
const ComfyNode_object_for_my_node = app.graph._nodes_by_id(my_node_id) 
ComfyNode_object_for_my_node.inputs.forEach(input => {
    const link_id = input.link;
    if (link_id) {
        const LLink_object = app.graph.links[link_id]
        const id_of_upstream_node = LLink_object.origin_id
        // etc
    }
});
```

## LLink

The `LLink` object, accessible through `graph.links`, represents a single link in the graph, from node `link.origin_id` output slot `link.origin_slot`
to node `link.target_id` slot `link.target_slot`. It also has a string representing the data type, in `link.type`, and `link.id`.

`LLink`s are created in the `connect` method of a `LGraphNode` (of which `ComfyNode` is a subclass).

<Tip>Avoid creating your own LLink objects - use the LiteGraph functions instead.</Tip>

## ComfyNode

`ComfyNode` is a subclass of `LGraphNode`, and the LiteGraph documentation is therefore helpful for more generic
operations. However, Comfy has significantly extended the LiteGraph core behavior, and also does not make
use of all LiteGraph functionality.

<Tip>The description that follows applies to a normal node.
Group nodes, primitive nodes, notes, and redirect nodes have different properties.</Tip>

A `ComfyNode` object represents a node in the current workflow. It has a number of important properties
that you may wish to make use of, a very large number of functions that you may wish to use, or hijack to
modify behavior.

To get a more complete sense of the node object, you may find it helpful to insert the following
code into your extension and place a breakpoint on the `console.log` command. When you then create a new node
you can use your favorite debugger to interrogate the node.

```Javascript
async nodeCreated(node) {
    console.log("nodeCreated")
}
```

### Properties

| property          | contents                                                                                                                            |
| ----------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| `bgcolor`         | The background color of the node, or undefined for the default                                                                      |
| `comfyClass`      | The Python class representing the node                                                                                              |
| `flags`           | A dictionary that may contain flags related to the state of the node. In particular, `flags.collapsed` is true for collapsed nodes. |
| `graph`           | A reference to the LGraph object                                                                                                    |
| `id`              | A unique id                                                                                                                         |
| `input_type`      | A list of the input types (eg "STRING", "MODEL", "CLIP" etc). Generally matches the Python INPUT\_TYPES                             |
| `inputs`          | A list of inputs (discussed below)                                                                                                  |
| `mode`            | Normally 0, set to 2 if the node is muted and 4 if the node is bypassed. Values of 1 and 3 are not used by Comfy                    |
| `order`           | The node's position in the execution order. Set by `LGraph.computeExecutionOrder()` when the prompt is submitted                    |
| `pos`             | The \[x,y] position of the node on the canvas                                                                                       |
| `properties`      | A dictionary containing `"Node name for S&R"`, used by LiteGraph                                                                    |
| `properties_info` | The type and default value of entries in `properties`                                                                               |
| `size`            | The width and height of the node on the canvas                                                                                      |
| `title`           | Display Title                                                                                                                       |
| `type`            | The unique name (from Python) of the node class                                                                                     |
| `widgets`         | A list of widgets (discussed below)                                                                                                 |
| `widgets_values`  | A list of the current values of widgets                                                                                             |

### Functions

There are a very large number of functions (85, last time I counted). A selection are listed below.
Most of these functions are unmodified from the LiteGraph core code.

#### Inputs, Outputs, Widgets

| function               | notes                                                                                              |
| ---------------------- | -------------------------------------------------------------------------------------------------- |
| Inputs / Outputs       | Most have output methods with the equivalent names: s/In/Out/                                      |
| `addInput`             | Create a new input, defined by name and type                                                       |
| `addInputs`            | Array version of `addInput`                                                                        |
| `findInputSlot`        | Find the slot index from the input name                                                            |
| `findInputSlotByType`  | Find an input matching the type. Options to prefer, or only use, free slots                        |
| `removeInput`          | By slot index                                                                                      |
| `getInputNode`         | Get the node connected to this input. The output equivalent is `getOutputNodes` and returns a list |
| `getInputLink`         | Get the LLink connected to this input. No output equivalent                                        |
| Widgets                |                                                                                                    |
| `addWidget`            | Add a standard Comfy widget                                                                        |
| `addCustomWidget`      | Add a custom widget (defined in the `getComfyWidgets` hook)                                        |
| `addDOMWidget`         | Add a widget defined by a DOM element                                                              |
| `convertWidgetToInput` | Convert a widget to an input if allowed by `isConvertableWidget` (in `widgetInputs.js`)            |

#### Connections

| function              | notes                                                                                             |
| --------------------- | ------------------------------------------------------------------------------------------------- |
| `connect`             | Connect this node's output to another node's input                                                |
| `connectByType`       | Connect output to another node by specifying the type - connects to first available matching slot |
| `connectByTypeOutput` | Connect input to another node output by type                                                      |
| `disconnectInput`     | Remove any link into the input (specified by name or index)                                       |
| `disconnectOutput`    | Disconnect an output from a specified node's input                                                |
| `onConnectionChange`  | Called on each node. `side==1` if it's an input on this node                                      |
| `onConnectInput`      | Called *before* a connection is made. If this returns `false`, the connection is refused          |

#### Display

| function           | notes                                                                                                  |
| ------------------ | ------------------------------------------------------------------------------------------------------ |
| `setDirtyCanvas`   | Specify that the foreground (nodes) and/or background (links and images) need to be redrawn            |
| `onDrawBackground` | Called with a `CanvasRenderingContext2D` object to draw the background. Used by Comfy to render images |
| `onDrawForeground` | Called with a `CanvasRenderingContext2D` object to draw the node.                                      |
| `getTitle`         | The title to be displayed.                                                                             |
| `collapse`         | Toggles the collapsed state of the node.                                                               |

<Warning>`collapse` is badly named; it *toggles* the collapsed state.
It takes a boolean parameter, which can be used to override
`node.collapsable === false`.</Warning>

#### Other

| function     | notes                                                              |
| ------------ | ------------------------------------------------------------------ |
| `changeMode` | Use to set the node to bypassed (`mode == 4`) or not (`mode == 0`) |

## Inputs and Widgets

Inputs and Widgets represent the two ways that data can be fed into a node. In general a widget can be
converted to an input, but not all inputs can be converted to a widget (as many datatypes can't be
entered through a UI element).

`node.inputs` is a list of the current inputs (colored dots on the left hand side of the node),
specifying their `.name`, `.type`, and `.link` (a reference to the connected `LLink` in `app.graph.links`).

If an input is a widget which has been converted, it also holds a reference to the, now inactive, widget in `.widget`.

`node.widgets` is a list of all widgets, whether or not they have been converted to an input. A widget has:

| property/function | notes                                                                     |
| ----------------- | ------------------------------------------------------------------------- |
| `callback`        | A function called when the widget value is changed                        |
| `last_y`          | The vertical position of the widget in the node                           |
| `name`            | The (unique within a node) widget name                                    |
| `options`         | As specified in the Python code (such as default, min, and max)           |
| `type`            | The name of the widget type (see below) in lowercase                      |
| `value`           | The current widget value. This is a property with `get` and `set` methods |

### Widget Types

`app.widgets` is a dictionary of currently registered widget types, keyed in the UPPER CASE version of the name of the type.
Build in Comfy widgets types include the self explanatory `BOOLEAN`, `INT`, and `FLOAT`,
as well as `STRING` (which comes in two flavours, single line and multiline),
`COMBO` for dropdown selection from a list, and `IMAGEUPLOAD`, used in Load Image nodes.

Custom widget types can be added by providing a `getCustomWidgets` method in your extension.

{/* TODO add link */}

### Linked widgets

Widgets can also be linked - the built in behavior of `seed` and `control_after_generate`, for example.
A linked widget has `.type = 'base_widget_type:base_widget_name'`; so `control_after_generate` may have
type `int:seed`.

## Prompt

When you press the `Queue Prompt` button in Comfy, the `app.graphToPrompt()` method is called to convert the
current graph into a prompt that can be sent to the server.

`app.graphToPrompt` returns an object (refered to herein as `prompt`) with two properties, `output` and `workflow`.

### output

`prompt.output` maps from the `node_id` of each node in the graph to an object with two properties.

* `prompt.output[node_id].class_type`, the unique name of the custom node class, as defined in the Python code
* `prompt.output[node_id].inputs`, which contains the value of each input (or widget) as a map from the input name to:
  * the selected value, if it is a widget, or
  * an array containing (`upstream_node_id`, `upstream_node_output_slot`) if there is a link connected to the input, or
  * undefined, if it is a widget that has been converted to an input and is not connected
  * other unconnected inputs are not included in `.inputs`

<Tip>Note that the `upstream_node_id` in the array describing a connected input is represented as a string, not an integer.</Tip>

### workflow

`prompt.workflow` contains the following properties:

* `config` - a dictionary of additional configuration options (empty by default)
* `extra` - a dictionary containing extra information about the workflow. By default it contains:
  * `extra.ds` - describes the current view of the graph (`scale` and `offset`)
* `groups` - all groups in the workflow
* `last_link_id` - the id of the last link added
* `last_node_id` - the id of the last node added
* `links` - a list of all links in the graph. Each entry is an array of five integers and one string:
  * (`link_id`, `upstream_node_id`, `upstream_node_output_slot`, `downstream_node_id`, `downstream_node_input_slot`, `data type`)
* `nodes` - a list of all nodes in the graph. Each entry is a map of a subset of the properties of the node as described [above](#comfynode)
  * The following properties are included: `flags`, `id`, `inputs`, `mode`, `order`, `pos`, `properties`, `size`, `type`, `widgets_values`
  * In addition, unless a node has no outputs, there is an `outputs` property, which is a list of the outputs of the node, each of which contains:
    * `name` - the name of the output
    * `type` - the data type of the output
    * `links` - a list of the `link_id` of all links from this output (if there are no connections, may be an empty list, or null),
    * `shape` - the shape used to draw the output (default 3 for a dot)
    * `slot_index` - the slot number of the output
* `version` - the LiteGraph version number (at time of writing, `0.4`)

<Tip>`nodes.output` is absent for nodes with no outputs, not an empty list.</Tip>


# Javascript Extensions
Source: https://docs.comfy.org/custom-nodes/js/javascript_overview



## Extending the Comfy Client

Comfy can be modified through an extensions mechanism. To add an extension you need to:

* Export `WEB_DIRECTORY` from your Python module,
* Place one or more `.js` files into that directory,
* Use `app.registerExtension` to register your extension.

These three steps are below. Once you know how to add an extension, look
through the [hooks](javascript_hooks) available to get your code called,
a description of various [Comfy objects](javascript_objects_and_hijacking) you might need,
or jump straight to some [example code snippets](javascript_examples).

### Exporting `WEB_DIRECTORY`

The Comfy web client can be extended by creating a subdirectory in your custom node directory, conventionally called `js`, and
exporting `WEB_DIRECTORY` - so your `__init_.py` will include something like:

```python
WEB_DIRECTORY = "./js"
__all__ = ["NODE_CLASS_MAPPINGS", "NODE_DISPLAY_NAME_MAPPINGS", "WEB_DIRECTORY"]
```

### Including `.js` files

<Tip>All Javascript `.js` files will be loaded by the browser as the Comfy webpage loads. You don't need to specify the file
your extension is in.</Tip>

*Only* `.js` files will be added to the webpage. Other resources (such as `.css` files) can be accessed
at `extensions/custom_node_subfolder/the_file.css` and added programmatically.

<Warning>That path does *not* include the name of the subfolder. The value of `WEB_DIRECTORY` is inserted by the server.</Warning>

### Registering an extension

The basic structure of an extension follows is to import the main Comfy `app` object, and call `app.registerExtension`,
passing a dictionary that contains a unique `name`,
and one or more functions to be called by hooks in the Comfy code.

A complete, trivial, and annoying, extension might look like this:

```Javascript
import { app } from "../../scripts/app.js";
app.registerExtension({ 
	name: "a.unique.name.for.a.useless.extension",
	async setup() { 
		alert("Setup complete!")
	},
})
```


# Settings
Source: https://docs.comfy.org/custom-nodes/js/javascript_settings



You can provide a settings object to ComfyUI that will show up when the user
opens the ComfyUI settings panel.

## Basic operation

### Add a setting

```javascript
import { app } from "../../scripts/app.js";

app.registerExtension({
    name: "My Extension",
    settings: [
        {
            id: "example.boolean",
            name: "Example boolean setting",
            type: "boolean",
            defaultValue: false,
        },
    ],
});
```

The `id` must be unique across all extensions and will be used to fetch values.

If you do not [provide a category](#categories), then the `id` will be split by
`.` to determine where it appears in the settings panel.

* If your `id` doesn't contain any `.` then it will appear in the "Other"
  category and your `id` will be used as the section heading.
* If your `id` contains at least one `.` then the leftmost part will be used
  as the setting category and the second part will be used as the section
  heading. Any further parts are ignored.

### Read a setting

```javascript
import { app } from "../../scripts/app.js";

if (app.extensionManager.setting.get('example.boolean')) {
    console.log("Setting is enabled.");
} else {
    console.log("Setting is disabled.");
}
```

### React to changes

The `onChange()` event handler will be called as soon as the user changes the
setting in the settings panel.

This will also be called when the extension is registered, on every page load.

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Write a setting

```javascript
import { app } from "../../scripts/app.js";

try {
    await app.extensionManager.setting.set("example.boolean", true);
} catch (error) {
    console.error(`Error changing setting: ${error}`);
}
```

### Extra configuration

The setting types are based on [PrimeVue](https://primevue.org/) components.
Props described in the PrimeVue documentation can be defined for ComfyUI
settings by adding them in an `attrs` field.

For instance, this adds increment/decrement buttons to a number input:

```javascript
{
    id: "example.number",
    name: "Example number setting",
    type: "number",
    defaultValue: 0,
    attrs: {
        showButtons: true,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

## Types

### Boolean

This shows an on/off toggle.

Based on the [ToggleSwitch PrimeVue
component](https://primevue.org/toggleswitch/).

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Text

This is freeform text.

Based on the [InputText PrimeVue component](https://primevue.org/inputtext/).

```javascript
{
    id: "example.text",
    name: "Example text setting",
    type: "text",
    defaultValue: "Foo",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Number

This for entering numbers.

To allow decimal places, set the `maxFractionDigits` attribute to a number greater than zero.

Based on the [InputNumber PrimeVue
component](https://primevue.org/inputnumber/).

```javascript
{
    id: "example.number",
    name: "Example number setting",
    type: "number",
    defaultValue: 42,
    attrs: {
        showButtons: true,
        maxFractionDigits: 1,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Slider

This lets the user enter a number directly or via a slider.

Based on the [Slider PrimeVue component](https://primevue.org/slider/). Ranges
are not supported.

```javascript
{
    id: "example.slider",
    name: "Example slider setting",
    type: "slider",
    attrs: {
        min: -10,
        max: 10,
        step: 0.5,
    },
    defaultValue: 0,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Combo

This lets the user pick from a drop-down list of values.

You can provide options either as a plain string or as an object with `text`
and `value` fields. If you only provide a plain string, then it will be used
for both.

You can let the user enter freeform text by supplying the `editable: true`
attribute, or search by supplying the `filter: true` attribute.

Based on the [Select PrimeVue component](https://primevue.org/select/). Groups
are not supported.

```javascript
{
    id: "example.combo",
    name: "Example combo setting",
    type: "combo",
    defaultValue: "first",
    options: [
        { text: "My first option", value: "first" },
        "My second option",
    ],
    attrs: {
        editable: true,
        filter: true,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Color

This lets the user select a color from a color picker or type in a hex
reference.

Note that the format requires six full hex digits - three digit shorthand does
not work.

Based on the [ColorPicker PrimeVue
component](https://primevue.org/colorpicker/).

```javascript
{
    id: "example.color",
    name: "Example color setting",
    type: "color",
    defaultValue: "ff0000",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Image

This lets the user upload an image.

The setting will be saved as a [data
URL](https://developer.mozilla.org/en-US/docs/Web/URI/Schemes/data).

Based on the [FileUpload PrimeVue
component](https://primevue.org/fileupload/).

```javascript
{
    id: "example.image",
    name: "Example image setting",
    type: "image",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Hidden

Hidden settings aren't displayed in the settings panel, but you can read and
write to them from your code.

```javascript
{
    id: "example.hidden",
    name: "Example hidden setting",
    type: "hidden",
}
```

## Other

### Categories

You can specify the categorisation of your setting separately to the `id`.
This means you can change the categorisation and naming without changing the
`id` and losing the values that have already been set by users.

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    category: ["Category name", "Section heading", "Setting label"],
}
```

### Tooltips

You can add extra contextual help with the `tooltip` field. This adds a small ℹ︎
icon after the field name that will show the help text when the user hovers
over it.

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    tooltip: "This is some helpful information",
}
```


# Overview
Source: https://docs.comfy.org/custom-nodes/overview



Custom nodes allow you to implement new features and share them with the wider community.

A custom node is like any Comfy node: it takes input, does something to it, and produces an output.
While some custom nodes perform highly complex tasks, many just do one thing. Here's an example of a
simple node that takes an image and inverts it.

![Unique Images Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/invert_image_node.png)

## Client-Server Model

Comfy runs in a client-server model. The server, written in Python, handles all the real work: data-processing, models, image diffusion etc. The client, written in Javascript, handles the user interface.

Comfy can also be used in API mode, in which a workflow is sent to the server by a non-Comfy client (such as another UI, or a command line script).

Custom nodes can be placed into one of four categories:

### Server side only

The majority of Custom Nodes run purely on the server side, by defining a Python class that specifies the input and output types, and provides a function that can be called to process inputs and produce an output.

### Client side only

A few Custom Nodes provide a modification to the client UI, but do not add core functionality. Despite the name, they may not even add new nodes to the system.

### Independent Client and Server

Custom nodes may provide additional server features, and additional (related) UI features (such as a new widget to deal with a new data type). In most cases, communication between the client and server can be handled by the Comfy data flow control.

### Connected Client and Server

In a small number of cases, the UI features and the server need to interact with each other directly.

<Warning>Any node that requires Client-Server communication will not be compatible with use through the API.</Warning>


# Tips
Source: https://docs.comfy.org/custom-nodes/tips



### Recommended Development Lifecycle


# Getting Started
Source: https://docs.comfy.org/custom-nodes/walkthrough



This page will take you step-by-step through the process of creating a custom node.

Our example will take a batch of images, and return one of the images. Initially, the node
will return the image which is, on average, the lightest in color; we'll then extend
it to have a range of selection criteria, and then finally add some client side code.

This page assumes very little knowledge of Python or Javascript.

After this walkthrough, dive into the details of [backend code](./backend/server_overview), and
[frontend code](./backend/server_overview).

## Write a basic node

### Prerequisites

* A working ComfyUI [installation](/installation/manual_install). For development, we recommend installing ComfyUI manually.
* A working comfy-cli [installation](/comfy-cli/getting-started).

### Setting up

```bash
cd ComfyUI/custom_nodes
comfy node scaffold
```

After answering a few questions, you'll have a new directory set up.

```bash
 ~  % comfy node scaffold
You've downloaded .cookiecutters/cookiecutter-comfy-extension before. Is it okay to delete and re-download it? [y/n] (y): y
  [1/9] full_name (): Comfy
  [2/9] email (you@gmail.com): me@comfy.org
  [3/9] github_username (your_github_username): comfy
  [4/9] project_name (My Custom Nodepack): FirstComfyNode
  [5/9] project_slug (firstcomfynode): 
  [6/9] project_short_description (A collection of custom nodes for ComfyUI): 
  [7/9] version (0.0.1): 
  [8/9] Select open_source_license
    1 - GNU General Public License v3
    2 - MIT license
    3 - BSD license
    4 - ISC license
    5 - Apache Software License 2.0
    6 - Not open source
    Choose from [1/2/3/4/5/6] (1): 1
  [9/9] include_web_directory_for_custom_javascript [y/n] (n): y
Initialized empty Git repository in firstcomfynode/.git/
✓ Custom node project created successfully!
```

### Defining the node

Add the following code to the end of `src/nodes.py`:

```Python src/nodes.py
class ImageSelector:
    CATEGORY = "example"
    @classmethod    
    def INPUT_TYPES(s):
        return { "required":  { "images": ("IMAGE",), } }
    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "choose_image"
```

<Info>The basic structure of a custom node is described in detail [here](/custom-nodes/backend/server_overview). </Info>

A custom node is defined using a Python class, which must include these four things: `CATEGORY`,
which specifies where in the add new node menu the custom node will be located,
`INPUT_TYPES`, which is a class method defining what inputs the node will take
(see [later](/custom-nodes/backend/server_overview#input-types) for details of the dictionary returned),
`RETURN_TYPES`, which defines what outputs the node will produce, and `FUNCTION`, the name
of the function that will be called when the node is executed.

<Tip>Notice that the data type for input and output is `IMAGE` (singular) even though
we expect to receive a batch of images, and return just one. In Comfy, `IMAGE` means
image batch, and a single image is treated as a batch of size 1.</Tip>

### The main function

The main function, `choose_image`, receives named arguments as defined in `INPUT_TYPES`, and
returns a `tuple` as defined in `RETURN_TYPES`. Since we're dealing with images, which are internally
stored as `torch.Tensor`,

```Python
import torch
```

Then add the function to your class. The datatype for image is `torch.Tensor` with shape `[B,H,W,C]`,
where `B` is the batch size and `C` is the number of channels - 3, for RGB. If we iterate over such
a tensor, we will get a series of `B` tensors of shape `[H,W,C]`. The `.flatten()` method turns
this into a one dimensional tensor, of length `H*W*C`, `torch.mean()` takes the mean, and `.item()`
turns a single value tensor into a Python float.

```Python
def choose_image(self, images):
    brightness = list(torch.mean(image.flatten()).item() for image in images)
    brightest = brightness.index(max(brightness))
    result = images[brightest].unsqueeze(0)
    return (result,)
```

Notes on those last two lines:

* `images[brightest]` will return a Tensor of shape `[H,W,C]`. `unsqueeze` is used to insert a (length 1) dimension at, in this case, dimension zero, to give
  us `[B,H,W,C]` with `B=1`: a single image.
* in `return (result,)`, the trailing comma is essential to ensure you return a tuple.

### Register the node

To make Comfy recognize the new node, it must be available at the package level. Modify the `NODE_CLASS_MAPPINGS` variable at the end of `src/nodes.py`. You must restart ComfyUI to see any changes.

```Python src/nodes.py

NODE_CLASS_MAPPINGS = {
    "Example" : Example,
    "Image Selector" : ImageSelector,
}

# Optionally, you can rename the node in the `NODE_DISPLAY_NAME_MAPPINGS` dictionary.
NODE_DISPLAY_NAME_MAPPINGS = {
    "Example": "Example Node",
    "Image Selector": "Image Selector",
}
```

<Info>For a detailed explanation of how ComfyUI discovers and loads custom nodes, see the [node lifecycle documentation](/custom-nodes/backend/lifecycle).</Info>

## Add some options

That node is maybe a bit boring, so we might add some options; a widget that allows you to
choose the brightest image, or the reddest, bluest, or greenest. Edit your `INPUT_TYPES` to look like:

```Python
@classmethod    
def INPUT_TYPES(s):
    return { "required":  { "images": ("IMAGE",), 
                            "mode": (["brightest", "reddest", "greenest", "bluest"],)} }
```

Then update the main function. We'll use a fairly naive definition of 'reddest' as being the average
`R` value of the pixels divided by the average of all three colors. So:

```Python
def choose_image(self, images, mode):
    batch_size = images.shape[0]
    brightness = list(torch.mean(image.flatten()).item() for image in images)
    if (mode=="brightest"):
        scores = brightness
    else:
        channel = 0 if mode=="reddest" else (1 if mode=="greenest" else 2)
        absolute = list(torch.mean(image[:,:,channel].flatten()).item() for image in images)
        scores = list( absolute[i]/(brightness[i]+1e-8) for i in range(batch_size) )
    best = scores.index(max(scores))
    result = images[best].unsqueeze(0)
    return (result,)
```

## Tweak the UI

Maybe we'd like a bit of visual feedback, so let's send a little text message to be displayed.

### Send a message from server

This requires two lines to be added to the Python code:

```Python
from server import PromptServer
```

and, at the end of the `choose_image` method, add a line to send a message to the front end (`send_sync` takes a message
type, which should be unique, and a dictionary)

```Python
PromptServer.instance.send_sync("example.imageselector.textmessage", {"message":f"Picked image {best+1}"})
return (result,)
```

### Write a client extension

To add some Javascript to the client, create a subdirectory, `web/js` in your custom node directory, and modify the end of `__init__.py`
to tell Comfy about it by exporting `WEB_DIRECTORY`:

```Python
WEB_DIRECTORY = "./web/js"
__all__ = ['NODE_CLASS_MAPPINGS', 'WEB_DIRECTORY']
```

The client extension is saved as a `.js` file in the `web/js` subdirectory, so create `image_selector/web/js/imageSelector.js` with the
code below. (For more, see [client side coding](./js/javascript_overview)).

```Javascript
app.registerExtension({
	name: "example.imageselector",
    async setup() {
        function messageHandler(event) { alert(event.detail.message); }
        app.api.addEventListener("example.imageselector.textmessage", messageHandler);
    },
})
```

All we've done is register an extension and add a listener for the message type we are sending in the `setup()` method. This reads the dictionary we sent (which is stored in `event.detail`).

Stop the Comfy server, start it again, reload the webpage, and run your workflow.

### The complete example

The complete example is available [here](https://gist.github.com/robinjhuang/fbf54b7715091c7b478724fc4dffbd03). You can download the example workflow [JSON file](https://github.com/Comfy-Org/docs/blob/main/public/workflow.json) or view it below:

<div align="center">
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/firstnodeworkflow.png" alt="Image Selector Workflow" width="100%" />
</div>


# Workflow templates
Source: https://docs.comfy.org/custom-nodes/workflow_templates



If you have example workflow files associated with your custom nodes
then ComfyUI can show these to the user in the template browser (`Workflow`/`Browse Templates` menu).
Workflow templates are a great way to support people getting started with your nodes.

All you have to do as a node developer is to create an `example_workflows` folder and place the `json` files there.
Optionally you can place `jpg` files with the same name to be shown as the template thumbnail.

Under the hood ComfyUI statically serves these files along with an endpoint (`/api/workflow_templates`)
that returns the collection of workflow templates.

## Example

Under `ComfyUI-MyCustomNodeModule/example_workflows/` directory:

* `My_example_workflow_1.json`
* `My_example_workflow_1.jpg`
* `My_example_workflow_2.json`

In this example ComfyUI's template browser shows a category called `ComfyUI-MyCustomNodeModule` with two items, one of which has a thumbnail.


# Messages
Source: https://docs.comfy.org/essentials/comfyui-server/comms_messages



## Messages

During execution (or when the state of the queue changes), the `PromptExecutor` sends messages back to the client
through the `send_sync` method of `PromptServer`.

These messages are received by a socket event listener defined in `api.js` (at time of writing around line 90, or search for `this.socket.addEventListener`),
which creates a `CustomEvent` object for any known message type, and dispatches it to any registered listeners.

An extension can register to receive events (normally done in the `setup()` function) following the standard Javascript idiom:

```Javascript
api.addEventListener(message_type, messageHandler);
```

If the `message_type` is not one of the built in ones, it will be added to the list of known message types automatically. The message `messageHandler`
will be called with a `CustomEvent` object, which extends the event raised by the socket to add a `.detail` property, which is a dictionary of
the data sent by the server. So usage is generally along the lines of:

```Javascript
function messageHandler(event) {
    if (event.detail.node == aNodeIdThatIsInteresting) {
        // do something with event.detail.other_things
    }
}
```

### Built in message types

During execution (or when the state of the queue changes), the `PromptExecutor` sends the following messages back to the client
through the `send_sync` method of `PromptServer`. An extension can register as a listener for any of these.

| event                   | when                                                                       | data                                                                                                    |
| ----------------------- | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |
| `execution_start`       | When a prompt is about to run                                              | `prompt_id`                                                                                             |
| `execution_error`       | When an error occurs during execution                                      | `prompt_id`, plus additional information                                                                |
| `execution_interrupted` | When execution is stopped by a node raising `InterruptProcessingException` | `prompt_id`, `node_id`, `node_type` and `executed` (a list of executed nodes)                           |
| `execution_cached`      | At the start of execution                                                  | `prompt_id`, `nodes` (a list of nodes which are being skipped because their cached outputs can be used) |
| `executing`             | When a new node is about to be executed                                    | `node` (node id or `None` to indicate completion), `prompt_id`                                          |
| `executed`              | When a node returns a ui element                                           | `node` (node id), `prompt_id`, `output`                                                                 |
| `progress`              | During execution of a node that implements the required hook               | `node` (node id), `prompt_id`, `value`, `max`                                                           |
| `status`                | When the state of the queue changes                                        | `exec_info`, a dictionary holding `queue_remaining`, the number of entries in the queue                 |

### Using executed

Despite the name, an `executed` message is not sent whenever a node completes execution (unlike `executing`), but only when the node
returns a ui update.

To do this, the main function needs to return a dictionary instead of a tuple:

```python
# at the end of my main method
        return { "ui":a_new_dictionary, "result": the_tuple_of_output_values }
```

`a_new_dictionary` will then be sent as the value of `output` in an `executed` message.
The `result` key can be omitted if the node has no outputs (see, for instance, the code for `SaveImage` in `nodes.py`)

### Custom message types

As indicated above, on the client side, a custom message type can be added simply by registering as a listener for a unique message type name.

```Javascript
api.addEventListener("my.custom.message", messageHandler);
```

On the server, the code is equally simple:

```Python
from server import PromptServer
# then, in your main execution function (normally)
        PromptServer.instance.send_sync("my.custom.message", a_dictionary)
```

#### Getting node\_id

Most of the built-in messages include the current node id in the value of `node`. It's likely that you will want to do the same.

The node\_id is available on the server side through a hidden input, which is obtained with the `hidden` key in the `INPUT_TYPES` dictionary:

```Python
    @classmethod    
    def INPUT_TYPES(s):
        return {"required" : { }, # whatever your required inputs are 
                "hidden": { "node_id": "UNIQUE_ID" } } # Add the hidden key

    def my_main_function(self, required_inputs, node_id):
        # do some things
        PromptServer.instance.send_sync("my.custom.message", {"node": node_id, "other_things": etc})
```


# Server Overview
Source: https://docs.comfy.org/essentials/comfyui-server/comms_overview



## Overview

The Comfy server runs on top of the [aiohttp framework](https://docs.aiohttp.org/), which in turn uses [asyncio](https://pypi.org/project/asyncio/).

Messages from the server to the client are sent by socket messages through the `send_sync` method of the server,
which is an instance of `PromptServer` (defined in `server.py`). They are processed
by a socket event listener registered in `api.js`. See [messages](./comms_messages).

Messages from the client to the server are sent by the `api.fetchApi()` method defined in `api.js`,
and are handled by http routes defined by the server. See [routes](./comms_routes).

<Tip>The client submits the whole workflow (widget values and all) when you queue a request.
The server does not receive any changes you make after you send a request to the queue.
If you want to modify server behavior during execution, you'll need routes.</Tip>


# Execution Model Inversion Guide
Source: https://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide



[PR #2666](https://github.com/comfyanonymous/ComfyUI/pull/2666) inverts the execution model from a back-to-front recursive model to a front-to-back topological sort. While most custom nodes should continue to "just work", this page is intended to serve as a guide for custom node creators to the things that *could* break.

## Breaking Changes

### Monkey Patching

Any code that monkey patched the execution model is likely to stop working. Note that the performance of execution with this PR exceeds that with the most popular monkey patches, so many of them will be unnecessary.

### Optional Input Validation

Prior to this PR, only nodes that were connected to outputs exclusively through a string of `"required"` inputs were actually validated. If you had custom nodes that were only ever connected to `"optional"` inputs, you previously wouldn't have been seeing that they failed validation.

<Tip>If your nodes' outputs could already be connected to `"required"` inputs, it is unlikely that anything in this section applies to you. It will primarily apply to custom node authors who use custom types and exclusively use `"optional"` inputs.</Tip>

Here are some of the things that could cause you to fail validation along with recommended solutions:

* Use of reserved [Additional Parameters](/custom-nodes/backend/datatypes#additional-parameters) like `min` and `max` on types that aren't comparable (e.g. dictionaries) in order to configure custom widgets.
  * Change the additional parameters used to non-reserved keys like `uiMin` and `uiMax`. *(Recommended Solution)*
    ```python
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "my_size": ("VEC2", {"uiMin": 0.0, "uiMax": 1.0}),
            }
        }
    ```

  * Define a custom [VALIDATE\_INPUTS](/custom-nodes/backend/server_overview#validate-inputs) function with this input so validation of it is skipped. *(Quick Solution)*
    ```python
    @classmethod
    def VALIDATE_INPUTS(cls, my_size):
        return True
    ```

* Use of composite types (e.g. `CUSTOM_A,CUSTOM_B`)
  * (When used as output) Define and use a wrapper like `MakeSmartType` [seen here in the PR's unit tests](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R2)
    ```python
    class MyCustomNode:

        @classmethod
        def INPUT_TYPES(cls):
            return {
                "required": {
                    "input": (MakeSmartType("FOO,BAR"), {}),
                }
            }

        RETURN_TYPES = (MakeSmartType("FOO,BAR"),)

        # ...
    ```
  * (When used as input) Define a custom[VALIDATE\_INPUTS](/custom-nodes/backend/server_overview#validate-inputs) function that takes a `input_types` argument so type validation is skipped.
    ```python
    @classmethod
    def VALIDATE_INPUTS(cls, input_types):
        return True
    ```
  * (Supports both, convenient) Define and use the `@VariantSupport` decorator [seen here in the PR's unit tests](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R15)
    ```python
    @VariantSupport
    class MyCustomNode:

        @classmethod
        def INPUT_TYPES(cls):
            return {
                "required": {
                    "input": ("FOO,BAR", {}),
                }
            }
        
        RETURN_TYPES = (MakeSmartType("FOO,BAR"),)

        # ...
    ```

* The use of lists (e.g. `[1, 2, 3]`) as constants in the graph definition (e.g. to represent a const `VEC3` input). This would have required a front-end extension before. Previously, lists of size exactly `2` would have failed anyway -- they would have been treated as broken links.
  * Wrap the lists in a dictionary like `{ "value": [1, 2, 3] }`

### Execution Order

Execution order has always changed depending on which nodes happen to have which IDs, but it may now change depending on which values are cached as well. In general, the execution order should be considered non-deterministic and subject to change (beyond what is enforced by the graph's structure).

Don't rely on the execution order.

*HIC SUNT DRACONES*

## New Functionality

### Validation Changes

A number of features were added to the `VALIDATE_INPUTS` function in order to lessen the impact of the [Optional Input Validation](#optional-input-validation) mentioned above.

* Default validation will now be skipped for inputs which are received by the `VALIDATE_INPUTS` function.
* The `VALIDATE_INPUTS` function can now take `**kwargs` which causes all inputs to be treated as validated by the node creator.
* The `VALIDATE_INPUTS` function can take an input named `input_types`. This input will be a dict mapping each input (connected via a link) to the type of the connected output. When this argument exists, type validation for the node's inputs is skipped.

You can read more at [VALIDATE\_INPUTS](/custom-nodes/backend/server_overview#validate-inputs).

### Lazy Evaluation

Inputs can be evaluated lazily (i.e. you can wait to see if they are needed before evaluating the attached node and all its ancestors). See [Lazy Evaluation](/custom-nodes/backend/lazy_evaluation) for more information.

### Node Expansion

At runtime, nodes can expand into a subgraph of nodes. This is what allows loops to be implemented (via tail-recursion). See [Node Expansion](/custom-nodes/backend/expansion) for more information.


# Dependencies
Source: https://docs.comfy.org/essentials/core-concepts/dependencies



{/*
  description: "Understand asset and software dependencies in ComfyUI"
  */}

## A workflow file depends on other files

A ComfyUI workflow is usually not self-contained. In other words, a workflow can't function unless it can access the other files it needs. These are called **dependencies** because the primary document, the workflow file, ***depends*** on other files in order to do its work.

## Assets

An AI model is an example of an ***asset***. In media production, an asset is some media file that supplies input data. For example, a video editing program operates on movie files stored on disk. The editing program’s project file holds links to these movie file assets, allowing non-destructive editing that doesn’t alter the original movie files.

ComfyUI works the same way. A workflow can only run if all of the required assets are found and loaded. Generative AI models, images, movies, and sounds are some examples of assets that a workflow might depend upon. These are therefore known as ***dependent assets*** or ***asset dependencies***.

## Software

An advanced application like ComfyUI also has ***software dependencies***. These are libraries of programming code and data that are required for the application to run. Custom nodes are examples of software dependencies. On an even more fundamental level, the Python programming environment is the ultimate dependency for ComfyUI. The correct version of Python is required to run a particular version of ComfyUI. Updates to Python, ComfyUI, and custom nodes can all be handled from the **ComfyUI Manager** window.

![ComfyUI Custom Nodes Manager](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_dependecies_custom-nodes-manager.png)


# Links
Source: https://docs.comfy.org/essentials/core-concepts/links



{/*
  description: "Understand connection links in ComfyUI"
  */}

## Links connect nodes

In the terminology of ComfyUI, the lines or curves between nodes are called ***links***. They’re also known as ***connections*** or wires. Links can be displayed in several ways, such as straight lines (the default), spline curves, or completely hidden. Display of links is controlled from the ComfyUI **Setup** window and the **Toggle Link Visibility** button on the display toolbar at the lower right of the ComfyUI main window.

Link display is crucial. Depending on the situation, it may be necessary to see all links. Especially when learning, sharing, or even just understanding workflows, the visibility of links enables users to follow the flow of data through the graph. For packaged workflows that aren’t intended to be altered, it might make sense to hide the links to reduce clutter.

![color-coded links](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_links.png)

### Reroute node

If legibility of the graph structure is important, then link wires can be manually routed in the 2D space of the graph with a tiny node called **Reroute**. Its purpose is to position the beginning and/or end points of link wires to ensure visibility. We can design a workflow so that link wires don’t pass behind nodes, don’t cross other link wires, and so on.

## Color-coding

The data type of node properties is indicated by color coding of input/output ports and link connection wires. We can always tell which inputs and outputs can be connected to one another by their color. Ports can only be connected to other ports of the same color.

Common data types:

| Data type                 | Color       |
| ------------------------- | ----------- |
| diffusion model           | lavender    |
| CLIP model                | yellow      |
| VAE model                 | rose        |
| conditioning              | orange      |
| latent image              | pink        |
| pixel image               | blue        |
| mask                      | green       |
| number (integer or float) | light green |


# Models
Source: https://docs.comfy.org/essentials/core-concepts/models



{/*
  description: "Understand AI models and their role in ComfyUI"
  */}

## Models are essential

Models are essential building blocks for media generation workflows. They can be combined and mixed to achieve different creative effects.

The word ***model*** has many different meanings. Here, it means a data file carrying information that is required for a node graph to do its work. Specifically, it’s a data structure that *models* some function. As a verb, to model something means to represent it or provide an example.

The primary example of a model data file in ComfyUI is an AI ***diffusion model***. This is a large set of data that represents the complex relationships among text strings and images, making it possible to translate words into pictures or vice versa. Other examples of common models used for image generation are language models such as CLIP, and upscaling models such as RealESRGAN.

## Model files

Model files are absolutely required for generative media production. Nothing can happen in a workflow if the model files are not found. Models are not included in the ComfyUI installation, but ComfyUI can often automatically download and install missing model files. Many models can be downloaded and installed from the **ComfyUI Manager** window. Models can also be found at websites such as [huggingface.co](https://huggingface.co), [civitai.green](https://civitai.green), and [github.com](https://github.com).

### Using Models in ComfyUI

1. Download and place them in the ComfyUI program directory
   1. Within the **models** folder, you'll find subfolders for various types of models, such as **checkpoints**
   2. The **ComfyUI Manager** helps to automate the process of searching, downloading, and installing
   3. Restart ComfyUI if it's running
2. In your workflow, create the node appropriate to the model type, e.g. **Load Checkpoint**, **Load LoRA**, **Load VAE**
3. In the loader node, choose the model you wish to use
4. Connect the loader node to other nodes in your workflow

### File size

Models can be extremely large files relative to image files. A typical uncompressed image may require a few megabytes of disk storage. Generative AI models can be tens of thousands of times larger, up to tens of gigabytes per model. They take up a great deal of disk space and take a long time to transfer over a network.

## Model training and refinement

A generative AI model is created by training a machine learning program on a very large set of data, such as pairs of images and text descriptions. An AI model doesn’t store the training data explicitly, but rather it stores the correlations that are implicit within the data.

Organizations and companies such as Stability AI and Black Forest Labs release “base” models that carry large amounts of generic information. These are general purpose generative AI models. Commonly, the base models need to be ***refined*** in order to get high quality generative outputs. A dedicated community of people work to refine the base models. The new, refined models produce better output, provide new or different functionality, and/or use fewer resources. Refined models can usually be run on systems with less computing power and/or memory.

## Auxiliary models

Model functionality can be extended with auxiliary models. For example, art directing a text-to-image workflow to achieve a specific result may be difficult or impossible using a diffusion model alone. Additional models can refine a diffusion model within the workflow graph to produce desired results. Examples include **LoRA** (Low Rank Adaptation), a small model that is trained on a specific subject; **ControlNet**, a model that helps control composition using a guide image; and **Inpainting**, a model that allows certain diffusion models to generate new content within an existing image.

![auxiliary models](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_auxiliary-model.png)


# Nodes
Source: https://docs.comfy.org/essentials/core-concepts/nodes



{/*
  description: "Understand the concept of a node in ComfyUI."
  */}

## Nodes perform operations

In computer science, a ***node*** is a container for information, usually including programmed instructions to perform some task. Nodes almost never exist in isolation, they’re almost always connected to other nodes in a networked graph. In ComfyUI, nodes take the visual form of boxes that are connected to each other.

ComfyUI nodes are usually ***function operators***. This means that they operate on some data to perform a function. A function is a process that accepts input data, performs some operation on it, and produces output data. In other words, nodes do some work, contributing to the completion of a task such as generating an image. So ComfyUI nodes almost always have at least one input or output, and usually have multiple inputs and outputs.

## Custom Nodes

ComfyUI includes many powerful nodes in the base installation package. These are known as **Comfy Core** nodes. Additionally, the ComfyUI community has created an amazing array of [***custom nodes***](https://registry.comfy.org) to perform a wide variety of functions.

## ComfyUI Manager

![ComfyUI Manager interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_nodes_manager.png)

The **ComfyUI Manager** window makes it easy to perform custom node management tasks such as search, install, update, disable, and uninstall. The Manager is included in the ComfyUI desktop application, but not in the ComfyUI server application.

### Installing the Manager

If you're running the ComfyUI server application, you need to install the Manager. If ComfyUI is running, shut it down before proceeding.

The first step is to install Git, a command-line application for software version control. Git will download the ComfyUI Manager from [github.com](https://github.com). Download Git from [git-scm.com](https://git-scm.com/) and install it.

Once Git is installed, navigate to the ComfyUI server program directory, to the folder labeled **custom\_nodes**. Open up a command window or terminal. Make sure that the command line displays the current directory path as **custom\_nodes**. Enter the following command. This will download the Manager. Technically, this is known as *cloning a Git repository*.

```bash
git clone https://github.com/ltdrdata/ComfyUI-Manager.git
```

For details or special cases, see [ComfyUI Manager Install](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#installation).


# Properties
Source: https://docs.comfy.org/essentials/core-concepts/properties



{/*
  description: "Understand node properties in ComfyUI"
  */}

## Nodes are containers for properties

Nodes usually have ***properties***. Also known as ***parameters*** or ***attributes***, node properties are variables that can be changed. Some properties can be adjusted manually by the user, using a data entry field called a ***widget***. Other properties can be driven automatically by other nodes connected to the property ***input slot*** or port. Usually, a property can be converted from widget to input and vice versa, allowing users to control property values manually or automatically.

Properties can take many forms and hold many different types of information. For example, a **Load Checkpoint** node has a single property:  the file path to the generative model checkpoint file. A **KSampler** node has multiple properties such as the number of sampling **steps**, **CFG** scale, **sampler\_name**, etc.

![node properties](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/concepts/core-concepts_properties.png)

## Data types

Information can come in many different forms, called ***data types***. For example, alphanumeric text is known as a ***string***, a whole number is an ***integer***, and a number with a decimal point is known as a ***floating point*** number or ***float***. New data types are always being added to ComfyUI.

ComfyUI is written in the Python scripting language, which is very forgiving about data types. By contrast, the ComfyUI environment is very ***strongly typed***. This means that different data types can’t be mixed up. For example, we can’t connect an image output to an integer input. This is a huge benefit to users, guiding them to proper workflow construction and preventing program errors.


# Workflow
Source: https://docs.comfy.org/essentials/core-concepts/workflow



{/*
  description: "Understand the concept of a workflow in ComfyUI."
  */}

## A graph of nodes

ComfyUI is an environment for building and running generative content ***workflows***. In this context, a workflow is defined as a collection of program objects called ***nodes*** that are connected to each other, forming a network. This network is also known as a ***graph***.

A ComfyUI workflow can generate any type of media: image, video, audio, AI model, AI agent, and so on.

## Sample workflows

To get started, try out some of the [official workflows](https://comfyanonymous.github.io/ComfyUI_examples). These use only the Core nodes included in the ComfyUI installation. A thriving community of developers has created a rich [ecosystem](https://registry.comfy.org) of custom nodes to extend the functionality of ComfyUI.

### Simple Example

![simple workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/simple_workflow.jpeg)

## Visual programming

A node-based computer program like ComfyUI provides a level of power and flexibility that can’t be achieved with traditional menu- and button-driven applications. The ComfyUI node graph is not limited by the tools provided in a traditional computer application. It’s a high-level ***visual programming environment*** allowing users to design complex systems without needing to write program code or understand advanced mathematics.

Many other computer applications use this same node graph paradigm. Examples include the compositing application called Nuke, the 3D programs Maya and Blender, the Unreal real-time graphics engine, and the interactive media authoring program called Max.

### More Complex Example

![complex workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/complex_workflow.jpeg)

## Procedural framework

Another term used to describe a node-based application is ***procedural framework***. Procedural means generative: some procedure or algorithm is employed to generate content such as a 3D model or a musical composition.

ComfyUI is all of these things: a node graph, a visual programming environment, and a procedural framework. What makes ComfyUI different (and amazing!) is that its radically open structure allows us to generate any type of media asset such as picture, movie, sound, 3D model, AI model, etc.

In the context of ComfyUI, the term ***workflow*** is a synonym for the node network or graph. It corresponds to the ***scene graph*** in a 3D or multimedia program: the network of all of the nodes within a particular disk file. 3D programs call this a ***scene file***. Video editing, compositing, and multimedia programs usually call it a ***project file***.

## Saving workflows

The ComfyUI workflow is automatically saved in the metadata of any generated image, allowing users to open and use the graph that generated the image. A workflow can also be stored in a human-readable text file that follows the JSON data format. This is necessary for media formats that don’t support metadata. ComfyUI workflows stored as JSON files are very small, allowing convenient versioning, archiving, and sharing of graphs, independently of any generated media.


# Keyboard and Mouse Shortcuts
Source: https://docs.comfy.org/essentials/shortcuts



{/*
  description: "A list of keyboard and mouse shortcuts for ComfyUI"
  */}

{/* TODO(yoland): Add this back to comfyUI readme page */}

<Tabs>
  <Tab title="Windows/Linux">
    | Shortcut                            | Command                                                                                                            |
    | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------ |
    | Ctrl + Enter                        | Queue up current graph for generation                                                                              |
    | Ctrl + Shift + Enter                | Queue up current graph as first for generation                                                                     |
    | Ctrl + Z / Ctrl + Y                 | Undo/Redo                                                                                                          |
    | Ctrl + S                            | Save workflow                                                                                                      |
    | Ctrl + O                            | Load workflow                                                                                                      |
    | Ctrl + A                            | Select all nodes                                                                                                   |
    | Alt + C                             | Collapse/uncollapse selected nodes                                                                                 |
    | Ctrl + M                            | Mute/unmute selected nodes                                                                                         |
    | Ctrl + B                            | Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)            |
    | Delete<br />Backspace               | Delete selected nodes                                                                                              |
    | Ctrl + Delete<br />Ctrl + Backspace | Delete the current graph                                                                                           |
    | Space                               | Move the canvas around when held and moving the cursor                                                             |
    | Ctrl + Click<br />Shift + Click     | Add clicked node to selection                                                                                      |
    | Ctrl + C/Ctrl + V                   | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)                     |
    | Ctrl + C/Ctrl + Shift + V           | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |
    | Shift + Drag                        | Move multiple selected nodes at the same time                                                                      |
    | Ctrl + D                            | Load default graph                                                                                                 |
    | Q                                   | Toggle visibility of the queue                                                                                     |
    | H                                   | Toggle visibility of history                                                                                       |
    | R                                   | Refresh graph                                                                                                      |
    | Double-Click LMB                    | Quick search for nodes to add                                                                                      |
  </Tab>

  <Tab title="MacOS">
    | Keybind                               | Explanation                                                                                                        |
    | ------------------------------------- | ------------------------------------------------------------------------------------------------------------------ |
    | Cmd ⌘ + Enter                         | Queue up current graph for generation                                                                              |
    | Cmd ⌘ + Shift + Enter                 | Queue up current graph as first for generation                                                                     |
    | Cmd ⌘ + Z/Cmd ⌘ + Y                   | Undo/Redo                                                                                                          |
    | Cmd ⌘ + S                             | Save workflow                                                                                                      |
    | Cmd ⌘ + O                             | Load workflow                                                                                                      |
    | Cmd ⌘ + A                             | Select all nodes                                                                                                   |
    | Opt ⌥ + C                             | Collapse/uncollapse selected nodes                                                                                 |
    | Cmd ⌘ + M                             | Mute/unmute selected nodes                                                                                         |
    | Cmd ⌘ + B                             | Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)            |
    | Delete<br />Backspace                 | Delete selected nodes                                                                                              |
    | Cmd ⌘ + Delete<br />Cmd ⌘ + Backspace | Delete the current graph                                                                                           |
    | Space                                 | Move the canvas around when held and moving the cursor                                                             |
    | Cmd ⌘ + Click<br />Shift + Click      | Add clicked node to selection                                                                                      |
    | Cmd ⌘ + C / Cmd ⌘ + V                 | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)                     |
    | Cmd ⌘ + C / Cmd ⌘ + Shift + V         | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |
    | Shift + Drag                          | Move multiple selected nodes at the same time                                                                      |
    | Cmd ⌘ + D                             | Load default graph                                                                                                 |
    | Q                                     | Toggle visibility of the queue                                                                                     |
    | H                                     | Toggle visibility of history                                                                                       |
    | R                                     | Refresh graph                                                                                                      |
    | Double-Click LMB                      | Quick search for nodes to add                                                                                      |
  </Tab>
</Tabs>


# Getting Started with AI Image Generation
Source: https://docs.comfy.org/get_started/first_generation

This tutorial will guide you through your first image generation with ComfyUI, covering basic interface operations like workflow loading, model installation, and image generation

This guide aims to help you understand ComfyUI's basic operations and complete your first image generation. We'll cover:

1. Loading example workflows
   * Loading from ComfyUI's workflow templates
   * Loading from images with workflow metadata
2. Model installation guidance
   * Automatic model installation
   * Manual model installation
   * Using ComfyUI Manager for model installation
3. Completing your first text-to-image generation

## About Text-to-Image

Text-to-Image is a fundamental AI drawing feature that generates images from text descriptions. It's one of the most commonly used functions in AI art generation. You can think of the process as telling your requirements (positive and negative prompts) to an artist (the drawing model), who will then create what you want. Detailed explanations about text-to-image will be covered in the [Text to Image](/tutorials/basic/text-to-image) chapter.

## ComfyUI Text-to-Image Workflow Tutorial

### 1. Launch ComfyUI

Make sure you've followed the installation guide to start ComfyUI and can successfully enter the ComfyUI interface.

![ComfyUI Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/comfyui-boot-screen.jpg)

If you have not installed ComfyUI, please choose a suitable version to install based on your device.

<AccordionGroup>
  <Accordion title="ComfyUI Desktop (Recommended)">
    ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

    * Code is open source on [Github](https://github.com/Comfy-Org/desktop)

    You can choose the appropriate installation for your system and hardware below

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI Desktop (Windows) Installation Guide" icon="link" href="/installation/desktop/windows">
          Suitable for **Windows** version with **Nvidia** GPU
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI Desktop (MacOS) Installation Guide" icon="link" href="/installation/desktop/macos">
          Suitable for MacOS with **Apple Silicon**
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](/installation/manual_install) section to install ComfyUI</Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI Portable (Windows)">
    <Card title="ComfyUI Portable (Windows) Installation Guide" icon="link" href="/installation/comfyui_portable_windows">
      Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.
    </Card>
  </Accordion>

  <Accordion title="Manual Installation">
    <Card title="ComfyUI Manual Installation Guide" icon="link" href="/installation/manual_install">
      Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)
    </Card>
  </Accordion>
</AccordionGroup>

### 2. Load Default Text-to-Image Workflow

ComfyUI usually loads the default text-to-image workflow automatically when launched. However, you can try different methods to load workflows to familiarize yourself with ComfyUI's basic operations:

<Tabs>
  <Tab title="Load from Workflow Template">
    ![ComfyUI Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1.jpg)
    Follow the numbered steps in the image:

    1. Click the **Fit View** button in the bottom right to ensure any loaded workflow isn't hidden
    2. Click the **folder icon (workflows)** in the sidebar
    3. Click the **Browse example workflows** button at the top of the Workflows panel

    Continue with:
    ![Load Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg)

    4. Select the first default workflow **Image Generation** to load it

    Alternatively, you can select **Browse workflow templates** from the workflow menu
    ![ComfyUI Menu - Browse Workflow Templates](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg)
  </Tab>

  <Tab title="Load from Images with Metadata">
    All images generated by ComfyUI contain metadata including workflow information. You can load workflows by:

    * Dragging and dropping a ComfyUI-generated image into the interface
    * Using menu **Workflows** -> **Open** to open an image

    Try loading the workflow using this example image:
    ![ComfyUI-Text to Image Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/text-to-image-workflow.png)
  </Tab>

  <Tab title="Load from workflow.json">
    ComfyUI workflows can be stored in JSON format. You can export workflows using menu **Workflows** -> **Export**.

    Try downloading and loading this example workflow:

    <a className="prose" href="https://github.com/Comfy-Org/docs/blob/main/public/text-to-image.json" download style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
      <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download text-to-image.json</p>
    </a>

    After downloading, use menu **Workflows** -> **Open** to load the JSON file.
  </Tab>
</Tabs>

### 3. Model Installation

Most ComfyUI installations don't include base models by default. After loading the workflow, if you don't have the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model installed, you'll see this prompt:

![Missing Models](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg)

All models are stored in `<your ComfyUI installation>/ComfyUI/models/` with subfolders like `checkpoints`, `embeddings`, `vae`, `lora`, `upscale_model`, etc. ComfyUI detects models in these folders and paths configured in `extra_models_config.yaml` at startup.

![ComfyUI Models Folder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg)

You can install models through:

<Tabs>
  <Tab title="Automatic Download">
    After you click the **Download** button, ComfyUI will execute the download, and different behaviors will be performed depending on the version you are using.

    <Tabs>
      <Tab title="ComfyUI Desktop">
        The desktop version will automatically complete the model download and save it to the `<your ComfyUI installation location>/ComfyUI/models/checkpoints` directory.
        You can wait for the installation to complete or view the installation progress in the model panel on the sidebar.

        ![Model Download Progress](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg)

        If everything goes smoothly, the model should be able to download locally. If the download fails for a long time, please try other installation methods.
      </Tab>

      <Tab title="ComfyUI Portable">
        The browser will execute file downloads. Please save the file to the `<your ComfyUI installation location>/ComfyUI_windows_portable/ComfyUI/models/checkpoints` directory after the download is complete.
      </Tab>
    </Tabs>
  </Tab>

  <Tab title="ComfyUI Manager">
    ComfyUI Manager is a tool for managing custom nodes, models, and plugins.

    <Steps>
      <Step title="Open ComfyUI Manager">
        ![ComfyUI Manager Installation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager.jpg)

        Click the `Manager` button to open ComfyUI Manager
      </Step>

      <Step title="Open Model Manager">
        ![ComfyUI Manager Model Management](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-model-manager.jpg)

        Click `Model Manager`
      </Step>

      <Step title="Search and Install Model">
        ![ComfyUI Manager Model Download](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-download-model.jpg)

        1. Search for `v1-5-pruned-emaonly.ckpt`
        2. Click `install` on the desired model
      </Step>
    </Steps>
  </Tab>

  <Tab title="Manual Installation">
    Visit [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) and follow this guide:

    ![Hugging Face Model Download](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-5-hugging-face.jpg)

    Save the downloaded file to:

    <Tabs>
      <Tab title="ComfyUI Desktop">
        Save to `<your ComfyUI installation>/ComfyUI/models/checkpoints`

        ![ComfyUI Desktop Model Save Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-6-2-desktop.jpg)
      </Tab>

      <Tab title="ComfyUI Portable">
        Save to `ComfyUI_windows_portable/ComfyUI/models/checkpoints`

        ![ComfyUI Portable Model Save Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-6-1-portable.jpg)
      </Tab>
    </Tabs>

    Refresh or restart ComfyUI after saving.
  </Tab>
</Tabs>

### 4. Load Model and Generate Your First Image

After installing the model:

![Image Generation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg)

1. In the **Load Checkpoint** node, ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected
2. Click `Queue` or press `Ctrl + Enter` to generate

The result will appear in the **Save Image** node. Right-click to save locally.

![ComfyUI First Image Generation Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg)

For detailed text-to-image instructions, see our comprehensive guide:

<Card title="ComfyUI Text-to-Image Workflow Guide" icon="link" href="/tutorials/basic/text-to-image">
  Click here for detailed text-to-image workflow instructions
</Card>

## Troubleshooting

### Model Loading Issues

If the `Load Checkpoint` node shows no models or displays "null", verify your model installation location and try refreshing or restarting ComfyUI.


# Introduction
Source: https://docs.comfy.org/get_started/introduction

Official documentation for ComfyUI. Contribute [here](https://github.com/Comfy-Org/docs).

<img className="block dark:hidden" src="https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui_screenshot.png" alt="Hero Light" />

<img className="hidden dark:block" src="https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui_screenshot.png" alt="Hero Dark" />

## [ComfyUI](https://github.com/comfyanonymous/ComfyUI)

The most powerful and modular stable diffusion GUI and backend. Written by [comfyanonymous](https://github.com/comfyanonymous) and other [contributors](https://github.com/comfyanonymous/ComfyUI/graphs/contributors).

* **ComfyUI** is a node-based interface and inference engine for generative AI
* Users can combine various AI models and operations through nodes to achieve highly customizable and controllable content generation
* ComfyUI is completely open source and can run on your local device

## Getting Started with ComfyUI

### ComfyUI Installation

ComfyUI currently offers multiple installation methods, supporting Windows, MacOS, and Linux systems:

<AccordionGroup>
  <Accordion title="ComfyUI Desktop (Recommended)">
    ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

    * Code is open source on [Github](https://github.com/Comfy-Org/desktop)

    You can choose the appropriate installation for your system and hardware below

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI Desktop (Windows) Installation Guide" icon="link" href="/installation/desktop/windows">
          Suitable for **Windows** version with **Nvidia** GPU
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI Desktop (MacOS) Installation Guide" icon="link" href="/installation/desktop/macos">
          Suitable for MacOS with **Apple Silicon**
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](/installation/manual_install) section to install ComfyUI</Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI Portable (Windows)">
    <Card title="ComfyUI Portable (Windows) Installation Guide" icon="link" href="/installation/comfyui_portable_windows">
      Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.
    </Card>
  </Accordion>

  <Accordion title="Manual Installation">
    <Card title="ComfyUI Manual Installation Guide" icon="link" href="/installation/manual_install">
      Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)
    </Card>
  </Accordion>
</AccordionGroup>

## Contributing to ComfyUI Ecosystem

If you're planning to develop ComfyUI custom nodes (plugins), please read the following section.

<Card title="Custom Node Development Guide" icon="link" href="/custom-nodes/overview">
  Learn how to build a custom node (plugin) for ComfyUI
</Card>

## Contributing to Documentation

Fork the documentation [repo](https://github.com/comfyanonymous/ComfyUI) on Github and submit a PR to us


# ComfyUI(portable) Windows
Source: https://docs.comfy.org/installation/comfyui_portable_windows

This tutorial will guide you on how to download and start using ComfyUI Portable and run the corresponding programs

<Tip>
  For Nvidia 50 series (Blackwell) GPUs, please refer to the [System Requirements](/installation/nvidia-50-series) section to ensure your system meets the requirements for ComfyUI.
</Tip>

**ComfyUI Portable** is a standalone packaged complete ComfyUI Windows version that has integrated an independent **Python (python\_embeded)** required for ComfyUI to run. You only need to extract it to use it. Currently, the portable version supports running through **Nvidia GPU** or **CPU**.

This guide section will walk you through installing ComfyUI Portable.

## Download ComfyUI Portable

You can get the latest ComfyUI Portable download link by clicking the link below

<a className="prose" href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download ComfyUI Portable</p>
</a>

After downloading, you can use decompression software like [7-ZIP](https://7-zip.org/) to extract the compressed package

The file structure and description after extracting the portable version are as follows:

```
ComfyUI_windows_portable
├── 📂ComfyUI                   // ComfyUI main program
├── 📂python_embeded            // Independent Python environment
├── 📂update                    // Batch scripts for upgrading portable version
├── README_VERY_IMPORTANT.txt   // ComfyUI Portable usage instructions in English
├── run_cpu.bat                 // Double click to start ComfyUI (CPU only)
└── run_nvidia_gpu.bat          // Double click to start ComfyUI (Nvidia GPU)
```

## How to Launch ComfyUI

Double click either `run_nvidia_gpu.bat` or `run_cpu.bat` depending on your computer's configuration to launch ComfyUI.
You will see the command running as shown in the image below

![ComfyUI Portable Command Prompt](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui-portable-cmd.png)

When you see something similar to the image

```
To see the GUI go to: http://127.0.0.1:8188
```

At this point, your ComfyUI service has started. Normally, ComfyUI will automatically open your default browser and navigate to `http://127.0.0.1:8188`. If it doesn't open automatically, please manually open your browser and visit this address.

<Warning>During use, please do not close the corresponding command line window, otherwise ComfyUI will stop running</Warning>

## First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey\~

<Card title="First Image Generation" icon="link" href="/get_started/first_generation">
  This tutorial will guide you through your first model installation and text-to-image generation
</Card>

## Additional ComfyUI Portable Instructions

### 1. Upgrading ComfyUI Portable

You can use the batch commands in the update folder to upgrade your ComfyUI Portable version

```
ComfyUI_windows_portable
└─ 📂update
   ├── update.py
   ├── update_comfyui.bat                          // Update ComfyUI to the latest commit version
   ├── update_comfyui_and_python_dependencies.bat  // Only use when you have issues with your runtime environment
   └── update_comfyui_stable.bat                   // Update ComfyUI to the latest stable version
```

### 2. ComfyUI Model Sharing and Custom Model Directory Configuration

If you are also using [A1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) or want to customize your model storage location, you can modify the following file to complete the configuration

```
ComfyUI_windows_portable
└─ 📂ComfyUI
  └── extra_model_paths.yaml.example  // This file is the configuration template
```

Please copy and rename the `extra_model_paths.yaml.example` to `extra_model_paths.yaml`.

Below is the original configuration file content, which you can modify according to your needs

```yaml
#Rename this to extra_model_paths.yaml and ComfyUI will load it


#config for a1111 ui
#all you have to do is change the base_path to where yours is installed
a111:
    base_path: path/to/stable-diffusion-webui/

    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet

#config for comfyui
#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.

#comfyui:
#     base_path: path/to/comfyui/
#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads
#     #is_default: true
#     checkpoints: models/checkpoints/
#     clip: models/clip/
#     clip_vision: models/clip_vision/
#     configs: models/configs/
#     controlnet: models/controlnet/
#     diffusion_models: |
#                  models/diffusion_models
#                  models/unet
#     embeddings: models/embeddings/
#     loras: models/loras/
#     upscale_models: models/upscale_models/
#     vae: models/vae/

#other_ui:
#    base_path: path/to/ui
#    checkpoints: models/checkpoints
#    gligen: models/gligen
#    custom_nodes: path/custom_nodes

```

For example, if your WebUI is located at `D:\stable-diffusion-webui\`, you can modify the corresponding configuration to

```yaml
a111:
    base_path: D:\stable-diffusion-webui\
    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet
```

This way, models under paths like `D:\stable-diffusion-webui\models\Stable-diffusion\` can be detected and used by ComfyUI. Similarly, you can add other custom model location configurations

### 3. Setting Up LAN Access for ComfyUI Portable

If your ComfyUI is running on a local network and you want other devices to access ComfyUI, you can modify the `run_nvidia_gpu.bat` or `run_cpu.bat` file using Notepad to complete the configuration. This is mainly done by adding `--listen` to specify the listening address.
Below is an example of the `run_nvidia_gpu.bat` file command with the `--listen` parameter added

```bat
.\python_embeded\python.exe -s ComfyUI\main.py --listen --windows-standalone-build
pause
```

After enabling ComfyUI, you will notice the final running address will become

```
Starting server

To see the GUI go to: http://0.0.0.0:8188
To see the GUI go to: http://[::]:8188
```

You can press `WIN + R` and type `cmd` to open the command prompt, then enter `ipconfig` to view your local IP address. Other devices can then access ComfyUI by entering `http://your-local-IP:8188` in their browser.


# Linux Desktop Version
Source: https://docs.comfy.org/installation/desktop/linux

This article introduces how to download, install and use ComfyUI Desktop for Linux

<Warning>Linux pre-built packages are not yet available. Please try [manual installation.](/installation/manual_install)</Warning>


# MacOS Desktop Version
Source: https://docs.comfy.org/installation/desktop/macos

This article introduces how to download, install and use ComfyUI Desktop for MacOS

export const log_path_0 = "~/Library/Logs/ComfyUI"

export const config_path_0 = "~/Library/Application Support/ComfyUI"

**ComfyUI Desktop** is a standalone installation version that can be installed like regular software.
It supports quick installation and automatic configuration of the **Python environment and dependencies**, and supports one-click import of existing ComfyUI settings, models, workflows, and files.

ComfyUI Desktop is an open source project, please visit the full code [here](https://github.com/Comfy-Org/desktop).

<Note>ComfyUI Desktop (MacOS) only supports Apple Silicon</Note>

This tutorial will guide you through the software installation process and explain related configuration details.

<Warning>As **ComfyUI Desktop** is still in **Beta** status, the actual installation process may change</Warning>

## ComfyUI Desktop (MacOS) Download

Please click the button below to download the installation package for MacOS **ComfyUI Desktop**

<a className="prose" href="https://download.comfy.org/mac/dmg/arm64" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download for MacOS</p>
</a>

## ComfyUI Desktop Installation Steps

Double-click the downloaded installation package file. As shown in the image, drag the **ComfyUI** application into the **Applications** folder following the arrow

![ComfyUI Installation Package](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0.png)

If your folder shows as below with a prohibition sign on the icon after opening the installation package, it means your current system version is not compatible with **ComfyUI Desktop**
![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0-1.png)

Then find the **ComfyUI icon** in **Launchpad** and click it to enter ComfyUI initialization settings
![ComfyUI Lanchpad](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-1.jpg)

## ComfyUI Desktop Initialization Process

<Steps>
  <Step title="Start Screen">
    <Tabs>
      <Tab title="Normal Start">
        ![ComfyUI Installation Steps - Start](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-2.png)

        Click **Get Started** to begin initialization
      </Tab>

      <Tab title="Maintenance Page">
        There are many reasons you might have issues installing ComfyUI. Maybe a network connection failed when installing pytorch (15 GB). Or you don’t have git installed. The maintenance page automatically opens when it detects an issue and provides a way to resolve the issue.

        You can use it to resolve most issues:

        * Create a python virtual environment
        * Reinstall all missing core dependencies to your Python virtual environment that’s managed by Desktop
        * Install git, VC redis
        * Choose a new install location

        The default maintenance page displays the current error content

        ![ComfyUI Maintenance Page](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-1.jpg)

        Clicking `All` allows you to view all the content that can be operated on currently

        ![ComfyUI Maintenance Page](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-2.jpg)
      </Tab>
    </Tabs>
  </Step>

  <Step title="Select GPU">
    ![ComfyUI Installation Steps - GPU Selection](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-3.png)

    The three options are:

    1. **MPS (Recommended):** Metal Performance Shaders (MPS) is an Apple framework that uses GPUs to accelerate computing and machine learning tasks on Apple devices, supporting frameworks like PyTorch.
    2. **Manual Configuration:** You need to manually install and configure the python runtime environment. Don't select this unless you know how to configure
    3. **Enable CPU Mode:** For developers and special cases only. Don't select this unless you're sure you need it

    Unless there are special circumstances, please select **MPS** as shown and click **Next** to proceed
  </Step>

  <Step title="Install location">
    ![ComfyUI Installation Steps - Installation Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-4.png)

    In this step, you will select the installation location for the following related content of ComfyUI:

    * **Python Environment**
    * **Models Model Files**
    * **Custom Nodes Custom Nodes**

    Recommendations:

    * Please create a separate empty folder as the installation directory for ComfyUI
    * Please ensure that the disk has at least **5G** of disk space to ensure the normal installation of **ComfyUI Desktop**

    <Note>Not all files are installed in this directory, some files will be located in the MacOS system directory, you can refer to the uninstallation section of this guide to complete the uninstallation of the ComfyUI desktop version</Note>
  </Step>

  <Step title="Migrate from Existing Installation (Optional)">
    ![ComfyUI Installation Steps - File Migration](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-5.png)

    In this step you can migrate your existing ComfyUI installation content to ComfyUI Desktop. Select your existing ComfyUI installation directory, and the installer will automatically recognize:

    * **User Files**
    * **Models:** Will not be copied, only linked with desktop version
    * **Custom Nodes:** Nodes will be reinstalled

    Don't worry, this step won't copy model files. You can check or uncheck options as needed. Click **Next** to continue
  </Step>

  <Step title="Desktop Settings">
    ![ComfyUI Installation Steps - Desktop Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-6.png)

    These are preference settings:

    1. **Automatic Updates:** Whether to set automatic updates when ComfyUI updates are available
    2. **Usage Metrics:** If enabled, we will collect **anonymous usage data** to help improve ComfyUI
    3. **Mirror Settings:** Since the program needs internet access to download Python and complete environment installation, if you see a red ❌ during installation indicating this may cause installation failure, please follow the steps below

    ![ComfyUI Installation Steps - Mirror Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-7.png)
    Expand the mirror settings to find the specific failing mirror. In this screenshot the error is **Python Install Mirror** failure.

    For different mirror errors, you can refer to the following content to try to manually find different mirrors and replace them

    The following cases mainly apply to users in China.

    #### Python Installation Mirror

    If the default mirror is unavailable, please try using the mirror below.

    ```
    https://python-standalone.org/mirror/astral-sh/python-build-standalone
    ```

    If you need to find other alternative GitHub mirror addresses, please look for and construct a mirror address pointing to the releases of the `python-build-standalone` repository.

    ```
    https://github.com/astral-sh/python-build-standalone/releases/download
    ```

    Build a link in the following pattern

    ```
    https://xxx/astral-sh/python-build-standalone/releases/download
    ```

    <info>Since most of the Github mirror services are provided by third parties, please pay attention to the security during use.</info>

    #### PyPI Mirror

    * Alibaba Cloud: [https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)
    * Tencent Cloud: [https://mirrors.cloud.tencent.com/pypi/simple/](https://mirrors.cloud.tencent.com/pypi/simple/)
    * University of Science and Technology of China: [https://pypi.mirrors.ustc.edu.cn/simple/](https://pypi.mirrors.ustc.edu.cn/simple/)
    * Shanghai Jiao Tong University: [https://pypi.sjtu.edu.cn/simple/](https://pypi.sjtu.edu.cn/simple/)

    #### Torch Mirror

    * Aliyun: [https://mirrors.aliyun.com/pytorch-wheels/cu121/](https://mirrors.aliyun.com/pytorch-wheels/cu121/)
  </Step>

  <Step title="Complete the installation">
    If everything is correct, the installer will complete and automatically enter the ComfyUI Desktop interface, then the installation is successful
    ![ComfyUI Desktop Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-interface.jpg)
  </Step>
</Steps>

## First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey\~

<Card title="First Image Generation" icon="link" href="/get_started/first_generation">
  This tutorial will guide you through your first model installation and text-to-image generation
</Card>

## How to Update ComfyUI Desktop

Currently, ComfyUI Desktop updates use automatic detection updates, please ensure that automatic updates are enabled in the settings

![ComfyUI Desktop Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg)

## How to Uninstall ComfyUI Desktop

For **ComfyUI Desktop**, you can directly delete **ComfyUI** from the **Applications** folder

If you want to completely remove all **ComfyUI Desktop** files, you can manually delete these folders:

* /Users/Library/Application Support/ComfyUI

The above operations will not delete your following folders. If you need to delete corresponding files, please delete manually:

* models files
* custom nodes
* input/output directories

## Troubleshooting

### ​Error identification​

If installation fails, you should see the following screen

![ComfyUI Installation Failed](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg)

It is recommended to take these steps to find the error cause:

1. Click `Show Terminal` to view error output
2. Click `Open Logs` to view installation logs
3. Visit official forum to search for error reports
4. Click `Reinstall` to try reinstalling

Before submitting feedback, it's recommended to provide the **error output** and **log files** to tools like **GPT**

![ComfyUI Installation Failed - Error Log](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg)
![ComfyUI Installation Failed - GPT Feedback](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg)

As shown above, ask the GPT for the cause of the corresponding error, or remove ComfyUI completely and retry the installation.

### Feedback Installation Failure

If you encounter any errors during installation, please check if there are similar error reports or submit errors to us through:

* Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)
* Comfy Official Forum: [https://forum.comfy.org/](https://forum.comfy.org/)

When submitting error reports, please ensure you include the following logs and configuration files to help us locate and investigate the issue:

1. Log Files

| Filename    | Description                                                                                     | Location     |
| ----------- | ----------------------------------------------------------------------------------------------- | ------------ |
| main.log    | Contains logs related to desktop application and server startup from the Electron process       | {log_path_0} |
| comfyui.log | Contains logs related to ComfyUI normal operation, such as core ComfyUI process terminal output | {log_path_0} |

![ComfyUI Log Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)

2. Configuration Files

| Filename                   | Description                                                                     | Location        |
| -------------------------- | ------------------------------------------------------------------------------- | --------------- |
| extra\_models\_config.yaml | Contains additional paths where ComfyUI will search for models and custom nodes | {config_path_0} |
| config.json                | Contains application configuration. This file should not be edited directly     | {config_path_0} |

![ComfyUI Config Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)


# Windows Desktop Version
Source: https://docs.comfy.org/installation/desktop/windows

This article introduces how to download, install and use ComfyUI Desktop for Windows

export const log_path_0 = "C:\Users\ <your username> \AppData\Roaming\ComfyUI\logs"

export const config_path_0 = "C:\Users\ <your username> \AppData\Roaming\ComfyUI"

**ComfyUI Desktop** is a standalone installation version that can be installed like regular software. It supports quick installation and automatic configuration of the **Python environment and dependencies**, and supports one-click import of existing ComfyUI settings, models, workflows, and files. You can quickly migrate from an existing [ComfyUI Portable version](/installation/comfyui_portable_windows) to the Desktop version.

ComfyUI Desktop is an open source project, please visit the full code [here](https://github.com/Comfy-Org/desktop)

ComfyUI Desktop hardware requirements:

* NVIDIA GPU

This tutorial will guide you through the software installation process and explain related configuration details.

<Warning>As **ComfyUI Desktop** is still in **Beta** status, the actual installation process may change</Warning>

## ComfyUI Desktop (Windows) Download

Please click the button below to download the installation package for Windows **ComfyUI Desktop**

<a className="prose" href="https://download.comfy.org/windows/nsis/x64" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download for Windows (NVIDIA)</p>
</a>

## ComfyUI Desktop Installation Steps

Double-click the downloaded installation package file, which will first perform an automatic installation and create a **ComfyUI Desktop** shortcut on the desktop

![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-shortcut.jpg)

Double-click the corresponding shortcut to enter ComfyUI initialization settings

### ComfyUI Desktop Initialization Process

<Steps>
  <Step title="Start Screen">
    <Tabs>
      <Tab title="Normal Start">
        ![ComfyUI Installation Steps - Start](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-1.jpg)

        Click **Get Started** to begin initialization
      </Tab>

      <Tab title="Maintenance Page">
        There are many reasons you might have issues installing ComfyUI. Maybe a network connection failed when installing pytorch (15 GB). Or you don’t have git installed. The maintenance page automatically opens when it detects an issue and provides a way to resolve the issue.

        You can use it to resolve most issues:

        * Create a python virtual environment
        * Reinstall all missing core dependencies to your Python virtual environment that’s managed by Desktop
        * Install git, VC redis
        * Choose a new install location

        The default maintenance page displays the current error content

        ![ComfyUI Maintenance Page](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-1.jpg)

        Clicking `All` allows you to view all the content that can be operated on currently

        ![ComfyUI Maintenance Page](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-2.jpg)
      </Tab>
    </Tabs>
  </Step>

  <Step title="Select GPU">
    ![ComfyUI Installation Steps - GPU Selection](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-2.jpg)

    The three options are:

    1. **Nvidia GPU (Recommended):** Direct support for pytorch and CUDA
    2. **Manual Configuration:** You need to manually install and configure the python runtime environment. Don't select this unless you know how to configure
    3. **Enable CPU Mode:** For developers and special cases only. Don't select this unless you're sure you need it

    Unless there are special circumstances, please select **NVIDIA** as shown and click **Next** to proceed
  </Step>

  <Step title="Install location">
    ![ComfyUI Installation Steps - Installation Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-3.jpg)

    In this step, you will select the installation location for the following ComfyUI content:

    * **Python Environment**
    * **Models Model Files**
    * **Custom Nodes Custom Nodes**

    Recommendations:

    * Please select a **solid-state drive** as the installation location, which will increase ComfyUI's performance when accessing models.
    * Please create a separate empty folder as the ComfyUI installation directory
    * Please ensure that the corresponding disk has at least around **15G** of disk space to ensure the installation of ComfyUI Desktop

    <Note>Not all files are installed in this directory, some files will still be installed on the C drive, and if you need to uninstall in the future, you can refer to the uninstallation section of this guide to complete the full uninstallation of ComfyUI Desktop</Note>

    After completing this step, click **Next** to proceed to the next step
  </Step>

  <Step title="Migrate from Existing Installation (Optional)">
    ![ComfyUI Installation Steps - File Migration](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-4.jpg)

    In this step you can migrate your existing ComfyUI installation content to ComfyUI Desktop. As shown, I selected my original **D:\ComfyUI\_windows\_portable\ComfyUI** installation directory. The installer will automatically recognize:

    * **User Files**
    * **Models:** Will not be copied, only linked with desktop version
    * **Custom Nodes:** Nodes will be reinstalled

    Don't worry, this step won't copy model files. You can check or uncheck options as needed. Click **Next** to continue
  </Step>

  <Step title="Desktop Settings">
    ![ComfyUI Installation Steps - Desktop Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-5.jpg)

    These are preference settings:

    1. **Automatic Updates:** Whether to set automatic updates when ComfyUI updates are available
    2. **Usage Metrics:** If enabled, we will collect **anonymous usage data** to help improve ComfyUI
    3. **Mirror Settings:** Since the program needs internet access to download Python and complete environment installation, if you see a red ❌ during installation indicating this may cause installation failure, please follow the steps below

    ![ComfyUI Installation Steps - Mirror Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-6.jpg)
    Expand the mirror settings to find the specific failing mirror. In this screenshot the error is **Python Install Mirror** failure.

    For different mirror errors, you can refer to the following content to try to manually find different mirrors and replace them

    The following cases mainly apply to users in China.

    #### Python Installation Mirror

    If the default mirror is unavailable, please try using the mirror below.

    ```
    https://python-standalone.org/mirror/astral-sh/python-build-standalone
    ```

    If you need to find other alternative GitHub mirror addresses, please look for and construct a mirror address pointing to the releases of the `python-build-standalone` repository.

    ```
    https://github.com/astral-sh/python-build-standalone/releases/download
    ```

    Build a link in the following pattern

    ```
    https://xxx/astral-sh/python-build-standalone/releases/download
    ```

    <info>Since most of the Github mirror services are provided by third parties, please pay attention to the security during use.</info>

    #### PyPI Mirror

    * Alibaba Cloud: [https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)
    * Tencent Cloud: [https://mirrors.cloud.tencent.com/pypi/simple/](https://mirrors.cloud.tencent.com/pypi/simple/)
    * University of Science and Technology of China: [https://pypi.mirrors.ustc.edu.cn/simple/](https://pypi.mirrors.ustc.edu.cn/simple/)
    * Shanghai Jiao Tong University: [https://pypi.sjtu.edu.cn/simple/](https://pypi.sjtu.edu.cn/simple/)

    #### Torch Mirror

    * Aliyun: [https://mirrors.aliyun.com/pytorch-wheels/cu121/](https://mirrors.aliyun.com/pytorch-wheels/cu121/)
  </Step>

  <Step title="Complete the installation">
    If everything is correct, the installer will complete and automatically enter the ComfyUI Desktop interface, then the installation is successful
    ![ComfyUI Desktop Interface](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-interface.jpg)
  </Step>
</Steps>

## First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey\~

<Card title="First Image Generation" icon="link" href="/get_started/first_generation">
  This tutorial will guide you through your first model installation and text-to-image generation
</Card>

## How to Update ComfyUI Desktop

Currently, ComfyUI Desktop updates use automatic detection updates, please ensure that automatic updates are enabled in the settings

![ComfyUI Desktop Settings](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg)

## How to Uninstall ComfyUI Desktop

For **ComfyUI Desktop** you can use the system uninstall function in Windows Settings to complete software uninstallation

![ComfyUI Desktop Uninstallation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-uninstall-comfyui.jpg)

If you want to completely remove all **ComfyUI Desktop** files, you can manually delete these folders:

* C:\Users\<your username>\AppData\Local\@comfyorgcomfyui-electron-updater
* C:\Users\<your username>\AppData\Local\Programs\@comfyorgcomfyui-electron
* C:\Users\<your username>\AppData\Roaming\ComfyUI

The above operations will not delete your following folders. If you need to delete corresponding files, please delete manually:

* models files
* custom nodes
* input/output directories

## Troubleshooting

### Display unsupported devices

![ComfyUI Installation Steps - Unsupported Device](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-0.jpg)

Since ComfyUI Desktop (Windows) only supports **NVIDIA GPUs with CUDA**, you may see this screen if your device is not supported

* Please switch to a supported device
* Or consider using [ComfyUI Portable](/installation/comfyui_portable_windows) or through [manual installation](/installation/manual_install) to use ComfyUI

### ​Error identification​

If installation fails, you should see the following screen

![ComfyUI Installation Failed](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg)

It is recommended to take these steps to find the error cause:

1. Click `Show Terminal` to view error output
2. Click `Open Logs` to view installation logs
3. Visit official forum to search for error reports
4. Click `Reinstall` to try reinstalling

Before submitting feedback, it's recommended to provide the **error output** and **log files** to tools like **GPT**

![ComfyUI Installation Failed - Error Log](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg)
![ComfyUI Installation Failed - GPT Feedback](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg)

As shown above, ask the GPT for the cause of the corresponding error, or remove ComfyUI completely and retry the installation.

### Feedback Installation Failure

If you encounter any errors during installation, please check if there are similar error reports or submit errors to us through:

* Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)
* Comfy Official Forum: [https://forum.comfy.org/](https://forum.comfy.org/)

When submitting error reports, please ensure you include the following logs and configuration files to help us locate and investigate the issue:

1. Log Files

| Filename    | Description                                                                                     | Location     |
| ----------- | ----------------------------------------------------------------------------------------------- | ------------ |
| main.log    | Contains logs related to desktop application and server startup from the Electron process       | {log_path_0} |
| comfyui.log | Contains logs related to ComfyUI normal operation, such as core ComfyUI process terminal output | {log_path_0} |

![ComfyUI Log Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)

2. Configuration Files

| Filename                   | Description                                                                     | Location        |
| -------------------------- | ------------------------------------------------------------------------------- | --------------- |
| extra\_models\_config.yaml | Contains additional paths where ComfyUI will search for models and custom nodes | {config_path_0} |
| config.json                | Contains application configuration. This file should not be edited directly     | {config_path_0} |

![ComfyUI Config Files Location](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)


# Manual Installation
Source: https://docs.comfy.org/installation/manual_install



<Tip>
  For Nvidia 50 series (Blackwell) GPUs, please refer to the [System Requirements](/installation/nvidia-50-series) section to ensure your system meets the requirements for ComfyUI.
</Tip>

<Tabs>
  <Tab title="Windows">
    ### Clone the repository

    ```bash
    git clone git@github.com:comfyanonymous/ComfyUI.git
    ```

    More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

    <Warning>If you have not installed Microsoft Visual C++ Redistributable, please install it [here.](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170)</Warning>

    ### Install Dependencies

    1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.

       Create an environment with Conda.

       ```
       conda create -n comfyenv
       conda activate comfyenv
       ```
    2. Install GPU Dependencies

       <Accordion title="Nvidia">
         ```
         conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
           ```
         </Accordion>
       </Accordion>

       <Accordion title="AMD">
         ```
         pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
           ```
         </Accordion>
       </Accordion>

       <Accordion title="Mac ARM Silicon">
         ```bash
         conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
         ```
       </Accordion>
    3. ```bash
       cd ComfyUI
       pip install -r requirements.txt
       ```
    4. Start the application
       ```
       cd ComfyUI
       python main.py
       ```
  </Tab>

  <Tab title="Linux">
    ### Clone the repository

    ```bash
    git clone git@github.com:comfyanonymous/ComfyUI.git
    ```

    More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

    ### Install Dependencies

    1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.

       Create an environment with Conda.

       ```
       conda create -n comfyenv
       conda activate comfyenv
       ```
    2. Install GPU Dependencies

       <Accordion title="Nvidia">
         ```
         conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
           ```
         </Accordion>
       </Accordion>

       <Accordion title="AMD">
         ```
         pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
           ```
         </Accordion>
       </Accordion>

       <Accordion title="Mac ARM Silicon">
         ```bash
         conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
         ```
       </Accordion>
    3. ```bash
       cd ComfyUI
       pip install -r requirements.txt
       ```
    4. Start the application
       ```
       cd ComfyUI
       python main.py
       ```
  </Tab>

  <Tab title="MacOS">
    ### Clone the repository

    Open [Terminal application](https://support.apple.com/guide/terminal/open-or-quit-terminal-apd5265185d-f365-44cb-8b09-71a064a42125/mac).

    ```bash
    git clone git@github.com:comfyanonymous/ComfyUI.git
    ```

    More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

    ### Install Dependencies

    1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.

       Create an environment with Conda.

       ```
       conda create -n comfyenv
       conda activate comfyenv
       ```
    2. Install GPU Dependencies

       <Accordion title="Nvidia">
         ```
         conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
           ```
         </Accordion>
       </Accordion>

       <Accordion title="AMD">
         ```
         pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
           ```
         </Accordion>
       </Accordion>

       <Accordion title="Mac ARM Silicon">
         ```bash
         conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
         ```
       </Accordion>
    3. ```bash
       cd ComfyUI
       pip install -r requirements.txt
       ```
    4. Start the application
       ```
       cd ComfyUI
       python main.py
       ```
  </Tab>
</Tabs>


# System Requirements
Source: https://docs.comfy.org/installation/system_requirements

This guide introduces some system requirements for ComfyUI, including hardware and software requirements

In this guide, we will introduce the system requirements for installing ComfyUI. Due to frequent updates of ComfyUI, this document may not be updated in a timely manner. Please refer to the relevant instructions in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).

Regardless of which version of ComfyUI you use, it runs in a separate Python environment.

You can refer to the following sections to learn about the installation methods for different systems and versions of ComfyUI. In the installation of different versions, we have simply described the system requirements.

<AccordionGroup>
  <Accordion title="ComfyUI Desktop (Recommended)">
    ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

    * Code is open source on [Github](https://github.com/Comfy-Org/desktop)

    You can choose the appropriate installation for your system and hardware below

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI Desktop (Windows) Installation Guide" icon="link" href="/installation/desktop/windows">
          Suitable for **Windows** version with **Nvidia** GPU
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI Desktop (MacOS) Installation Guide" icon="link" href="/installation/desktop/macos">
          Suitable for MacOS with **Apple Silicon**
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](/installation/manual_install) section to install ComfyUI</Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI Portable (Windows)">
    <Card title="ComfyUI Portable (Windows) Installation Guide" icon="link" href="/installation/comfyui_portable_windows">
      Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.
    </Card>
  </Accordion>

  <Accordion title="Manual Installation">
    <Card title="ComfyUI Manual Installation Guide" icon="link" href="/installation/manual_install">
      Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)
    </Card>
  </Accordion>
</AccordionGroup>

## Nvidia 50 Series GPU Requirements

To make your Nvidia 50 series GPU (Blackwell architecture) work properly with ComfyUI, you need a PyTorch version that supports CUDA 12.8 or newer. Currently (March 2025), the stable version of PyTorch does not yet support the Blackwell architecture, so you need to use a nightly build version.

Related discussions on this issue are concentrated [here](https://github.com/comfyanonymous/ComfyUI/discussions/6643).

### For Windows Users

**Recommended Option:**
Download the standalone ComfyUI Portable version with nightly pytorch 2.7 cu128:

* [Click here to download](https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_windows_portable_nvidia_or_cpu_nightly_pytorch.7z)

**Other Options:**
Older torch 2.6 Windows package:

* [Click here to download the standalone ComfyUI package with a cuda 12.8 torch build](https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_cu128_50XX.7z)

### Manual Installation

Windows and Linux users can install the PyTorch nightly version using the following command:

```bash
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
```

### Docker Container Alternative

You can try the PyTorch container provided by Nvidia, which might offer better performance.

Container address: [https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch)

Usage method:

```bash
docker run -p 8188:8188 --gpus all -it --rm nvcr.io/nvidia/pytorch:25.01-py3
```

Inside the Docker container, execute the following commands:

```bash
git clone https://github.com/comfyanonymous/ComfyUI
cd ComfyUI
grep -v 'torchaudio\|torchvision' requirements.txt > temp_requirements.txt
pip install -r temp_requirements.txt
python main.py --listen
```


# Create a new custom node
Source: https://docs.comfy.org/registry/api-reference/nodes/create-a-new-custom-node

post /publishers/{publisherId}/nodes



# Delete a specific node
Source: https://docs.comfy.org/registry/api-reference/nodes/delete-a-specific-node

delete /publishers/{publisherId}/nodes/{nodeId}



# Retrieve a specific node by ID
Source: https://docs.comfy.org/registry/api-reference/nodes/retrieve-a-specific-node-by-id

get /nodes/{nodeId}
Returns the details of a specific node.



# Retrieve all nodes
Source: https://docs.comfy.org/registry/api-reference/nodes/retrieve-all-nodes

get /publishers/{publisherId}/nodes



# Retrieves a list of nodes
Source: https://docs.comfy.org/registry/api-reference/nodes/retrieves-a-list-of-nodes

get /nodes
Returns a paginated list of nodes across all publishers.



# Returns a node version to be installed.
Source: https://docs.comfy.org/registry/api-reference/nodes/returns-a-node-version-to-be-installed

get /nodes/{nodeId}/install
Retrieves the node data for installation, either the latest or a specific version.



# Update a specific node
Source: https://docs.comfy.org/registry/api-reference/nodes/update-a-specific-node

put /publishers/{publisherId}/nodes/{nodeId}



# API Overview
Source: https://docs.comfy.org/registry/api-reference/overview



## Overview

The Custom Node Registry follows this structure:

```mermaid
erDiagram
    PUBLISHER {
        string id PK
    }

    USER {
        string id PK
        string name
        string publisher_id FK
    }

    CUSTOM_NODE {
        string id PK
        string name
        string publisher_id FK
    }

    NODE_VERSION {
        string id PK
        string version
        string node_id FK
    }

    PUBLISHER ||--o{ USER: "has many"
    PUBLISHER ||--o{ CUSTOM_NODE: "has many"
    CUSTOM_NODE ||--o{ NODE_VERSION: "has many"
```

## Commonly Used APIs

* **List All Nodes** [API](/registry/api-reference/nodes/retrieves-a-list-of-nodes)
* **Install a Node** [API](/registry/api-reference/nodes/returns-a-node-version-to-be-installed)


# Create a new publisher
Source: https://docs.comfy.org/registry/api-reference/publishers/create-a-new-publisher

post /publishers



# Delete a publisher
Source: https://docs.comfy.org/registry/api-reference/publishers/delete-a-publisher

delete /publishers/{publisherId}



# Retrieve a publisher by ID
Source: https://docs.comfy.org/registry/api-reference/publishers/retrieve-a-publisher-by-id

get /publishers/{publisherId}



# Retrieve all publishers
Source: https://docs.comfy.org/registry/api-reference/publishers/retrieve-all-publishers

get /publishers



# Retrieve all publishers for a given user
Source: https://docs.comfy.org/registry/api-reference/publishers/retrieve-all-publishers-for-a-given-user

get /users/publishers/



# Update a publisher
Source: https://docs.comfy.org/registry/api-reference/publishers/update-a-publisher

put /publishers/{publisherId}



# Validate if a publisher username is available
Source: https://docs.comfy.org/registry/api-reference/publishers/validate-if-a-publisher-username-is-available

get /publishers/validate
Checks if the publisher username is already taken.



# Create a new personal access token
Source: https://docs.comfy.org/registry/api-reference/token-management/create-a-new-personal-access-token

post /publishers/{publisherId}/tokens



# Delete a specific personal access token
Source: https://docs.comfy.org/registry/api-reference/token-management/delete-a-specific-personal-access-token

delete /publishers/{publisherId}/tokens/{tokenId}



# Retrieve all personal access tokens for a publisher
Source: https://docs.comfy.org/registry/api-reference/token-management/retrieve-all-personal-access-tokens-for-a-publisher

get /publishers/{publisherId}/tokens



# List all versions of a node
Source: https://docs.comfy.org/registry/api-reference/versions/list-all-versions-of-a-node

get /nodes/{nodeId}/versions



# Publish a new version of a node
Source: https://docs.comfy.org/registry/api-reference/versions/publish-a-new-version-of-a-node

post /publishers/{publisherId}/nodes/{nodeId}/versions



# Retrieve a specific version of a node
Source: https://docs.comfy.org/registry/api-reference/versions/retrieve-a-specific-version-of-a-node

get /nodes/{nodeId}/versions/{versionId}



# Unpublish (delete) a specific version of a node
Source: https://docs.comfy.org/registry/api-reference/versions/unpublish-delete-a-specific-version-of-a-node

delete /publishers/{publisherId}/nodes/{nodeId}/versions/{versionId}



# Update changelog and deprecation status of a node version
Source: https://docs.comfy.org/registry/api-reference/versions/update-changelog-and-deprecation-status-of-a-node-version

put /publishers/{publisherId}/nodes/{nodeId}/versions/{versionId}
Update only the changelog and deprecated status of a specific version of a node.



# Custom Node CI/CD
Source: https://docs.comfy.org/registry/cicd



## Introduction

When making changes to custom nodes, it's not uncommon to break things in Comfy or other custom nodes. It is often unrealistic to test on every operating system and different configurations of Pytorch.

### Run Comfy Workflows using Github Actions

[Comfy-Action](https://github.com/Comfy-Org/comfy-action) allows you to run a Comfy workflow\.json file on Github Actions. It supports downloading models, custom nodes, and runs on Linux/Mac/Windows.

### Results

Output files are uploaded to the [CI/CD Dashboard](https://comfyci.org) and can be viewed as a last step before commiting new changes or publishing new versions of the custom node.

![ComfyCI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyci.png)


# Overview
Source: https://docs.comfy.org/registry/overview



## Introduction

The Registry is a public collection of custom nodes. Developers can publish, version, deprecate, and track metrics related to their custom nodes. ComfyUI users can discover, install, and rate custom nodes from the registry.

## Why use the Registry?

The Comfy Registry helps the community by standardizing the development of custom nodes:

<Icon icon="timeline" iconType="solid" size={20} />  **Node Versioning:** Developers frequently publish new versions of their custom nodes which often break workflows that rely on them. With registry nodes being [semantically versioned](https://semver.org/), users can now choose to safely upgrade, deprecate, or lock their node versions in place, knowing in advance how their actions will impact their workflows. The workflow JSON will store the version of the node used, so you can always reliably reproduce your workflows.

<Icon icon="shield" iconType="solid" size={20} />  **Node Security:** The registry will serve as a backend for the [ComfyUI-manager](https://github.com/ltdrdata/ComfyUI-Manager). All nodes will be scanned for malicious behaviour such as custom pip wheels, arbitrary system calls, etc. Nodes that pass these checks will have a verification flag (<Icon icon="check" iconType="solid" />) beside their name on the UI-manager. For a list of security standards, see the [standards](/registry/standards).

<Icon icon="magnifying-glass" iconType="solid" size={20} />  **Search:** Search across all nodes on the Registry to find existing nodes for your workflow\.x

## Publishing Nodes

Get started publishing your first node by following the [tutorial](/registry/publishing).

## Frequently Asked Questions

<AccordionGroup>
  <Accordion title="Do registry nodes have unique identifiers?">
    Yes, a custom node on the Registry has a globally unique name and this allows Comfy Workflow JSON files to uniquely identify any custom node without collisions.
  </Accordion>

  <Accordion title="Are there any restrictions on what I can publish?">
    Check the [standards](/registry/standards) for more information.
  </Accordion>

  <Accordion title="How do you ensure node stability?">
    Once a custom node version is published, it cannot be changed. This ensures that users can rely on the stability of the custom node over time.
  </Accordion>

  <Accordion title="How are nodes versioned?">
    Custom nodes are versioned using [semantic versioning](https://semver.org/). This allows users to understand the impact of upgrading to a new version.
  </Accordion>

  <Accordion title="How do I deprecate a node version?">
    You can deprecate a version in the Comfy Registry website by clicking **More Actions > Deprecate**. Users who installed this version will be shown the deprecation message and be encouraged to upgrade to a newer version.

    Deprecating versions is useful when an issue is discovered after publishing.
  </Accordion>
</AccordionGroup>


# Publishing Nodes
Source: https://docs.comfy.org/registry/publishing



## Set up a Registry Account

Follow the steps below to set up a registry account and publish your first node.

### Watch a Tutorial

<iframe height="415" src="https://www.youtube.com/embed/WhOZZOgBggU?si=6TyvhJJadmQ65uXC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen style={{ width: "100%", borderRadius: "0.5rem" }} />

### Create a Publisher

A publisher is an identity that can publish custom nodes to the registry. Every custom node needs to include a publisher identifier in the pyproject.toml [file]().

Go to [Comfy Registry](https://registry.comfy.org), and create a publisher account. Your publisher id is globally unique, and cannot be changed later because it is used in the URL of your custom node.

Your publisher id is found after the `@` symbol on your profile page.

<img className="block" src="https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/publisherid.png" alt="Hero Dark" />

### Create an API Key for publishing

Go [here](https://registry.comfy.org/nodes) and click on the publisher you want to create an API key for. This will be used to publish a custom node via the CLI.

![Create key for Specific Publisher](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/pat-1.png)

Name the API key and save it somewhere safe. If you lose it, you'll have to create a new key.

![Create API Key](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/pat-2.png)

### Add Metadata

<Tip>Have you installed the comfy-cli? [Do that first](/comfy-cli/getting-started).</Tip>

```bash
comfy node init
```

This command will generate the following metadata:

```toml
# pyproject.toml
[project]
name = "" # Unique identifier for your node. Immutable after creation.
description = ""
version = "1.0.0" # Custom Node version. Must be semantically versioned.
license = { file = "LICENSE.txt" }
dependencies  = [] # Filled in from requirements.txt

[project.urls]
Repository = "https://github.com/..."

[tool.comfy]
PublisherId = "" # TODO (fill in Publisher ID from Comfy Registry Website).
DisplayName = "" # Display name for the Custom Node. Can be changed later.
Icon = "https://example.com/icon.png" # SVG, PNG, JPG or GIF (MAX. 800x400px)
```

Add this file to your repository. Check the [specifications](/registry/specifications) for more information on the pyproject.toml file.

## Publish to the Registry

### Option 1: Comfy CLI

Run the command below to manually publish your node to the registry.

```bash
comfy node publish
```

You'll be prompted for the API key.

```bash
API Key for publisher '<publisher id>': ****************************************************

...Version 1.0.0 Published. 
See it here: https://registry.comfy.org/publisherId/your-node
```

<Warning>
  Keep in mind that the API key is hidden by default.
</Warning>

<Warning>
  When copy-pasting, your API key might have an additional \x16 at the back when using CTRL+V (for Windows), eg: \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\x16.

  It is recommended to copy-paste your API key via right-clicking instead.
</Warning>

### Option 2: Github Actions

Automatically publish your node through github actions.

<Steps>
  <Step title="Set up a Github Secret">
    Go to Settings -> Secrets and Variables -> Actions -> Under Secrets Tab and Repository secrets -> New Repository Secret.

    Create a secret called `REGISTRY_ACCESS_TOKEN` and store your API key as the value.
  </Step>

  <Step title="Create a Github Action">
    Copy the code below and paste it here `/.github/workflows/publish_action.yml`

    ```bash
    name: Publish to Comfy registry
    on:
      workflow_dispatch:
      push:
        branches:
          - main
        paths:
          - "pyproject.toml"

    jobs:
      publish-node:
        name: Publish Custom Node to registry
        runs-on: ubuntu-latest
        steps:
          - name: Check out code
            uses: actions/checkout@v4
          - name: Publish Custom Node
            uses: Comfy-Org/publish-node-action@main
            with:
              personal_access_token: ${{ secrets.REGISTRY_ACCESS_TOKEN }} ## Add your own personal access token to your Github Repository secrets and reference it here.
    ```

    <Warning>If your working branch is named something besides `main`, such as `master`, add the name under the branches section.</Warning>
  </Step>

  <Step title="Test the Github Action">
    Push an update to your `pyproject.toml`'s version number. You should see your updated node on the registry.

    <Tip>The github action will automatically run every time you push an update to your `pyproject.toml` file</Tip>
  </Step>
</Steps>


# pyproject.toml
Source: https://docs.comfy.org/registry/specifications



# Node ID

The node id ("name" field in toml file) uniquely identifies the custom node, and will be used in URLs from the registry. Users can also install the node by referencing the name.

`comfy node install <node-id>`

The node id must be less than 100 characters and can only contain alphanumeric characters, hyphens, underscores, and periods. There should not be consecutive special characters and the id cannot start with a number or special character.

Comparison of node ids is case-insensitive. See the official [python documentation](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#name) for more information.

We recommend using a short name for your node id, and don't include "ComfyUI" in the name.

# Version Number

The registry uses [semantic versioning](https://semver.org/) which indicates the specific release of a custom node through a three-digit version number X.Y.Z.

X - **MAJOR** change that breaks previous updates

Y - **MINOR** change that adds new features and is backwards compatible

Z - **PATCH** change that fixes a bug

# License

An optional field that expects a relative path to your license file (usually named `LICENSE` or `LICENSE.txt`).

* `license = { file = "LICENSE" }` ✅
* `license = "LICENSE"` ❌

Alternatively, it can also be referenced by name. Common licenses include [MIT](https://opensource.org/license/mit), [GPL](https://www.gnu.org/licenses/gpl-3.0.en.html), or [Apache](https://www.apache.org/licenses/LICENSE-2.0).

* `license = {text = "MIT License"}` ✅
* `license = "MIT LICENSE"` ❌

Read up more on toml file standards [here](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license)

# Publisher ID

The publisher id uniquely identifies a publisher and is ideally the same as your github username. It is also referred to as the username on the registry and can be found after the `@` symbol on the profile page.

# Icon

An optional field that expects a icon URL. It accepts extensions SVG, PNG, JPG or GIF with a maximum resolution of 800px by 400px.


# Standards
Source: https://docs.comfy.org/registry/standards

Security and other standards for publishing to the Registry

## Base Standards

### 1. Community Value

Custom nodes must provide valuable functionality to the ComfyUI community

Avoid:

* Excessive self-promotion
* Impersonation or misleading behavior
* Malicious behavior
* Self-promotion is permitted only within your designated settings menu section
* Top and side menus should contain only useful functionality

### 2. Node Compatibility

Do not interfere with other custom nodes' operations (installation, updates, removal)

* For dependencies on other custom nodes:
  * Display clear warnings when dependent functionality is used
  * Provide example workflows demonstrating required nodes

### 3. Legal Compliance

Must comply with all applicable laws and regulations

### 5. Quality Requirements

Nodes must be fully functional, well documented, and actively maintained.

### 6. Fork Guidelines

Forked nodes must:

* Have clearly distinct names from original
* Provide significant differences in functionality or code

Below are standards that must be met to publish custom nodes to the registry.

## Security Standards

Custom nodes should be secure. We will start working with custom nodes that violate these standards to be rewritten. If there is some major functionality that should be exposed by core, please request it in the [rfcs repo](https://github.com/comfy-org/rfcs).

### eval/exec Calls

#### Policy

The use of `eval` and `exec` functions is prohibited in custom nodes due to security concerns.

#### Reasoning

These functions can enable arbitrary code execution, creating potential Remote Code Execution (RCE) vulnerabilities when processing user inputs. Workflows containing nodes that pass user inputs into `eval` or `exec` could be exploited for various cyberattacks, including:

* Keylogging
* Ransomware
* Other malicious code execution

### subprocess for pip install

#### Policy

Runtime package installation through subprocess calls is not permitted.

#### Reasoning

* First item
  ComfyUI manager will ship with ComfyUI and lets the user install dependencies
* Centralized dependency management improves security and user experience
* Helps prevent potential supply chain attacks
* Eliminates need for multiple ComfyUI reloads

### Code Obfuscation

#### Policy

Code obfuscation is prohibited in custom nodes.

#### Reasoning

Obfuscated code:

* Impossible to review and likely to be malicious


# Node Definition JSON
Source: https://docs.comfy.org/specs/nodedef_json

JSON schema for a ComfyUI Node.

The node definition JSON is defined using [JSON Schema](https://json-schema.org/). Changes to this schema will be discussed in the [rfcs repo](https://github.com/comfy-org/rfcs).

## v2.0 (Latest)

```json Node Definition v2.0
{
  "$ref": "#/definitions/ComfyNodeDefV2",
  "definitions": {
    "ComfyNodeDefV2": {
      "type": "object",
      "properties": {
        "inputs": {
          "type": "object",
          "additionalProperties": {
            "anyOf": [
              {
                "type": "object",
                "properties": {
                  "default": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "array",
                        "items": {
                          "type": "number"
                        }
                      }
                    ]
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "min": {
                    "type": "number"
                  },
                  "max": {
                    "type": "number"
                  },
                  "step": {
                    "type": "number"
                  },
                  "display": {
                    "type": "string",
                    "enum": [
                      "slider",
                      "number",
                      "knob"
                    ]
                  },
                  "control_after_generate": {
                    "type": "boolean"
                  },
                  "type": {
                    "type": "string",
                    "const": "INT"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "array",
                        "items": {
                          "type": "number"
                        }
                      }
                    ]
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "min": {
                    "type": "number"
                  },
                  "max": {
                    "type": "number"
                  },
                  "step": {
                    "type": "number"
                  },
                  "display": {
                    "type": "string",
                    "enum": [
                      "slider",
                      "number",
                      "knob"
                    ]
                  },
                  "round": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "boolean",
                        "const": false
                      }
                    ]
                  },
                  "type": {
                    "type": "string",
                    "const": "FLOAT"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "type": "boolean"
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "label_on": {
                    "type": "string"
                  },
                  "label_off": {
                    "type": "string"
                  },
                  "type": {
                    "type": "string",
                    "const": "BOOLEAN"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "type": "string"
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "multiline": {
                    "type": "boolean"
                  },
                  "dynamicPrompts": {
                    "type": "boolean"
                  },
                  "defaultVal": {
                    "type": "string"
                  },
                  "placeholder": {
                    "type": "string"
                  },
                  "type": {
                    "type": "string",
                    "const": "STRING"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {},
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "control_after_generate": {
                    "type": "boolean"
                  },
                  "image_upload": {
                    "type": "boolean"
                  },
                  "image_folder": {
                    "type": "string",
                    "enum": [
                      "input",
                      "output",
                      "temp"
                    ]
                  },
                  "allow_batch": {
                    "type": "boolean"
                  },
                  "video_upload": {
                    "type": "boolean"
                  },
                  "remote": {
                    "type": "object",
                    "properties": {
                      "route": {
                        "anyOf": [
                          {
                            "type": "string",
                            "format": "uri"
                          },
                          {
                            "type": "string",
                            "pattern": "^\\/"
                          }
                        ]
                      },
                      "refresh": {
                        "anyOf": [
                          {
                            "type": "number",
                            "minimum": -9007199254740991,
                            "maximum": 9007199254740991
                          },
                          {
                            "type": "number",
                            "maximum": 9007199254740991,
                            "minimum": -9007199254740991
                          }
                        ]
                      },
                      "response_key": {
                        "type": "string"
                      },
                      "query_params": {
                        "type": "object",
                        "additionalProperties": {
                          "type": "string"
                        }
                      },
                      "refresh_button": {
                        "type": "boolean"
                      },
                      "control_after_refresh": {
                        "type": "string",
                        "enum": [
                          "first",
                          "last"
                        ]
                      },
                      "timeout": {
                        "type": "number",
                        "minimum": 0
                      },
                      "max_retries": {
                        "type": "number",
                        "minimum": 0
                      }
                    },
                    "required": [
                      "route"
                    ],
                    "additionalProperties": false
                  },
                  "options": {
                    "type": "array",
                    "items": {
                      "type": [
                        "string",
                        "number"
                      ]
                    }
                  },
                  "type": {
                    "type": "string",
                    "const": "COMBO"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {},
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "type": {
                    "type": "string"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              }
            ]
          }
        },
        "outputs": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "index": {
                "type": "number"
              },
              "name": {
                "type": "string"
              },
              "type": {
                "type": "string"
              },
              "is_list": {
                "type": "boolean"
              },
              "options": {
                "type": "array"
              },
              "tooltip": {
                "type": "string"
              }
            },
            "required": [
              "index",
              "name",
              "type",
              "is_list"
            ],
            "additionalProperties": false
          }
        },
        "hidden": {
          "type": "object",
          "additionalProperties": {}
        },
        "name": {
          "type": "string"
        },
        "display_name": {
          "type": "string"
        },
        "description": {
          "type": "string"
        },
        "category": {
          "type": "string"
        },
        "output_node": {
          "type": "boolean"
        },
        "python_module": {
          "type": "string"
        },
        "deprecated": {
          "type": "boolean"
        },
        "experimental": {
          "type": "boolean"
        }
      },
      "required": [
        "inputs",
        "outputs",
        "name",
        "display_name",
        "description",
        "category",
        "output_node",
        "python_module"
      ],
      "additionalProperties": false
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```


# Node Definition JSON 1.0
Source: https://docs.comfy.org/specs/nodedef_json_1_0

JSON schema for a ComfyUI Node.

## v1.0

```json Node Definition v1.0
{
  "$ref": "#/definitions/ComfyNodeDefV1",
  "definitions": {
    "ComfyNodeDefV1": {
      "type": "object",
      "properties": {
        "input": {
          "type": "object",
          "properties": {
            "required": {
              "type": "object",
              "additionalProperties": {
                "anyOf": [
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "INT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "FLOAT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "round": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "boolean",
                                    "const": false
                                  }
                                ]
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "BOOLEAN"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "boolean"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "label_on": {
                                "type": "string"
                              },
                              "label_off": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "STRING"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "string"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "multiline": {
                                "type": "boolean"
                              },
                              "dynamicPrompts": {
                                "type": "boolean"
                              },
                              "defaultVal": {
                                "type": "string"
                              },
                              "placeholder": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "array",
                        "items": {
                          "type": [
                            "string",
                            "number"
                          ]
                        }
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "COMBO"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  }
                ]
              }
            },
            "optional": {
              "type": "object",
              "additionalProperties": {
                "anyOf": [
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "INT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "FLOAT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "round": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "boolean",
                                    "const": false
                                  }
                                ]
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "BOOLEAN"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "boolean"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "label_on": {
                                "type": "string"
                              },
                              "label_off": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "STRING"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "string"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "multiline": {
                                "type": "boolean"
                              },
                              "dynamicPrompts": {
                                "type": "boolean"
                              },
                              "defaultVal": {
                                "type": "string"
                              },
                              "placeholder": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "array",
                        "items": {
                          "type": [
                            "string",
                            "number"
                          ]
                        }
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "COMBO"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  }
                ]
              }
            },
            "hidden": {
              "type": "object",
              "additionalProperties": {}
            }
          },
          "additionalProperties": false
        },
        "output": {
          "type": "array",
          "items": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "array",
                "items": {
                  "type": [
                    "string",
                    "number"
                  ]
                }
              }
            ]
          }
        },
        "output_is_list": {
          "type": "array",
          "items": {
            "type": "boolean"
          }
        },
        "output_name": {
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "output_tooltips": {
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "name": {
          "type": "string"
        },
        "display_name": {
          "type": "string"
        },
        "description": {
          "type": "string"
        },
        "category": {
          "type": "string"
        },
        "output_node": {
          "type": "boolean"
        },
        "python_module": {
          "type": "string"
        },
        "deprecated": {
          "type": "boolean"
        },
        "experimental": {
          "type": "boolean"
        }
      },
      "required": [
        "name",
        "display_name",
        "description",
        "category",
        "output_node",
        "python_module"
      ],
      "additionalProperties": false
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```


# Workflow JSON
Source: https://docs.comfy.org/specs/workflow_json

JSON schema for a ComfyUI workflow.

The workflow JSON is defined using [JSON Schema](https://json-schema.org/). Changes to this schema will be discussed in the [rfcs repo](https://github.com/comfy-org/rfcs).

## Version 1.0 (Latest)

```json ComfyUI Workflow v1.0
{
  "$ref": "#/definitions/ComfyWorkflow1_0",
  "definitions": {
    "ComfyWorkflow1_0": {
      "type": "object",
      "properties": {
        "version": {
          "type": "number",
          "const": 1
        },
        "config": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "links_ontop": {
                      "type": "boolean"
                    },
                    "align_to_grid": {
                      "type": "boolean"
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "state": {
          "type": "object",
          "properties": {
            "lastGroupid": {
              "type": "number"
            },
            "lastNodeId": {
              "type": "number"
            },
            "lastLinkId": {
              "type": "number"
            },
            "lastRerouteId": {
              "type": "number"
            }
          },
          "additionalProperties": true
        },
        "groups": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "title": {
                "type": "string"
              },
              "bounding": {
                "type": "array",
                "minItems": 4,
                "maxItems": 4,
                "items": [
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "font_size": {
                "type": "number"
              },
              "locked": {
                "type": "boolean"
              }
            },
            "required": [
              "title",
              "bounding"
            ],
            "additionalProperties": true
          }
        },
        "nodes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "type": "string"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "size": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "flags": {
                "type": "object",
                "properties": {
                  "collapsed": {
                    "type": "boolean"
                  },
                  "pinned": {
                    "type": "boolean"
                  },
                  "allow_interaction": {
                    "type": "boolean"
                  },
                  "horizontal": {
                    "type": "boolean"
                  },
                  "skip_repeated_outputs": {
                    "type": "boolean"
                  }
                },
                "additionalProperties": true
              },
              "order": {
                "type": "number"
              },
              "mode": {
                "type": "number"
              },
              "inputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "link": {
                      "type": [
                        "number",
                        "null"
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "outputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "links": {
                      "anyOf": [
                        {
                          "type": "array",
                          "items": {
                            "type": "number"
                          }
                        },
                        {
                          "type": "null"
                        }
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "properties": {
                "type": "object",
                "properties": {
                  "Node name for S&R": {
                    "type": "string"
                  }
                },
                "additionalProperties": true
              },
              "widgets_values": {
                "anyOf": [
                  {
                    "type": "array"
                  },
                  {
                    "type": "object",
                    "additionalProperties": {}
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "bgcolor": {
                "type": "string"
              }
            },
            "required": [
              "id",
              "type",
              "pos",
              "size",
              "flags",
              "order",
              "mode",
              "properties"
            ],
            "additionalProperties": true
          }
        },
        "links": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "type": "number"
              },
              "origin_id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "origin_slot": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "target_id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "target_slot": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "anyOf": [
                  {
                    "type": "string"
                  },
                  {
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "parentId": {
                "type": "number"
              }
            },
            "required": [
              "id",
              "origin_id",
              "origin_slot",
              "target_id",
              "target_slot",
              "type"
            ],
            "additionalProperties": true
          }
        },
        "reroutes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "type": "number"
              },
              "parentId": {
                "type": "number"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "linkIds": {
                "anyOf": [
                  {
                    "type": "array",
                    "items": {
                      "type": "number"
                    }
                  },
                  {
                    "type": "null"
                  }
                ]
              }
            },
            "required": [
              "id",
              "pos"
            ],
            "additionalProperties": true
          }
        },
        "extra": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "ds": {
                      "type": "object",
                      "properties": {
                        "scale": {
                          "type": "number"
                        },
                        "offset": {
                          "anyOf": [
                            {
                              "type": "object",
                              "properties": {
                                "0": {
                                  "type": "number"
                                },
                                "1": {
                                  "type": "number"
                                }
                              },
                              "required": [
                                "0",
                                "1"
                              ],
                              "additionalProperties": true
                            },
                            {
                              "type": "array",
                              "minItems": 2,
                              "maxItems": 2,
                              "items": [
                                {
                                  "type": "number"
                                },
                                {
                                  "type": "number"
                                }
                              ]
                            }
                          ]
                        }
                      },
                      "required": [
                        "scale",
                        "offset"
                      ],
                      "additionalProperties": true
                    },
                    "info": {
                      "type": "object",
                      "properties": {
                        "name": {
                          "type": "string"
                        },
                        "author": {
                          "type": "string"
                        },
                        "description": {
                          "type": "string"
                        },
                        "version": {
                          "type": "string"
                        },
                        "created": {
                          "type": "string"
                        },
                        "modified": {
                          "type": "string"
                        },
                        "software": {
                          "type": "string"
                        }
                      },
                      "required": [
                        "name",
                        "author",
                        "description",
                        "version",
                        "created",
                        "modified",
                        "software"
                      ],
                      "additionalProperties": true
                    },
                    "linkExtensions": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          }
                        },
                        "required": [
                          "id",
                          "parentId"
                        ],
                        "additionalProperties": true
                      }
                    },
                    "reroutes": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          },
                          "pos": {
                            "anyOf": [
                              {
                                "type": "object",
                                "properties": {
                                  "0": {
                                    "type": "number"
                                  },
                                  "1": {
                                    "type": "number"
                                  }
                                },
                                "required": [
                                  "0",
                                  "1"
                                ],
                                "additionalProperties": true
                              },
                              {
                                "type": "array",
                                "minItems": 2,
                                "maxItems": 2,
                                "items": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "number"
                                  }
                                ]
                              }
                            ]
                          },
                          "linkIds": {
                            "anyOf": [
                              {
                                "type": "array",
                                "items": {
                                  "type": "number"
                                }
                              },
                              {
                                "type": "null"
                              }
                            ]
                          }
                        },
                        "required": [
                          "id",
                          "pos"
                        ],
                        "additionalProperties": true
                      }
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "models": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": {
                "type": "string"
              },
              "url": {
                "type": "string",
                "format": "uri"
              },
              "hash": {
                "type": "string"
              },
              "hash_type": {
                "type": "string"
              },
              "directory": {
                "type": "string"
              }
            },
            "required": [
              "name",
              "url",
              "directory"
            ],
            "additionalProperties": false
          }
        }
      },
      "required": [
        "version",
        "state",
        "nodes"
      ],
      "additionalProperties": true
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```

## Older versions

* [0.4](./workflow_json_0.4.mdx)


# Workflow JSON 0.4
Source: https://docs.comfy.org/specs/workflow_json_0.4

JSON schema for a ComfyUI workflow.

## v0.4

```json
{
  "$ref": "#/definitions/ComfyWorkflow0_4",
  "definitions": {
    "ComfyWorkflow0_4": {
      "type": "object",
      "properties": {
        "last_node_id": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "string"
            }
          ]
        },
        "last_link_id": {
          "type": "number"
        },
        "nodes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "type": "string"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "size": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "flags": {
                "type": "object",
                "properties": {
                  "collapsed": {
                    "type": "boolean"
                  },
                  "pinned": {
                    "type": "boolean"
                  },
                  "allow_interaction": {
                    "type": "boolean"
                  },
                  "horizontal": {
                    "type": "boolean"
                  },
                  "skip_repeated_outputs": {
                    "type": "boolean"
                  }
                },
                "additionalProperties": true
              },
              "order": {
                "type": "number"
              },
              "mode": {
                "type": "number"
              },
              "inputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "link": {
                      "type": [
                        "number",
                        "null"
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "outputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "links": {
                      "anyOf": [
                        {
                          "type": "array",
                          "items": {
                            "type": "number"
                          }
                        },
                        {
                          "type": "null"
                        }
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "properties": {
                "type": "object",
                "properties": {
                  "Node name for S&R": {
                    "type": "string"
                  }
                },
                "additionalProperties": true
              },
              "widgets_values": {
                "anyOf": [
                  {
                    "type": "array"
                  },
                  {
                    "type": "object",
                    "additionalProperties": {}
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "bgcolor": {
                "type": "string"
              }
            },
            "required": [
              "id",
              "type",
              "pos",
              "size",
              "flags",
              "order",
              "mode",
              "properties"
            ],
            "additionalProperties": true
          }
        },
        "links": {
          "type": "array",
          "items": {
            "type": "array",
            "minItems": 6,
            "maxItems": 6,
            "items": [
              {
                "type": "number"
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "string"
                  },
                  {
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  {
                    "type": "number"
                  }
                ]
              }
            ]
          }
        },
        "groups": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "title": {
                "type": "string"
              },
              "bounding": {
                "type": "array",
                "minItems": 4,
                "maxItems": 4,
                "items": [
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "font_size": {
                "type": "number"
              },
              "locked": {
                "type": "boolean"
              }
            },
            "required": [
              "title",
              "bounding"
            ],
            "additionalProperties": true
          }
        },
        "config": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "links_ontop": {
                      "type": "boolean"
                    },
                    "align_to_grid": {
                      "type": "boolean"
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "extra": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "ds": {
                      "type": "object",
                      "properties": {
                        "scale": {
                          "type": "number"
                        },
                        "offset": {
                          "anyOf": [
                            {
                              "type": "object",
                              "properties": {
                                "0": {
                                  "type": "number"
                                },
                                "1": {
                                  "type": "number"
                                }
                              },
                              "required": [
                                "0",
                                "1"
                              ],
                              "additionalProperties": true
                            },
                            {
                              "type": "array",
                              "minItems": 2,
                              "maxItems": 2,
                              "items": [
                                {
                                  "type": "number"
                                },
                                {
                                  "type": "number"
                                }
                              ]
                            }
                          ]
                        }
                      },
                      "required": [
                        "scale",
                        "offset"
                      ],
                      "additionalProperties": true
                    },
                    "info": {
                      "type": "object",
                      "properties": {
                        "name": {
                          "type": "string"
                        },
                        "author": {
                          "type": "string"
                        },
                        "description": {
                          "type": "string"
                        },
                        "version": {
                          "type": "string"
                        },
                        "created": {
                          "type": "string"
                        },
                        "modified": {
                          "type": "string"
                        },
                        "software": {
                          "type": "string"
                        }
                      },
                      "required": [
                        "name",
                        "author",
                        "description",
                        "version",
                        "created",
                        "modified",
                        "software"
                      ],
                      "additionalProperties": true
                    },
                    "linkExtensions": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          }
                        },
                        "required": [
                          "id",
                          "parentId"
                        ],
                        "additionalProperties": true
                      }
                    },
                    "reroutes": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          },
                          "pos": {
                            "anyOf": [
                              {
                                "type": "object",
                                "properties": {
                                  "0": {
                                    "type": "number"
                                  },
                                  "1": {
                                    "type": "number"
                                  }
                                },
                                "required": [
                                  "0",
                                  "1"
                                ],
                                "additionalProperties": true
                              },
                              {
                                "type": "array",
                                "minItems": 2,
                                "maxItems": 2,
                                "items": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "number"
                                  }
                                ]
                              }
                            ]
                          },
                          "linkIds": {
                            "anyOf": [
                              {
                                "type": "array",
                                "items": {
                                  "type": "number"
                                }
                              },
                              {
                                "type": "null"
                              }
                            ]
                          }
                        },
                        "required": [
                          "id",
                          "pos"
                        ],
                        "additionalProperties": true
                      }
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "version": {
          "type": "number"
        },
        "models": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": {
                "type": "string"
              },
              "url": {
                "type": "string",
                "format": "uri"
              },
              "hash": {
                "type": "string"
              },
              "hash_type": {
                "type": "string"
              },
              "directory": {
                "type": "string"
              }
            },
            "required": [
              "name",
              "url",
              "directory"
            ],
            "additionalProperties": false
          }
        }
      },
      "required": [
        "last_node_id",
        "last_link_id",
        "nodes",
        "links",
        "version"
      ],
      "additionalProperties": true
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```


# ComfyUI Hunyuan3D-2 Examples
Source: https://docs.comfy.org/tutorials/3d/hunyuan3D-2

This guide will demonstrate how to use Hunyuan3D-2 in ComfyUI to generate 3D assets.

# Hunyuan3D 2.0 Introduction

![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-1.gif)
![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-2.gif)

[Hunyuan3D 2.0](https://github.com/Tencent/Hunyuan3D-2) is an open-source 3D asset generation model released by Tencent, capable of generating high-fidelity 3D models with high-resolution texture maps through text or images.

Hunyuan3D 2.0 adopts a two-stage generation approach, first generating a geometry model without textures, then synthesizing high-resolution texture maps. This effectively separates the complexity of shape and texture generation. Below are the two core components of Hunyuan3D 2.0:

1. **Geometry Generation Model (Hunyuan3D-DiT)**: Based on a flow diffusion Transformer architecture, it generates untextured geometric models that precisely match input conditions.
2. **Texture Generation Model (Hunyuan3D-Paint)**: Combines geometric conditions and multi-view diffusion techniques to add high-resolution textures to models, supporting PBR materials.

**Key Advantages**

* **High-Precision Generation**: Sharp geometric structures, rich texture colors, support for PBR material generation, achieving near-realistic lighting effects.
* **Diverse Usage Methods**: Provides code calls, Blender plugins, Gradio applications, and online experience through the official website, suitable for different user needs.
* **Lightweight and Compatibility**: The Hunyuan3D-2mini model requires only 5GB VRAM, the standard version needs 6GB VRAM for shape generation, and the complete process (shape + texture) requires only 12GB VRAM.

Recently (March 18, 2025), Hunyuan3D 2.0 also introduced a multi-view shape generation model (Hunyuan3D-2mv), which supports generating more detailed geometric structures from inputs at different angles.

This example includes three workflows:

* Using Hunyuan3D-2mv with multiple view inputs to generate 3D models
* Using Hunyuan3D-2mv-turbo with multiple view inputs to generate 3D models
* Using Hunyuan3D-2 with a single view input to generate 3D models

<Tip>
  ComfyUI now natively supports Hunyuan3D-2mv, but does not yet support texture and material generation. Please make sure you have updated to the latest version of [ComfyUI](https://github.com/comfyanonymous/ComfyUI) before starting.

  The workflow example PNG images in this tutorial contain workflow JSON in their metadata:

  * You can drag them directly into ComfyUI
  * Or use the menu `Workflows` -> `Open (ctrl+o)`

  This will load the corresponding workflow and prompt you to download the required models. The generated `.glb` format models will be output to the `ComfyUI/output/mesh` folder.
</Tip>

## ComfyUI Hunyuan3D-2mv Workflow Example

In the Hunyuan3D-2mv workflow, we'll use multi-view images to generate a 3D model. Note that multiple view images are not mandatory in this workflow - you can use only the `front` view image to generate a 3D model.

### 1. Workflow

Please download the images below and drag  into ComfyUI to load the workflow.
![Hunyuan3D-2mv workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/hunyuan-3d-multiview-elf.webp)

Download the images below we will use them as input images.

<div class="flex space-x-4">
  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/front.png" alt="input image" class="w-1/3" />

  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/left.png" alt="input image" class="w-1/3" />

  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/back.png" alt="input image" class="w-1/3" />
</div>

<Tip>
  In this example, the input images have already been preprocessed to remove excess background. In actual use, you can use custom nodes like [ComfyUI\_essentials](https://github.com/cubiq/ComfyUI_essentials) to automatically remove excess background.
</Tip>

### 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

* hunyuan3d-dit-v2-mv: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv.safetensors`

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2-mv.safetensors  // renamed file
```

### 3. Steps to Run the Workflow

![ComfyUI hunyuan3d\_2mv](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg)

1. Ensure that the Image Only Checkpoint Loader(img2vid model) has loaded our downloaded and renamed `hunyuan3d-dit-v2-mv.safetensors` model
2. Load the corresponding view images in each of the `Load Image` nodes
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

If you need to add more views, make sure to load other view images in the `Hunyuan3Dv2ConditioningMultiView` node, and ensure that you load the corresponding view images in the `Load Image` nodes.

## Hunyuan3D-2mv-turbo Workflow

In the Hunyuan3D-2mv-turbo workflow, we'll use the Hunyuan3D-2mv-turbo model to generate 3D models. This model is a step distillation version of Hunyuan3D-2mv, allowing for faster 3D model generation. In this version of the workflow, we set `cfg` to 1.0 and add a `flux guidance` node to control the `distilled cfg` generation.

### 1. Workflow

Please download the images below and drag into ComfyUI to load the workflow.

![Hunyuan3D-2mv-turbo workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/hunyuan-3d-turbo.webp)

Download the images below we will use them as input images.

<div class="flex space-x-4">
  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/front.png" alt="input image" class="w-1/2" />

  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/right.png" alt="input image" class="w-1/2" />
</div>

### 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

* hunyuan3d-dit-v2-mv-turbo: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv-turbo/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv-turbo.safetensors`

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2-mv-turbo.safetensors  // renamed file
```

### 3. Steps to Run the Workflow

![ComfyUI hunyuan3d\_2mv\_turbo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg)

1. Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2-mv-turbo.safetensors` model
2. Load the corresponding view images in each of the `Load Image` nodes
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Hunyuan3D-2 Single View Workflow

In the Hunyuan3D-2 workflow, we'll use the Hunyuan3D-2 model to generate 3D models. This model is not a multi-view model. In this workflow, we use the `Hunyuan3Dv2Conditioning` node instead of the `Hunyuan3Dv2ConditioningMultiView` node.

### 1. Workflow

Please download the image below and drag it into ComfyUI to load the workflow.

![Hunyuan3D-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d-non-multiview-train.webp)

Download the image below we will use it as input image.
![ComfyUI Hunyuan 3D 2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan_3d_v2_non_multiview_train.png)

### 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

* hunyuan3d-dit-v2-0: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2/resolve/main/hunyuan3d-dit-v2-0/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2.safetensors`

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2.safetensors  // renamed file
```

### 3. Steps to Run the Workflow

![ComfyUI hunyuan3d\_2](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg)

1. Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2.safetensors` model
2. Load the image in the `Load Image` node
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Community Resources

Below are ComfyUI community resources related to Hunyuan3D-2

* [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)
* [Kijai/Hunyuan3D-2\_safetensors](https://huggingface.co/Kijai/Hunyuan3D-2_safetensors/tree/main)
* [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)

## Hunyuan3D 2.0 Open-Source Model Series

Currently, Hunyuan3D 2.0 has open-sourced multiple models covering the complete 3D generation process. You can visit [Hunyuan3D-2](https://github.com/Tencent/Hunyuan3D-2) for more information.

**Hunyuan3D-2mini Series**

| Model                 | Description               | Date       | Parameters | Huggingface                                                                             |
| --------------------- | ------------------------- | ---------- | ---------- | --------------------------------------------------------------------------------------- |
| Hunyuan3D-DiT-v2-mini | Mini Image to Shape Model | 2025-03-18 | 0.6B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini) |

**Hunyuan3D-2mv Series**

| Model                    | Description                                                                                                 | Date       | Parameters | Huggingface                                                                              |
| ------------------------ | ----------------------------------------------------------------------------------------------------------- | ---------- | ---------- | ---------------------------------------------------------------------------------------- |
| Hunyuan3D-DiT-v2-mv-Fast | Guidance Distillation Version, can halve DIT inference time                                                 | 2025-03-18 | 1.1B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast) |
| Hunyuan3D-DiT-v2-mv      | Multi-view Image to Shape Model, suitable for 3D creation requiring multiple angles to understand the scene | 2025-03-18 | 1.1B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv)      |

**Hunyuan3D-2 Series**

| Model                   | Description                 | Date       | Parameters | Huggingface                                                                           |
| ----------------------- | --------------------------- | ---------- | ---------- | ------------------------------------------------------------------------------------- |
| Hunyuan3D-DiT-v2-0-Fast | Guidance Distillation Model | 2025-02-03 | 1.1B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast) |
| Hunyuan3D-DiT-v2-0      | Image to Shape Model        | 2025-01-21 | 1.1B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0)      |
| Hunyuan3D-Paint-v2-0    | Texture Generation Model    | 2025-01-21 | 1.3B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0)    |
| Hunyuan3D-Delight-v2-0  | Image Delight Model         | 2025-01-21 | 1.3B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0)  |


# ComfyUI Native HiDream-I1 Text-to-Image Workflow Example
Source: https://docs.comfy.org/tutorials/advanced/hidream

This guide will walk you through completing a ComfyUI native HiDream-I1 text-to-image workflow example

![HiDream-I1 Demo](https://raw.githubusercontent.com/HiDream-ai/HiDream-I1/main/assets/demo.jpg)

HiDream-I1 is a text-to-image model officially open-sourced by HiDream-ai on April 7, 2025. The model has 17B parameters and is released under the [MIT license](https://github.com/HiDream-ai/HiDream-I1/blob/main/LICENSE), supporting personal projects, scientific research, and commercial use.
It currently performs excellently in multiple benchmark tests.

## Model Features

**Hybrid Architecture Design**
A combination of Diffusion Transformer (DiT) and Mixture of Experts (MoE) architecture:

* Based on Diffusion Transformer (DiT), with dual-stream MMDiT modules processing multimodal information and single-stream DiT modules optimizing global consistency.
* Dynamic routing mechanism flexibly allocates computing resources, enhancing complex scene processing capabilities and delivering excellent performance in color restoration, edge processing, and other details.

**Multimodal Text Encoder Integration**
Integrates four text encoders:

* OpenCLIP ViT-bigG, OpenAI CLIP ViT-L (visual semantic alignment)
* T5-XXL (long text parsing)
* Llama-3.1-8B-Instruct (instruction understanding)
  This combination achieves SOTA performance in complex semantic parsing of colors, quantities, spatial relationships, etc., with Chinese prompt support significantly outperforming similar open-source models.

**Original Model Versions**

HiDream-ai provides three versions of the HiDream-I1 model to meet different needs. Below are the links to the original model repositories:

| Model Name      | Description    | Inference Steps | Repository Link                                                         |
| --------------- | -------------- | --------------- | ----------------------------------------------------------------------- |
| HiDream-I1-Full | Full version   | 50              | [🤗 HiDream-I1-Full](https://huggingface.co/HiDream-ai/HiDream-I1-Full) |
| HiDream-I1-Dev  | Distilled dev  | 28              | [🤗 HiDream-I1-Dev](https://huggingface.co/HiDream-ai/HiDream-I1-Dev)   |
| HiDream-I1-Fast | Distilled fast | 16              | [🤗 HiDream-I1-Fast](https://huggingface.co/HiDream-ai/HiDream-I1-Fast) |

## About This Workflow Example

In this example, we will use the repackaged version from ComfyOrg. You can find all the model files we'll use in this example in the [HiDream-I1\_ComfyUI](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/) repository.

<Tip>
  Before starting, please update your ComfyUI version to ensure it's at least after this [commit](https://github.com/comfyanonymous/ComfyUI/commit/9ad792f92706e2179c58b2e5348164acafa69288) to make sure your ComfyUI has native support for HiDream
</Tip>

## HiDream-I1 Workflow

The model requirements for different ComfyUI native HiDream-I1 workflows are basically the same, with only the [diffusion models](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files/diffusion_models) files being different.

If you don't know which version to choose, please refer to the following suggestions:

* **HiDream-I1-Full** can generate the highest quality images
* **HiDream-I1-Dev** balances high-quality image generation with speed
* **HiDream-I1-Fast** can generate images in just 16 steps, suitable for scenarios requiring real-time iteration

For the **dev** and **fast** versions, negative prompts are not needed, so please set the `cfg` parameter to `1.0` during sampling. We have noted the corresponding parameter settings in the relevant workflows.

<Tip>
  The full versions of all three versions require a lot of VRAM - you may need more than 27GB of VRAM to run them smoothly. In the corresponding workflow tutorials,
  we will use the **fp8** version as a demonstration example to ensure that most users can run it smoothly.
  However, we will still provide download links for different versions of the model in the corresponding examples, and you can choose the appropriate file based on your VRAM situation.
</Tip>

### Model Installation

The following model files are common files that we will use.
Please click on the corresponding links to download and save them according to the model file save location.
We will guide you to download the corresponding **diffusion models** in the corresponding workflows.

**text\_encoders**：

* [clip\_l\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)
* [clip\_g\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)
* [t5xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) This model has been used in many workflows, you may have already downloaded this file.
* [llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)

**VAE**

* [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) This is Flux's VAE model, if you have used Flux's workflow before, you may have already downloaded this file.

**diffusion models**
We will guide you to download the corresponding model files in the corresponding workflows.

Model file save location

```
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 text_encoders/
│   │   ├─── clip_l_hidream.safetensors
│   │   ├─── clip_g_hidream.safetensors
│   │   ├─── t5xxl_fp8_e4m3fn_scaled.safetensors
│   │   └─── llama_3.1_8b_instruct_fp8_scaled.safetensors
│   └── 📂 vae/
│   │   └── ae.safetensors
│   └── 📂 diffusion_models/
│       └── ...               # We will guide you to install in the corresponding version workflow       
```

### HiDream-I1 Full Version Workflow

#### 1. Model File Download

Please select the appropriate version based on your hardware. Click the link and download the corresponding model file to save it to the `ComfyUI/models/diffusion_models/` folder.

* FP8 version: [hidream\_i1\_full\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp8.safetensors?download=true) requires more than 16GB of VRAM
* Full version: [hidream\_i1\_full\_f16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp16.safetensors?download=true) requires more than 27GB of VRAM

#### 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow
![HiDream-I1 Full Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_full.png)

#### 3. Complete the Workflow Step by Step

![HiDream-I1 Full Version Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg)

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_full_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **full** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`
5. For the `Ksampler` node, you need to make the following settings
   * Set `steps` to `50`
   * Set `cfg` to `5.0`
   * (Optional) Set `sampler` to `lcm`
   * (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

### HiDream-I1 Dev Version Workflow

#### 1. Model File Download

Please select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.

* FP8 version: [hidream\_i1\_dev\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_fp8.safetensors?download=true) requires more than 16GB of VRAM
* Full version: [hidream\_i1\_dev\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_bf16.safetensors?download=true) requires more than 27GB of VRAM

#### 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

![HiDream-I1 Dev Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_dev.png)

#### 3. Complete the Workflow Step by Step

![HiDream-I1 Dev Version Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg)
Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_dev_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **dev** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `6.0`
5. For the `Ksampler` node, you need to make the following settings
   * Set `steps` to `28`
   * (Important) Set `cfg` to `1.0`
   * (Optional) Set `sampler` to `lcm`
   * (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

### HiDream-I1 Fast Version Workflow

#### 1. Model File Download

Please select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.

* FP8 version: [hidream\_i1\_fast\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) requires more than 16GB of VRAM
* Full version: [hidream\_i1\_fast\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) requires more than 27GB of VRAM

#### 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

![HiDream-I1 Fast Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_fast.png)

#### 3. Complete the Workflow Step by Step

![HiDream-I1 Fast Version Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg)

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_fast_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **fast** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`
5. For the `Ksampler` node, you need to make the following settings
   * Set `steps` to `16`
   * (Important) Set `cfg` to `1.0`
   * (Optional) Set `sampler` to `lcm`
   * (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Other Related Resources

### GGUF Version Models

* [HiDream-I1-Full-gguf](https://huggingface.co/city96/HiDream-I1-Full-gguf)
* [HiDream-I1-Dev-gguf](https://huggingface.co/city96/HiDream-I1-Dev-gguf)

You need to use the “Unet Loader (GGUF)” node in City96's [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF) to replace the “Load Diffusion Model” node.

### NF4 Version Models

* [HiDream-I1-nf4](https://github.com/hykilpikonna/HiDream-I1-nf4)
* Use the [ComfyUI-HiDream-Sampler](https://github.com/SanDiegoDude/ComfyUI-HiDream-Sampler) node to use the NF4 version model.


# ComfyUI Image to Image Workflow
Source: https://docs.comfy.org/tutorials/basic/image-to-image

This guide will help you understand and complete an image to image workflow

## What is Image to Image

Image to Image is a workflow in ComfyUI that allows users to input an image and generate a new image based on it.

Image to Image can be used in scenarios such as:

* Converting original image styles, like transforming realistic photos into artistic styles
* Converting line art into realistic images
* Image restoration
* Colorizing old photos
* ... and other scenarios

To explain it with an analogy:
It's like asking an artist to create a specific piece based on your reference image.

If you carefully compare this tutorial with the [Text to Image](/tutorials/basic/text-to-image) tutorial,
you'll notice that the Image to Image process is very similar to Text to Image,
just with an additional input reference image as a condition. In Text to Image, we let the artist (image model) create freely based on our prompts,
while in Image to Image, we let the artist create based on both our reference image and prompts.

## ComfyUI Image to Image Workflow Example Guide

### Model Installation

Download the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) file and put it in your `ComfyUI/models/checkpoints` folder.

### Image to Image Workflow and Input Image

Download the image below and **drag it into ComfyUI** to load the workflow:
![Image to Image Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image_to_image.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

Download the image below and we will use it as the input image:
![Example Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/input.jpeg)

### Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

![ComfyUI Image to Image Workflow - Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image-to-image-02-guide.jpg)

1. Ensure `Load Checkpoint` loads  **v1-5-pruned-emaonly-fp16.safetensors**
2. Upload the input image to the `Load Image` node
3. Click `Queue` or press `Ctrl/Cmd + Enter` to generate

## Key Points of Image to Image Workflow

The key to the Image to Image workflow lies in the `denoise` parameter in the `KSampler` node, which should be **less than 1**

If you've adjusted the `denoise` parameter and generated images, you'll notice:

* The smaller the `denoise` value, the smaller the difference between the generated image and the reference image
* The larger the `denoise` value, the larger the difference between the generated image and the reference image

This is because `denoise` determines the strength of noise added to the latent space image after converting the reference image. If `denoise` is 1, the latent space image will become completely random noise, making it the same as the latent space generated by the `empty latent image` node, losing all characteristics of the reference image.

For the corresponding principles, please refer to the principle explanation in the [Text to Image](/tutorials/basic/text-to-image) tutorial.

## Try It Yourself

1. Try modifying the `denoise` parameter in the **KSampler** node, gradually changing it from 1 to 0, and observe the changes in the generated images
2. Replace with your own prompts and reference images to generate your own image effects


# ComfyUI Inpainting Workflow
Source: https://docs.comfy.org/tutorials/basic/inpaint

This guide will introduce you to the inpainting workflow in ComfyUI, walk you through an inpainting example, and cover topics like using the mask editor

This article will introduce the concept of inpainting in AI image generation and guide you through creating an inpainting workflow in ComfyUI. We'll cover:

* Using inpainting workflows to modify images
* Using the ComfyUI mask editor to draw masks
* `VAE Encoder (for Inpainting)` node

## About Inpainting

In AI image generation, we often encounter situations where we're satisfied with the overall image but there are elements we don't want or that contain errors. Simply regenerating might produce a completely different image, so using inpainting to fix specific parts becomes very useful.

It's like having an **artist (AI model)** paint a picture, but we're still not satisfied with the specific details. We need to tell the artist **which areas to adjust (mask)**, and then let them **repaint (inpaint)** according to our requirements.

Common inpainting scenarios include:

* **Defect Repair:** Removing unwanted objects, fixing incorrect AI-generated body parts, etc.
* **Detail Optimization:** Precisely adjusting local elements (like modifying clothing textures, adjusting facial expressions)
* And other scenarios

## ComfyUI Inpainting Workflow Example

### Model and Resource Preparation

#### 1. Model Installation

Download the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)  file and put it in your `ComfyUI/models/checkpoints` folder:

#### 2. Inpainting Asset

Please download the following image which we'll use as input:

![ComfyUI Inpainting Input Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/input.png)

<Note>This image already contains an alpha channel (transparency mask), so you don't need to manually draw a mask. This tutorial will also cover how to use the mask editor to draw masks.</Note>

#### 3. Inpainting Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

![ComfyUI Inpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

### ComfyUI Inpainting Workflow Example Explanation

Follow the steps in the diagram below to ensure the workflow runs correctly.

![ComfyUI Inpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_workflow.png)

1. Ensure `Load Checkpoint` loads `512-inpainting-ema.safetensors`
2. Upload the input image to the `Load Image` node
3. Click `Queue` or use `Ctrl + Enter` to generate

![Inpainting Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png)

For comparison, here's the result using the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model:

![SD1.5 Inpainting Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png)

You will find that the results generated by the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) model have better inpainting effects and more natural transitions.
This is because this model is specifically designed for inpainting, which helps us better control the generation area, resulting in improved inpainting effects.

Do you remember the analogy we've been using? Different models are like artists with varying abilities, but each artist has their own limits. Choosing the right model can help you achieve better generation results.

You can try these approaches to achieve better results:

1. Modify positive and negative prompts with more specific descriptions
2. Try multiple runs using different seeds in the `KSampler` for different generation results
3. After learning about the mask editor in this tutorial, you can re-inpaint the generated results to achieve satisfactory outcomes.

Next, we'll learn about using the **Mask Editor**. While our input image already includes an `alpha` transparency channel (the area we want to edit),
so manual mask drawing isn't necessary, you'll often use the Mask Editor to create masks in practical applications.

### Using the Mask Editor

First right-click the `Save Image` node and select `Copy(Clipspace)`:

![Copy Image to Clipboard](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png)

Then right-click the **Load Image** node and select `Paste(Clipspace)`:

![Paste Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png)

Right-click the **Load Image** node again and select `Open in MaskEditor`:

![Open Mask Editor](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg)

![Mask Editor Demo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint-maskeditor.gif)

1. Adjust brush parameters on the right panel
2. Use eraser to correct mistakes
3. Click `Save` when finished

The drawn content will be used as a Mask input to the VAE Encoder (for Inpainting) node for encoding

Then try adjusting your prompts and generating again until you achieve satisfactory results.

## VAE Encoder (for Inpainting) Node

Comparing this workflow with [Text-to-Image](/tutorials/basic/text-to-image) and [Image-to-Image](/tutorials/basic/image-to-image), you'll notice the main differences are in the VAE section's conditional inputs.
In this workflow, we use the **VAE Encoder (for Inpainting)** node, specifically designed for inpainting to help us better control the generation area and achieve better results.

![VAE Encoder (for Inpainting) Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/lantent/inpaint/vae_encode_for_inpainting.jpg)

**Input Types**

| Parameter Name | Function                                                                                                                                              |
| -------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| `pixels`       | Input image to be encoded into latent space.                                                                                                          |
| `vae`          | VAE model used to encode the image from pixel space to latent space.                                                                                  |
| `mask`         | Image mask specifying which areas need modification.                                                                                                  |
| `grow_mask_by` | Pixel value to expand the original mask outward, ensuring a transition area around the mask to avoid hard edges between inpainted and original areas. |

**Output Types**

| Parameter Name | Function                                    |
| -------------- | ------------------------------------------- |
| `latent`       | Image encoded into latent space by the VAE. |


# ComfyUI LoRA Example
Source: https://docs.comfy.org/tutorials/basic/lora

This guide will help you understand and use a single LoRA model

**LoRA (Low-Rank Adaptation)** is an efficient technique for fine-tuning large generative models like Stable Diffusion.
It introduces trainable low-rank matrices to the pre-trained model, adjusting only a portion of parameters rather than retraining the entire model,
thus achieving optimization for specific tasks at a lower computational cost.
Compared to base models like SD1.5, LoRA models are smaller and easier to train.

![LoRA Model vs Base Model Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/compare.png)

The image above compares generation with the same parameters using [dreamshaper\_8](https://civitai.com/models/4384?modelVersionId=128713) directly versus using the [blindbox\_V1Mix](https://civitai.com/models/25995/blindbox) LoRA model.
As you can see, by using a LoRA model, we can generate images in different styles without adjusting the base model.

We will demonstrate how to use a LoRA model. All LoRA variants: Lycoris, loha, lokr, locon, etc... are used in the same way.

In this example, we will learn how to load and use a LoRA model in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), covering the following topics:

1. Installing a LoRA model
2. Generating images using a LoRA model
3. A simple introduction to the `Load LoRA` node

## Required Model Installation

Download the [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder.

Download the [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model\&format=SafeTensor\&size=full\&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

## LoRA Workflow File

Download the image below and **drag it into ComfyUI** to load the workflow.
![ComfyUI Workflow - LoRA](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/lora.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

## Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

![ComfyUI Workflow - LoRA Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/flow_diagram.png)

1. Ensure `Load Checkpoint` loads `dreamshaper_8.safetensors`
2. Ensure `Load LoRA` loads `blindbox_V1Mix.safetensors`
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

## Load LoRA Node Introduction

![Load LoRA Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_lora.jpg)

Models in the `ComfyUI\models\loras` folder will be detected by ComfyUI and can be loaded using this node.

### Input Types

| Parameter Name   | Function                                                                                               |
| ---------------- | ------------------------------------------------------------------------------------------------------ |
| `model`          | Connect to the base model                                                                              |
| `clip`           | Connect to the CLIP model                                                                              |
| `lora_name`      | Select the LoRA model to load and use                                                                  |
| `strength_model` | Affects how strongly the LoRA influences the model weights; higher values make the LoRA style stronger |
| `strength_clip`  | Affects how strongly the LoRA influences the CLIP text embeddings                                      |

### Output Types

| Parameter Name | Function                                             |
| -------------- | ---------------------------------------------------- |
| `model`        | Outputs the model with LoRA adjustments applied      |
| `clip`         | Outputs the CLIP model with LoRA adjustments applied |

This node supports chain connections, allowing multiple `Load LoRA` nodes to be linked in series to apply multiple LoRA models. For more details, please refer to [ComfyUI Multiple LoRAs Example](/tutorials/basic/multiple-loras)

![LoRA Node Chain Connection](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png)

## Try It Yourself

1. Try modifying the prompt or adjusting different parameters of the `Load LoRA` node, such as `strength_model`, to observe changes in the generated images and become familiar with the `Load LoRA` node.
2. Visit [CivitAI](https://civitai.com/models) to download other kinds of LoRA models and try using them.


# ComfyUI Multiple LoRAs Example
Source: https://docs.comfy.org/tutorials/basic/multiple-loras

This guide demonstrates how to apply multiple LoRA models simultaneously in ComfyUI

In our [ComfyUI LoRA Example](/tutorials/basic/lora), we introduced how to load and use a single LoRA model, mentioning the node's chain connection capability.

![LoRA Node Chaining](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png)

This tutorial demonstrates chaining multiple `Load LoRA` nodes to apply two LoRA models simultaneously: [blindbox\_V1Mix](https://civitai.com/models/25995?modelVersionId=32988) and [MoXinV1](https://civitai.com/models/12597?modelVersionId=14856).

The comparison below shows individual effects of these LoRAs using identical parameters:

![Single LoRA Effects Comparison](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/compare.png)

By chaining multiple LoRA models, we achieve a blended style in the final output:

![Multi-LoRA Application Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png)

## Model Installation

Download the [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder.

Download the [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model\&format=SafeTensor\&size=full\&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

Download the [MoXinV1.safetensors](https://civitai.com/api/download/models/14856?type=Model\&format=SafeTensor\&size=full\&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

## Multi-LoRA Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:
![ComfyUI Workflow - Multiple LoRAs](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

## Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

![Multi-LoRA Workflow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/flow_diagram.png)

1. Ensure `Load Checkpoint` loads  **dreamshaper\_8.safetensors**
2. Ensure first `Load LoRA` loads **blindbox\_V1Mix.safetensors**
3. Ensure second `Load LoRA` loads **MoXinV1.safetensors**
4. Click `Queue` or press `Ctrl/Cmd + Enter` to generate

## Try It Yourself

1. Adjust `strength_model` values in both `Load LoRA` nodes to control each LoRA's influence
2. Explore [CivitAI](https://civitai.com/models) for additional LoRAs and create custom combinations


# ComfyUI Outpainting Workflow Example
Source: https://docs.comfy.org/tutorials/basic/outpaint

This guide will introduce you to the outpainting workflow in ComfyUI and walk you through an outpainting example

This guide will introduce you to the concept of outpainting in AI image generation and how to create an outpainting workflow in ComfyUI. We will cover:

* Using outpainting workflow to extend an image
* Understanding and using outpainting-related nodes in ComfyUI
* Mastering the basic outpainting process

## About Outpainting

In AI image generation, we often encounter situations where an existing image has good composition but the canvas area is too small, requiring us to extend the canvas to get a larger scene. This is where outpainting comes in.

Basically, it requires similar content to [Inpainting](/tutorials/basic/inpaint), but we use different nodes to **build the mask**.

Outpainting applications include:

* **Scene Extension:** Expand the scene range of the original image to show a more complete environment
* **Composition Adjustment:** Optimize the overall composition by extending the canvas
* **Content Addition:** Add more related scene elements to the original image

## ComfyUI Outpainting Workflow Example Explanation

### Preparation

#### 1. Model Installation

Download the following model file and save it to `ComfyUI/models/checkpoints` directory:

* [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)

#### 2. Input Image

Prepare an image you want to extend. In this example, we will use the following image:

![ComfyUI Outpainting Input Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/input.png)

#### 3. Outpainting Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

![ComfyUI Outpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpaint.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

### Outpainting Workflow Usage Explanation

![ComfyUI Outpainting Workflow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpainting_workflow.jpg)

The key steps of the outpainting workflow are as follows:

1. Load the locally installed model file in the `Load Checkpoint` node
2. Click the `Upload` button in the `Load Image` node to upload your image
3. Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute the image generation

In this workflow, we mainly use the `Pad Image for outpainting` node to control the direction and range of image extension. This is actually an [Inpaint](/tutorials/basic/inpaint.mdx) workflow, but we use different nodes to build the mask.

### Pad Image for outpainting Node

![Pad Image for outpainting Node](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/pad_image_for_outpainting.jpg)

This node accepts an input image and outputs an extended image with a corresponding mask, where the mask is built based on the node parameters.

#### Input Parameters

| Parameter Name | Function                                                                                                                              |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| `image`        | Input image                                                                                                                           |
| `left`         | Left padding amount                                                                                                                   |
| `top`          | Top padding amount                                                                                                                    |
| `right`        | Right padding amount                                                                                                                  |
| `bottom`       | Bottom padding amount                                                                                                                 |
| `feathering`   | Controls the smoothness of the transition between the original image and the added padding, higher values create smoother transitions |

#### Output Parameters

| Parameter Name | Function                                                                   |
| -------------- | -------------------------------------------------------------------------- |
| `image`        | Output `image` represents the padded image                                 |
| `mask`         | Output `mask` indicates the original image area and the added padding area |

#### Node Output Content

After processing by the `Pad Image for outpainting` node, the output image and mask preview are as follows:

![Pad Image for outpainting Node Results](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg)

You can see the corresponding output results:

* The `Image` output is the extended image
* The `Mask` output is the mask marking the extension areas


# ComfyUI Text to Image Workflow
Source: https://docs.comfy.org/tutorials/basic/text-to-image

This guide will help you understand the concept of text-to-image in AI art generation and complete a text-to-image workflow in ComfyUI

This guide aims to introduce you to ComfyUI's text-to-image workflow and help you understand the functionality and usage of various ComfyUI nodes.

In this document, we will:

* Complete a text-to-image workflow
* Gain a basic understanding of diffusion model principles
* Learn about the functions and roles of workflow nodes
* Get an initial understanding of the SD1.5 model

We'll start by running a text-to-image workflow, followed by explanations of related concepts. Please choose the relevant sections based on your needs.

## About Text to Image

**Text to Image** is a fundamental process in AI art generation that creates images from text descriptions, with **diffusion models** at its core.

The text-to-image process requires the following elements:

* **Artist:** The image generation model
* **Canvas:** The latent space
* **Image Requirements (Prompts):** Including positive prompts (elements you want in the image) and negative prompts (elements you don't want)

This text-to-image generation process can be simply understood as telling your requirements (positive and negative prompts) to an **artist (the image model)**, who then creates what you want based on these requirements.

## ComfyUI Text to Image Workflow Example Guide

### 1. Preparation

Ensure you have at least one SD1.5 model file in your `ComfyUI/models/checkpoints` folder, such as [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)

If you haven't installed it yet, please refer to the model installation section in [Getting Started with ComfyUI AI Art Generation](/get_started/first_generation).

### 2. Loading the Text to Image Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/text-to-image-workflow.png" alt="ComfyUI-Text to Image Workflow" />

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

### 3. Loading the Model and Generating Your First Image

After installing the image model, follow the steps in the image below to load the model and generate your first image

![Image Generation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg)

Follow these steps according to the image numbers:

1. In the **Load Checkpoint** node, use the arrows or click the text area to ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected, and the left/right arrows don't show **null** text
2. Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute image generation

After the process completes, you should see the resulting image in the **Save Image** node interface, which you can right-click to save locally

![ComfyUI First Image Generation Result](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg)

<Tip>If you're not satisfied with the result, try running the generation multiple times. Each time you run it, **KSampler** will use a different random seed based on the `seed` parameter, so each generation will produce different results</Tip>

### 4. Start Experimenting

Try modifying the text in the **CLIP Text Encoder**

![CLIP Text Encoder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg)

The `Positive` connection to the KSampler node represents positive prompts, while the `Negative` connection represents negative prompts

Here are some basic prompting principles for the SD1.5 model:

* Use English whenever possible
* Separate prompts with English commas `,`
* Use phrases rather than long sentences
* Use specific descriptions
* Use expressions like `(golden hour:1.2)` to increase the weight of specific keywords, making them more likely to appear in the image. `1.2` is the weight, `golden hour` is the keyword
* Use keywords like `masterpiece, best quality, 4k` to improve generation quality

Here are several prompt examples you can try, or use your own prompts for generation:

**1. Anime Style**

Positive prompts:

```
anime style, 1girl with long pink hair, cherry blossom background, studio ghibli aesthetic, soft lighting, intricate details

masterpiece, best quality, 4k
```

Negative prompts:

```
low quality, blurry, deformed hands, extra fingers
```

**2. Realistic Style**

Positive prompts:

```
(ultra realistic portrait:1.3), (elegant woman in crimson silk dress:1.2), 
full body, soft cinematic lighting, (golden hour:1.2), 
(fujifilm XT4:1.1), shallow depth of field, 
(skin texture details:1.3), (film grain:1.1), 
gentle wind flow, warm color grading, (perfect facial symmetry:1.3)
```

Negative prompts:

```
(deformed, cartoon, anime, doll, plastic skin, overexposed, blurry, extra fingers)
```

**3. Specific Artist Style**

Positive prompts:

```
fantasy elf, detailed character, glowing magic, vibrant colors, long flowing hair, elegant armor, ethereal beauty, mystical forest, magical aura, high detail, soft lighting, fantasy portrait, Artgerm style
```

Negative prompts:

```
blurry, low detail, cartoonish, unrealistic anatomy, out of focus, cluttered, flat lighting
```

## Text to Image Working Principles

The entire text-to-image process can be understood as a **reverse diffusion process**. The [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) we downloaded is a pre-trained model that can **generate target images from pure Gaussian noise**. We only need to input our prompts, and it can generate target images through denoising random noise.

```mermaid
graph LR
A[Pure Gaussian Noise] --> B[Iterative Denoising]
B --> C[Intermediate Latents]
C --> D[Final Generated Image]
E[Text Prompts] --> F[CLIP Encoder]
F --> G[Semantic Vectors]
G --> B
```

We need to understand two concepts:

1. **Latent Space:** Latent Space is an abstract data representation method in diffusion models. Converting images from pixel space to latent space reduces storage space and makes it easier to train diffusion models and reduce denoising complexity. It's like architects using blueprints (latent space) for design rather than designing directly on the building (pixel space), maintaining structural features while significantly reducing modification costs
2. **Pixel Space:** Pixel Space is the storage space for images, which is the final image we see, used to store pixel values.

If you want to learn more about diffusion models, you can read these papers:

* [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/pdf/2006.11239)
* [Denoising Diffusion Implicit Models (DDIM)](https://arxiv.org/pdf/2010.02502)
* [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752)

## ComfyUI Text to Image Workflow Node Explanation

![ComfyUI Text to Image Workflow Explanation](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/text-image-workflow.jpg)

### A. Load Checkpoint Node

![Load Checkpoint](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_checkpoint.jpg)

This node is typically used to load the image generation model. A `checkpoint` usually contains three components: `MODEL (UNet)`, `CLIP`, and `VAE`

* `MODEL (UNet)`: The UNet model responsible for noise prediction and image generation during the diffusion process
* `CLIP`: The text encoder that converts our text prompts into vectors that the model can understand, as the model cannot directly understand text prompts
* `VAE`: The Variational AutoEncoder that converts images between pixel space and latent space, as diffusion models work in latent space while our images are in pixel space

### B. Empty Latent Image Node

![Empty Latent Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/lantent/empty_latent_image.jpg)

Defines a latent space that outputs to the KSampler node. The Empty Latent Image node constructs a **pure noise latent space**

You can think of its function as defining the canvas size, which determines the dimensions of our final generated image

### C. CLIP Text Encoder Node

![CLIP Text Encoder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg)

Used to encode prompts, which are your requirements for the image

* The `Positive` condition input connected to the KSampler node represents positive prompts (elements you want in the image)
* The `Negative` condition input connected to the KSampler node represents negative prompts (elements you don't want in the image)

The prompts are encoded into semantic vectors by the `CLIP` component from the `Load Checkpoint` node and output as conditions to the KSampler node

### D. KSampler Node

![KSampler](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/sampling/k_sampler.jpg)

The **KSampler** is the core of the entire workflow, where the entire noise denoising process occurs, ultimately outputting a latent space image

```mermaid
graph LR
A[Diffusion Model] --> B{KSampler}
C[Random Noise<br>Latent Space] --> B
D[CLIP Semantic Vectors] --> B
B --> E[Denoised Latent]
```

Here's an explanation of the KSampler node parameters:

| Parameter Name               | Description                        | Function                                                                                                    |
| ---------------------------- | ---------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| **model**                    | Diffusion model used for denoising | Determines the style and quality of generated images                                                        |
| **positive**                 | Positive prompt condition encoding | Guides generation to include specified elements                                                             |
| **negative**                 | Negative prompt condition encoding | Suppresses unwanted content                                                                                 |
| **latent\_image**            | Latent space image to be denoised  | Serves as the input carrier for noise initialization                                                        |
| **seed**                     | Random seed for noise generation   | Controls generation randomness                                                                              |
| **control\_after\_generate** | Seed control mode after generation | Determines seed variation pattern in batch generation                                                       |
| **steps**                    | Number of denoising iterations     | More steps mean finer details but longer processing time                                                    |
| **cfg**                      | Classifier-free guidance scale     | Controls prompt constraint strength (too high leads to overfitting)                                         |
| **sampler\_name**            | Sampling algorithm name            | Determines the mathematical method for denoising path                                                       |
| **scheduler**                | Scheduler type                     | Controls noise decay rate and step size allocation                                                          |
| **denoise**                  | Denoising strength coefficient     | Controls noise strength added to latent space, 0.0 preserves original input features, 1.0 is complete noise |

In the KSampler node, the latent space uses `seed` as an initialization parameter to construct random noise, and semantic vectors `Positive` and `Negative` are input as conditions to the diffusion model

Then, based on the number of denoising steps specified by the `steps` parameter, denoising is performed. Each denoising step uses the denoising strength coefficient specified by the `denoise` parameter to denoise the latent space and generate a new latent space image

### E. VAE Decode Node

![VAE Decode](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/lantent/vae_decode.jpg)

Converts the latent space image output from the **KSampler** into a pixel space image

### F. Save Image Node

![Save Image](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/save_image.jpg)

Previews and saves the decoded image from latent space to the local `ComfyUI/output` folder

## Introduction to SD1.5 Model

**SD1.5 (Stable Diffusion 1.5)** is an AI image generation model developed by [Stability AI](https://stability.ai/). It's the foundational version of the Stable Diffusion series, trained on **512×512** resolution images, making it particularly good at generating images at this resolution. With a size of about 4GB, it runs smoothly on **consumer-grade GPUs (e.g., 6GB VRAM)**. Currently, SD1.5 has a rich ecosystem, supporting various plugins (like ControlNet, LoRA) and optimization tools.
As a milestone model in AI art generation, SD1.5 remains the best entry-level choice thanks to its open-source nature, lightweight architecture, and rich ecosystem. Although newer versions like SDXL/SD3 have been released, its value for consumer-grade hardware remains unmatched.

### Basic Information

* **Release Date**: October 2022
* **Core Architecture**: Based on Latent Diffusion Model (LDM)
* **Training Data**: LAION-Aesthetics v2.5 dataset (approximately 590M training steps)
* **Open Source Features**: Fully open-source model/code/training data

### Advantages and Limitations

Model Advantages:

* Lightweight: Small size, only about 4GB, runs smoothly on consumer GPUs
* Low Entry Barrier: Supports a wide range of plugins and optimization tools
* Mature Ecosystem: Extensive plugin and tool support
* Fast Generation: Smooth operation on consumer GPUs

Model Limitations:

* Detail Handling: Hands/complex lighting prone to distortion
* Resolution Limits: Quality degrades for direct 1024x1024 generation
* Prompt Dependency: Requires precise English descriptions for control


# ComfyUI Image Upscale Workflow
Source: https://docs.comfy.org/tutorials/basic/upscale

This guide explains the concept of image upscaling in AI drawing and demonstrates how to implement an image upscaling workflow in ComfyUI

## What is Image Upscaling?

Image Upscaling is the process of converting low-resolution images to high-resolution using algorithms.
Unlike traditional interpolation methods, AI upscaling models (like ESRGAN) can intelligently reconstruct details while maintaining image quality.

For instance, the default SD1.5 model often struggles with large-size image generation.
To achieve high-resolution results,we typically generate smaller images first and then use upscaling techniques.

This article covers one of many upscaling methods in ComfyUI. In this tutorial, we'll guide you through:

1. Downloading and installing upscaling models
2. Performing basic image upscaling
3. Combining text-to-image workflows with upscaling

## Upscaling Workflow

### Model Installation

Required ESRGAN models download:

<Steps>
  <Step title="Visit OpenModelDB">
    Visit [OpenModelDB](https://openmodeldb.info/) to search and download upscaling models (e.g., RealESRGAN)

    ![openmodeldb](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_OpenModelDB.jpg)

    As shown:

    1. Filter models by image type using the category selector
    2. The model's magnification factor is indicated in the top-right corner (e.g., 2x in the screenshot)

    We'll use the [4x-ESRGAN](https://openmodeldb.info/models/4x-ESRGAN) model for this tutorial. Click the `Download` button on the model detail page.

    ![OpenModelDB\_download](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_OpenModelDB_download.jpg)
  </Step>

  <Step title="Save Model Files in Directory">
    Save the model file (.pth) in `ComfyUI/models/upscale_models` directory
  </Step>
</Steps>

### Workflow and Assets

Download and drag the following image into ComfyUI to load the basic upscaling workflow:
![Upscale workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_workflow.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

Use this image in smaller size as input:
![Upscale-input](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale-input.jpg)

### Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

![Upscale workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_simple_workflow.jpg)

1. Ensure `Load Upscale Model` loads `4x-ESRGAN.pth`
2. Upload the input image to the `Load Image` node
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

The core components are the `Load Upscale Model` and `Upscale Image (Using Model)` nodes, which receive an image input and upscale it using the selected model.

## Text-to-Image Combined Workflow

After mastering basic upscaling, we can combine it with the [text-to-image](/tutorials/basic/text-to-image) workflow. For text-to-image basics, refer to the [text-to-image tutorial](/tutorials/basic/text-to-image).

Download and drag this image into ComfyUI to load the combined workflow:
![Text-to-image upscale workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/esrgan_example.png)

This workflow connects the text-to-image output image directly to the upscaling nodes for final processing.

## Additional Tips

<Tip>
  Model characteristics:

  * **RealESRGAN**: General-purpose upscaling
  * **BSRGAN**: Excels with text and sharp edges
  * **SwinIR**: Preserves natural textures, ideal for landscapes
</Tip>

1. **Chained Upscaling**: Combine multiple upscale nodes (e.g., 2x → 4x) for ultra-high magnification
2. **Hybrid Workflow**: Connect upscale nodes after generation for "generate+enhance" pipelines
3. **Comparative Testing**: Different models perform better on specific image types - test multiple options


# ComfyUI ControlNet Usage Example
Source: https://docs.comfy.org/tutorials/controlnet/controlnet

This guide will introduce you to the basic concepts of ControlNet and demonstrate how to generate corresponding images in ComfyUI

Achieving precise control over image creation in AI image generation cannot be done with just one click.
It typically requires numerous generation attempts to produce a satisfactory image. However, the emergence of **ControlNet** has effectively addressed this challenge.

ControlNet is a conditional control generation model based on diffusion models (such as Stable Diffusion),
first proposed by [Lvmin Zhang](https://lllyasviel.github.io/) and Maneesh Agrawala et al. in 2023 in the paper [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).

ControlNet models significantly enhance the controllability of image generation and the ability to reproduce details by introducing multimodal input conditions,
such as edge detection maps, depth maps, and pose keypoints.

These conditioning constraints make image generation more controllable, allowing multiple ControlNet models to be used simultaneously during the drawing process for better results.

Before ControlNet, we could only rely on the model to generate images repeatedly until we were satisfied with the results, which involved a lot of randomness.

![Images generated with random seeds in ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/generated_with_random_seed.jpg)

With the advent of ControlNet, we can control image generation by introducing additional conditions.
For example, we can use a simple sketch to guide the image generation process, producing images that closely align with our sketch.

![Sketch-controlled image generation in ComfyUI](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/scribble_example.jpg)

In this example, we will guide you through installing and using ControlNet models in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), and complete a sketch-controlled image generation example.

![ComfyUI ControlNet Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)

<Tip>
  The workflows for other types of ControlNet V1.1 models are similar to this example. You only need to select the appropriate model and upload the corresponding reference image based on your needs.
</Tip>

## ControlNet Image Preprocessing Information

Different types of ControlNet models typically require different types of reference images:

![Reference Images](https://github.com/Fannovel16/comfyui_controlnet_aux/blob/main/examples/CNAuxBanner.jpg?raw=true)

> Image source: [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

Since the current **Comfy Core** nodes do not include all types of **preprocessors**, in the actual examples in this documentation, we will provide pre-processed images.
However, in practical use, you may need to use custom nodes to preprocess images to meet the requirements of different ControlNet models. Below are some relevant custom nodes:

* [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
* [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## ComfyUI ControlNet Workflow Example Explanation

### 1. ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)

<Tip>
  Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
  This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.
</Tip>

Please download the image below, which we will use as input:

![ComfyUI Sketch Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_input.png)

### 2. Manual Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [dreamCreationVirtual3DECommerce\_v10.safetensors](https://civitai.com/api/download/models/731340?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)
* [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── dreamCreationVirtual3DECommerce_v10.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
│   └── controlnet/
│       └── control_v11p_sd15_scribble_fp16.safetensors
```

<Note>
  In this example, you could also use the VAE model embedded in dreamCreationVirtual3DECommerce\_v10.safetensors, but we're following the model author's recommendation to use a separate VAE model.
</Note>

### 3. Step-by-Step Workflow Execution

![ComfyUI Workflow - ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_scribble.png)

1. Ensure that `Load Checkpoint` can load **dreamCreationVirtual3DECommerce\_v10.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**
3. Click `Upload` in the `Load Image` node to upload the input image provided earlier
4. Ensure that `Load ControlNet` can load **control\_v11p\_sd15\_scribble\_fp16.safetensors**
5. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Related Node Explanations

### Load ControlNet Node Explanation

![load controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_controlnet_model.jpg)

Models located in `ComfyUI\models\controlnet` will be detected by ComfyUI and can be loaded through this node.

### Apply ControlNet Node Explanation

![apply controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg)

This node accepts the ControlNet model loaded by `load controlnet` and generates corresponding control conditions based on the input image.

**Input Types**

| Parameter Name  | Function                                                                                                                                   |
| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| `positive`      | Positive conditioning                                                                                                                      |
| `negative`      | Negative conditioning                                                                                                                      |
| `control_net`   | The ControlNet model to be applied                                                                                                         |
| `image`         | Preprocessed image used as reference for ControlNet application                                                                            |
| `vae`           | VAE model input                                                                                                                            |
| `strength`      | Strength of ControlNet application; higher values increase ControlNet's influence on the generated image                                   |
| `start_percent` | Determines when to start applying ControlNet as a percentage; e.g., 0.2 means ControlNet guidance begins when 20% of diffusion is complete |
| `end_percent`   | Determines when to stop applying ControlNet as a percentage; e.g., 0.8 means ControlNet guidance stops when 80% of diffusion is complete   |

**Output Types**

| Parameter Name | Function                                           |
| -------------- | -------------------------------------------------- |
| `positive`     | Positive conditioning data processed by ControlNet |
| `negative`     | Negative conditioning data processed by ControlNet |

You can use chain connections to apply multiple ControlNet models, as shown in the image below. You can also refer to the [Mixing ControlNet Models](/tutorials/controlnet/mixing-controlnets.mdx) guide to learn more about combining multiple ControlNet models.
![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg)

<Note>
  You might see the `Apply ControlNet(Old)` node in some early workflows, which is an early version of the ControlNet node. It is currently deprecated and not visible by default in search and node lists.
  ![apply controlnet old](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/controlnet/apply_controlnet_old.jpg)
  To enable it, go to **Settings** --> **comfy** --> **Node** and enable the `Show deprecated nodes in search` option. However, it's recommended to use the new node.
</Note>

## Start Your Exploration

1. Try creating similar sketches, or even draw your own, and use ControlNet models to generate images to experience the benefits of ControlNet.
2. Adjust the `Control Strength` parameter in the Apply ControlNet node to control the influence of the ControlNet model on the generated image.
3. Visit the [ControlNet-v1-1\_fp16\_safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/tree/main) repository to download other types of ControlNet models and try using them to generate images.


# ComfyUI Depth ControlNet Usage Example
Source: https://docs.comfy.org/tutorials/controlnet/depth-controlnet

This guide will introduce you to the basic concepts of Depth ControlNet and demonstrate how to generate corresponding images in ComfyUI

## Introduction to Depth Maps and Depth ControlNet

A depth map is a special type of image that uses grayscale values to represent the distance between objects in a scene and the observer or camera. In a depth map, the grayscale value is inversely proportional to distance: brighter areas (closer to white) indicate objects that are closer, while darker areas (closer to black) indicate objects that are farther away.

![Depth Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png)

Depth ControlNet is a ControlNet model specifically trained to understand and utilize depth map information. It helps AI correctly interpret spatial relationships, ensuring that generated images conform to the spatial structure specified by the depth map, thereby enabling precise control over three-dimensional spatial layouts.

### Application Scenarios for Depth Maps with ControlNet

Depth maps have numerous applications in various scenarios:

1. **Portrait Scenes**: Control the spatial relationship between subjects and backgrounds, avoiding distortion in critical areas such as faces
2. **Landscape Scenes**: Control the hierarchical relationships between foreground, middle ground, and background
3. **Architectural Scenes**: Control the spatial structure and perspective relationships of buildings
4. **Product Showcase**: Control the separation and spatial positioning of products against their backgrounds

In this example, we will use a depth map to generate an architectural visualization scene.

## ComfyUI ControlNet Workflow Example Explanation

### 1. ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

![Depth Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_controlnet.png)

<Tip>
  Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
  This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.
</Tip>

Please download the image below, which we will use as input:

![Depth Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png)

### 2. Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [architecturerealmix\_v11.safetensors](https://civitai.com/api/download/models/431755?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [control\_v11f1p\_sd15\_depth\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors?download=true)

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── architecturerealmix_v11.safetensors
│   └── controlnet/
│       └── control_v11f1p_sd15_depth_fp16.safetensors
```

### 3. Step-by-Step Workflow Execution

![ComfyUI Workflow - Depth ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth.jpg)

1. Ensure that `Load Checkpoint` can load **architecturerealmix\_v11.safetensors**
2. Ensure that `Load ControlNet` can load **control\_v11f1p\_sd15\_depth\_fp16.safetensors**
3. Click `Upload` in the `Load Image` node to upload the depth image provided earlier
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Combining Depth Control with Other Techniques

Based on different creative needs, you can combine Depth ControlNet with other types of ControlNet to achieve better results:

1. **Depth + Lineart**: Maintain spatial relationships while reinforcing outlines, suitable for architecture, products, and character design
2. **Depth + Pose**: Control character posture while maintaining correct spatial relationships, suitable for character scenes

For more information on using multiple ControlNet models together, please refer to the [Mixing ControlNet](/tutorials/controlnet/mixing-controlnets.mdx) example.


# ComfyUI Depth T2I Adapter Usage Example
Source: https://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter

This guide will introduce you to the basic concepts of Depth T2I Adapter and demonstrate how to generate corresponding images in ComfyUI

## Introduction to T2I Adapter

[T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter) is a lightweight adapter developed by [Tencent ARC Lab](https://github.com/TencentARC) designed to enhance the structural, color, and style control capabilities of text-to-image generation models (such as Stable Diffusion).
It works by aligning external conditions (such as edge detection maps, depth maps, sketches, or color reference images) with the model's internal features, achieving high-precision control without modifying the original model structure. With only about 77M parameters (approximately 300MB in size), its inference speed is about 3 times faster than [ControlNet](https://github.com/lllyasviel/ControlNet-v1-1-nightly), and it supports multiple condition combinations (such as sketch + color grid). Application scenarios include line art to image conversion, color style transfer, multi-element scene generation, and more.

### Comparison Between T2I Adapter and ControlNet

Although their functions are similar, there are notable differences in implementation and application:

1. **Lightweight Design**: T2I Adapter has fewer parameters and a smaller memory footprint
2. **Inference Speed**: T2I Adapter is typically about 3 times faster than ControlNet
3. **Control Precision**: ControlNet offers more precise control in certain scenarios, while T2I Adapter is more suitable for lightweight control
4. **Multi-condition Combination**: T2I Adapter shows more significant resource advantages when combining multiple conditions

### Main Types of T2I Adapter

T2I Adapter provides various types to control different aspects:

* **Depth**: Controls the spatial structure and depth relationships in images
* **Line Art (Canny/Sketch)**: Controls image edges and lines
* **Keypose**: Controls character poses and actions
* **Segmentation (Seg)**: Controls scene layout through semantic segmentation
* **Color**: Controls the overall color scheme of images

In ComfyUI, using T2I Adapter is similar to [ControlNet](/tutorials/controlnet/controlnet.mdx) in terms of interface and workflow. In this example, we will demonstrate how to use a depth T2I Adapter to control an interior scene.

![ComfyUI Depth T2I Adapter Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)

## Value of Depth T2I Adapter Applications

Depth maps have several important applications in image generation:

1. **Spatial Layout Control**: Accurately describes three-dimensional spatial structures, suitable for interior design and architectural visualization
2. **Object Positioning**: Controls the relative position and size of objects in a scene, suitable for product showcases and scene construction
3. **Perspective Relationships**: Maintains reasonable perspective and proportions, suitable for landscape and urban scene generation
4. **Light and Shadow Layout**: Natural light and shadow distribution based on depth information, enhancing realism

We will use interior design as an example to demonstrate how to use the depth T2I Adapter, but these techniques are applicable to other scenarios as well.

## ComfyUI Depth T2I Adapter Workflow Example Explanation

### 1. Depth T2I Adapter Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

![ComfyUI Workflow - Depth T2I Adapter](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)

<Tip>
  Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
  This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.
</Tip>

Please download the image below, which we will use as input:

![ComfyUI Interior Depth Map](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png)

### 2. Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [interiordesignsuperm\_v2.safetensors](https://civitai.com/api/download/models/93152?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [t2iadapter\_depth\_sd15v2.pth](https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd15v2.pth?download=true)

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── interiordesignsuperm_v2.safetensors
│   └── controlnet/
│       └── t2iadapter_depth_sd15v2.pth
```

### 3. Step-by-Step Workflow Execution

![ComfyUI Workflow - Depth T2I Adapter Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg)

1. Ensure that `Load Checkpoint` can load **interiordesignsuperm\_v2.safetensors**
2. Ensure that `Load ControlNet` can load **t2iadapter\_depth\_sd15v2.pth**
3. Click `Upload` in the `Load Image` node to upload the input image provided earlier
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## General Tips for Using T2I Adapter

### Input Image Quality Optimization

Regardless of the application scenario, high-quality input images are key to successfully using T2I Adapter:

1. **Moderate Contrast**: Control images (such as depth maps, line art) should have clear contrast, but not excessively extreme
2. **Clear Boundaries**: Ensure that major structures and element boundaries are clearly distinguishable in the control image
3. **Noise Control**: Try to avoid excessive noise in control images, especially for depth maps and line art
4. **Reasonable Layout**: Control images should have a reasonable spatial layout and element distribution

## Characteristics of T2I Adapter Usage

One major advantage of T2I Adapter is its ability to easily combine multiple conditions for complex control effects:

1. **Depth + Edge**: Control spatial layout while maintaining clear structural edges, suitable for architecture and interior design
2. **Line Art + Color**: Control shapes while specifying color schemes, suitable for character design and illustrations
3. **Pose + Segmentation**: Control character actions while defining scene areas, suitable for complex narrative scenes

Mixing different T2I Adapters, or combining them with other control methods (such as ControlNet, regional prompts, etc.), can further expand creative possibilities. To achieve mixing, simply chain multiple `Apply ControlNet` nodes together in the same way as described in [Mixing ControlNet](/tutorials/controlnet/mixing-controlnets.mdx).


# ComfyUI Mixing ControlNet Examples
Source: https://docs.comfy.org/tutorials/controlnet/mixing-controlnets

In this example, we will demonstrate how to mix multiple ControlNets and learn to use multiple ControlNet models to control image generation

In AI image generation, a single control condition often fails to meet the requirements of complex scenes. Mixing multiple ControlNets allows you to control different regions or aspects of an image simultaneously, achieving more precise control over image generation.

In certain scenarios, mixing ControlNets can leverage the characteristics of different control conditions to achieve more refined conditional control:

1. **Scene Complexity**: Complex scenes require multiple control conditions working together
2. **Fine-grained Control**: By adjusting the strength parameter of each ControlNet, you can precisely control the degree of influence for each part
3. **Complementary Effects**: Different types of ControlNets can complement each other, compensating for the limitations of single controls
4. **Creative Expression**: Combining different controls can produce unique creative effects

### How to Mix ControlNets

When mixing multiple ControlNets, each ControlNet influences the image generation process according to its applied area. ComfyUI enables multiple ControlNet conditions to be applied sequentially in a layered manner through chain connections in the `Apply ControlNet` node:

![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg)

## ComfyUI ControlNet Regional Division Mixing Example

In this example, we will use a combination of **Pose ControlNet** and **Scribble ControlNet** to generate a scene containing multiple elements: a character on the left controlled by Pose ControlNet and a cat on a scooter on the right controlled by Scribble ControlNet.

### 1. ControlNet Mixing Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Mixing ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets.png)

<Tip>
  This workflow image contains Metadata, and can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`. The system will automatically detect and prompt to download the required models.
</Tip>

Input pose image (controls the character pose on the left):

![ComfyUI Workflow - Mixing ControlNet Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input.png)

Input scribble image (controls the cat and scooter on the right):

![ComfyUI Workflow - Mixing ControlNet Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input_scribble.png)

### 2. Manual Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [awpainting\_v14.safetensors](https://civitai.com/api/download/models/624939?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)
* [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── awpainting_v14.safetensors
│   ├── controlnet/
│   │   └── control_v11p_sd15_scribble_fp16.safetensors
│   │   └── control_v11p_sd15_openpose_fp16.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
```

### 3. Step-by-Step Workflow Execution

![ComfyUI Workflow - Mixing ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg)

Follow these steps according to the numbered markers in the image:

1. Ensure that `Load Checkpoint` can load **awpainting\_v14.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**

First ControlNet group using the Openpose model:
3\. Ensure that `Load ControlNet Model` loads **control\_v11p\_sd15\_openpose\_fp16.safetensors**
4\. Click `Upload` in the `Load Image` node to upload the pose image provided earlier

Second ControlNet group using the Scribble model:
5\. Ensure that `Load ControlNet Model` loads **control\_v11p\_sd15\_scribble\_fp16.safetensors**
6\. Click `Upload` in the `Load Image` node to upload the scribble image provided earlier
7\. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Workflow Explanation

#### Strength Balance

When controlling different regions of an image, balancing the strength parameters is particularly important:

* If the ControlNet strength for one region is significantly higher than another, it may cause that region's control effect to overpower and suppress the other region
* It's recommended to set similar strength values for ControlNets controlling different regions, for example, both set to 1.0

#### Prompt Techniques

For regional division mixing, the prompt needs to include descriptions of both regions:

```
"A woman in red dress, a cat riding a scooter, detailed background, high quality"
```

Such a prompt covers both the character and the cat on the scooter, ensuring the model pays attention to both control regions.

## Multi-dimensional Control Applications for a Single Subject

In addition to the regional division mixing shown in this example, another common mixing approach is to apply multi-dimensional control to the same subject. For example:

* **Pose + Depth**: Control character posture and spatial sense
* **Pose + Canny**: Control character posture and edge details
* **Pose + Reference**: Control character posture while referencing a specific style

In this type of application, reference images for multiple ControlNets should be aligned to the same subject, and their strengths should be adjusted to ensure proper balance.

By combining different types of ControlNets and specifying their control regions, you can achieve precise control over elements in your image.


# ComfyUI Pose ControlNet Usage Example
Source: https://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass

This guide will introduce you to the basic concepts of Pose ControlNet, and demonstrate how to generate large-sized images in ComfyUI using a two-pass generation approach

## Introduction to OpenPose

[OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) is an open-source real-time multi-person pose estimation system developed by Carnegie Mellon University (CMU), representing a significant breakthrough in the field of computer vision. The system can simultaneously detect multiple people in an image, capturing:

* **Body skeleton**: 18 keypoints, including head, shoulders, elbows, wrists, hips, knees, and ankles
* **Facial expressions**: 70 facial keypoints for capturing micro-expressions and facial contours
* **Hand details**: 21 hand keypoints for precisely expressing finger positions and gestures
* **Foot posture**: 6 foot keypoints, recording standing postures and movement details

![OpenPose Example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/openpose_example.jpg)

In AI image generation, skeleton structure maps generated by OpenPose serve as conditional inputs for ControlNet, enabling precise control over the posture, actions, and expressions of generated characters. This allows us to generate realistic human figures with expected poses and actions, greatly improving the controllability and practical value of AI-generated content.
Particularly for early Stable Diffusion 1.5 series models, skeletal maps generated by OpenPose can effectively prevent issues with distorted character actions, limbs, and expressions.

## ComfyUI 2-Pass Pose ControlNet Usage Example

### 1. Pose ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

![ComfyUI Workflow - Pose ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass.png)

<Tip>
  Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
  This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.
</Tip>

Please download the image below, which we will use as input:

![ComfyUI Pose Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass_input.png)

### 2. Manual Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
* [majicmixRealistic\_v7.safetensors](https://civitai.com/api/download/models/176425?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16)
* [japaneseStyleRealistic\_v20.safetensors](https://civitai.com/api/download/models/85426?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── majicmixRealistic_v7.safetensors
│   │   └── japaneseStyleRealistic_v20.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
│   └── controlnet/
│       └── control_v11p_sd15_openpose_fp16.safetensors
```

### 3. Step-by-Step Workflow Execution

![ComfyUI Workflow - Pose ControlNet Flow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg)

Follow these steps according to the numbered markers in the image:

1. Ensure that `Load Checkpoint` can load **majicmixRealistic\_v7.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**
3. Ensure that `Load ControlNet Model` can load **control\_v11p\_sd15\_openpose\_fp16.safetensors**
4. Click the select button in the `Load Image` node to upload the pose input image provided earlier, or use your own OpenPose skeleton map
5. Ensure that `Load Checkpoint` can load **japaneseStyleRealistic\_v20.safetensors**
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Explanation of the Pose ControlNet 2-Pass Workflow

This workflow uses a two-pass image generation approach, dividing the image creation process into two phases:

### First Phase: Basic Pose Image Generation

In the first phase, the **majicmixRealistic\_v7** model is combined with Pose ControlNet to generate an initial character pose image:

1. First, load the majicmixRealistic\_v7 model via the `Load Checkpoint` node
2. Load the pose control model through the `Load ControlNet Model` node
3. The input pose image is fed into the `Apply ControlNet` node and combined with positive and negative prompt conditions
4. The first `KSampler` node (typically using 20-30 steps) generates a basic character pose image
5. The pixel-space image for the first phase is obtained through `VAE Decode`

This phase primarily focuses on correct character posture, pose, and basic structure, ensuring that the generated character conforms to the input skeletal pose.

### Second Phase: Style Optimization and Detail Enhancement

In the second phase, the output image from the first phase is used as a reference, with the **japaneseStyleRealistic\_v20** model performing stylization and detail enhancement:

1. The image generated in the first phase creates a larger resolution latent space through the `Upscale latent` node
2. The second `Load Checkpoint` loads the japaneseStyleRealistic\_v20 model, which focuses on details and style
3. The second `KSampler` node uses a lower `denoise` strength (typically 0.4-0.6) for refinement, preserving the basic structure from the first phase
4. Finally, a higher quality, larger resolution image is output through the second `VAE Decode` and `Save Image` nodes

This phase primarily focuses on style consistency, detail richness, and enhancing overall image quality.

## Advantages of 2-Pass Image Generation

Compared to single-pass generation, the two-pass image generation method offers the following advantages:

1. **Higher Resolution**: Two-pass processing can generate high-resolution images beyond the capabilities of single-pass generation
2. **Style Blending**: Can combine advantages of different models, such as using a realistic model in the first phase and a stylized model in the second phase
3. **Better Details**: The second phase can focus on optimizing details without having to worry about overall structure
4. **Precise Control**: Once pose control is completed in the first phase, the second phase can focus on refining style and details
5. **Reduced GPU Load**: Generating in two passes allows for high-quality large images with limited GPU resources

<Tip>
  To learn more about techniques for mixing multiple ControlNets, please refer to the [Mixing ControlNet Models](/tutorials/controlnet/mixing-controlnets.mdx) tutorial.
</Tip>


# ComfyUI Flux.1 ControlNet Examples
Source: https://docs.comfy.org/tutorials/flux/flux-1-controlnet

This guide will demonstrate workflow examples using Flux.1 ControlNet.

![Flux.1 Canny Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-canny-controlnet.png)
![Flux.1 Depth Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-depth-controlnet.png)

## FLUX.1 ControlNet Model Introduction

FLUX.1 Canny and Depth are two powerful models from the [FLUX.1 Tools](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/). This toolkit is designed to add control and guidance capabilities to FLUX.1, enabling users to modify and recreate real or generated images.

**FLUX.1-Depth-dev** and **FLUX.1-Canny-dev** are both 12B parameter Rectified Flow Transformer models that can generate images based on text descriptions while maintaining the structural features of the input image.
The Depth version maintains the spatial structure of the source image through depth map extraction techniques, while the Canny version uses edge detection techniques to preserve the structural features of the source image, allowing users to choose the appropriate control method based on different needs.

Both models have the following features:

* Top-tier output quality and detail representation
* Excellent prompt following ability while maintaining consistency with the original image
* Trained using guided distillation techniques for improved efficiency
* Open weights for the research community
* API interfaces (pro version) and open-source weights (dev version)

Additionally, Black Forest Labs also provides **FLUX.1-Depth-dev-lora** and **FLUX.1-Canny-dev-lora** adapter versions extracted from the complete models.
These can be applied to the FLUX.1 \[dev] base model to provide similar functionality with smaller file size, especially suitable for resource-constrained environments.

We will use the full version of **FLUX.1-Canny-dev** and **FLUX.1-Depth-dev-lora** to complete the workflow examples.

<Tip>
  All workflow images's Metadata contains the corresponding model download information. You can load the workflows by:

  * Dragging them directly into ComfyUI
  * Or using the menu `Workflows` -> `Open（ctrl+o）`

  If you're not using the Desktop Version or some models can't be downloaded automatically, please refer to the manual installation sections to save the model files to the corresponding folder.

  For image preprocessors, you can use the following custom nodes to complete image preprocessing. In this example, we will provide processed images as input.

  * [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
  * [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)
</Tip>

## FLUX.1-Canny-dev Complete Version Workflow

### 1. Workflow and Asset

Please download the workflow image below and drag it into ComfyUI to load the workflow

![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev.png)

Please download the image below, which we will use as the input image

![ComfyUI Flux.1 Canny Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev-input.png)

### 2. Manual Models Installation

<Note>
  If you have previously used the [complete version of Flux related workflows](/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-canny-dev.safetensors** model file.
  Since you need to first agree to the terms of [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev), please visit the [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev) page and make sure you have agreed to the corresponding terms as shown in the image below.
  ![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux1_canny_dev_agreement.jpg)
</Note>

Complete model list:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-canny-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/resolve/main/flux1-canny-dev.safetensors?download=true) (Please ensure you have agreed to the corresponding repo's terms)

File storage location:

```
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-canny-dev.safetensors
```

### 3. Step-by-Step Workflow Execution

![ComfyUI Flux.1 Canny Controlnet Step Process](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg)

1. Make sure `ae.safetensors` is loaded in the `Load VAE` node
2. Make sure `flux1-canny-dev.safetensors` is loaded in the `Load Diffusion Model` node
3. Make sure the following models are loaded in the `DualCLIPLoader` node:
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
4. Upload the provided input image in the `Load Image` node
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### 4. Start Your Experimentation

Try using the [FLUX.1-Depth-dev](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev) model to complete the Depth version of the workflow

You can use the image below as input
![ComfyUI Indoor Depth Map](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png)

Or use the following custom nodes to complete image preprocessing:

* [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
* [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## FLUX.1-Depth-dev-lora Workflow

The LoRA version workflow builds on the complete version by adding the LoRA model. Compared to the [complete version of the Flux workflow](/tutorials/flux/flux-1-text-to-image), it adds nodes for loading and using the corresponding LoRA model.

### 1. Workflow and Asset

Please download the workflow image below and drag it into ComfyUI to load the workflow

![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora.png)

Please download the image below, which we will use as the input image

![ComfyUI Flux.1 Depth Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora-input.png)

### 2. Manual Model Download

<Tip>
  If you have previously used the [complete version of Flux related workflows](/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-depth-dev-lora.safetensors** model file.
</Tip>

Complete model list:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors?download=true)
* [flux1-depth-dev-lora.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora/resolve/main/flux1-depth-dev-lora.safetensors?download=true)

File storage location:

```
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   ├── diffusion_models/
│   │   └── flux1-dev.safetensors
│   └── loras/
│       └── flux1-depth-dev-lora.safetensors
```

### 3. Step-by-Step Workflow Execution

![ComfyUI Flux.1 Depth Controlnet Step Process](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg)

1. Make sure `flux1-dev.safetensors` is loaded in the `Load Diffusion Model` node
2. Make sure `flux1-depth-dev-lora.safetensors` is loaded in the `LoraLoaderModelOnly` node
3. Make sure the following models are loaded in the `DualCLIPLoader` node:
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
4. Upload the provided input image in the `Load Image` node
5. Make sure `ae.safetensors` is loaded in the `Load VAE` node
6. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### 4. Start Your Experimentation

Try using the [FLUX.1-Canny-dev-lora](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora) model to complete the Canny version of the workflow

Use [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet) or [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to complete image preprocessing

## Community Versions of Flux Controlnets

XLab and InstantX + Shakker Labs have released Controlnets for Flux.

**InstantX:**

* [FLUX.1-dev-Controlnet-Canny](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Canny/blob/main/diffusion_pytorch_model.safetensors)
* [FLUX.1-dev-ControlNet-Depth](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Depth/blob/main/diffusion_pytorch_model.safetensors)
* [FLUX.1-dev-ControlNet-Union-Pro](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/blob/main/diffusion_pytorch_model.safetensors)

**XLab**: [flux-controlnet-collections](https://huggingface.co/XLabs-AI/flux-controlnet-collections)

Place these files in the `ComfyUI/models/controlnet` directory.

You can visit [Flux Controlnet Example](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/flux_controlnet_example.png) to get the corresponding workflow image, and use the image from [here](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/girl_in_field.png) as the input image.


# ComfyUI Flux.1 fill dev Example
Source: https://docs.comfy.org/tutorials/flux/flux-1-fill-dev

This guide demonstrates how to use Flux.1 fill dev to create Inpainting and Outpainting workflows.

![Flux.1 fill dev](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-fill-dev-demo.jpeg)

## Introduction to Flux.1 fill dev Model

Flux.1 fill dev is one of the core tools in the [FLUX.1 Tools suite](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/), specifically designed for image inpainting and outpainting.

Key features of Flux.1 fill dev:

* Powerful image inpainting and outpainting capabilities, with results second only to the commercial version FLUX.1 Fill \[pro].
* Excellent prompt understanding and following ability, precisely capturing user intent while maintaining high consistency with the original image.
* Advanced guided distillation training technology, making the model more efficient while maintaining high-quality output.
* Friendly licensing terms, with generated outputs usable for personal, scientific, and commercial purposes, please refer to the [FLUX.1 \[dev\] non-commercial license](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) for details.

Open Source Repository: [FLUX.1 \[dev\]](https://huggingface.co/black-forest-labs/FLUX.1-dev)

This guide will demonstrate inpainting and outpainting workflows based on the Flux.1 fill dev model.
If you're not familiar with inpainting and outpainting workflows, you can refer to [ComfyUI Layout Inpainting Example](/tutorials/basic/inpaint) and [ComfyUI Image Extension Example](/tutorials/basic/outpaint) for some related explanations.

## Flux.1 Fill dev and related models installation

Before we begin, let's complete the installation of the Flux.1 Fill dev model files. The inpainting and outpainting workflows will use exactly the same model files.
If you've previously used the full version of the [Flux.1 Text-to-Image workflow](/tutorials/flux/flux-1-text-to-image),
then you only need to download the **flux1-fill-dev.safetensors** model file in this section.

However, since downloading the corresponding model requires agreeing to the corresponding usage agreement, please visit the [black-forest-labs/FLUX.1-Fill-dev](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev) page and make sure you have agreed to the corresponding agreement as shown in the image below.
![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux1_fill_dev_agreement.jpg)

Complete model list:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-fill-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/resolve/main/flux1-fill-dev.safetensors?download=true)

File storage location:

```
ComfyUI/
├── models/
│   ├── text_encoders/
│   │    ├── clip_l.safetensors
│   │    └── t5xxl_fp16.safetensors
│   ├── vae/
│   │    └── ae.safetensors
│   └── diffusion_models/
│        └── flux1-fill-dev.safetensors
```

## Flux.1 Fill dev inpainting workflow

### 1. Inpainting workflow and asset

Please download the image below and drag it into ComfyUI to load the corresponding workflow
![ComfyUI Flux.1 inpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint.png)

Please download the image below, we will use it as the input image
![ComfyUI Flux.1 inpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input.png)

<Note>
  The corresponding image already contains an alpha channel, so you don't need to draw a mask separately.
  If you want to draw your own mask, please [click here](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input_original.png) to get the image without a mask, and refer to the MaskEditor usage section in the [ComfyUI Layout Inpainting Example](/tutorials/basic/inpaint#using-the-mask-editor) to learn how to draw a mask in the `Load Image` node.
</Note>

### 2. Steps to run the workflow

![ComfyUI Flux.1 Fill dev Inpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_inpaint.jpg)

1. Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.
2. Ensure the `DualCLIPLoader` node has the following models loaded:
   * clip\_name1: `t5xxl_fp16.safetensors`
   * clip\_name2: `clip_l.safetensors`
3. Ensure the `Load VAE` node has `ae.safetensors` loaded.
4. Upload the input image provided in the document to the `Load Image` node; if you're using the version without a mask, remember to complete the mask drawing using the mask editor
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Flux.1 Fill dev Outpainting Workflow

### 1. Outpainting workflow and asset

Please download the image below and drag it into ComfyUI to load the corresponding workflow
![ComfyUI Flux.1 outpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint.png)

Please download the image below, we will use it as the input image
![ComfyUI Flux.1 outpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint_input.png)

### 2. Steps to run the workflow

![ComfyUI Flux.1 Fill dev Outpainting Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_outpaint.jpg)

1. Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.
2. Ensure the `DualCLIPLoader` node has the following models loaded:
   * clip\_name1: `t5xxl_fp16.safetensors`
   * clip\_name2: `clip_l.safetensors`
3. Ensure the `Load VAE` node has `ae.safetensors` loaded.
4. Upload the input image provided in the document to the `Load Image` node
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow


# ComfyUI Flux.1 Text-to-Image Workflow Example
Source: https://docs.comfy.org/tutorials/flux/flux-1-text-to-image

This guide provides a brief introduction to the Flux.1 model and guides you through using the Flux.1 model for text-to-image generation with examples including the full version and the FP8 Checkpoint version.

![Flux](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_example.png)
Flux is one of the largest open-source text-to-image generation models, with 12B parameters and an original file size of approximately 23GB. It was developed by [Black Forest Labs](https://blackforestlabs.ai/), a team founded by former Stable Diffusion team members.
Flux is known for its excellent image quality and flexibility, capable of generating high-quality, diverse images.

Currently, the Flux.1 model has several main versions:

* **Flux.1 Pro:** The best performing model, closed-source, only available through API calls.
* **[Flux.1 \[dev\]：](https://huggingface.co/black-forest-labs/FLUX.1-dev)** Open-source but limited to non-commercial use, distilled from the Pro version, with performance close to the Pro version.
* **[Flux.1 \[schnell\]：](https://huggingface.co/black-forest-labs/FLUX.1-schnell)** Uses the Apache2.0 license, requires only 4 steps to generate images, suitable for low-spec hardware.

**Flux.1 Model Features**

* **Hybrid Architecture:** Combines the advantages of Transformer networks and diffusion models, effectively integrating text and image information, improving the alignment accuracy between generated images and prompts, with excellent fidelity to complex prompts.
* **Parameter Scale:** Flux has 12B parameters, capturing more complex pattern relationships and generating more realistic, diverse images.
* **Supports Multiple Styles:** Supports diverse styles, with excellent performance for various types of images.

In this example, we'll introduce text-to-image examples using both Flux.1 Dev and Flux.1 Schnell versions, including the full version model and the simplified FP8 Checkpoint version.

* **Flux Full Version:** Best performance, but requires larger VRAM resources and installation of multiple model files.
* **Flux FP8 Checkpoint:** Requires only one fp8 version of the model, but quality is slightly reduced compared to the full version.

<Tip>
  All workflow images's Metadata contains the corresponding model download information. You can load the workflows by:

  * Dragging them directly into ComfyUI
  * Or using the menu `Workflows` -> `Open（ctrl+o）`

  If you're not using the Desktop Version or some models can't be downloaded automatically, please refer to the manual installation sections to save the model files to the corresponding folder.
  Make sure your ComfyUI is updated to the latest version before starting.
</Tip>

## Flux.1 Full Version Text-to-Image Example

<Note>
  If you can't download models from [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), make sure you've logged into Huggingface and agreed to the corresponding repository's license agreement.
  ![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_agreement.jpg)
</Note>

### Flux.1 Dev

#### 1. Workflow File

Please download the image below and drag it into ComfyUI to load the workflow.
![Flux Dev Original Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_t5fp16.png)

#### 2. Manual Model Installation

<Note>
  * The `flux1-dev.safetensors` file requires agreeing to the [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) agreement before downloading via browser.
  * If your VRAM is low, you can try using [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true) to replace the `t5xxl_fp16.safetensors` file.
</Note>

Please download the following model files:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true) Recommended when your VRAM is greater than 32GB.
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors)

Storage location:

```
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-dev.safetensors
```

#### 3. Steps to Run the Workflow

Please refer to the image below to ensure all model files are loaded correctly

![ComfyUI Flux Dev Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg)

1. Ensure the `DualCLIPLoader` node has the following models loaded:
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
2. Ensure the `Load Diffusion Model` node has `flux1-dev.safetensors` loaded
3. Make sure the `Load VAE` node has `ae.safetensors` loaded
4. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

<Tip>
  Thanks to Flux's excellent prompt following capability, we don't need any negative prompts
</Tip>

### Flux.1 Schnell

#### 1. Workflow File

Please download the image below and drag it into ComfyUI to load the workflow.

![Flux Schnell Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_t5fp8.png)

#### 2. Manual Models Installation

<Note>
  In this workflow, only two model files are different from the Flux1 Dev version workflow. For t5xxl, you can still use the fp16 version for better results.

  * **t5xxl\_fp16.safetensors** -> **t5xxl\_fp8.safetensors**
  * **flux1-dev.safetensors** -> **flux1-schnell.safetensors**
</Note>

Complete model file list:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-schnell.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors)

File storage location:

```
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp8_e4m3fn.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-schnell.safetensors
```

#### 3. Steps to Run the Workflow

![Flux Schnell Version Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg)

1. Ensure the `DualCLIPLoader` node has the following models loaded:
   * clip\_name1: t5xxl\_fp8\_e4m3fn.safetensors
   * clip\_name2: clip\_l.safetensors
2. Ensure the `Load Diffusion Model` node has `flux1-schnell.safetensors` loaded
3. Ensure the `Load VAE` node has `ae.safetensors` loaded
4. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Flux.1 FP8 Checkpoint Version Text-to-Image Example

The fp8 version is a quantized version of the original Flux.1 fp16 version.
To some extent, the quality of this version will be lower than that of the fp16 version,
but it also requires less VRAM, and you only need to install one model file to try running it.

### Flux.1 Dev

Please download the image below and drag it into ComfyUI to load the workflow.

![Flux Dev fp8 Checkpoint Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_fp8.png)

Please download [flux1-dev-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory.

Ensure that the corresponding `Load Checkpoint` node loads `flux1-dev-fp8.safetensors`, and you can try to run the workflow.

### Flux.1 Schnell

Please download the image below and drag it into ComfyUI to load the workflow.

![Flux Schnell fp8 Checkpoint Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_fp8.png)

Please download [flux1-schnell-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-schnell/resolve/main/flux1-schnell-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory.

Ensure that the corresponding `Load Checkpoint` node loads `flux1-schnell-fp8.safetensors`, and you can try to run the workflow.


# ComfyUI Hunyuan Video Examples
Source: https://docs.comfy.org/tutorials/video/hunyuan-video

This guide shows how to use Hunyuan Text-to-Video and Image-to-Video workflows in ComfyUI

<video controls className="w-full aspect-video" src="https://github.com/user-attachments/assets/442afb73-3092-454f-bc46-02361c285930" />

Hunyuan Video series is developed and open-sourced by [Tencent](https://huggingface.co/tencent), featuring a hybrid architecture that supports both [Text-to-Video](https://github.com/Tencent/HunyuanVideo) and [Image-to-Video](https://github.com/Tencent/HunyuanVideo-I2V) generation with a parameter scale of 13B.

Technical features:

* **Core Architecture:** Uses a DiT (Diffusion Transformer) architecture similar to Sora, effectively fusing text, image, and motion information to improve consistency, quality, and alignment between generated video frames. A unified full-attention mechanism enables multi-view camera transitions while ensuring subject consistency.
* **3D VAE:** The custom 3D VAE compresses videos into a compact latent space, making image-to-video generation more efficient.
* **Superior Image-Video-Text Alignment:** Utilizing MLLM text encoders that excel in both image and video generation, better following text instructions, capturing details, and performing complex reasoning.

You can learn more through the official repositories: [Hunyuan Video](https://github.com/Tencent/HunyuanVideo) and [Hunyuan Video-I2V](https://github.com/Tencent/HunyuanVideo-I2V).

This guide will walk you through setting up both **Text-to-Video** and **Image-to-Video** workflows in ComfyUI.

<Tip>
  The workflow images in this tutorial contain metadata with model download information.

  Simply drag them into ComfyUI or use the menu `Workflows` -> `Open (ctrl+o)` to load the corresponding workflow, which will prompt you to download the required models.

  Alternatively, this guide provides direct model links if automatic downloads fail or you are not using the Desktop version. All models are available [here](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/tree/main/split_files) for download.
</Tip>

## Shared Models for All Workflows

The following models are used in both Text-to-Video and Image-to-Video workflows. Please download and save them to the specified directories:

* [clip\_l.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors?download=true)
* [llava\_llama3\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors?download=true)
* [hunyuan\_video\_vae\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors?download=true)

Storage location:

```
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── llava_llama3_fp8_scaled.safetensors
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors
```

## Hunyuan Text-to-Video Workflow

Hunyuan Text-to-Video was open-sourced in December 2024, supporting 5-second short video generation through natural language descriptions in both Chinese and English.

### 1. Workflow

Download the image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Hunyuan Text-to-Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/t2v/kitchen.webp)

### 2. Manual Models Installation

Download [hunyuan\_video\_t2v\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_t2v_720p_bf16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models` folder.

Ensure you have all these model files in the correct locations:

```
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                       // Shared model
│   │   └── llava_llama3_fp8_scaled.safetensors      // Shared model
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors       // Shared model
│   └── diffusion_models/
│       └── hunyuan_video_t2v_720p_bf16.safetensors  // T2V model
```

### 3. Steps to Run the Workflow

![ComfyUI Hunyuan Video T2V Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg)

1. Ensure the `DualCLIPLoader` node has loaded these models:
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_t2v_720p_bf16.safetensors`
3. Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

<Tip>
  When the `length` parameter in the `EmptyHunyuanLatentVideo` node is set to 1, the model can generate a static image.
</Tip>

## Hunyuan Image-to-Video Workflow

Hunyuan Image-to-Video model was open-sourced on March 6, 2025, based on the HunyuanVideo framework. It transforms static images into smooth, high-quality videos and also provides LoRA training code to customize special video effects like hair growth, object transformation, etc.

Currently, the Hunyuan Image-to-Video model has two versions:

* v1 "concat": Better motion fluidity but less adherence to the image guidance
* v2 "replace": Updated the day after v1, with better image guidance but seemingly less dynamic compared to v1

<div class="flex justify-between">
  <div class="text-center">
    <p>v1 "concat"</p>

    <img src="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video.webp" alt="HunyuanVideo v1" />
  </div>

  <div class="text-center">
    <p>v2 "replace"</p>

    <img src="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video_v2.webp" alt="HunyuanVideo v2" />
  </div>
</div>

### Shared Model for v1 and v2 Versions

Download the following file and save it to the `ComfyUI/models/clip_vision` directory:

* [llava\_llama3\_vision.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors?download=true)

### V1 "concat" Image-to-Video Workflow

#### 1. Workflow and Asset

Download the workflow image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Hunyuan Image-to-Video v1](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v1_robot.webp)

Download the image below, which we'll use as the starting frame for the image-to-video generation:
![Starting Frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/hunyuan-video/i2v/robot-ballet.png)

#### 2. Related models manual installation

* [hunyuan\_video\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_image_to_video_720p_bf16.safetensors?download=true)

Ensure you have all these model files in the correct locations:

```
ComfyUI/
├── models/
│   ├── clip_vision/
│   │   └── llava_llama3_vision.safetensors                     // I2V shared model
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                                  // Shared model
│   │   └── llava_llama3_fp8_scaled.safetensors                 // Shared model
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors                  // Shared model
│   └── diffusion_models/
│       └── hunyuan_video_image_to_video_720p_bf16.safetensors  // I2V v1 "concat" version model
```

#### 3. Steps to Run the Workflow

![ComfyUI Hunyuan Video I2V v1 Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg)

1. Ensure that `DualCLIPLoader` has loaded these models:
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure that `Load CLIP Vision` has loaded `llava_llama3_vision.safetensors`
3. Ensure that `Load Image Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. Ensure that `Load VAE` has loaded `vae_name: hunyuan_video_vae_bf16.safetensors`
5. Ensure that `Load Diffusion Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### v2 "replace" Image-to-Video Workflow

The v2 workflow is essentially the same as the v1 workflow. You just need to download the **replace** model and use it in the `Load Diffusion Model` node.

#### 1. Workflow and Asset

Download the workflow image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Hunyuan Image-to-Video v2](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v2_fennec_gril.webp)

Download the image below, which we'll use as the starting frame for the image-to-video generation:
![Starting Frame](https://comfyanonymous.github.io/ComfyUI_examples/flux/flux_dev_example.png)

#### 2. Related models manual installation

* [hunyuan\_video\_v2\_replace\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors?download=true)

Ensure you have all these model files in the correct locations:

```
ComfyUI/
├── models/
│   ├── clip_vision/
│   │   └── llava_llama3_vision.safetensors                                // I2V shared model
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                                             // Shared model
│   │   └── llava_llama3_fp8_scaled.safetensors                            // Shared model
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors                             // Shared model
│   └── diffusion_models/
│       └── hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors  // V2 "replace" version model
```

#### 3. Steps to Run the Workflow

![ComfyUI Hunyuan Video I2V v2 Workflow](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg)

1. Ensure the `DualCLIPLoader` node has loaded these models:
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure the `Load CLIP Vision` node has loaded `llava_llama3_vision.safetensors`
3. Ensure the `Load Image Model` node has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`
5. Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors`
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Try it yourself

Here are some images and prompts we provide. Based on that content or make an adjustment to create your own video.

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png)

```
Futuristic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene
```

***

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/samurai.png)

```
Samurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic
```

***

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/a_flying_car.png)

```
flying car fastly moving and flying through the city
```

***

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png)

```
cyberpunk car race in night city, dynamic, super fast, fast shot
```


# LTX-Video
Source: https://docs.comfy.org/tutorials/video/ltxv



[LTX-Video](https://huggingface.co/Lightricks/LTX-Video) is a very efficient video model by lightricks. The important thing with this model is to give it long descriptive prompts.

## Multi Frame Control

Allows you to control the video with a series of images. You can download the input images: [starting frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house1.png) and [ending frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house2.png).

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/workflow.webp" alt="LTX-Video Multi Frame Control" />

<Tip>
  Drag the video directly into ComfyUI to run the workflow.
</Tip>

## Image to Video

Allows you to control the video with a first [frame image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/girl1.png).

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/workflow.webp" alt="LTX-Video Image to Video" />

<Tip>
  Drag the video directly into ComfyUI to run the workflow.
</Tip>

## Text to Video

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/t2v.webp" alt="LTX-Video Text to Video" />

<Tip>
  Drag the video directly into ComfyUI to run the workflow.
</Tip>

## Requirements

Download the following models and place them in the locations specified below:

* [ltx-video-2b-v0.9.5.safetensors](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/mochi_preview_repackaged/resolve/main/split_files/text_encoders/t5xxl_fp16.safetensors?download=true)

```
├── checkpoints/
│   └── ltx-video-2b-v0.9.5.safetensors
└── text_encoders/
    └── t5xxl_fp16.safetensors
```


# ComfyUI Wan2.1 Fun Control Video Examples
Source: https://docs.comfy.org/tutorials/video/wan/fun-control

This guide demonstrates how to use Wan2.1 Fun Control in ComfyUI to generate videos with control videos

## About Wan2.1-Fun-Control

**Wan2.1-Fun-Control** is an open-source video generation and control project developed by Alibaba team.
It introduces innovative Control Codes mechanisms combined with deep learning and multimodal conditional inputs to generate high-quality videos that conform to preset control conditions. The project focuses on precisely guiding generated video content through multimodal control conditions.

Currently, the Fun Control model supports various control conditions, including **Canny (line art), Depth, OpenPose (human posture), MLSD (geometric edges), and trajectory control.**
The model also supports multi-resolution video prediction with options for 512, 768, and 1024 resolutions at 16 frames per second, generating videos up to 81 frames (approximately 5 seconds) in length.

Model versions:

* **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**
* **14B** High-performance: Model size reaches 32GB+, offering better results but **requiring higher VRAM**

Here are the relevant code repositories:

* [Wan2.1-Fun-1.3B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control)
* [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control)
* Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

ComfyUI now **natively supports** the Wan2.1 Fun Control model. Before starting this tutorial, please update your ComfyUI to ensure you're using a version after [this commit](https://github.com/comfyanonymous/ComfyUI/commit/3661c833bcc41b788a7c9f0e7bc48524f8ee5f82).

In this guide, we'll provide two workflows:

1. A workflow using only native Comfy Core nodes
2. A workflow using custom nodes

<Tip>
  Due to current limitations in native nodes for video support, the native-only workflow ensures users can complete the process without installing custom nodes.
  However, we've found that providing a good user experience for video generation is challenging without custom nodes, so we're providing both workflow versions in this guide.
</Tip>

## Model Installation

You only need to install these models once. The workflow images also contain model download information, so you can choose your preferred download method.

The following models can be found at [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334)

Click the corresponding links to download. If you've used Wan-related workflows before, you only need to download the **Diffusion models**.

**Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:

* [wan2.1\_fun\_control\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_control_1.3B_bf16.safetensors?download=true)
* [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/blob/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-Control.safetensors` after downloading

**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage location:

```
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 diffusion_models/
│   │   └── wan2.1_fun_control_1.3B_bf16.safetensors
│   ├── 📂 text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── 📂 vae/
│   │   └── wan_2.1_vae.safetensors
│   └── 📂 clip_vision/
│       └──  clip_vision_h.safetensors                 
```

## ComfyUI Native Workflow

In this workflow, we use videos converted to **WebP format** since the `Load Image` node doesn't currently support mp4 format. We also use **Canny Edge** to preprocess the original video.
Because many users encounter installation failures and environment issues when installing custom nodes, this version of the workflow uses only native nodes to ensure a smoother experience.

Thanks to our powerful ComfyUI authors who provide feature-rich nodes. If you want to directly check the related version, see [Workflow Using Custom Nodes](#workflow-using-custom-nodes).

### 1. Workflow File Download

#### 1.1 Workflow File

Download the image below and drag it into ComfyUI to load the workflow:

![Wan2.1 Fun Control Native Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_native.webp)

#### 1.2 Input Images and Videos Download

Please download the following image and video for input:

![Input Reference Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_remix.png)

![Input Reference Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_video.webp)

### 2. Complete the Workflow Step by Step

![Wan2.1 Fun Control Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_native_flow_diagram.png)

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node (renamed to `Start_image`)
6. Upload the control video to the second `Load Image` node. Note: This node currently doesn't support mp4, only WebP videos
7. (Optional) Modify the prompt (both English and Chinese are supported)
8. (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions
9. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### 3. Usage Notes

* Since we need to input the same number of frames as the control video into the `WanFunControlToVideo` node, if the specified frame count exceeds the actual control video frames, the excess frames may display scenes not conforming to control conditions. We'll address this issue in the [Workflow Using Custom Nodes](#workflow-using-custom-nodes)
* Avoid setting overly large dimensions, as this can make the sampling process very time-consuming. Try generating smaller images first, then upscale
* Use your imagination to build upon this workflow by adding text-to-image or other types of workflows to achieve direct text-to-video generation or style transfer
* Use tools like [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) for richer control options

## Workflow Using Custom Nodes

We'll need to install the following two custom nodes:

* [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
* [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

You can use [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) to install missing nodes or follow the installation instructions for each custom node package.

### 1. Workflow File Download

#### 1.1 Workflow File

Download the image below and drag it into ComfyUI to load the workflow:

![Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.webp)

<Note>
  Due to the large size of video files, you can also click [here](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.json) to download the workflow file in JSON format.
</Note>

#### 1.2 Input Images and Videos Download

Please download the following image and video for input:
![Input Reference Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/02-robot's_eye.png)

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/02-man's_eye.mp4" />

### 2. Complete the Workflow Step by Step

![Wan2.1 Fun Control Workflow Using Custom Nodes Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png)

> The model part is essentially the same. If you've already experienced the native-only workflow, you can directly upload the corresponding images and run it.

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node
6. Upload an mp4 format video to the `Load Video(Upload)` custom node. Note that the workflow has adjusted the default `frame_load_cap`
7. For the current image, the `DWPose Estimator` only uses the `detect_face` option
8. (Optional) Modify the prompt (both English and Chinese are supported)
9. (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions
10. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### 3. Workflow Notes

Thanks to the ComfyUI community authors for their custom node packages:

* This example uses `Load Video(Upload)` to support mp4 videos
* The `video_info` obtained from `Load Video(Upload)` allows us to maintain the same `fps` for the output video
* You can replace `DWPose Estimator` with other preprocessors from the `ComfyUI-comfyui_controlnet_aux` node package
* Prompts support multiple languages

## Usage Tips

![Apply Multi Control Videos](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/apply_multi_control_videos.jpg)

* A useful tip is that you can combine multiple image preprocessing techniques and then use the `Image Blend` node to achieve the goal of applying multiple control methods simultaneously.

* You can use the `Video Combine` node from `ComfyUI-VideoHelperSuite` to save videos in mp4 format

* We use `SaveAnimatedWEBP` because we currently don't support embedding workflow into **mp4** and some other custom nodes may not support embedding workflow too. To preserve the workflow in the video, we choose  `SaveAnimatedWEBP` node.

* In the `WanFunControlToVideo` node, `control_video` is not mandatory, so sometimes you can skip using a control video, first generate a very small video size like 320x320, and then use them as control video input to achieve consistent results.

* [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)

* [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)


# ComfyUI Wan2.1 Fun InP Video Examples
Source: https://docs.comfy.org/tutorials/video/wan/fun-inp

This guide demonstrates how to use Wan2.1 Fun InP in ComfyUI to generate videos with first and last frame control

## About Wan2.1-Fun-InP

**Wan-Fun InP** is an open-source video generation model released by Alibaba, part of the Wan2.1-Fun series, focusing on generating videos from images with first and last frame control.

**Key features**:

* **First and last frame control**: Supports inputting both first and last frame images to generate transitional video between them, enhancing video coherence and creative freedom. Compared to earlier community versions, Alibaba's official model produces more stable and significantly higher quality results.
* **Multi-resolution support**: Supports generating videos at 512×512, 768×768, 1024×1024 and other resolutions to accommodate different scenario requirements.

**Model versions**:

* **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**
* **14B** High-performance: Model size reaches 32GB+, offering better results but requiring **higher VRAM**

Below are the relevant model weights and code repositories:

* [Wan2.1-Fun-1.3B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Input)
* [Wan2.1-Fun-14B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Input)
* Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

<Tip>
  Currently, ComfyUI natively supports the Wan2.1 Fun InP model. Before starting this tutorial, please update your ComfyUI to ensure your version is after [this commit](https://github.com/comfyanonymous/ComfyUI/commit/0a1f8869c9998bbfcfeb2e97aa96a6d3e0a2b5df).
</Tip>

## Wan2.1 Fun InP Workflow

Download the image below and drag it into ComfyUI to load the workflow:

![Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_inp/wan2.1_fun_inp.webp)

### 1. Workflow File Download

### 2. Manual Model Installation

If automatic model downloading is ineffective, please download the models manually and save them to the corresponding folders.

The following models can be found at [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334)

**Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:

* [wan2.1\_fun\_inp\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_inp_1.3B_bf16.safetensors?download=true)
* [Wan2.1-Fun-14B-InP](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP/resolve/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-InP.safetensors` after downloading

**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage location:

```
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 diffusion_models/
│   │   └── wan2.1_fun_inp_1.3B_bf16.safetensors
│   ├── 📂 text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── 📂 vae/
│   │   └── wan_2.1_vae.safetensors
│   └── 📂 clip_vision/
│       └──  clip_vision_h.safetensors                 
```

### 3. Complete the Workflow Step by Step

![ComfyUI Wan2.1 Fun InP Video Generation Workflow Diagram](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_inp_flow_diagram.png)

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_inp_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node (renamed to `Start_image`)
6. Upload the ending frame to the second `Load Image` node
7. (Optional) Modify the prompt (both English and Chinese are supported)
8. (Optional) Adjust the video size in `WanFunInpaintToVideo`, avoiding overly large dimensions
9. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### 4. Workflow Notes

<Tip>
  Please make sure to use the correct model, as `wan2.1_fun_inp_1.3B_bf16.safetensors` and `wan2.1_fun_control_1.3B_bf16.safetensors` are stored in the same folder and have very similar names. Ensure you're using the right model.
</Tip>

* When using Wan Fun InP, you may need to frequently modify prompts to ensure the accuracy of the corresponding scene transitions.

## Other Wan2.1 Fun InP or video-related custom node packages

* [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
* [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)
* [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)


# ComfyUI Wan2.1 Video Examples
Source: https://docs.comfy.org/tutorials/video/wan/wan-video

This guide demonstrates how to generate videos with first and last frames using Wan2.1 Video in ComfyUI

Wan2.1 Video series is a video generation model open-sourced by Alibaba in February 2025 under the [Apache 2.0 license](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file).
It offers two versions:

* 14B (14 billion parameters)
* 1.3B (1.3 billion parameters)
  Covering multiple tasks including text-to-video (T2V) and image-to-video (I2V).
  The model not only outperforms existing open-source models in performance but more importantly, its lightweight version requires only 8GB of VRAM to run, significantly lowering the barrier to entry.

<video controls>
  <source src="https://github.com/user-attachments/assets/4aca6063-60bf-4953-bfb7-e265053f49ef" type="video/mp4" />
</video>

* [Wan2.1 Code Repository](https://github.com/Wan-Video/Wan2.1)
* [Wan2.1 Model Repository](https://huggingface.co/Wan-AI)

## Wan2.1 ComfyUI Native Workflow Examples

<Tip>
  Please update ComfyUI to the latest version before starting the examples to make sure you have native Wan Video support.
</Tip>

## Model Installation

All models mentioned in this guide can be found [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files). Below are the common models you'll need for the examples in this guide, which you can download in advance:

Choose one version from **Text encoders** to download:

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage locations:

```
ComfyUI/
├── models/
│   ├── diffusion_models/
│   ├── ...                  # Let's download the models in the corresponding workflow
│   ├── text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── vae/
│   │   └──  wan_2.1_vae.safetensors
│   └── clip_vision/
│       └──  clip_vision_h.safetensors   
```

<Note>
  For diffusion models, we'll use the fp16 precision models in this guide because we've found that they perform better than the bf16 versions. If you need other precision versions, please visit [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models) to download them.
</Note>

## Wan2.1 Text-to-Video Workflow

Before starting the workflow, please download [wan2.1\_t2v\_1.3B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

> If you need other t2v precision versions, please visit [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models) to download them.

### 1. Workflow File Download

Download the file below and drag it into ComfyUI to load the corresponding workflow:

![Wan2.1 Text-to-Video Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_t2v_1.3b.webp)

### 2. Complete the Workflow Step by Step

![ComfyUI Wan2.1 Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg)

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_t2v_1.3B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. (Optional) You can modify the video dimensions in the `EmptyHunyuanLatentVideo` node if needed
5. (Optional) If you need to modify the prompts (positive and negative), make changes in the `CLIP Text Encoder` node at number `5`
6. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

## Wan2.1 Image-to-Video Workflow

**Since Wan Video separates the 480P and 720P models**, we'll need to provide examples for both resolutions in this guide. In addition to using different models, they also have slight parameter differences.

### 480P Version

#### 1. Workflow and Input Image

Download the image below and drag it into ComfyUI to load the corresponding workflow:
![Wan2.1 Image-to-Video Workflow 14B 480P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_480P.webp)

We'll use the following image as input:

![Wan2.1 Image-to-Video Workflow 14B 480P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/flux_dev_example.png)

#### 2. Model Download

Please download [wan2.1\_i2v\_480p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

#### 3. Complete the Workflow Step by Step

![ComfyUI Wan2.1 Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg)

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_480p_14B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model
5. Upload the provided input image in the `Load Image` node
6. (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node
7. (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed
8. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

### 720P Version

#### 1. Workflow and Input Image

Download the image below and drag it into ComfyUI to load the corresponding workflow:
![Wan2.1 Image-to-Video Workflow 14B 720P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_720P.webp)

We'll use the following image as input:

![Wan2.1 Image-to-Video Workflow 14B 720P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/magician.png)

#### 2. Model Download

Please download [wan2.1\_i2v\_720p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_720p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

#### 3. Complete the Workflow Step by Step

![ComfyUI Wan2.1 Workflow Steps](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg)

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_720p_14B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model
5. Upload the provided input image in the `Load Image` node
6. (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node
7. (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed
8. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation


# 开始 ComfyUI 的 AI 绘图之旅
Source: https://docs.comfy.org/zh-CN/get_started/first_generation

本部分教程将会带你完成首次 ComfyUI 的图片生成，了解并熟悉 ComfyUI 中的一些界面基础操作，如工作流加载、模型安装、图片生成等

本篇的主要目的是带你初步了解 ComfyUI 熟悉 ComfyUI 的一些基础操作，并引导你首次的图片生成

1. 加载示例工作流
   * 从 ComfyUI 加载`Workflows template`中的`Text to Image`工作流
   * 使用带有`metadata` 的图片中加载工作流
2. 指导你完成模型
   * 自动安装模型
   * 手动安装模型
   * 使用 **ComfyUI Manager** 的模型管理功能安装模型
3. 进行一次文本到图片的生成

## 关于文生图的说明

**文生图（Text to Image）**，是 AI 绘图的基础，通过输入文本描述来生成对应的图片，是 AI 绘图最常用的功能之一，你可以理解成你把你的**绘图要求(正向提示词、负向提示词)**告诉一个**画家(绘图模型)**，画家会根据你的要求，画出你想要的内容，由于本篇教程主要是为了引导你开始 ComfyUI 的使用，对于文生图的详细说明，我们将在[文生图](/zh-CN/tutorials/basic/image-to-image)章节进行详细讲解

## ComfyUI 文生图工作流教程讲解

### 1. 启动 ComfyUI

请确定你已经按照安装部分的指南完成了 ComfyUI 的启动，并可以成功打开 ComfyUI 的页面

![ComfyUI界面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/interface/comfyui-boot-screen.jpg)

如果你还未安装 ComfyUI 请根据你的设备情况选择一个合适的版本进行安装

<AccordionGroup>
  <Accordion title="ComfyUI 桌面版(推荐)">
    ComfyUI 桌面版目前支持 **Windows 及 MacOS(Apple Silicon)** 的独立安装，目前仍在 Beta 版本

    * 代码开源在 [Github](https://github.com/Comfy-Org/desktop)

    你可以从下面选择适合你的系统和硬件开始安装 ComfyUI

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI桌面版(Windows)安装指南" icon="link" href="/zh-CN/installation/desktop/windows">
          适合带有 **Nvdia** 显卡 **Windows** 版本的 ComfyUI 桌面版
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI桌面版(MacOS)安装指南" icon="link" href="/zh-CN/installation/desktop/macos">
          适合带有 **Apple Silicon** 的 MacOS ComfyUI 桌面版
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI桌面版，**暂时没有 Linux 的预构建**，请访问[手动安装](/zh-CN/installation/manual_install)部分进行 ComfyUI 的安装</Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI 便携版(Windows)">
    <Card title="ComfyUI桌面版(Windows)安装指南" icon="link" href="/zh-CN/installation/comfyui_portable_windows">
      支持 **Navida 显卡** 和在 **CPU** 运行的 **Windows** ComfyUI 版本，始终使用最新 commit 的代码
    </Card>
  </Accordion>

  <Accordion title="手动安装 ComfyUI">
    <Card title="ComfyUI 手动安装教程" icon="link" href="/zh-CN/installation/manual_install">
      支持所有的系统类型和 GPU 类型（Nvidia、AMD、Intel、Apple Silicon、Ascend NPU、寒武纪 MLU）的用户都可以尝试使用手动安装 ComfyUI
    </Card>
  </Accordion>
</AccordionGroup>

### 2. 加载默认文生图工作流

正常情况下，打开 ComfyUI 后是会自动加载默认的文生图工作流的, 不过你仍旧可以尝试以下不同方式加载工作流来熟悉 ComfyUI 的一些基础操作

<Tabs>
  <Tab title="从 Workflow Template 加载">
    ![ComfyUI 界面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1.jpg)
    请对照图片中序号所对应的顺序进行操作

    1. 点击 ComfyUI 界面右下角的**Fit View**按钮，防止已加载工作流是在视图外导致不可见
    2. 点击侧边栏的**文件夹图标（workflows）**
    3. 点击 工作流（Workflows）面板顶部的**浏览工作流示例（Browse example workflows）** 按钮

    下图继续

    ![加载工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg)

    4. 选择默认的第一个工作流 **Image Generation** 以加载图标

    或者你也可以从`workflow`菜单中选择**Browse workflow templates** 浏览工作流模板
    ![ComfyUI 菜单 - 浏览工作流模板](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg)
  </Tab>

  <Tab title="从带有 metadata 的图片中加载">
    所有用 ComfyUI 生成的图片，都会带有 metadata 信息，这些信息会包含图片的 workflow 信息，你可以通过这些信息来加载对应的 workflow。

    现在，让我们尝试一下，请保存下面的工作流图片，然后直接拖入 ComfyUI 的界面中，或者使用菜单 **工作流（Workflows）** -> **打开（Open）** 打开这个图片以加载对应的 workflow

    ![ComfyUI-文生图工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/text-to-image-workflow.png)
  </Tab>

  <Tab title="从 workflow.json 文件中加载">
    ComfyUI 工作流还可以 json 格式存储，当我们完成一个工作流后，可以使用菜单 **工作流（Workflows）** -> **导出（Export）** 导出，这样对应的工作流就可以被保存为 json 文件中加载

    现在，让我们尝试一下，点击下面的按钮，前往 Github 下载对应的 text-to-image.json 文件

    <a className="prose" href="https://github.com/Comfy-Org/docs/blob/main/public/text-to-image.json" download style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
      <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>前往下载 text-to-image.json 文件</p>
    </a>

    下载完成后，请使用菜单 **工作流（Workflows）** -> **打开（Open）** 打开这个 json 文件以加载对应的 workflow
  </Tab>
</Tabs>

### 3. 安装绘图模型

通常在 ComfyUI 的初始安装中，并不会包含任何的绘图模型，但是模型是我们运行图片生成必不可少的部分。

在你完成第二步，工作流的加载后，如果你的电脑上没有安装[v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) 这个模型文件时，一般会出现下图的提示

![模型缺失](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg)

你可以直接选择点击 `Download` 按钮，让 ComfyUI 自动完成对应的模型的下载，但由于在有些地区不能够顺利访问对应模型的下载源，所以在这个步骤中，我将说明几种不同的模型安装方法。

无论使用哪种方法，模型都会被保存到 `<你的 ComfyUI 安装位置>/ComfyUI/models/` 文件夹下，你可以在你的电脑上尝试找到这个文件夹位置，你可以看到许多文件夹比如 `checkpoints`、`embeddings`、`vae`、`lora`、`upscale_model` 等，这些都是不同的模型保存的文件夹，通常以文件夹名称区分，ComfyUI 在启动时会检测这些文件夹下的模型文件，以及`extra_models_config.yaml` 文件中配置的文件路径

![ComfyUI 模型文件夹](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg)

被检测到的不同的文件夹里的模型将可以在 ComfyUI 的不同 **模型加载节点** 里使用，下面让我们开始了解不同模型的安装方式：

<Tabs>
  <Tab title="自动下载模型">
    在你点击 **Download** 按钮后，ComfyUI 将会执行下载,根据你使用的版本不同，将会执行不同的行为

    <Tabs>
      <Tab title="ComfyUI 桌面版">
        桌面版将自动完成模型的下载并保存到 `<你的 ComfyUI 安装位置>/ComfyUI/models/checkpoints` 目录下
        你可以等待安装完成或者在侧边栏的模型面板里查看安装进度

        ![模型下载进度](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg)

        如果一切顺利，模型应该可以自动下载到本地，如果长时间未下载成功，请尝试其它安装方法
      </Tab>

      <Tab title="ComfyUI 便携版">
        浏览器将会执行文件下载，请在下载完成后，将文件保存到的 `<你的 ComfyUI 安装位置>/ComfyUI_windows_portable/ComfyUI/models/checkpoints` 目录下
      </Tab>
    </Tabs>
  </Tab>

  <Tab title="使用 ComfyUI Manager 安装模型">
    [ComfyUI Manager](https://github.com/ltdrdata/ComfyUI-Manager) 是由 [ltdrdata](https://github.com/ltdrdata) 开发的用于扩展和管理自定义节点、模型及插件的工具，目前 ComfyUI 的安装过程会自动完成 ComfyUI Manager 的安装，下面的步骤将会引导你使用 ComfyUI Manager 安装模型

    <Steps>
      <Step title="打开 ComfyUI Manager">
        ![ComfyUI Manager 安装](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager.jpg)

        如图，点击对应的 `Manager` 按钮，即可打开 ComfyUI Manager 的界面
      </Step>

      <Step title="打开模型管理界面（Model Manager）">
        ![ComfyUI Manager 模型管理](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-model-manager.jpg)

        如图，点击对应的 `Model Manager` 按钮，即可打开模型管理界面
      </Step>

      <Step title="搜索模型并安装模型">
        ![ComfyUI Manager 模型下载](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-download-model.jpg)

        1. 请在搜索框输入`v1-5-pruned-emaonly.ckpt`
        2. 在搜索结果里，点击对应的 `install` 按钮，即可下载模型

        不过由于目前各类模型的更新迭代速度较快，你不一定可以在这里找到所有的模型，另外在某些地区因为无法正常访问 ComfyUI Manager 的模型下载源，所以会存在下载不成功的情况，如果长时间无法下载成功，请尝试手动安装
      </Step>
    </Steps>
  </Tab>

  <Tab title="手动完成模型的安装">
    请访问模型地址：[前往下载 v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)
    参考下面图片完成对应模型的下载

    ![Hugging Face 模型下载](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-5-hugging-face.jpg)

    下载完成后，请将对应的**v1-5-pruned-emaonly-fp16.safetensors** 文件保存到以下位置

    <Tabs>
      <Tab title="ComfyUI 桌面版">
        请找到你在安装过程中设置的 ComfyUI 安装位置，将对应模型文件保存到以下文件夹位置 `<你的 ComfyUI 安装位置>/ComfyUI/models/checkpoints`

        ![ComfyUI 桌面版模型保存位置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-6-2-desktop.jpg)
      </Tab>

      <Tab title="ComfyUI便携版本">
        找到你解压后的便携版的文件夹，在**ComfyUI\_windows\_portable/ComfyUI/models/checkpoints** 文件夹下完成模型的保存
        ![ComfyUI 便携版模型保存位置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-6-1-portable.jpg)
      </Tab>

      <Tab title="其它版本">
        请参考 桌面版和便携版部分的说明查找 **ComfyUI/models/checkpoints**文件夹位置
      </Tab>
    </Tabs>

    完成对应保存操作后，请刷新或者重启 ComfyUI 保证对应模型可以被 ComfyUI 检测
  </Tab>
</Tabs>

### 4. 加载模型，并进行第一次图片生成

在完成了对应的绘图模型安装后，请参考下图步骤加载对应的模型，并进行第一次图片的生成

![图片生成](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg)
请对应图片序号，完成下面操作

1. 请在 **Load Checkpoint** 节点使用箭头或者点击文本区域确保 **v1-5-pruned-emaonly-fp16.safetensors** 被选中，且左右切换箭头不会出现 **null** 的文本
2. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl + enter(回车)` 来执行图片生成

等待对应流程执行完成后，你应该可以在界面的 **保存图像(Save Image)** 节点中看到对应的图片结果，可以在上面右键保存到本地

![ComfyUI 首次图片生成结果](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg)

对于文生图的详细说明，下面的指南中会有详细的说明和指导

<Card title="ComfyUI 文生图工作流示例说明" icon="link" href="/zh-CN/tutorials/basic/text-to-image">
  点击这里查看文生图工作流的详细说明
</Card>

## 故障排除

### 模型加载问题

如果 `Load Checkpoint` 节点没有任何模型可以选择，或者显示为 **null**，请先确认你的模型安装位置正确，或者尝试 **刷新** 或者 **重启 ComfyUI** 使得对应文件夹下的模型可以被检测到


# 关于 ComfyUI
Source: https://docs.comfy.org/zh-CN/get_started/introduction

ComfyUI 的官方文档。欢迎在[这里](https://github.com/Comfy-Org/docs)贡献文档。

<img className="block dark:hidden" src="https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui_screenshot.png" alt="Hero Light" />

<img className="hidden dark:block" src="https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui_screenshot.png" alt="Hero Dark" />

## [ComfyUI](https://github.com/comfyanonymous/ComfyUI)

最强大和模块化的稳定扩散 GUI 和后端。由 [comfyanonymous](https://github.com/comfyanonymous) 和其他[贡献者](https://github.com/comfyanonymous/ComfyUI/graphs/contributors)编写。

* **ComfyUI** 是一个基于**节点**的生成式人工智能的界面和推理引擎
* 用户可以通过节点组合各种AI模型和操作，实现更高的定制化和可控的内容生成
* ComfyUI 完全开源，并可以在你的本地设备上运行

## 开始使用 ComfyUI

### ComfyUI 安装

ComfyUI 目前提供多种安装方式，支持 Windows、MacOS 以及 Linux 系统:

<AccordionGroup>
  <Accordion title="ComfyUI 桌面版(推荐)">
    ComfyUI 桌面版目前支持 **Windows 及 MacOS(Apple Silicon)** 的独立安装，目前仍在 Beta 版本

    * 代码开源在 [Github](https://github.com/Comfy-Org/desktop)

    你可以从下面选择适合你的系统和硬件开始安装 ComfyUI

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI桌面版(Windows)安装指南" icon="link" href="/zh-CN/installation/desktop/windows">
          适合带有 **Nvdia** 显卡 **Windows** 版本的 ComfyUI 桌面版
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI桌面版(MacOS)安装指南" icon="link" href="/zh-CN/installation/desktop/macos">
          适合带有 **Apple Silicon** 的 MacOS ComfyUI 桌面版
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI桌面版，**暂时没有 Linux 的预构建**，请访问[手动安装](/zh-CN/installation/manual_install)部分进行 ComfyUI 的安装</Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI 便携版(Windows)">
    <Card title="ComfyUI桌面版(Windows)安装指南" icon="link" href="/zh-CN/installation/comfyui_portable_windows">
      支持 **Navida 显卡** 和在 **CPU** 运行的 **Windows** ComfyUI 版本，始终使用最新 commit 的代码
    </Card>
  </Accordion>

  <Accordion title="手动安装 ComfyUI">
    <Card title="ComfyUI 手动安装教程" icon="link" href="/zh-CN/installation/manual_install">
      支持所有的系统类型和 GPU 类型（Nvidia、AMD、Intel、Apple Silicon、Ascend NPU、寒武纪 MLU）的用户都可以尝试使用手动安装 ComfyUI
    </Card>
  </Accordion>
</AccordionGroup>

## 参与 ComfyUI 生态建设

如果你正准备开发 ComfyUI 自定义节点（插件）相关功能，请访问以下板块了解开发相关内容
<Note>由于目前中文版本翻译暂未完善，所以请参考英文部分的开发文档说明</Note>

<Card title="自定义节点开发指南" icon="link" href="/custom-nodes/overview">
  了解如何构建一个 ComfyUI 的自定义节点(插件)
</Card>

## 参与贡献撰写文档

在 Github 上 Fork 本文档的[repo](https://github.com/comfyanonymous/ComfyUI), 并提交 PR 给我们


# ComfyUI便携版 Windows
Source: https://docs.comfy.org/zh-CN/installation/comfyui_portable_windows

本篇教程将指导你如何下载和开始使用 ComfyUI Portable(便携版) 并运行对应的程序

**ComfyUI Portable(便携版)** 是一个独立封装完整的 ComfyUI Windows 版本，内部已经整合了 ComfyUI 运行所需的独立的 **Python(python\_embeded)**,只需要解压即可使用,目前便携版本支持通过 **Nvidia** 显卡或者 **CPU** 运行。

本部分指南将引导你完成对应的安装。

<Tip>
  对于 Nvidia 50 系列(Blackwell)显卡，请参考 [系统要求](/zh-CN/installation/nvidia-50-series)部分,确保你的系统环境能够满足 ComfyUI 的运行要求。
</Tip>

## 下载 ComfyUI Portable(便携版)

您可通过点击下面的链接来获取最新的 **ComfyUI Portable(便携版)** 下载链接

<a className="prose" href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>下载 ComfyUI Portable(便携版)</p>
</a>

下载后你可以使用类似解压软件如 [7-ZIP](https://7-zip.org/) 对压缩包进行解压

便携版解压后对应的文件结构及说明如下：

```
ComfyUI_windows_portable
├── 📂ComfyUI                   // ComfyUI 程序主体
├── 📂python_embeded            // 独立的 Python 环境
├── 📂update                    // 用于升级便携版安装包的批处理脚本
├── README_VERY_IMPORTANT.txt   // 英文版本的 ComfyUI 便携版使用说明
├── run_cpu.bat                 // 双击启动 ComfyUI（仅支持 CPU）
└── run_nvidia_gpu.bat          // 双击启动 ComfyUI（仅支持 Nvidia 显卡）
```

## 如何启动 ComfyUI

根据你的电脑情况双击 `run_nvidia_gpu.bat ` 或者 `run_cpu.bat` 来启动 ComfyUI，你会看到对应下图所示的命令的运行

![ComfyUI便携版运行命令提示符](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfyui-portable-cmd.png)

当你看到类似图片中的

```
To see the GUI go to: http://127.0.0.1:8188
```

此时你的 ComfyUI 服务已经启动，正常情况下 ComfyUI 会自动打开你的默认浏览器并访问 `http://127.0.0.1:8188` 地址，如果没有自动打开，请手动打开浏览器并访问该地址。

<Note>使用过程中请不要关闭对应的命令行窗口，否则 ComfyUI 将会停止运行</Note>

## 进行第一次图片生成

安装成功后，你可以参考访问下面的章节，开始你的 ComfyUI 之路。

<Card title="进行第一次图片生成" icon="link" href="/zh-CN/get_started/first_generation">
  本教程将引导你完成第一次的模型安装以及对应的文本到图片的生成
</Card>

## 其它 ComfyUI 便携版相关说明

### 1. ComfyUI 便携版升级

你可以使用 **update** 文件夹下的相关批处理命令完成 ComfyUI 便携版的升级

```
ComfyUI_windows_portable
└─ 📂update
   ├── update.py
   ├── update_comfyui.bat            // 更新 ComfyUI 到最新的 Commit 版本
   ├── update_comfyui_and_python_dependencies.bat  // 请仅在你的运行环境存在问题时使用
   └── update_comfyui_stable.bat       // 更新 ComfyUI 为最新的 stable 版本
```

### 2. ComfyUI 模型共享或自定义模型文件夹存储位置配置

如果你同时有使用 [A1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) 或者想要自定义模型存储位置，你可以通过修改下面的文件来完成配置

```
ComfyUI_windows_portable
└─ 📂ComfyUI
   └── extra_model_paths.yaml.example  // 此文件为配置模板
```

请复制 `extra_model_paths.yaml.example` 并重命名为 `extra_model_paths.yaml`

下面是对应的原始配置文件内容

```yaml
#Rename this to extra_model_paths.yaml and ComfyUI will load it


#config for a1111 ui
#all you have to do is change the base_path to where yours is installed
a111:
    base_path: path/to/stable-diffusion-webui/

    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet

#config for comfyui
#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.

#comfyui:
#     base_path: path/to/comfyui/
#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads
#     #is_default: true
#     checkpoints: models/checkpoints/
#     clip: models/clip/
#     clip_vision: models/clip_vision/
#     configs: models/configs/
#     controlnet: models/controlnet/
#     diffusion_models: |
#                  models/diffusion_models
#                  models/unet
#     embeddings: models/embeddings/
#     loras: models/loras/
#     upscale_models: models/upscale_models/
#     vae: models/vae/

#other_ui:
#    base_path: path/to/ui
#    checkpoints: models/checkpoints
#    gligen: models/gligen
#    custom_nodes: path/custom_nodes

```

比如你的 WebUI 位于 `D:\stable-diffusion-webui\ ` 下，你可以修改对应的配置为

```yaml
a111:
    base_path: D:\stable-diffusion-webui\
    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet
```

这样类似 `D:\stable-diffusion-webui\models\Stable-diffusion\` 下的模型就可以被 ComfyUI 便携版检测到并使用，同样你也可以自定添加其它自定义模型位置配置

### 3. ComfyUI 便携版设置局域网访问

如果你的 ComfyUI 运行在局域网内，想要其它的设备也可以访问到 ComfyUI，你可以通过记事本修改 `run_nvidia_gpu.bat` 或者 `run_cpu.bat` 文件来完成配置，主要通过添加`--listen`来添加监听地址
下面的示例是添加了 `--listen` 参数的 `run_nvidia_gpu.bat` 文件命令

```bat
.\python_embeded\python.exe -s ComfyUI\main.py --listen --windows-standalone-build
pause
```

当启用 ComfyUI 后您会发现最后的运行地址会变为

```
Starting server

To see the GUI go to: http://0.0.0.0:8188
To see the GUI go to: http://[::]:8188
```

你可以通过 `WIN + R` 输入`cmd` 打开命令行，输入 `ipconfig` 来查看你的局域网 IP 地址，然后在其它设备上输入 `http://你的局域网IP:8188` 来访问 ComfyUI


# Linux桌面版
Source: https://docs.comfy.org/zh-CN/installation/desktop/linux

本文将介绍 ComfyUI Desktop MacOS 版本的下载以及安装使用

<Warning>Linux预建包尚不可用。请尝试[手动安装](/zh-CN/installation/manual_install)。 </Warning>


# MacOs桌面版
Source: https://docs.comfy.org/zh-CN/installation/desktop/macos

本文将介绍 ComfyUI Desktop MacOS 版本的下载以及安装使用

export const log_path_0 = "~/Library/Logs/ComfyUI"

export const config_path_0 = "~/Library/Application Support/ComfyUI"

**ComfyUI 桌面版（Desktop）** 是一个独立的安装版本，可以像常规软件一样安装，支持快捷安装自动配置 **Python环境及依赖** ，支持导入已有的 ComfyUI 设置、模型、工作流和文件

ComfyUI 桌面版是一个开源项目，完整代码请访问 [这里](https://github.com/Comfy-Org/desktop)

<Note>ComfyUI 桌面版（MacOS） 目前仅支持 Apple Silicon</Note>

本篇教程将引导你完成对应的软件安装，并说明相关安装配置说明。

<Warning>由于 **ComfyUI 桌面版** 仍旧处于 **Beta** 状态，实际的安装过程可能会发生变化</Warning>

## ComfyUI 桌面版（MacOS）下载

请点击下面的按钮下载对应的针对 MacOS 系统的 **ComfyUI 桌面版** 安装包

<a className="prose" href="https://download.comfy.org/mac/dmg/arm64" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download for MacOS</p>
</a>

## ComfyUI 桌面版安装步骤

双击下载到的安装包文件，如图所示，请将**ComfyUI**程序，按键头所示拖入**Applications**文件夹

![ComfyUI 安装包](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0.png)

如果打开安装包后你的文件夹显示如下，在图标上出现禁止符号，说明你当前的系统版本与当前 **ComfyUI 桌面版**并不兼容
![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-0-1.png)

然后在 **启动台(Lanchpad)** 找到对应的 **ComfyUI 图标** 点击将进入 ComfyUI 的初始化设置
![ComfyUI Lanchpad](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-1.jpg)

## ComfyUI 桌面版初始化流程

<Steps>
  <Step title="开始界面">
    <Tabs>
      <Tab title="正常启动">
        ![ComfyUI 安装步骤 - 起始](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-2.png)
        点击 **Get Started** 开始初始化步骤
      </Tab>

      <Tab title="维护页">
        安装 ComfyUI 可能会出现许多问题。也许在安装 pytorch（15 GB）时网络连接失败，或者你没有安装 git，当检测到问题时，维护页面会自动打开，并提供解决问题的方法。

        你可以使用它来解决大多数问题：

        * 创建一个 Python 虚拟环境
        * 重新安装所有缺失的核心依赖项到由桌面管理的 Python 虦虚拟环境
        * 安装 git，VC redis
        * 选择一个新的安装位置

        默认维护页面会显示当前报错的内容

        ![ComfyUI 维护页面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-1.jpg)

        点击 `All` 可以切换查看可以操作的所有内容

        ![ComfyUI 维护页面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-2.jpg)
      </Tab>
    </Tabs>
  </Step>

  <Step title="Select GPU(GPU选择)">
    ![ComfyUI 安装步骤 - GPU 选择](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-3.png)
    对应三个选项为：

    1. **MPS（推荐）:**  Metal Performance Shaders (MPS) 是苹果的优化框架，让开发者能在苹果设备上利用 GPU 加速包括机器学习在内的高性能计算任务，且支持 PyTorch 等框架使用 GPU 提升模型训练和推理效率。
    2. **Manual Configuration 手动配置:** 你需要手动安装和配置 python 运行环境，除非你知道应该如何配置，否则请不要选择
    3. **Enable CPU Mode 启用 CPU 模式:** 仅适用于开发人员和特殊情况，除非你确定你需要使用这个模式，否则请不要选择

    如无特殊情况，请按截图所示选择 **MPS**，并点击 **Next** 进入下一步
  </Step>

  <Step title="Install location（安装位置）">
    ![ComfyUI 安装步骤 - 安装位置设置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-4.png)
    在这一步将选择 ComfyUI 以下相关内容的安装位置：

    * **Python 环境**
    * **Models 模型文件**
    * **Custom Nodes 自定义节点**
      建议：
    * 请新建一个单独的空白文件夹作为 ComfyUI 的安装目录
    * 请保证磁盘至少有 **5G** 左右的磁盘空间，以保证 **ComfyUI桌面版** 的正常安装

    <Note>ComfyUI 并非所有文件都安装在此目录下，部分文件将会位于 MacOS 的系统目录下，你可以参考本篇指南的卸载部分完成完整的 ComfyUI 桌面版的卸载</Note>
    完成后点击 **Next** 进入下一步
  </Step>

  <Step title="Migrate from Existing Installation（从已有安装迁移 - 可选）">
    ![ComfyUI 安装步骤 - 文件迁移](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-5.png)
    在这一步你可以将你已有的 ComfyUI 安装内容迁移到 ComfyUI 桌面版中，选择你电脑上已有的 **ComfyUI** 安装目录，安装程序会自动识别对应目录下的：

    * **User Files 用户文件**
    * **Models 模型文件:** 不会进行复制，只是与桌面版进行关联
    * **Custom Nodes 自定义节点:** 节点将会重新进行安装

    不要担心，这个步骤并不会复制模型文件，你可以按你的需要勾选或者取消勾选对应的选项，点击 **Next** 进入下一步
  </Step>

  <Step title="Desktop Setting(桌面版设置)">
    ![ComfyUI 安装步骤 - 桌面版设置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-6.png)
    这一步是偏好设置

    1. **Automatic Updates 自动更新:** 是否设置在 ComfyUI 更新可用时自动更新
    2. **Usage Metrics 使用情况分析:** 如果启用，我们将收集**匿名的使用数据** 用于帮助我们改进 ComfyUI
    3. **Mirror Settings 镜像设置:** 由于程序需要联网下载 Python 完成相关环境安装，如果你在安装时候也如图所示出现了红色的❌，提示这可能会导致后续安装过程的失败，则请参考下面步骤进行处理
       ![ComfyUI 安装步骤 - 镜像设置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/mac-comfyui-desktop-7.png)
       展开对应的镜像设置，找到具体失败的镜像，在当前截图中错误为 **Python Install Mirror** 镜像失败。

    对于不同的镜像错误，你可以参考下面的内容尝试手动查找不同的镜像，并进行替换

    以下情况主要针对中国境内用户

    #### Python 安装镜像

    如果默认镜像无法使用，请尝试使用下面的镜像

    ```
    https://python-standalone.org/mirror/astral-sh/python-build-standalone
    ```

    如果你需要查找其它备选 GitHub 的镜像地址，请查找并构建指向 `python-build-standalone` 仓库releases的镜像地址

    ```
    https://github.com/astral-sh/python-build-standalone/releases/download
    ```

    构建类似下面格式的链接

    ```
    https://xxx/astral-sh/python-build-standalone/releases/download
    ```

    <info>由于大多 Github 镜像服务都由第三方提供，所以请注意使用过程中的安全性。</info>

    #### PyPI 镜像

    * 阿里云：[https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)
    * 腾讯云：[https://mirrors.cloud.tencent.com/pypi/simple/](https://mirrors.cloud.tencent.com/pypi/simple/)
    * 中国科技大学：[https://pypi.mirrors.ustc.edu.cn/simple/](https://pypi.mirrors.ustc.edu.cn/simple/)
    * 上海交通大学：[https://pypi.sjtu.edu.cn/simple/](https://pypi.sjtu.edu.cn/simple/)

    #### Torch 镜像

    * 阿里云: [https://mirrors.aliyun.com/pytorch-wheels/cu121/](https://mirrors.aliyun.com/pytorch-wheels/cu121/)
  </Step>

  <Step title="完成安装">
    如果一切无误，安装程序将完成安装并自动进入 ComfyUI 桌面版界面, 则说明已经安装成功
    ![ComfyUI 桌面版界面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-interface.jpg)
  </Step>
</Steps>

## 进行第一次图片生成

安装成功后，你可以参考访问下面的章节，开始你的 ComfyUI 之路。

<Card title="进行第一次图片生成" icon="link" href="/zh-CN/get_started/first_generation">
  本教程将引导你完成第一次的模型安装以及对应的文本到图片的生成
</Card>

## 如何更新 ComfyUI 桌面版

目前 ComfyUI 桌面版更新采用自动检测更新，请确保在设置中已经启用自动更新

![ComfyUI 桌面版设置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg)

## 如何卸载 ComfyUI 桌面版

对于 **ComfyUI 桌面版** 的卸载你可以直接在 **Application** 文件夹内删除 **ComfyUI**

如果你想要完全删除 **ComfyUI 桌面版** 的所有文件，你可以手动删除以下文件夹：

* /Users/Library/Application Support/ComfyUI

以上的操作并不会删除以下你的以下文件夹，如果你需要删除对应文件的话，请手动删除：

* models 模型文件
* custom nodes 自定义节点
* input/output directories. 图片输入/输出目录

## 故障排除

### 如何定位安装错误

如果安装失败，你应该可以看到下面的界面显示

![ComfyUI 安装失败](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg)

此时建议你采取以下几种方式查找错误原因

1. 点击 `Show Teriminal` 查看错误问题输出
2. 点击 `Open Logs` 查看安装过程日志
3. 访问官方论坛查找错误反馈
4. 点击`Reinstall`尝试重新安装

建议在提交反馈之前，你可以将对应的**错误输出**以及 **log 文件**信息提供给类似 **GPT**一类的工具

![ComfyUI 安装失败-错误日志](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg)
![ComfyUI 安装失败-GPT 反馈](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg)

如上图，询问对应错误的原因，或者完全删除 ComfyUI 后进行安装重试

### 反馈错误

如果在安装过程中，你发生了任何错误，请通过以下任意方式查看是否有类似错误反馈，或者向我们提交错误

* Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)
* Comfy 官方论坛: [https://forum.comfy.org/](https://forum.comfy.org/)

请在提交错误时确保提交了以下日志以及配置文件，方便我们进行问题的定位和查找

1. 日志文件

| 文件名         | 描述                                          | 位置           |
| ----------- | ------------------------------------------- | ------------ |
| main.log    | 包含与桌面应用和服务器启动相关的日志，来自桌面的 Electron 进程。       | {log_path_0} |
| comfyui.log | 包含与 ComfyUI 正常运行相关的日志，例如核心 ComfyUI 进程的终端输出。 | {log_path_0} |

![ComfyUI 日志文件输出位置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)

2. 配置文件

| 文件名                        | 描述                           | 位置              |
| -------------------------- | ---------------------------- | --------------- |
| extra\_models\_config.yaml | 包含 ComfyUI 将搜索模型和自定义节点的额外路径。 | {config_path_0} |
| config.json                | 包含应用配置。此文件通常不应直接编辑。          | {config_path_0} |

![ComfyUI 配置文件位置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)


# Windows桌面版
Source: https://docs.comfy.org/zh-CN/installation/desktop/windows

本文将介绍 ComfyUI Desktop Windows 版本的下载以及安装使用

export const log_path_0 = "C:\Users\ <你的用户名> \AppData\Roaming\ComfyUI\logs"

export const config_path_0 = "C:\Users\ <你的用户名> \AppData\Roaming\ComfyUI"

**ComfyUI 桌面版（Desktop）** 是一个独立的安装版本，可以像常规软件一样进行安装，支持快捷安装自动配置 **Python环境及依赖** ，支持导入已有的 ComfyUI 设置、模型、工作流和文件，可以快速从已有的[ComfyUI 便携版](/zh-CN/installation/comfyui_portable_windows)迁移到桌面版

ComfyUI 桌面版是一个开源项目，完整代码请访问 [这里](https://github.com/Comfy-Org/desktop)

ComfyUI 桌面版(Windows)硬件要求：

* NVIDIA 显卡

本篇教程将引导你完成对应的软件安装，并说明相关安装配置说明。

<Warning>由于 **ComfyUI 桌面版** 仍旧处于 **Beta** 状态，实际的安装过程可能会发生变化</Warning>

## ComfyUI 桌面版（Windows）下载

请点击下面的按钮下载对应的针对 Windows 系统的 **ComfyUI 桌面版** 安装包

<a className="prose" href="https://download.comfy.org/windows/nsis/x64" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download for Windows (NVIDIA)</p>
</a>

## ComfyUI 桌面版安装步骤

双击下载到的安装包文件，首先将会执行一次自动安装，并在桌面生成一个 **ComfyUI 桌面版** 的快捷方式

![ComfyUI logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-shortcut.jpg)

双击对应的快捷，进入 ComfyUI 的初始化设置

### ComfyUI 桌面版初始化流程

<Steps>
  <Step title="开始界面">
    <Tabs>
      <Tab title="正常启动">
        ![ComfyUI 安装步骤 - 起始](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-1.jpg)
        点击 **Get Started** 开始初始化步骤
      </Tab>

      <Tab title="维护页">
        安装 ComfyUI 可能会出现许多问题。也许在安装 pytorch（15 GB）时网络连接失败，或者你没有安装 git，当检测到问题时，维护页面会自动打开，并提供解决问题的方法。

        你可以使用它来解决大多数问题：

        * 创建一个 Python 虚拟环境
        * 重新安装所有缺失的核心依赖项到由桌面管理的 Python 虦虚拟环境
        * 安装 git，VC redis
        * 选择一个新的安装位置

        默认维护页面会显示当前报错的内容

        ![ComfyUI 维护页面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-1.jpg)

        点击 `All` 可以切换查看可以操作的所有内容

        ![ComfyUI 维护页面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/maintenance-2.jpg)
      </Tab>
    </Tabs>
  </Step>

  <Step title="Select GPU(GPU选择)">
    ![ComfyUI 安装步骤 - GPU 选择](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-2.jpg)
    对应三个选项为：

    1. **Nvidia 显卡（推荐）:** 直接支持使用 pytorch 和 CUDA
    2. **Manual Configuration 手动配置:** 你需要手动安装和配置 python 运行环境，除非你知道应该如何配置，否则请不要选择
    3. **Enable CPU Mode 启用 CPU 模式:** 仅适用于开发人员和特殊情况，除非你确定你需要使用这个模式，否则请不要选择

    如无特殊情况，请按截图所示选择**NVIDIA**，并点击 **Next** 进入下一步
  </Step>

  <Step title="Install location（安装位置）">
    ![ComfyUI 安装步骤 - 安装位置设置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-3.jpg)

    在这一步将选择 ComfyUI 以下内容的安装位置：

    * **Python 环境**
    * **Models 模型文件**
    * **Custom Nodes 自定义节点**

    建议：

    * 请选择**固态硬盘**作为安装位置，这将提高 ComfyUI 访问模型的速度。
    * 请新建一个单独的空白文件夹作为 ComfyUI 的安装目录
    * 请保证对应磁盘至少有 **15G** 左右的磁盘空间，以保证 ComfyUI Desktop 的安装

    <Note>ComfyUI 并非所有文件都安装在此目录下，部分文件依然会安装在 C 盘，后期如需卸载，你可以参考本篇指南的卸载部分完成完整的 ComfyUI 桌面版的卸载</Note>

    完成后点击 **Next** 进入下一步
  </Step>

  <Step title="Migrate from Existing Installation（从已有安装迁移 - 可选）">
    ![ComfyUI 安装步骤 - 文件迁移](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-4.jpg)

    在这一步你可以将你已有的 ComfyUI 安装内容迁移到 ComfyUI 桌面版中，如图所示，选择了原本的 **D:\ComfyUI\_windows\_portable\ComfyUI** 安装目录，安装程序会自动识别对应目录下的：

    * **User Files 用户文件**
    * **Models 模型文件:** 不会进行复制，只是与桌面版进行关联
    * **Custom Nodes 自定义节点:** 自定义节点将会重新进行安装

    不要担心，这个步骤并不会复制模型文件，你可以按你的需要勾选或者取消勾选对应的选项，点击 **Next** 进入下一步
  </Step>

  <Step title="Desktop Setting(桌面版设置)">
    ![ComfyUI 安装步骤 - 桌面版设置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-5.jpg)

    这一步是偏好设置

    1. **Automatic Updates 自动更新:** 是否设置在 ComfyUI 更新可用时自动更新
    2. **Usage Metrics 使用情况分析:** 如果启用，我们将收集**匿名的使用数据** 用于帮助我们改进 ComfyUI
    3. **Mirror Settings 镜像设置:** 由于程序需要联网下载 Python 完成相关环境安装，如果你在安装时候也如图所示出现了红色的❌，提示这可能会导致后续安装过程的失败，则请参考下面步骤进行处理

    ![ComfyUI 安装步骤 - 镜像设置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-6.jpg)
    展开对应的镜像设置，找到具体失败的镜像，在当前截图中错误为 **Python Install Mirror** 镜像失败。

    对于不同的镜像错误，你可以参考下面的内容尝试手动查找不同的镜像，并进行替换

    以下情况主要针对中国境内用户

    #### Python 安装镜像

    如果默认镜像无法使用，请尝试使用下面的镜像

    ```
    https://python-standalone.org/mirror/astral-sh/python-build-standalone
    ```

    如果你需要查找其它备选 GitHub 的镜像地址，请查找并构建指向 `python-build-standalone` 仓库releases的镜像地址

    ```
    https://github.com/astral-sh/python-build-standalone/releases/download
    ```

    构建类似下面格式的链接

    ```
    https://xxx/astral-sh/python-build-standalone/releases/download
    ```

    <info>由于大多 Github 镜像服务都由第三方提供，所以请注意使用过程中的安全性。</info>

    #### PyPI 镜像

    * 阿里云：[https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)
    * 腾讯云：[https://mirrors.cloud.tencent.com/pypi/simple/](https://mirrors.cloud.tencent.com/pypi/simple/)
    * 中国科技大学：[https://pypi.mirrors.ustc.edu.cn/simple/](https://pypi.mirrors.ustc.edu.cn/simple/)
    * 上海交通大学：[https://pypi.sjtu.edu.cn/simple/](https://pypi.sjtu.edu.cn/simple/)

    #### Torch 镜像

    * 阿里云: [https://mirrors.aliyun.com/pytorch-wheels/cu121/](https://mirrors.aliyun.com/pytorch-wheels/cu121/)
  </Step>

  <Step title="完成安装">
    如果一切无误，安装程序将完成安装并自动进入 ComfyUI 桌面版界面, 则说明已经安装成功
    ![ComfyUI 桌面版界面](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-interface.jpg)
  </Step>
</Steps>

## 进行第一次图片生成

安装成功后，你可以参考访问下面的章节，开始你的 ComfyUI 之路。

<Card title="进行第一次图片生成" icon="link" href="/zh-CN/get_started/first_generation">
  本教程将引导你完成第一次的模型安装以及对应的文本到图片的生成
</Card>

## 如何更新 ComfyUI 桌面版

目前 ComfyUI 桌面版更新采用自动检测更新，请确保在设置中已经启用自动更新

![ComfyUI 桌面版设置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/comfyui-desktop-update-setting.jpg)

## 如何卸载 ComfyUI 桌面版

对于 **ComfyUI 桌面版** 你可以在 Windows 的系统设置中使用系统的卸载功能来完成对应软件的卸载操作

![ComfyUI 桌面版卸载](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-uninstall-comfyui.jpg)

如果你想要完全删除 **ComfyUI 桌面版** 的所有文件，你可以手动删除以下文件夹：

* C:\Users\<你的用户名>\AppData\Local\@comfyorgcomfyui-electron-updater
* C:\Users\<你的用户名>\AppData\Local\Programs\@comfyorgcomfyui-electron
* C:\Users\<你的用户名>\AppData\Roaming\ComfyUI

以上的操作并不会删除以下你的以下文件夹，如果你需要删除对应文件的话，请手动删除：

* models 模型文件
* custom nodes 自定义节点
* input/output directories. 图片输入/输出目录

## 故障排除

### 显示不支持的设备

![ComfyUI 安装步骤 - 不支持的设备](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-0.jpg)

由于 ComfyUI 桌面版（Windows）仅支持可以使用 **CUDA 的 Nvdia 显卡** 所以如果你的设备不支持，可能会出现此界面

* 请更换使用支持的设备
* 或者考虑使用 [ComfyUI便携版](/zh-CN/installation/comfyui_portable_windows) 或者通过[手动安装](/zh-CN/installation/manual_install)来使用 ComfyUI

### 如何定位安装错误

如果安装失败，你应该可以看到下面的界面显示

![ComfyUI 安装失败](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-7.jpg)

此时建议你采取以下几种方式查找错误原因

1. 点击 `Show Teriminal` 查看错误问题输出
2. 点击 `Open Logs` 查看安装过程日志
3. 访问官方论坛查找错误反馈
4. 点击`Reinstall`尝试重新安装

建议在提交反馈之前，你可以将对应的**错误输出**以及 **log 文件**信息提供给类似 **GPT**一类的工具

![ComfyUI 安装失败-错误日志](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-8.jpg)
![ComfyUI 安装失败-GPT 反馈](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-9.jpg)

如上图，询问对应错误的原因，或者完全删除 ComfyUI 后进行安装重试

### 反馈错误

如果在安装过程中，你发生了任何错误，请通过以下任意方式查看是否有类似错误反馈，或者向我们提交错误

* Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)
* Comfy 官方论坛: [https://forum.comfy.org/](https://forum.comfy.org/)

请在提交错误时确保提交了以下日志以及配置文件，方便我们进行问题的定位和查找

1. 日志文件

| 文件名         | 描述                                          | 位置           |
| ----------- | ------------------------------------------- | ------------ |
| main.log    | 包含与桌面应用和服务器启动相关的日志，来自桌面的 Electron 进程。       | {log_path_0} |
| comfyui.log | 包含与 ComfyUI 正常运行相关的日志，例如核心 ComfyUI 进程的终端输出。 | {log_path_0} |

![ComfyUI 日志文件输出位置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-10-logs.jpg)

2. 配置文件

| 文件名                        | 描述                           | 位置              |
| -------------------------- | ---------------------------- | --------------- |
| extra\_models\_config.yaml | 包含 ComfyUI 将搜索模型和自定义节点的额外路径。 | {config_path_0} |
| config.json                | 包含应用配置。此文件通常不应直接编辑。          | {config_path_0} |

![ComfyUI 配置文件位置](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/desktop/win-comfyui-desktop-11-config.jpg)


# 手动安装
Source: https://docs.comfy.org/zh-CN/installation/manual_install

本部分将指导你完成在 Windows、MacOS 以及 Linux 的手动安装过程

<Tip>
  对于 Nvidia 50 系列(Blackwell)显卡，请参考 [系统要求](/zh-CN/installation/nvidia-50-series)部分,确保你的系统环境能够满足 ComfyUI 的运行要求。
</Tip>

<Tabs>
  <Tab title="Windows">
    ### 克隆代码仓库

    ```bash
    git clone git@github.com:comfyanonymous/ComfyUI.git
    ```

    More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

    <Warning>如果你还没有安装 Microsoft Visual C++ Redistributable，请在[这里安装](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170)</Warning>

    ### 安装依赖

    1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.

       Create an environment with Conda.

       ```
       conda create -n comfyenv
       conda activate comfyenv
       ```
    2. Install GPU Dependencies

       <Accordion title="Nvidia">
         ```
         conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
           ```
         </Accordion>
       </Accordion>

       <Accordion title="AMD">
         ```
         pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
           ```
         </Accordion>
       </Accordion>

       <Accordion title="Mac ARM Silicon">
         ```bash
         conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
         ```
       </Accordion>
    3. ```bash
       cd ComfyUI
       pip install -r requirements.txt
       ```
    4. Start the application
       ```
       cd ComfyUI
       python main.py
       ```
  </Tab>

  <Tab title="Linux">
    ### 克隆代码仓库

    ```bash
    git clone git@github.com:comfyanonymous/ComfyUI.git
    ```

    More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

    ### 安装依赖

    1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.

       Create an environment with Conda.

       ```
       conda create -n comfyenv
       conda activate comfyenv
       ```
    2. Install GPU Dependencies

       <Accordion title="Nvidia">
         ```
         conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
           ```
         </Accordion>
       </Accordion>

       <Accordion title="AMD">
         ```
         pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
           ```
         </Accordion>
       </Accordion>

       <Accordion title="Mac ARM Silicon">
         ```bash
         conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
         ```
       </Accordion>
    3. ```bash
       cd ComfyUI
       pip install -r requirements.txt
       ```
    4. Start the application
       ```
       cd ComfyUI
       python main.py
       ```
  </Tab>

  <Tab title="MacOS">
    ### 克隆代码仓库

    打开[终端应用程序](https://support.apple.com/guide/terminal/open-or-quit-terminal-apd5265185d-f365-44cb-8b09-71a064a42125/mac)。

    ```bash
    git clone git@github.com:comfyanonymous/ComfyUI.git
    ```

    More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

    ### 安装依赖

    1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.

       Create an environment with Conda.

       ```
       conda create -n comfyenv
       conda activate comfyenv
       ```
    2. Install GPU Dependencies

       <Accordion title="Nvidia">
         ```
         conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
           ```
         </Accordion>
       </Accordion>

       <Accordion title="AMD">
         ```
         pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
         ```

         Alternatively, you can install the nightly version of PyTorch.

         <Accordion title="Install Nightly">
           <Warning>Install Nightly version (might be more risky)</Warning>

           ```
           pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
           ```
         </Accordion>
       </Accordion>

       <Accordion title="Mac ARM Silicon">
         ```bash
         conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
         ```
       </Accordion>
    3. ```bash
       cd ComfyUI
       pip install -r requirements.txt
       ```
    4. Start the application
       ```
       cd ComfyUI
       python main.py
       ```
  </Tab>
</Tabs>


# 系统要求
Source: https://docs.comfy.org/zh-CN/installation/system_requirements

本文将介绍 ComfyUI 目前的一些系统要求，包括硬件及软件要求

在本篇我们将介绍安装 ComfyUI 的系统要求, 由于 ComfyUI 的更新频繁，本篇文档未必能够及时更新，请参考[ComfyUI](https://github.com/comfyanonymous/ComfyUI)中的相关说明。

无论是哪个版本的 ComfyUI，都是运行在一个独立的 Python 环境中。

你可以参考下面的章节来了解不同系统和版本 ComfyUI 的安装方式，在不同版本的安装中我们简单对安装的系统要求进行了说明。

<AccordionGroup>
  <Accordion title="ComfyUI 桌面版(推荐)">
    ComfyUI 桌面版目前支持 **Windows 及 MacOS(Apple Silicon)** 的独立安装，目前仍在 Beta 版本

    * 代码开源在 [Github](https://github.com/Comfy-Org/desktop)

    你可以从下面选择适合你的系统和硬件开始安装 ComfyUI

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI桌面版(Windows)安装指南" icon="link" href="/zh-CN/installation/desktop/windows">
          适合带有 **Nvdia** 显卡 **Windows** 版本的 ComfyUI 桌面版
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI桌面版(MacOS)安装指南" icon="link" href="/zh-CN/installation/desktop/macos">
          适合带有 **Apple Silicon** 的 MacOS ComfyUI 桌面版
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI桌面版，**暂时没有 Linux 的预构建**，请访问[手动安装](/zh-CN/installation/manual_install)部分进行 ComfyUI 的安装</Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI 便携版(Windows)">
    <Card title="ComfyUI桌面版(Windows)安装指南" icon="link" href="/zh-CN/installation/comfyui_portable_windows">
      支持 **Navida 显卡** 和在 **CPU** 运行的 **Windows** ComfyUI 版本，始终使用最新 commit 的代码
    </Card>
  </Accordion>

  <Accordion title="手动安装 ComfyUI">
    <Card title="ComfyUI 手动安装教程" icon="link" href="/zh-CN/installation/manual_install">
      支持所有的系统类型和 GPU 类型（Nvidia、AMD、Intel、Apple Silicon、Ascend NPU、寒武纪 MLU）的用户都可以尝试使用手动安装 ComfyUI
    </Card>
  </Accordion>
</AccordionGroup>

## Nvdia 50 系显卡运行要求

如果要使你的 Nvdia 50 系 GPU（Blackwell架构）可以正常运行 ComfyUI，你需要一个针对 CUDA 12.8 或更新版本的 PyTorch, 目前（2025年3月） PyTorch 的稳定版本尚未支持 Blackwell 架构，所以你需要使用夜间构建的版本。

另外对应的相关问题讨论将集中在[这里](https://github.com/comfyanonymous/ComfyUI/discussions/6643)。

### 针对 Windows 的用户

**推荐方案：**
下载带有 nightly pytorch 2.7 cu128 的独立 ComfyUI Portable 版本：

* [点击这里进行下载](https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_windows_portable_nvidia_or_cpu_nightly_pytorch.7z)

**其他选择：**
旧版本 torch 2.6 Windows 包：

* [点击这里下载带有 cuda 12.8 torch 构建的独立 ComfyUI 包](https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_cu128_50XX.7z)

### 手动安装

Windows 和 Linux 用户可以使用以下命令安装 PyTorch nightly 版本：

```bash
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
```

### Docker 容器替代方案

你可以尝试由 Nvidia 提供的 PyTorch 容器，这可能会提供更好的性能。

容器地址: [https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch)

使用方法：

```bash
docker run -p 8188:8188 --gpus all -it --rm nvcr.io/nvidia/pytorch:25.01-py3
```

在Docker容器内部，执行以下命令：

```bash
git clone https://github.com/comfyanonymous/ComfyUI
cd ComfyUI
grep -v 'torchaudio\|torchvision' requirements.txt > temp_requirements.txt
pip install -r temp_requirements.txt
python main.py --listen
```


# ComfyUI Hunyuan3D-2 示例
Source: https://docs.comfy.org/zh-CN/tutorials/3d/hunyuan3D-2

本文将使用 Hunyuan3D-2 来完成在 ComfyUI 中 3D 资产生成的工作流示例。

# 混元3D 2.0 简介

![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-1.gif)
![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-2.gif)

[混元3D 2.0](https://github.com/Tencent/Hunyuan3D-2) 是腾讯推出的开源 3D 资产生成模型，可以通过文本、图像或草图生成带有高分辨率纹理贴图的高保真 3D 模型。

混元3D 2.0采用两阶段生成，首先采用生成无纹理的几何模型，再合成高分辨率的纹理贴图，有效分离了形状和纹理生成的复杂性，下面是混元3D 2.0的两个核心组件:

1. **几何生成模型（Hunyuan3D-DiT）**：基于流扩散的Transformer架构，生成无纹理的几何模型，可精准匹配输入条件。
2. **纹理生成模型（Hunyuan3D-Paint）**：结合几何条件和多视图扩散技术，为模型添加高分辨率纹理，支持PBR材质。

**主要优势**

* **高精度生成**：几何结构锐利，纹理色彩丰富，支持PBR材质生成，实现接近真实的光影效果。
* **多样化使用方式**：提供代码调用、Blender插件、Gradio应用及官网在线体验，适合不同用户需求。
* **轻量化与兼容性**：Hunyuan3D-2mini模型仅需5GB显存，标准版本形状生成需6GB显存，完整流程（形状+纹理）仅需12GB显存。

近期（2025 年 3 月 18 日），混元3D 2.0 还提供多视角形状生成模型（Hunyuan3D-2mv），支持从不同视角输入生成更精细的几何结构。

在本示例中包含三个工作流：

* 使用 Hunyuan3D-2mv 配合多个视图输入生成3D模型
* 使用 Hunyuan3D-2mv-turbo 配合多个视图输入生成3D模型
* 使用 Hunyuan3D-2 配合单个视图输入生成3D模型

<Tip>
  目前 ComfyUI 已原生支持 Hunyuan3D-2mv，暂未支持纹理和材质的生成，请在开始之前确保你已升级到最新版本的 [ComfyUI](https://github.com/comfyanonymous/ComfyUI)。

  本示例中工作流部分的输入图片示例 png 格式的图片的 Metadata 中包含工作流 json 的图片

  * 直接拖入 ComfyUI
  * 使用菜单 `Workflows` -> `Open（ctrl+o）`

  可以加载对应的工作流并提示完成模型下载，对应的 `.glb` 格式模型将输出至 `ComfyUI/output/mesh` 文件夹。
</Tip>

## ComfyUI Hunyuan3D-2mv 工作流示例

Hunyuan3D-2mv 工作流中，我们将使用多视角的图片来生成3D模型，另外多个视角的图片在这个工作流中并不是必须的，你可以只输入 `front` 视角的图片来生成3D模型。

### 1. 工作流

请下载下面的图片，并拖入 ComfyUI 以加载工作流,

![Hunyuan3D-2mv workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/hunyuan-3d-multiview-elf.webp)

下载下面的图片，同时我们将使用这些图片作为图片输入

<div class="flex space-x-4">
  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/front.png" alt="input image" class="w-1/3" />

  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/left.png" alt="input image" class="w-1/3" />

  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/back.png" alt="input image" class="w-1/3" />
</div>

<Tip>
  在本示例中提供的输入图片都已经过提前处理去除了多余的背景，在实际的使用中，你可以借助类似[ComfyUI\_essentials](https://github.com/cubiq/ComfyUI_essentials) 这样的自定义来完成多余背景的自动去除。
</Tip>

### 2. 手动安装模型

下载下面的模型，并保存到对应的 ComfyUI 文件夹

* hunyuan3d-dit-v2-mv: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv/model.fp16.safetensors?download=true)  下载后可重命名为 `hunyuan3d-dit-v2-mv.safetensors`

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2-mv.safetensors  // 重命名后的文件
```

### 3. 按步骤运行工作流

![ComfyUI hunyuan3d\_2mv](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg)

1. 确保 Image Only Checkpoint Loader(img2vid model) 加载了我们下载并重命名的 `hunyuan3d-dit-v2-mv.safetensors` 模型
2. 在 `Load Image` 节点的各个视角中加载了对应视角的图片
3. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流

如果你需要增加更多的视角，请确保 `Hunyuan3Dv2ConditioningMultiView` 节点中加载了其它视角的图片，并确保在 `Load Image` 节点中加载了对应视角的图片。

## 使用 Hunyuan3D-2mv-turbo 工作流

Hunyuan3D-2mv-turbo 工作流中，我们将使用 Hunyuan3D-2mv-turbo 模型来生成3D模型，这个模型是 Hunyuan3D-2mv 的分步蒸馏（Step Distillation）版本，可以更快地生成3D模型，在这个版本的工作流中我们设置 `cfg` 为 1.0 并添加 `flux guidance` 节点来控制 `distilled cfg` 的生成。

### 1. 工作流

请下载下面的图片，并拖入 ComfyUI 以加载工作流,

![Hunyuan3D-2mv-turbo workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/hunyuan-3d-turbo.webp)

我们将使用下面的图片作为多视角的输入

<div class="flex space-x-4">
  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/front.png" alt="input image" class="w-1/2" />

  <img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/right.png" alt="input image" class="w-1/2" />
</div>

### 2. 手动安装模型

下载下面的模型，并保存到对应的 ComfyUI 文件夹

* hunyuan3d-dit-v2-mv-turbo: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv-turbo/model.fp16.safetensors?download=true)  下载后可重命名为 `hunyuan3d-dit-v2-mv-turbo.safetensors`

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2-mv-turbo.safetensors  // 重命名后的文件
```

### 3. 按步骤运行工作流

![ComfyUI hunyuan3d\_2mv\_turbo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg)

1. 确保 `Image Only Checkpoint Loader(img2vid model)` 节点加载了我们重命名后的 `hunyuan3d-dit-v2-mv-turbo.safetensors` 模型
2. 在 `Load Image` 节点的各个视角中加载了对应视角的图片
3. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流

## 使用 Hunyuan3D-2 单视图工作流

Hunyuan3D-2 工作流中，我们将使用 Hunyuan3D-2 模型来生成3D模型，这个模型不是一个多视角的模型，在这个工作流中，我们使用`Hunyuan3Dv2Conditioning` 节点替换掉 `Hunyuan3Dv2ConditioningMultiView` 节点。

### 1. 工作流

请下载下面的图片，并拖入 ComfyUI 以加载工作流

![Hunyuan3D-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d-non-multiview-train.webp)

同时我们将使用这张图片作为图片输入

![ComfyUI Hunyuan 3D 2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan_3d_v2_non_multiview_train.png)

### 2. 手动安装模型

下载下面的模型，并保存到对应的 ComfyUI 文件夹

* hunyuan3d-dit-v2-0: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2/resolve/main/hunyuan3d-dit-v2-0/model.fp16.safetensors?download=true)  下载后可重命名为 `hunyuan3d-dit-v2.safetensors`

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2.safetensors  // 重命名后的文件
```

### 3. 按步骤运行工作流

![ComfyUI hunyuan3d\_2](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg)

1. 确保 `Image Only Checkpoint Loader(img2vid model)` 节点加载了我们重命名后的 `hunyuan3d-dit-v2.safetensors` 模型
2. 在 `Load Image` 节点中加载了对应视角的图片
3. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流

## 社区资源

下面是 Hunyuan3D-2 的相关的 ComfyUI 社区资源

* [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)
* [Kijai/Hunyuan3D-2\_safetensors](https://huggingface.co/Kijai/Hunyuan3D-2_safetensors/tree/main)
* [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)

## 混元3D 2.0 开源模型系列

目前混元3D 2.0 开源了多个模型，覆盖了完整的3D生成流程，你可以访问 [Hunyuan3D-2](https://github.com/Tencent/Hunyuan3D-2) 了解更多。

**Hunyuan3D-2mini 系列**

| 模型                    | 描述           | 日期         | 参数   | Huggingface                                                                          |
| --------------------- | ------------ | ---------- | ---- | ------------------------------------------------------------------------------------ |
| Hunyuan3D-DiT-v2-mini | Mini 图像到形状模型 | 2025-03-18 | 0.6B | [前往](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini) |

**Hunyuan3D-2mv 系列**

| 模型                       | 描述                              | 日期         | 参数   | Huggingface                                                                           |
| ------------------------ | ------------------------------- | ---------- | ---- | ------------------------------------------------------------------------------------- |
| Hunyuan3D-DiT-v2-mv-Fast | 指导蒸馏版本，可以将 DIT 推理时间减半           | 2025-03-18 | 1.1B | [前往](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast) |
| Hunyuan3D-DiT-v2-mv      | 多视角图像到形状模型，适合需要用多个角度理解场景的 3D 创作 | 2025-03-18 | 1.1B | [前往](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv)      |

**Hunyuan3D-2 系列**

| 模型                      | 描述      | 日期         | 参数   | Huggingface                                                                        |
| ----------------------- | ------- | ---------- | ---- | ---------------------------------------------------------------------------------- |
| Hunyuan3D-DiT-v2-0-Fast | 指导蒸馏模型  | 2025-02-03 | 1.1B | [前往](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast) |
| Hunyuan3D-DiT-v2-0      | 图像到形状模型 | 2025-01-21 | 1.1B | [前往](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0)      |
| Hunyuan3D-Paint-v2-0    | 纹理生成模型  | 2025-01-21 | 1.3B | [前往](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0)    |
| Hunyuan3D-Delight-v2-0  | 图像去光影模型 | 2025-01-21 | 1.3B | [前往](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0)  |


# ComfyUI 原生版本 HiDream-I1 文生图工作流实例
Source: https://docs.comfy.org/zh-CN/tutorials/advanced/hidream

本篇将引导了解并完成 ComfyUI 原生版本 HiDream-I1 文生图工作流实例

![HiDream-I1 演示](https://raw.githubusercontent.com/HiDream-ai/HiDream-I1/main/assets/demo.jpg)

HiDream-I1 是智象未来(HiDream-ai)于2025年4月7日正式开源的文生图模型。该模型拥有17B参数规模，采用 [MIT 许可证](https://github.com/HiDream-ai/HiDream-I1/blob/main/LICENSE) 发布，支持用于个人项目、科学研究以及商用，目前在多项基准测试中该模型表现优异。

## 模型特点

**混合架构设计**
采用​​扩散模型（DiT）​​与​​混合专家系统（MoE）​​的结合架构：

* 主体基于Diffusion Transformer（DiT），通过双流MMDiT模块处理多模态信息，单流DiT模块优化全局一致性。
* 动态路由机制灵活分配计算资源，提升复杂场景处理能力，在色彩还原、边缘处理等细节上表现优异。

**多模态文本编码器集成**
整合四个文本编码器：

* OpenCLIP ViT-bigG、OpenAI CLIP ViT-L（视觉语义对齐）
* T5-XXL（长文本解析）
* Llama-3.1-8B-Instruct（指令理解）
  这一组合使其在颜色、数量、空间关系等复杂语义解析上达到SOTA水平，中文提示词支持显著优于同类开源模型。

**原始模型版本**

智象未来(HiDream-ai)提供了三个版本的 HiDream-I1 模型，以满足不同场景的需求，下面是原始的模型仓库链接：

* 完整版本：[🤗 HiDream-I1-Full](https://huggingface.co/HiDream-ai/HiDream-I1-Full) 推理步数为 50
* 蒸馏开发版本：[🤗 HiDream-I1-Dev](https://huggingface.co/HiDream-ai/HiDream-I1-Dev) 推理步数为 28
* 蒸馏快速版本：[🤗 HiDream-I1-Fast](https://huggingface.co/HiDream-ai/HiDream-I1-Fast) 推理步数为 16

## 关于本篇工作流示例

我们将在本篇示例中使用 ComfyOrg 的 repackaged 的版本，你可以在 [HiDream-I1\_ComfyUI](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/) 仓库中找到我们将在本篇示例中使用的所有模型文件。

<Tip>
  在开始前请更新你的 ComfyUI 版本，至少保证在这个[提交](https://github.com/comfyanonymous/ComfyUI/commit/9ad792f92706e2179c58b2e5348164acafa69288) 之后才能确保你的 ComfyUI 有 HiDream 的原生支持
</Tip>

## HiDream-I1 工作流

对应不同 ComfyUI 原生版本 HiDream-I1 工作流的模型要求基本上是相同的，只有使用过的 [diffusion models](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files/diffusion_models) 文件不同。

如果你不知道如何选择合适的版本，请参考以下建议：

* **HiDream-I1-Full** 可以生成质量最高的图像
* **HiDream-I1-Dev** 在生成较高质量的图像的同时，又兼顾速度
* **HiDream-I1-Fast** 只需要 16 步就可以生成图像，适合需要实时迭代的场景

对于 **dev** 和 **fast** 版本并不需要负向提示词，所以请在采样时设置`cfg` 参数为 `1.0`，我们对应参数设置已在相关工作流中备注。

<Tip>
  以上三个版本的完整版本对显存要求较高，你可能需要 27GB 以上的显存才能顺利运行。在对应版本的工作流教程中，我们将会使用 **fp8** 版本作为示例演示，以保证大多用户都可以顺利运行，不过我们仍会在对应示例中提供不同版本的模型下载链接，你可以根据你的显存情况来选择合适的文件。
</Tip>

### 模型安装

下面的模型文件是我们会共用的模型文件，请点击对应的链接进行下载，并参照模型文件保存位置进行保存，对应的 **diffusion models** 模型我们会在对应工作流中引导你进行下载。

**text\_encoders**：

* [clip\_l\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)
* [clip\_g\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)
* [t5xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) 这个模型在许多的工作流中都有使用过，你可能已经下载了这个文件。
* [llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)

**VAE**

* [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) 这个是 Flux 的 VAE 模型，如果你之前使用过 Flux 的工作流，你可能已经下载了这个文件。

**diffusion models**
这部分我们将在对应工作流中具体引导下载对应的模型文件。

模型文件保存位置

```
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 text_encoders/
│   │   ├─── clip_l_hidream.safetensors
│   │   ├─── clip_g_hidream.safetensors
│   │   ├─── t5xxl_fp8_e4m3fn_scaled.safetensors
│   │   └─── llama_3.1_8b_instruct_fp8_scaled.safetensors
│   └── 📂 vae/
│   │   └── ae.safetensors
│   └── 📂 diffusion_models/
│       └── ...               # 将在对应版本的工作流中引导你进行安装            
```

### HiDream-I1 full 版本工作流

#### 1. 模型文件下载

请根据你的硬件情况选择合适的版本，点击链接并下载对应的模型文件保存到 `ComfyUI/models/diffusion_models/` 文件夹下。

* FP8 版本：[hidream\_i1\_full\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp8.safetensors?download=true) 需要 16GB 以上的显存
* 完整版本：[hidream\_i1\_full\_f16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp16.safetensors?download=true) 需要 27GB 以上的显存

#### 2. 工作流文件下载

请下载下面的图片，并拖入 ComfyUI 中以加载对应的工作流
![HiDream-I1 full 版本工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_full.png)

#### 3. 按步骤完成工作流的运行

![HiDream-I1 full 版本步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg)

按步骤完成工作流的运行

1. 确保`Load Diffusion Model` 节点中使用的是 `hidream_i1_full_fp8.safetensors` 文件
2. 确保`QuadrupleCLIPLoader` 中四个对应的 text encoder 被正确加载
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. 确保`Load VAE` 节点中使用的是 `ae.safetensors` 文件
4. 对于 **full** 版本你需要设置 `ModelSamplingSD3` 中的 `shift` 参数为 `3.0`
5. 对于 `Ksampler` 节点，你需要进行以下设置
   * `steps` 设置为 `50`
   * `cfg` 设置为 `5.0`
   * (可选) `sampler` 设置为 `lcm`
   * (可选) `scheduler` 设置为 `normal`
6. 点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片生成

### HiDream-I1 dev 版本工作流

#### 1. 模型文件下载

请根据你的硬件情况选择合适的版本，点击链接并下载对应的模型文件保存到 `ComfyUI/models/diffusion_models/` 文件夹下。

* FP8 版本：[hidream\_i1\_dev\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_fp8.safetensors?download=true) 需要 16GB 以上的显存
* 完整版本：[hidream\_i1\_dev\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_bf16.safetensors?download=true) 需要 27GB 以上的显存

#### 2. 工作流文件下载

请下载下面的图片，并拖入 ComfyUI 中以加载对应的工作流

![HiDream-I1 dev 版本工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_dev.png)

#### 3. 按步骤完成工作流的运行

![HiDream-I1 dev 版本步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg)
按步骤完成工作流的运行

1. 确保`Load Diffusion Model` 节点中使用的是 `hidream_i1_dev_fp8.safetensors` 文件
2. 确保`QuadrupleCLIPLoader` 中四个对应的 text encoder 被正确加载
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. 确保`Load VAE` 节点中使用的是 `ae.safetensors` 文件
4. 对于 **dev** 版本你需要设置 `ModelSamplingSD3` 中的 `shift` 参数为 `6.0`
5. 对于 `Ksampler` 节点，你需要进行以下设置
   * `steps` 设置为 `28`
   * (重要) `cfg` 设置为 `1.0`
   * (可选) `sampler` 设置为 `lcm`
   * (可选) `scheduler` 设置为 `normal`
6. 点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片生成

### HiDream-I1 fast 版本工作流

#### 1. 模型文件下载

请根据你的硬件情况选择合适的版本，点击链接并下载对应的模型文件保存到 `ComfyUI/models/diffusion_models/` 文件夹下。

* FP8 版本：[hidream\_i1\_fast\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) 需要 16GB 以上的显存
* 完整版本：[hidream\_i1\_fast\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) 需要 27GB 以上的显存

#### 2. 工作流文件下载

请下载下面的图片，并拖入 ComfyUI 中以加载对应的工作流

![HiDream-I1 fast 版本工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_fast.png)

#### 3. 按步骤完成工作流的运行

![HiDream-I1 fast 版本步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg)

按步骤完成工作流的运行

1. 确保`Load Diffusion Model` 节点中使用的是 `hidream_i1_fast_fp8.safetensors` 文件
2. 确保`QuadrupleCLIPLoader` 中四个对应的 text encoder 被正确加载
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. 确保`Load VAE` 节点中使用的是 `ae.safetensors` 文件
4. 对于 **fast** 版本你需要设置 `ModelSamplingSD3` 中的 `shift` 参数为 `3.0`
5. 对于 `Ksampler` 节点，你需要进行以下设置
   * `steps` 设置为 `16`
   * (重要) `cfg` 设置为 `1.0`
   * (可选) `sampler` 设置为 `lcm`
   * (可选) `scheduler` 设置为 `normal`
6. 点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片生成

## 使用建议

* 虽然 HiDream-I1 支持中文提示词，但建议还是优先使用英文提示词来保证准确性
* 你可以使用 fast 版本来快速生成示例验证，然后再用完整版本的模型来生成较高质量的图像

## 其它相关资源

### GGUF 版本模型

* [HiDream-I1-Full-gguf](https://huggingface.co/city96/HiDream-I1-Full-gguf)
* [HiDream-I1-Dev-gguf](https://huggingface.co/city96/HiDream-I1-Dev-gguf)

你需要使用 City96 的 [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF)  中的 `Unet Loader (GGUF)`节点替换掉 `Load Diffusion Model` 节点来使用 GGUF 版本模型。

* [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF)

### NF4 版本模型

* [HiDream-I1-nf4](https://github.com/hykilpikonna/HiDream-I1-nf4)
* 使用 [ComfyUI-HiDream-Sampler](https://github.com/SanDiegoDude/ComfyUI-HiDream-Sampler) 节点来使用 NF4 版本模型。


# ComfyUI 图生图工作流
Source: https://docs.comfy.org/zh-CN/tutorials/basic/image-to-image

本篇将引导了解并完成图生图工作流

## 什么是图生图

图生图（Image to Image）是 ComfyUI 中的一种工作流，它允许用户将一张图像作为输入，并生成一张新的图像。

图生图可以使用在以下场景中：

* 原始图像风格的转换，如把写实照片转为艺术风格
* 将线稿图像转换为写实图像
* 图像的修复
* 老照片着色
* ... 等其它场景

用一个比喻来讲解的话，大概是这样：
你需要画家根据你的参考图片，画出符合你要求特定效果的作品。

如果你仔细比对本篇教程和[文生图](/zh-CN/tutorials/basic/text-to-image)教程，你会发现图生图的流程和文生图的流程非常相似，只是多了个输入的参考图片作为输入条件，也就是在文生图中，我们是让画家（绘图模型）根据我们的提示词生成自由发挥，而在图生图中，我们是让画家（绘图模型）根据我们的参考图片和提示词生成图片。

## ComfyUI 图生图工作流示例讲解

### 1. 模型安装

请确保你已经在 `ComfyUI/models/checkpoints` 文件夹至少有一个 SD1.5 的模型文件，如果你还不了解如何安装模型，请参[开始 ComfyUI 的 AI 绘图之旅](/zh-CN/get_started/first_generation#3-安装绘图模型)章节中关于模型安装的部分说明。

你可以使用下面的这些模型：

* [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)
* [Dreamshaper 8](https://civitai.com/models/4384?modelVersionId=128713)
* [Anything V5](https://civitai.com/models/9409?modelVersionId=30163)

### 2. 图生图工作流相关文件

保存并下载下面的图片到本地，然后 **拖拽或使用 ComfyUI 打开** 它，就会加载对应的工作流

![图生图工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image_to_image.png)

或在 ComfyUI 的 **workflow template** 中加载 **image to image** 工作流
![ComfyUI 工作流模板 - 图生图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/image-to-image-01-template.jpg)

下载下面的图片作为使用示例，我们会在后面的步骤中使用它
![图片示例](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/input.jpeg)

### 3. 开始图生图工作流

![ComfyUI 图生图工作流 - 步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/img2img/image-to-image-02-guide.jpg)

在加载图生图工作流后，请对照图片，按照序号完成以下操作，完成示例工作流的生成

1. 在 **Load Checkpoint** 节点中加载好你本地的绘图模型
2. 在 **Load Image** 节点点击 `upload` 按钮，上传准备步骤中提供的图片
3. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl + Enter(回车)` 来执行图片生成

## 开始你自己的尝试

1. 试着修改 **KSampler** 节点中的 `denoise` 参数，逐渐从 1 到 0 变化，观察生成图片的变化
2. 更换你自己的提示词和参考图片，生成属于你自己的图片效果

## 图生图工作流核心要点

图生图工作流的核心在于在于 `KSampler` 节点中的 `denoise` 参数要是 **小于 1**

如果你调整过 `denoise` 参数，进行生成后会发现：

* `denoise` 越小，生成图片和参考图片的差异就会越小，
* `denoise` 越大，生成图片和参考图片的差异就会越大。

因为 `denoise` 决定了对应图片转换为潜空间图像后，向潜在空间图像添加的噪声强度，如果 `denoise` 为 1，对应潜空间图像就会变成一个完全随机的噪声，那这样就和`empty latent image`节点生成的潜在空间一样了，就会丢失参考图片的所有特征。

对应原理可以参考[文生图](/zh-CN/tutorials/basic/text-to-image)教程中的原理讲解。


# ComfyUI 局部重绘工作流
Source: https://docs.comfy.org/zh-CN/tutorials/basic/inpaint

本篇指南将带你了解 ComfyUI 中的局部重绘工作流，并带你完成一个局部重绘的示例，以及遮罩编辑器的使用等

本篇将引导了解 AI 绘图中，局部重绘的概念，并在 ComfyUI 中完成局部重绘工作流生成，我们将接触以下内容：

* 使用局部重绘工作流完成画面的修改
* 了解并使用 ComfyUI 中遮罩编辑器
* 了解相关节点 VAE Encoder (for Inpainting)

## 关于局部重绘

在 AI 图像生成过程中，我们常会遇到生成的画面整体较为满意，但是画面中存在一些不希望出现或者错误的元素，但是重新生成可能会生成另外一张完全不同的图片，所以这时候利用局部重绘来修复这部分的元素就非常有必要了。

这就像让 **画家(AI 绘图模型)** 画了一幅画，但是总是会有稍微有 **局部区域需要调整**，我们需要向画家说明**需要调整的区域(遮罩)**，然后让画家会根据我们的要求进行 **重新绘制(重绘)**。

局部重绘的场景包括：

* **瑕疵修复：** 消除照片中多余物体、错误的AI生成的画面的肢体等
* **细节优化：** 精准调整局部元素（如修改服装纹理、调整面部表情）
* 等其它场景

## ComfyUI 局部重绘工作流示例讲解

### 模型及相关素材准备

#### 1. 模型安装

下载下面的模型文件，并保存到`ComfyUI/models/checkpoints`目录下

* [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)

#### 2. 局部重绘素材

请下载下面的图片，我们将在这个示例中使用这个图片作为输入使用

![ComfyUI局部重绘输入图片](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/input.png)

<Note>这张照片已经包含了 alpha 透明通道，所以并不需要你手动绘制蒙版，在本篇教程也会涉及如何使用遮罩编辑器来绘制蒙版的部分，我们会引导你一步步来完成整个局部重绘的过程</Note>

#### 3. 局部重绘工作流

下面这张图的 metadata 包含的对应的json工作流，请将其下载后 **拖入** ComfyUI 界面或者使用菜单 **工作流(Workflow)** --> **打开工作流(Open,快捷键 `Ctrl + O`)** 来加载这个局部重绘工作流

![ComfyUI局部重绘工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png)

### ComfyUI 局部重绘工作流示例讲解

![ComfyUI 局部重绘工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_workflow.png)

请参照图片序号对照下面的提示完下操作：

1. 请确保已经加载了你所下载使用的模型
2. 请在在 `Load Image` 节点中加载局部重绘的素材
3. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl + Enter(回车)` 来执行图片生成

![ComfyUI局部重绘工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/sd1.5_inpaint.png)

此外我们在这里可以对比一下，下图是使用[v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) 模型来进行 inpainting 的结果。

![ComfyUI 局部重绘工作流 - SD1.5](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png)

你会发现 [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) 模型生成的结果局部重绘的效果更好过渡更自然。
这因为这个模型是专为 inpainting 设计的模型，它可以帮助我们更好地控制生成区域，从而获得更好的局部重绘效果。

记得我们一直用的比喻吗？不同的模型就像能力不同的画家一样，但每个画家都有自己能力的上限，选择合适的模型可以让你的生成效果更好。

你可以进行下面的尝试来让画面达到你想要的效果:

1. 修改正向 、负向提示词，使用更具体的描述
2. 尝试多次运行，让 `KSampler` 使用不同的种子，从而带来不同的生成效果
3. 在了解本篇遮罩编辑器使用的部分后，对于生成的结果再次进行重绘以获得满意的结果。

接下来我们将简单了解如何使用 **遮罩编辑器(Mask Editor)** ，因为之前提供的输入图片中是已经包含了`alpha`透明通道（也就是我们希望在绘图过程中进行编辑的区域），所以并不需要你手动绘制，但在日常使用中我们会更经常使用 **遮罩编辑器(Mask Editor)** 来绘制 蒙版(Mask)

### 使用遮罩编辑器(Mask Editor) 绘制蒙版

首先在上一步工作流中的`Save Image` 节点上右键，你可以在右键菜单中看到`复制(Clipspace)` 选项，点击后会复制当前图片到剪贴板

![ComfyUI 局部重绘 - 复制图片](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png)

然后在 **加载图像(Load Image)** 节点上右键，你可以在右键菜单中看到`Paste(Clipspace)` 选项，点击后会从剪贴板中粘贴图片

![ComfyUI 局部重绘 - 粘贴图片](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png)

然后在 **加载图像(Load Image)** 节点上右键，你可以在右键菜单中看到`在遮罩编辑器中打开(Open in MaskEditor)` 选项，点击后会打开遮罩编辑器

![打开遮罩编辑器](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg)

![遮罩编辑器](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/inpaint/inpaint-maskeditor.gif)

1. 你可以右侧编辑相关参数，比如调整画笔大小、透明度等等
2. 绘制错误区域可以使用橡皮檫来擦除
3. 绘制完成后点击 `Save` 按钮保存蒙版

这样绘制完成的内容就会作为 遮罩(Mask) 输入到 VAE Encoder (for Inpainting) 节点中一起进行编码

然后试着调整提示词，再次进行生成，直到你可以完成满意的生成结果。

## 局部重绘制相关节点

通过[文生图](/zh-CN/tutorials/basic/text-to-image)、[图生图](/zh-CN/tutorials/basic/image-to-image) 和本篇的工作流对比，我想你应该可以看到这几个工作流主要的差异都在于 VAE 部分这部分的条件输入,
在这个工作流中我们使用到的是 **VAE 内部编码器** 节点，这个节点是专门用于局部重绘的节点，它可以帮助我们更好地控制生成区域，从而获得更好的生成效果。

![VAE Encoder (for Inpainting) 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/lantent/inpaint/vae_encode_for_inpainting.jpg)

**输入类型**

| 参数名称           | 作用                                                     |
| -------------- | ------------------------------------------------------ |
| `pixels`       | 需要编码到潜空间的输入图像。                                         |
| `vae`          | 用于将图片从像素空间编码到潜在空间的 VAE 模型。                             |
| `mask`         | 图片遮罩，用来具体指明哪个区域需要进行修改。                                 |
| `grow_mask_by` | 在原有的遮罩基础上，向外扩展的像素值，保证在遮罩区域外围有一定的过度区域，避免重绘区域与原图存在生硬的过渡。 |

**输出类型**

| 参数名称     | 作用                |
| -------- | ----------------- |
| `latent` | 经过 VAE 编码后的潜空间图像。 |


# ComfyUI LoRA 使用示例
Source: https://docs.comfy.org/zh-CN/tutorials/basic/lora

本篇将引导了解并完成单个 LoRA 模型的使用

**LoRA 模型​（Low-Rank Adaptation）** 是一种用于微调大型生成模型（如 Stable Diffusion）的高效技术。
它通过在预训练模型的基础上引入可训练的低秩矩阵，仅调整部分参数，而非重新训练整个模型，从而以较低的计算成本实现特定任务的优化，相对于类似 SD1.5 这样的大模型，LoRA 模型更小，更容易训练。

![LoRA 模型与基础模型对比](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/compare.png)

上面的图片对比了同样参数下 [dreamshaper\_8](https://civitai.com/models/4384?modelVersionId=128713) 直接生成和使用 [blindbox\_V1Mix](https://civitai.com/models/25995/blindbox) LoRA 模型生成的图片对比，我们可以看到通过使用 LoRA 模型，可以在不调整基础模型的情况下，生成更符合我们需求的图片。

我们将演示如何使用 LoRA 的示例。所有 LoRA 变体：Lycoris, loha, lokr, locon, 等... 都是以这种方式使用。

在本示例中，我们将完成以下内容来学习[ComfyUI](https://github.com/comfyanonymous/ComfyUI) 中加载并使用 LoRA 模型，将涉及以下内容：

1. 安装 LoRA 模型
2. 使用 LoRA 模型生成图片
3. `Load LoRA` 节点的简单介绍

## 相关模型安装

请下载 [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16) 并保存至 `ComfyUI/models/checkpoints` 目录

请下载 [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model\&format=SafeTensor\&size=full\&fp=fp16) 并保存至 `ComfyUI/models/loras` 目录

## LoRA 工作流文件

请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流
![ComfyUI 工作流 - LoRA](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/lora.png)

<Tip>
  Metadata 中包含工作流 json 的图片可直接拖入 ComfyUI 或使用菜单 `Workflows` -> `Open（ctrl+o）` 来加载对应的工作流。
</Tip>

## 按步骤完成工作流的运行

请参照下图步骤，来确保对应的工作流可以正常运行

![ComfyUI 工作流 - LoRA 流程图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/flow_diagram.png)

1. 确保`Load Checkpoint` 加载了 `dreamshaper_8.safetensors`
2. 确保`Load LoRA` 加载了 `blindbox_V1Mix.safetensors`
3. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成

## Load LoRA 节点介绍

![Load LoRA 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_lora.jpg)

位于`ComfyUI\models\loras` 的模型会被 ComfyUI 检测到，并在这个节点中加载

### 输入类型

| 参数名称             | 作用                                        |
| ---------------- | ----------------------------------------- |
| `model`          | 连接基础模型                                    |
| `clip`           | 连接 CLIP 模型                                |
| `lora_name`      | 选择要加载使用的 LoRA 模型                          |
| `strength_model` | 影响 LoRA 对 模型权重（model）的影响程度，数值越大 LoRA 风格越强 |
| `strength_clip`  | 影响 LoRA 对 CLIP 词嵌入（clip）的影响程度             |

### 输出类型

| 参数名称    | 作用                     |
| ------- | ---------------------- |
| `model` | 输出应用了 LoRA 调整的模型       |
| `clip`  | 输出应用了 LoRA 调整的 CLIP 模型 |

该节点支持链式连接，可以将多个`Load LoRA` 节点串联来应用多个 LoRA 模型，具体请参考[ComfyUI 应用多个 LoRA 示例](/zh-CN/tutorials/basic/multiple-loras)

![LoRA 节点链式连接](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png)

## 开始你的尝试

1. 试着修改提示词，或者调整 `Load LoRA`  节点的不同参数，比如 `strength_model` ，来观察生成图片的变化，熟悉对应节点。
2. 访问 [CivitAI](https://civitai.com/models) 网站，下载其它风格的 LoRA 模型，尝试使用。


# ComfyUI 应用多个 LoRA 示例
Source: https://docs.comfy.org/zh-CN/tutorials/basic/multiple-loras

本篇将引导你了解并完成在 ComfyUI 中同时应用多个 LoRA 模型

在 [ComfyUI LoRA 使用示例](/zh-CN/tutorials/basic/lora) 中，我们介绍了如何在 ComfyUI 中加载并使用 LoRA 模型，也提及了该节点支持链式连接。

![LoRA 节点链式连接](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/lora/chain_link.png)

在本篇中我们将使用链式连接`Load LoRA`节点的方式来同时使用多个 LoRA 模型，在本示例中，我们将使用 [blindbox\_V1Mix](https://civitai.com/models/25995?modelVersionId=32988) 和 [MoXinV1](https://civitai.com/models/12597?modelVersionId=14856) 两个 LoRA 模型。

下图是这两个 LoRA 模型在同样参数下单独使用的效果

![ComfyUI 中 LoRA 模型单独使用效果](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/compare.png)

但通过多个 LoRA 模型链式连接后，我们可以在最终的效果中看到两种风格融合在一起的效果

![ComfyUI 中多 LoRA 模型应用示例结果](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png)

## 相关模型安装

请下载 [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16) 并保存至 `ComfyUI/models/checkpoints` 目录

请下载 [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model\&format=SafeTensor\&size=full\&fp=fp16) 并保存至 `ComfyUI/models/loras` 目录

请下载 [MoXinV1.safetensors](https://civitai.com/api/download/models/14856?type=Model\&format=SafeTensor\&size=full\&fp=fp16) 并保存至 `ComfyUI/models/loras` 目录

## 多 LoRA 模型应用示例工作流

请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流
![ComfyUI 工作流 - 多 LoRA 模型应用示例](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/multiple_loras.png)

<Tip>
  Metadata 中包含工作流 json 的图片可直接拖入 ComfyUI 或使用菜单 `Workflows` -> `Open（ctrl+o）` 来加载对应的工作流。
</Tip>

## 按步骤完成工作流的运行

请参照下图步骤完成，确保工作流能够正常运行

![ComfyUI 工作流 - 多 LoRA 模型应用示例流程图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/multiple_loras/flow_diagram.png)

1. 确保`Load Checkpoint`可以加载 **dreamshaper\_8.safetensors**
2. 确保`Load LoRA`可以加载 **blindbox\_V1Mix.safetensors**
3. 确保`Load LoRA`可以加载 **MoXinV1.safetensors**
4. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成

## 开始你的尝试

1. 试着调整两个 `Load LoRA` 的 `strength_model` 参数，来修改不同 LoRA 模型对最终生成图片的影响
2. 访问 [CivitAI](https://civitai.com/models) 网站，下载其它风格的 LoRA 模型，组合出你满意的效果


# ComfyUI 扩图（Outpaint）工作流示例
Source: https://docs.comfy.org/zh-CN/tutorials/basic/outpaint

本篇指南将带你了解 ComfyUI 中的扩图工作流，带你完成一个扩图的示例

本篇将引导了解 AI 绘图中扩图的概念，并在 ComfyUI 中完成扩图工作流生成。我们将接触以下内容：

* 使用扩图工作流完成画面的扩展
* 了解并使用 ComfyUI 中的扩图相关节点
* 掌握扩图的基本操作流程

## 关于扩图

在 AI 图像生成过程中，我们经常会遇到这样的需求：已有的图片构图很好，但是画面范围太小，需要扩展画布来获得更大的场景，这时候就需要用到扩图功能。

这就像让 **画家(AI 绘图模型)** 在已有的画作基础上，向外延伸绘制更大的场景。我们需要告诉画家 **需要扩展的方向和范围**，画家会根据已有的画面内容，合理地延伸和扩展场景。

基本上它要求的内容与[局部重绘](/zh-CN/tutorials/basic/inpaint)相似，只不过我们用来**构建遮罩（Mask）的节点不同**

扩图的应用场景包括：

* **场景扩展：** 扩大原有画面的场景范围，展现更完整的环境
* **构图调整：** 通过扩展画布来优化整体构图
* **内容补充：** 为原有画面添加更多相关的场景元素

## ComfyUI 扩图工作流示例讲解

### 准备工作

#### 1. 模型安装

请确保你已经在 `ComfyUI/models/checkpoints` 文件夹至少有一个 SD1.5 的模型文件，如果你还不了解如何安装模型，请参[开始 ComfyUI 的 AI 绘图之旅](/zh-CN/get_started/first_generation#3-安装绘图模型)章节中关于模型安装的部分说明。

你可以使用下面的这些模型：

* [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)
* [Dreamshaper 8](https://civitai.com/models/4384?modelVersionId=128713)
* [Anything V5](https://civitai.com/models/9409?modelVersionId=30163)

- [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)

#### 2. 输入图片

请准备一张你想要进行扩展的图片。在本例中，我们将使用下面这张图片作为示例：

![ComfyUI扩图输入图片](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/input.png)

#### 3. 扩图工作流

请下载下面的图片，并将其 **拖入** ComfyUI 界面或使用菜单 **工作流(Workflow)** --> **打开工作流(Open,快捷键 `Ctrl + O`)** 来加载这个扩图工作流

![ComfyUI扩图工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpaint.png)

### 扩图工作流使用讲解

![ComfyUI 扩图工作流示意图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/outpainting_workflow.jpg)

扩图工作流的关键步骤如下：

1. 请在 `加载模型(Load Checkpoint)` 节点中加载你本地安装的模型文件
2. 请在 `加载图片(Load Image)` 节点中点击 `Upload` 按钮上传
3. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl + Enter(回车)` 来执行图片生成

在这个工作流中主要是通过 `Pad Image for outpainting` 节点来控制图片的扩展方向和范围，其实这也是一个 [局部重绘(Inpaint)](/zh-CN/tutorials/basic/inpaint.mdx) 工作流，只不过我们用来构建遮罩（Mask）的节点不同。

### Pad Image for outpainting 节点

![Pad Image for outpainting 节点](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/pad_image_for_outpainting.jpg)

这个节点接受一个输入图片，并输出一张扩展过的图像和对应的遮罩（Mask），其中遮罩由于对应的节点参数构建。

#### 输入参数

| 参数名称         | 作用                           |
| ------------ | ---------------------------- |
| `image`      | 输入图片                         |
| `left`       | 左侧填充量                        |
| `top`        | 顶部填充量                        |
| `right`      | 右侧填充量                        |
| `bottom`     | 底部填充量                        |
| `feathering` | 控制原始图像与添加的填充内容之间的过渡平滑度，越大越平滑 |

#### 输出参数

| 参数名称    | 作用                     |
| ------- | ---------------------- |
| `image` | 输出`image`代表已填充的图像      |
| `mask`  | 输出`mask`指示原始图像和添加的填充区域 |

#### 节点输出内容

经过 `Pad Image for outpainting` 节点处理后，输出的图片和蒙版预览如下：

![Pad Image for outpainting 节点结果](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg)

你可以看到对应的输出结果

* `Image` 输出的是扩展后的图像
* `Mask` 输出的是标记了扩展区域的蒙版


# ComfyUI 文生图工作流
Source: https://docs.comfy.org/zh-CN/tutorials/basic/text-to-image

本篇将引导了解 AI 绘图中，文生图的概念，并在 ComfyUI 中完成文生图工作流生成

本篇目的主要带你初步了解 ComfyUI 的文生图的工作流，并初步了解一些 ComfyUI 相关节点的功能和使用。

在本篇文档中我们将完成以下内容：

* 完成一次文生图工作流
* 简单了解扩散模型原理
* 了解工作流中的节点的功能和作用
* 初步了解 SD1.5 模型

我们将会先进行文生图工作流的运行，然后进行相关内容的讲解，请按你的需要选择对应部分开始。

## 关于文生图

**文生图(Text to Image)** ，是 AI 绘图中的基础流程，通过输入文本描述来生成对应的图片，它的核心是 **扩散模型**。

在文生图过程中我们需要以下条件：

* **画家：** 绘图模型
* **画布：** 潜在空间
* \*\*对画面的要求（提示词）：\*\*提示词，包括正向提示词（希望在画面中出现的元素）和负向提示词（不希望在画面中出现的元素）

这个文本到图片图片生成过程，可以简单理解成你把你的**绘图要求(正向提示词、负向提示词)**告诉一个**画家(绘图模型)**，画家会根据你的要求，画出你想要的内容。

## ComfyUI 文生图工作流示例讲解

### 1. 开始开始前的准备

请确保你已经在 `ComfyUI/models/checkpoints` 文件夹至少有一个 SD1.5 的模型文件，如果你还不了解如何安装模型，请参[开始 ComfyUI 的 AI 绘图之旅](/zh-CN/get_started/first_generation#3-安装绘图模型)章节中关于模型安装的部分说明。

你可以使用下面的这些模型：

* [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)
* [Dreamshaper 8](https://civitai.com/models/4384?modelVersionId=128713)
* [Anything V5](https://civitai.com/models/9409?modelVersionId=30163)

### 2. 加载文生图工作流

请下载下面的图片，并将图片拖入 ComfyUI 的界面中，或者使用菜单 **工作流（Workflows）** -> **打开（Open）** 打开这个图片以加载对应的 workflow

![ComfyUI-文生图工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/text-to-image-workflow.png)

也可以从菜单 **工作流（Workflows）** -> **浏览工作流示例（Browse example workflows）** 中选择 **Text to Image** 工作流

### 3. 加载模型，并进行第一次图片生成

在完成了对应的绘图模型安装后，请参考下图步骤加载对应的模型，并进行第一次图片的生成

![图片生成](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg)

请对应图片序号，完成下面操作

1. 请在**Load Checkpoint** 节点使用箭头或者点击文本区域确保 **v1-5-pruned-emaonly-fp16.safetensors** 被选中，且左右切换箭头不会出现**null** 的文本
2. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl + Enter(回车)` 来执行图片生成

等待对应流程执行完成后，你应该可以在界面的\*\*保存图像（Save Image）\*\*节点中看到对应的图片结果，可以在上面右键保存到本地

![ComfyUI 首次图片生成结果](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/gettingstarted/first-image-generation-8-result.jpg)

<Tip>如果生成结果不满意，可以多运行几次图片生成，因为每次运行图片生成，**KSampler** 根据 `seed` 参数会使用不同的随机种子，所以每次生成的结果都会有所不同</Tip>

### 4. 开始你的尝试

你可以尝试修改**CLIP Text Encoder**处的文本

![CLIP Text Encoder](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg)

其中连接到 KSampler 节点的`Positive`为正向提示词，连接到 KSampler 节点的`Negative`为负向提示词

下面是针对 SD1.5 模型的一些简单提示词原则

* 尽量使用英文
* 提示词之间使用英文逗号 `,` 隔开
* 尽量使用短语而不是长句子
* 使用更具体的描述
* 可以使用类似 `(golden hour:1.2)` 这样的表达来提升特定关键词的权重，这样它在画面中出现的概率会更高，`1.2` 为权重，`golden hour` 为关键词
* 可以使用类似 `masterpiece, best quality, 4k` 等关键词来提升生成质量

下面是几组不同的 prompt 示例，你可以尝试使用这些 prompt 来查看生成的效果，或者使用你自己的 prompt 来尝试生成

**1. 二次元动漫风格**

正向提示词：

```
anime style, 1girl with long pink hair, cherry blossom background, studio ghibli aesthetic, soft lighting, intricate details

masterpiece, best quality, 4k
```

负向提示词：

```
low quality, blurry, deformed hands, extra fingers
```

**2. 写实风格**

正向提示词：

```
(ultra realistic portrait:1.3), (elegant woman in crimson silk dress:1.2), 
full body, soft cinematic lighting, (golden hour:1.2), 
(fujifilm XT4:1.1), shallow depth of field, 
(skin texture details:1.3), (film grain:1.1), 
gentle wind flow, warm color grading, (perfect facial symmetry:1.3)
```

负向提示词：

```
(deformed, cartoon, anime, doll, plastic skin, overexposed, blurry, extra fingers)
```

**3. 特定艺术家风格**

正向提示词：

```
fantasy elf, detailed character, glowing magic, vibrant colors, long flowing hair, elegant armor, ethereal beauty, mystical forest, magical aura, high detail, soft lighting, fantasy portrait, Artgerm style
```

负向提示词：

```
blurry, low detail, cartoonish, unrealistic anatomy, out of focus, cluttered, flat lighting
```

## 文生图工作原理

整个文生图的过程，我们可以理解成是**扩散模型的反扩散过程**，我们下载的 [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) 是一个已经训练好的可以 **从纯高斯噪声生成目标图片的模型**，我们只需要输入我们的提示词，它就可以通随机的噪声降噪生成目标图片。

```mermaid
graph LR
A[纯高斯噪声] --> B[迭代去噪]
B --> C[中间潜在变量]
C --> D[最终生成图像]
E[文本提示词] --> F[CLIP编码器]
F --> G[语义向量]
G --> B
```

我们可能需要了解下两个概念，

1. **潜在空间：** 潜在空间（Latent Space）是扩散模型中的一种抽象数据表示方式，通过把图片从像素空间转换为潜在空间，可以减少图片的存储空间，并且可以更容易的进行扩散模型的训练和减少降噪的复杂度，就像建筑师设计建筑时使用蓝图（潜在空间）来进行设计，而不是直接在建筑上进行设计（像素空间），这种方式可以保持结构特征的同时，又大幅度降低修改成本
2. **像素空间：** 像素空间（Pixel Space）是图片的存储空间，就是我们最终看到的图片，用于存储图片的像素值。

如果你想要了解更多扩散模型相关内容，可以阅读下面的文章：

* [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/pdf/2006.11239)
* [Denoising Diffusion Implicit Models (DDIM)](https://arxiv.org/pdf/2010.02502)
* [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752)

## ComfyUI 文生图工作流节点讲解

![ComfyUI 文生图工作流讲解](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/text-image-workflow.jpg)

### A. 加载模型（Load Checkpoint）节点

![加载模型](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_checkpoint.jpg)

这个节点通常用于加载绘图模型, 通常 `checkpoint` 中会包含 `MODEL（UNet）`、`CLIP` 和 `VAE` 三个组件

* `MODEL（UNet）`：为对应模型的 UNet 模型, 负责扩散过程中的噪声预测和图像生成,驱动扩散过程
* `CLIP`：这个是文本编码器,因为模型并不能直接理解我们的文本提示词（prompt）,所以需要将我们的文本提示词（prompt）编码为向量,转换为模型可以理解的语义向量
* `VAE`：这个是变分自编码器,我们的扩散模型处理的是潜在空间,而我们的图片是像素空间,所以需要将图片转换为潜在空间,然后进行扩散,最后将潜在空间转换为图片

### B. 空Latent图像（Empty Latent Image）节点

![空Latent图像](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/lantent/empty_latent_image.jpg)

定义一个潜在空间（Latent Space）,它输出到 KSampler 节点，空Latent图像节点构建的是一个 **纯噪声的潜在空间**

它的具体的作用你可以理解为定义画布尺寸的大小，也就是我们最终生成图片的尺寸

### C. CLIP文本编码器（CLIP Text Encoder）节点

![CLIP文本编码器](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/clip_text_encode.jpg)

用于编码提示词，也就是输入你对画面的要求

* 连接到 KSampler 节点的 `Positive` 条件输入的为正向提示词（希望在画面中出现的元素）
* 连接到 KSampler 节点的 `Negative` 条件输入的为负向提示词（不希望在画面中出现的元素）

对应的提示词被来自 `Load Checkpoint` 节点的 `CLIP` 组件编码为语义向量，然后作为条件输出到 KSampler 节点

### D. K 采样器（KSampler）节点

![K 采样器](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/sampling/k_sampler.jpg)

**K 采样器** 是整个工作流的核心，整个噪声降噪的过程都在这个节点中完成，并最后输出一个潜空间图像

```mermaid
graph LR
A[随机噪声<br>潜在空间] --> B{KSampler}
C[扩散模型] --> B
D[CLIP语义向量] --> B
B --> E[去噪Latent]
```

KSampler 节点的参数说明如下

| 参数名称                         | 描述         | 作用                                   |
| ---------------------------- | ---------- | ------------------------------------ |
| **model**                    | 去噪使用的扩散模型  | 决定生成图像的风格与质量                         |
| **positive**                 | 正向提示词条件编码  | 引导生成包含指定元素的内容                        |
| **negative**                 | 负向提示词条件编码  | 抑制生成不期望的内容                           |
| **latent\_image**            | 待去噪的潜在空间图像 | 作为噪声初始化的输入载体                         |
| **seed**                     | 噪声生成的随机种子  | 控制生成结果的随机性                           |
| **control\_after\_generate** | 种子生成后控制模式  | 决定多批次生成时种子的变化规律                      |
| **steps**                    | 去噪迭代步数     | 步数越多细节越精细但耗时增加                       |
| **cfg**                      | 分类器自由引导系数  | 控制提示词约束强度（过高导致过拟合）                   |
| **sampler\_name**            | 采样算法名称     | 决定去噪路径的数学方法                          |
| **scheduler**                | 调度器类型      | 控制噪声衰减速率与步长分配                        |
| **denoise**                  | 降噪强度系数     | 控制添加到潜在空间的噪声强度，0.0保留原始输入特征，1.0为完全的噪声 |

在 KSampler 节点中，潜在空间使用 `seed` 作为初始化参数构建随机的噪声,语义向量 `Positive` 和 `Negative` 会作为条件输入到扩散模型中

然后根据 `steps` 参数指定的去噪步数，进行去噪，每次去噪会根据 `denoise` 参数指定的降噪强度系数，对潜在空间进行降噪，并生成新的潜在空间图像

### E. VAE 解码（VAE Decode）节点

![VAE 解码](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/lantent/vae_decode.jpg)

将 **K 采样器(KSampler)** 输出的潜在空间图像转换为像素空间图像

### F. 保存图像（Save Image）节点

![保存图像](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/image/save_image.jpg)

预览并保存从潜空间解码的图像，并保存到本地`ComfyUI/output`文件夹下

## SD1.5 模型简介

**SD1.5(Stable Diffusion 1.5)** 是一个由[Stability AI](https://stability.ai/)开发的AI绘图模型，Stable Diffusion系列的基础版本，基于 **512×512** 分辨率图片训练，所以其对 **512×512** 分辨率图片生成支持较好，体积约为4GB，可以在\*\*消费级显卡（如6GB显存）\*\*上流畅运行。目前 SD1.5 的相关周边生态非常丰富，它支持广泛的插件（如ControlNet、LoRA）和优化工具。
作为AI绘画领域的里程碑模型，SD1.5凭借其开源特性、轻量架构和丰富生态，至今仍是最佳入门选择。尽管后续推出了SDXL/SD3等升级版本，但其在消费级硬件上的性价比仍无可替代。

### 基础信息

* **发布时间**：2022年10月
* **核心架构**：基于Latent Diffusion Model (LDM)
* **训练数据**：LAION-Aesthetics v2.5数据集（约5.9亿步训练）
* **开源特性**：完全开源模型/代码/训练数据

### 优缺点

模型优势：

* 轻量化：体积小，仅 4GB 左右，在消费级显卡上流畅运行
* 使用门槛低：支持广泛的插件和优化工具
* 生态成熟：支持广泛的插件和优化工具
* 生成速度快：在消费级显卡上流畅运行

模型局限：

* 细节处理：手部/复杂光影易畸变
* 分辨率限制：直接生成1024x1024质量下降
* 提示词依赖：需精确英文描述控制效果


# ComfyUI 图像放大工作流
Source: https://docs.comfy.org/zh-CN/tutorials/basic/upscale

本篇将引导了解 AI 绘图中，放大图片的概念，并在 ComfyUI 中完成放大图片工作流生成

## 什么是图像放大

图像放大（Image Upscaling）是通过算法将低分辨率图像转换为高分辨率图像的过程。与传统插值放大不同，AI 放大模型（如 ESRGAN）能智能重建细节，保持图像质量。
比如默认通过 SD1.5 模型对于大尺寸的图片生成表现不佳，如果需要高分辨率，我们通常会先生成小尺寸的图像，然后使用图像放大来提升图片的分辨率。

当然本文介绍的只是诸多 ComfyUI 中图像放大方法中的一种，在这篇讲解中，我们将带你完成以下内容：

* 下载并安装放大模型
* 使用放大模型进行一次简单的放大
* 结合文生图工作流，完成图像的放大

## 下载并安装放大模型

额外需要下载 ESRGAN 等放大模型（必须）：

<Steps>
  <Step title="访问 OpenModelDB">
    访问 [OpenModelDB](https://openmodeldb.info/) 搜索下载需要的放大模型（如 RealESRGAN）

    ![openmodeldb](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_OpenModelDB.jpg)

    如图所示：

    1. 你可以在筛选处根据你的图像类型选来过滤对应的模型类型
    2. 对应模型右上角会有标注具体放大的倍数，比如在我们提供的截图里，对应的这个模型是将图像放大2倍的模型

    本篇教程中我们将使用 [4x-ESRGAN](https://openmodeldb.info/models/4x-ESRGAN) 模型，点击进入详情页，点击 `Download` 下载模型

    ![OpenModelDB\_download](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_OpenModelDB_download.jpg)
  </Step>

  <Step title="将模型文件放入指定目录">
    将模型文件（.pth）放入 `ComfyUI\models\upscale_models` 目录
  </Step>
</Steps>

## 简单放大工作流

### 1. 工作流及素材

请下载下面的图片，并拖入到 ComfyUI 中，加载简单版本放大工作流
![放大工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_workflow.png)

请下载下面这张小尺寸的图片作为输入
![Upscale-input](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale-input.jpg)

### 2. 工作流讲解

![放大工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/upscale_simple_workflow.jpg)

1. 在`加载放大模型(Load Upscale Model)`节点中选择我们之前下载的放大模型
2. 在`加载图片(Load Image)`节点中选择我们之前准备的输入图片
3. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl + Enter(回车)` 来执行图片生成

通过以上步骤，我们就可以完成一个图片的放大，你可以看到在这个工作流中，核心主要在于 `Load Upscale Model` 和 `Upscale Image(Using Model)` 的组合，他们通过接收一个图像的输入，然后使用放大模型将图像放大。

## 结合文生图的放大工作流

在完成了简单的放大工作流后，我们就可以尝试结合[文生图](/zh-CN/tutorials/basic/text-to-image)的工作流来完成一个完整放大工作的流程，关于文生图的基础部分及相关模型要求，请参考[文生图](/zh-CN/tutorials/basic/text-to-image)的部分的说明完成。

请将下面的图片下载并保存后拖入到 ComfyUI 中，加载结合文生图的放大工作流
![结合文生图的放大工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/basic/upscale/esrgan_example.png)

你可以看到在这个工作流里，就是在文生图工作流之后把对应的图片输入到放大工作流中完成了对应图片的放大。

## 其它相关补充

<Tip>
  不同放大模型特性：

  * **RealESRGAN**: 通用型放大，适合大多数场景
  * **BSRGAN**: 擅长处理文字和锐利边缘
  * **SwinIR**: 保持自然纹理，适合风景照片
</Tip>

1. **链式放大**：对于需要超高倍率放大的情况，可以串联多个放大节点（如先2x再4x）
2. **混合放大**：在生成工作流后接放大节点，实现"生成+增强"一体化流程
3. **对比测试**：不同模型对特定类型图片效果差异较大，建议同时测试多个模型


# ComfyUI ControlNet 使用示例
Source: https://docs.comfy.org/zh-CN/tutorials/controlnet/controlnet

本篇将引导了解基础的 ControlNet 概念，并在 ComfyUI 中完成对应的图像生成

在 AI 图像生成过程中，要精确控制图像生成并不是一键容易的事情，通常需要通过许多次的图像生成才可能生成满意的图像，但随着 **ControlNet** 的出现，这个问题得到了很好的解决。

ControlNet 是一种基于扩散模型（如 Stable Diffusion）的条件控制生成模型，最早由[Lvmin Zhang](https://lllyasviel.github.io/)与 Maneesh Agrawala 等人于 2023 年提出[Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543)

ControlNet 模型通过引入多模态输入条件（如边缘检测图、深度图、姿势关键点等），显著提升了图像生成的可控性和细节还原能力。
使得我们可以进一步开始控制图像的风格、细节、人物姿势、画面结构等等，这些限定条件让图像生成变得更加可控，在绘图过程中也可以同时使用多个 ControlNet 模型，以达到更好的效果。

在没有 ControlNet 之前，我们每次只能让模型生成图像，直到生成我们满意的图像，充满了随机性。

![ComfyUI 随机种子生成的图片](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/generated_with_random_seed.jpg)

但随着 ControlNet 的出现，我们可以通过引入额外的条件，来控制图像的生成，比如我们可以使用一张简单的涂鸦，来控制图像的生成，就可以生成差不多类似的图片。

![ComfyUI 涂鸦控制图像生成](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/scribble_example.jpg)

在本示例中，我们将引导你完成在 [ComfyUI](https://github.com/comfyanonymous/ComfyUI) 中 ControlNet 模型的安装与使用, 并完成一个涂鸦控制图像生成的示例。

![ComfyUI ControlNet 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)

<Tip>
  ControlNet V1.1 其它类型的 ControlNet 模型的工作流也与都与本篇示例相同，你只需要根据需要选择对应的模型和上传对应的参考图即可。
</Tip>

## ControlNet 图片预处理相关说明

不同类型的 ControlNet 模型，通常需要使用不同类型的参考图：

![参考图](https://github.com/Fannovel16/comfyui_controlnet_aux/blob/main/examples/CNAuxBanner.jpg?raw=true)

> 图源：[ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

由于目前 **Comfy Core** 节点中，不包含所有类型的 **预处理器** 类型，但在本文档的实际示例中，我们都将提供已经经过处理后的图片，
但在实际使用过程中，你可能需要借助一些自定义节点来对图片进行预处理，以满足不同 ControlNet 模型的需求，下面是一些相关的插件

* [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
* [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## ComfyUI ControlNet 工作流示例讲解

### 1. ControlNet 工作流素材

请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流

![ComfyUI 工作流 - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)

<Tip>
  Metadata 中包含工作流 json 的图片可直接拖入 ComfyUI 或使用菜单 `Workflows` -> `Open（ctrl+o）` 来加载对应的工作流。
  该图片已包含对应模型的下载链接，直接拖入 ComfyUI 将会自动提示下载。
</Tip>

请下载下面的图片，我们将会将它作为输入

![ComfyUI 涂鸦图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_input.png)

### 2.  手动模型安装

<Note>
  如果你网络无法顺利完成对应模型的自动下载，请尝试手动下载下面的模型，并放置到指定目录中
</Note>

* [dreamCreationVirtual3DECommerce\_v10.safetensors](https://civitai.com/api/download/models/731340?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)
* [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── dreamCreationVirtual3DECommerce_v10.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
│   └── controlnet/
│       └── control_v11p_sd15_scribble_fp16.safetensors
```

<Note>
  本示例中 vae 模型也可以使用 dreamCreationVirtual3DECommerce\_v10.safetensors 模型中的 vae 模型，这里我们遵循模型作者建议使用单独的 vae 模型。
</Note>

### 3. 按步骤完成工作流的运行

![ComfyUI 工作流 - ControlNet 流程图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_scribble.png)

1. 确保`Load Checkpoint`可以加载 **dreamCreationVirtual3DECommerce\_v10.safetensors**
2. 确保`Load VAE`可以加载 **vae-ft-mse-840000-ema-pruned.safetensors**
3. 在`Load Image`中点击`Upload` 上传之前提供的输入图片
4. 确保`Load ControlNet`可以加载 **control\_v11p\_sd15\_scribble\_fp16.safetensors**
5. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成

## 相关节点讲解

### Load ControlNet 节点讲解

![load controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/loaders/load_controlnet_model.jpg)

位于`ComfyUI\models\controlnet` 的模型会被 ComfyUI 检测到，并在这个节点中识别并加载

### Apply ControlNet 节点讲解

![apply controlnet ](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg)

这个节点接受 `load controlnet` 加载的 ControlNet 模型，并根据输入的图片，生成对应的控制条件。

**输入类型**

| 参数名称            | 作用                                                                |
| --------------- | ----------------------------------------------------------------- |
| `positive`      | 正向条件                                                              |
| `negative`      | 负向条件                                                              |
| `control_net`   | 要应用的controlNet模型                                                  |
| `image`         | 用于 controlNet 应用参考的预处理器处理图片                                       |
| `vae`           | Vae模型输入                                                           |
| `strength`      | 应用 ControlNet 的强度，越大则 ControlNet 对生成图像的影响越大                       |
| `start_percent` | 确定开始应用controlNet的百分比，比如取值0.2，意味着ControlNet的引导将在扩散过程完成20%时开始影响图像生成 |
| `end_percent`   | 确定结束应用controlNet的百分比，比如取值0.8，意味着ControlNet的引导将在扩散过程完成80%时停止影响图像生成 |

**输出类型**

| 参数名称       | 作用                        |
| ---------- | ------------------------- |
| `positive` | 应用了 ControlNet 处理后的正向条件数据 |
| `negative` | 应用了 ControlNet 处理后的负向条件数据 |

你可以使用链式链接来应用多个 ControlNet 模型，如下图所示，你也可以参考 [混合 ControlNet 模型](/zh-CN/tutorials/controlnet/mixing-controlnets.mdx) 部分的指南来了解更多关于混合 ControlNet 模型的使用
![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg)

<Note>
  你也许会在有些早期的工作流中看到如下的`Apply ControlNet(Old)` 节点，这个节点是早期 ControlNet 的节点，目前已弃用状态，默认在搜索和节点列表不可见
  ![apply controlnet old](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/comfy_core/conditioning/controlnet/apply_controlnet_old.jpg)
  如需启用，请在**设置**--> **comfy** --> **Node** 中，启用`Show deprecated nodes in search` 选项，推荐使用新节点
</Note>

## 开始你的尝试

1. 试着制作类似的涂鸦图片，甚至自己手绘，并使用 ControlNet 模型生成图像，体验 ControlNet 带来的乐趣
2. 调整 Apply ControlNet 节点的 `Control Strength` 参数，来控制 ControlNet 模型对生成图像的影响
3. 访问 [ControlNet-v1-1\_fp16\_safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/tree/main)  仓库下载其它类型的 ControlNet 模型，并尝试使用它们生成图像


# ComfyUI Depth ControlNet 使用示例
Source: https://docs.comfy.org/zh-CN/tutorials/controlnet/depth-controlnet

本篇将引导了解基础的 Depth ControlNet 概念，并在 ComfyUI 中完成对应的图像生成

## 深度图与 Depth ControlNet 介绍

深度图(Depth Map)是一种特殊的图像，它通过灰度值表示场景中各个物体与观察者或相机的距离。在深度图中，灰度值与距离成反比：​越亮的区域（接近白色）表示距离越近，​越暗的区域（接近黑色）表示距离越远。

![Depth 图像](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png)

Depth ControlNet 是专门训练用于理解和利用深度图信息的 ControlNet 模型。它能够帮助 AI 正确解读空间关系，使生成的图像符合深度图指定的空间结构，从而实现对三维空间布局的精确控制。

### 深度图结合 ControlNet 应用场景

深度图在多种场景中都有比较多的应用：

1. **人像场景**：控制人物与背景的空间关系，避免面部等关键部位畸变
2. **风景场景**：控制近景、中景、远景的层次关系
3. **建筑场景**：控制建筑物的空间结构和透视关系
4. **产品展示**：控制产品与背景的分离度和空间位置

本篇示例中，我们将使用深度图生成建筑可视化的场景生成。

## ComfyUI ControlNet 工作流示例讲解

### 1. ControlNet 工作流素材

请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流

![Depth 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_controlnet.png)

<Tip>
  Metadata 中包含工作流 json 的图片可直接拖入 ComfyUI 或使用菜单 `Workflows` -> `Open（ctrl+o）` 来加载对应的工作流。
  该图片已包含对应模型的下载链接，直接拖入 ComfyUI 将会自动提示下载。
</Tip>

请下载下面的图片，我们将会将它作为输入。

![Depth 图像](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png)

### 2. 模型安装

<Note>
  如果你网络无法顺利完成对应模型的自动下载，请尝试手动下载下面的模型，并放置到指定目录中
</Note>

* [architecturerealmix\_v11.safetensors](https://civitai.com/api/download/models/431755?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [control\_v11f1p\_sd15\_depth\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors?download=true)

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── architecturerealmix_v11.safetensors
│   └── controlnet/
│       └── control_v11f1p_sd15_depth_fp16.safetensors
```

### 3. 按步骤完成工作流的运行

![ComfyUI 工作流 - Depth ControlNet 流程图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth.jpg)

1. 确保`Load Checkpoint`可以加载 **architecturerealmix\_v11.safetensors**
2. 确保`Load ControlNet`可以加载 **control\_v11f1p\_sd15\_depth\_fp16.safetensors**
3. 在`Load Image`中点击`Upload` 上传之前提供的 Depth 图像
4. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成

## 混合深度控制与其他技术

根据不同创作需求，可以将深度图 ControlNet 与其它类型的 ControlNet 混合使用来达到更好的效果：

1. **Depth + Lineart**：保持空间关系的同时强化轮廓，适用于建筑、产品、角色设计
2. **Depth + Pose**：控制人物姿态的同时维持正确的空间关系，适用于人物场景

关于多个 ControlNet 混合使用，可以参考 [混合 ControlNet](/zh-CN/tutorials/controlnet/mixing-controlnets.mdx) 示例。


# ComfyUI Depth T2I Adapter 使用示例
Source: https://docs.comfy.org/zh-CN/tutorials/controlnet/depth-t2i-adapter

本篇将引导了解基础的 Depth T2I Adapter ，并在 ComfyUI 中完成对应的图像生成

## T2I Adapter 介绍

[T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter) 是由 ​[腾讯ARC实验室](https://github.com/TencentARC) 开发的轻量级适配器，用于增强文本到图像生成模型（如Stable Diffusion）的结构、颜色和风格控制能力。
它通过外部条件（如边缘检测图、深度图、草图或颜色参考图）与模型内部特征对齐，实现高精度控制，无需修改原模型结构。其参数仅约77M（体积约300MB），推理速度比 [ControlNet](https://github.com/lllyasviel/ControlNet-v1-1-nightly) 快约3倍，支持多条件组合（如草图+颜色网格）。应用场景包括线稿转图像、色彩风格迁移、多元素场景生成等。

### T2I Adapter 与 ControlNet 的对比

虽然功能相似，但两者在实现和应用上有明显区别：

1. **轻量级设计**：T2I Adapter 参数量更少，占用内存更小
2. **推理速度**：T2I Adapter 通常比 ControlNet 快约3倍
3. **控制精度**：ControlNet 在某些场景下控制更精确，而 T2I Adapter 更适合轻量级控制
4. **多条件组合**：T2I Adapter 在多条件组合时资源占用优势更明显

### T2I Adapter 主要类型

T2I Adapter 提供多种类型以控制不同方面：

* **深度 (Depth)**：控制图像的空间结构和深度关系
* **线稿 (Canny/Sketch)**：控制图像的边缘和线条
* **关键点 (Keypose)**：控制人物姿态和动作
* **分割 (Seg)**：通过语义分割控制场景布局
* **颜色 (Color)**：控制图像的整体配色方案

在 ComfyUI 中，使用 T2I Adapter 与 [ControlNet](/zh-CN/tutorials/controlnet/controlnet.mdx) 的界面和工作流相似。在本篇示例中，我们将以深度 T2I Adapter 控制室内场景为例，展示其使用方法。

![ComfyUI Depth T2I Adapter 工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)

## 深度 T2I Adapter 应用价值

深度图（Depth Map）在图像生成中有多种重要应用：

1. **空间布局控制**：准确描述三维空间结构，适用于室内设计、建筑可视化
2. **物体定位**：控制场景中物体的相对位置和大小，适用于产品展示、场景构建
3. **透视关系**：维持合理的透视和比例，适用于风景、城市场景生成
4. **光影布局**：基于深度信息的自然光影分布，增强真实感

我们将以室内设计为例，展示深度 T2I Adapter 的使用方法，但这些技巧也适用于其他应用场景。

## ComfyUI Depth T2I Adapter工作流示例讲解

### 1. Depth T2I Adapter 工作流素材

请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流

![ComfyUI 工作流 - Depth T2I Adapter](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)

<Tip>
  Metadata 中包含工作流 json 的图片可直接拖入 ComfyUI 或使用菜单 `Workflows` -> `Open（ctrl+o）` 来加载对应的工作流。
  该图片已包含对应模型的下载链接，直接拖入 ComfyUI 将会自动提示下载。
</Tip>

请下载下面的图片，我们将会将它作为输入

![ComfyUI 室内深度图](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png)

### 2. 模型安装

<Note>
  如果你网络无法顺利完成对应模型的自动下载，请尝试手动下载下面的模型，并放置到指定目录中
</Note>

* [interiordesignsuperm\_v2.safetensors](https://civitai.com/api/download/models/93152?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [t2iadapter\_depth\_sd15v2.pth](https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd15v2.pth?download=true)

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── interiordesignsuperm_v2.safetensors
│   └── controlnet/
│       └── t2iadapter_depth_sd15v2.pth
```

### 3. 按步骤完成工作流的运行

![ComfyUI 工作流 - Depth T2I Adapter 流程图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg)

1. 确保`Load Checkpoint`可以加载 **interiordesignsuperm\_v2.safetensors**
2. 确保`Load ControlNet`可以加载 **t2iadapter\_depth\_sd15v2.pth**
3. 在`Load Image`中点击`Upload` 上传之前提供的输入图片
4. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成

## T2I Adapter 通用使用技巧

### 输入图像质量优化

无论应用场景如何，高质量的输入图像都是成功使用 T2I Adapter 的关键：

1. **对比度适中**：控制图像（如深度图、线稿）应有明确的对比，但不要过度极端
2. **清晰的边界**：确保主要结构和元素边界在控制图像中清晰可辨
3. **噪点控制**：尽量避免控制图像中有过多噪点，特别是深度图和线稿
4. **合理的布局**：控制图像应当具有合理的空间布局和元素分布

## T2I Adapter 的使用特点

T2I Adapter 的一大优势是可以轻松组合多个条件，实现复杂的控制效果：

1. **深度 + 边缘**：控制空间布局的同时保持结构边缘清晰，适用于建筑、室内设计
2. **线稿 + 颜色**：控制形状的同时指定配色方案，适用于角色设计、插画
3. **姿态 + 分割**：控制人物动作的同时定义场景区域，适用于复杂叙事场景

T2I Adapter 之间的混合，或与其他控制方法（如ControlNet、区域提示词等）的组合，可以进一步扩展创作可能性。要实现混合，只需按照与 [混合 ControlNet](/zh-CN/tutorials/controlnet/mixing-controlnets.mdx) 相同的方式，通过链式连接多个 `Apply ControlNet` 节点即可。


# ComfyUI ControlNet 混合使用示例
Source: https://docs.comfy.org/zh-CN/tutorials/controlnet/mixing-controlnets

我们将在本篇示例中，完成多个 ControlNet 混合使用，学会使用多个 ControlNet 模型来控制图像生成

在 AI 图像生成中，单一的控制条件往往难以满足复杂场景的需求。混合使用多个 ControlNet 可以同时控制图像的不同区域或不同方面，实现更精确的图像生成控制。

在一些场景下，混合使用 ControlNet 可以利用不同控制条件的特性，来达到更精细的条件控制：

1. **场景复杂性**：复杂场景需要多种控制条件共同作用
2. **精细控制**：通过调整每个 ControlNet 的强度参数，可以精确控制各部分的影响程度
3. **互补效果**：不同类型的 ControlNet 可以互相补充，弥补单一控制的局限性
4. **创意表达**：组合不同控制可以产生独特的创意效果

### 混合 ControlNet 的使用方法

当我们混合使用多个 ControlNet 时，每个 ControlNet 会根据其应用的区域对图像生成过程施加影响。ComfyUI 通过 `Apply ControlNet` 节点的链式连接方式，允许多个 ControlNet 条件按顺序叠加应用混合控制条件：

![apply controlnet chain link](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/apply_controlnet_chain_link.jpg)

## ComfyUI ControlNet 区域分治混合示例

在本示例中，我们将使用 **Pose ControlNet** 和 **Scribble ControlNet** 的组合来生成一张包含多个元素的场景：左侧由 Pose ControlNet 控制的人物和右侧由 Scribble ControlNet 控制的猫咪滑板车。

### 1. ControlNet 混合使用工作流素材

请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流
![ComfyUI 工作流 - Mixing ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets.png)

<Tip>
  该工作流图片包含 Metadata 数据，可直接拖入 ComfyUI 或使用菜单 `Workflows` -> `Open（ctrl+o）` 加载。系统会自动检测并提示下载所需模型。
</Tip>

用于输入的 pose 图片（控制左侧人物姿态）:

![ComfyUI 工作流 - Mixing ControlNet 输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input.png)

用于输入的 scribble 图片（控制右侧猫咪和滑板车）:

![ComfyUI 工作流 - Mixing ControlNet 输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input_scribble.png)

### 2. 手动模型安装

<Note>
  如果你网络无法顺利完成对应模型的自动下载，请尝试手动下载下面的模型，并放置到指定目录中
</Note>

* [awpainting\_v14.safetensors](https://civitai.com/api/download/models/624939?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)
* [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── awpainting_v14.safetensors
│   ├── controlnet/
│   │   └── control_v11p_sd15_scribble_fp16.safetensors
│   │   └── control_v11p_sd15_openpose_fp16.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
```

### 3. 按步骤完成工作流的运行

![ComfyUI 工作流 - Mixing ControlNet 流程图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg)

按照图片中的数字标记，执行以下步骤：

1. 确保`Load Checkpoint`可以加载 **awpainting\_v14.safetensors**
2. 确保`Load VAE`可以加载 **vae-ft-mse-840000-ema-pruned.safetensors**

第一组 ControlNet 使用 Openpose 模型:
3\. 确保`Load ControlNet Model`加载 **control\_v11p\_sd15\_openpose\_fp16.safetensors**
4\. 在`Load Image`中点击`Upload` 上传之前提供的 pose 图片

第二组 ControlNet 使用 Scribble 模型:
5\. 确保`Load ControlNet Model`加载 **control\_v11p\_sd15\_scribble\_fp16.safetensors**
6\. 在`Load Image`中点击`Upload` 上传之前提供的 scribble 图片
7\. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行图片的生成

## 工作流讲解

#### 强度平衡

当控制图像不同区域时，强度参数的平衡尤为重要：

* 如果一个区域的 ControlNet 强度明显高于另一个，可能导致该区域的控制效果过强而抑制另一区域
* 推荐为不同区域的 ControlNet 设置相似的强度值，例如都设为 1.0

#### 提示词技巧

在区域分治混合中，提示词需要同时包含两个区域的描述：

```
"A woman in red dress, a cat riding a scooter, detailed background, high quality"
```

这样的提示词同时涵盖了人物和猫咪滑板车，确保模型能够同时关注两个控制区域。

## 同一主体多维控制的混合应用

除了本例展示的区域分治混合外，另一种常见的混合方式是对同一主体进行多维控制。例如：

* **Pose + Depth**：控制人物姿势及空间感
* **Pose + Canny**：控制人物姿势及边缘细节
* **Pose + Reference**：控制人物姿势但参考特定风格

在这种应用中，多个 ControlNet 的参考图应该对准同一主体，并调整各自的强度确保适当平衡。

通过组合不同类型的 ControlNet 并指定其控制区域，你可以对画面元素进行精确控制。


# ComfyUI Pose ControlNet 使用示例
Source: https://docs.comfy.org/zh-CN/tutorials/controlnet/pose-controlnet-2-pass

本篇将引导了解基础的 Pose ControlNet，并通过二次图生图的方式，在 ComfyUI 中完成大尺寸的图像生成

## OpenPose 简介

[OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) 是由卡耐基梅隆大学（CMU）开发的开源实时多人姿态估计系统，是计算机视觉领域的重要技术突破。该系统能够同时检测图像中多个人的：

* **人体骨架**：18个关键点，包括头部、肩膀、手肘、手腕、髋部、膝盖和脚踝等
* **面部表情**：70个面部关键点，用于捕捉微表情和面部轮廓
* **手部细节**：21个手部关键点，精确表达手指姿势和手势
* **脚部姿态**：6个脚部关键点，记录站立姿势和动作细节

![OpenPose 示例](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/openpose_example.jpg)

在 AI 图像生成领域，OpenPose 生成的骨骼结构图作为 ControlNet 的条件输入，能够精确控制生成人物的姿势、动作和表情，让我们能够按照预期的姿态和动作生成逼真的人物图像，极大提高了 AI 生成内容的可控性和实用价值。
特别针对早期 Stable diffusion 1.5 系列的模型，通过 OpenPose 生成的骨骼图，可以有效避免人物动作、肢体、表情畸变的问题。

## ComfyUI 2 Pass Pose ControlNet 使用示例

### 1. Pose ControlNet 工作流素材

请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流

![ComfyUI 工作流 - Pose ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass.png)

<Tip>
  Metadata 中包含工作流 json 的图片可直接拖入 ComfyUI 或使用菜单 `Workflows` -> `Open（ctrl+o）` 来加载对应的工作流。
  该图片已包含对应模型的下载链接，直接拖入 ComfyUI 将会自动提示下载。
</Tip>

请下载下面的图片，我们将会将它作为输入

![ComfyUI Pose 输入图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass_input.png)

### 2. 手动模型安装

<Note>
  如果你网络无法顺利完成对应模型的自动下载，请尝试手动下载下面的模型，并放置到指定目录中
</Note>

* [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
* [majicmixRealistic\_v7.safetensors](https://civitai.com/api/download/models/176425?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16)
* [japaneseStyleRealistic\_v20.safetensors](https://civitai.com/api/download/models/85426?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── majicmixRealistic_v7.safetensors
│   │   └── japaneseStyleRealistic_v20.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
│   └── controlnet/
│       └── control_v11p_sd15_openpose_fp16.safetensors
```

### 3. 按步骤完成工作流的运行

![ComfyUI 工作流 - Pose ControlNet 流程图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg)

按照图片中的数字标记，执行以下步骤：

1. 确保`Load Checkpoint`可以加载 **majicmixRealistic\_v7.safetensors**
2. 确保`Load VAE`可以加载 **vae-ft-mse-840000-ema-pruned.safetensors**
3. 确保`Load ControlNet Model`可以加载 **control\_v11p\_sd15\_openpose\_fp16.safetensors**
4. 在`Load Image`节点中点击选择按钮，上传之前提供的姿态输入图片，或者使用你自己的OpenPose骨骼图
5. 确保`Load Checkpoint`可以加载 **japaneseStyleRealistic\_v20.safetensors**
6. 点击`Queue`按钮或使用快捷键`Ctrl(cmd) + Enter(回车)`来执行图片的生成

## Pose ControlNet 二次图生图工作流讲解

本工作流采用二次图生图（2-pass）的方式，将图像生成分为两个阶段：

### 第一阶段：基础姿态图像生成

在第一阶段，使用**majicmixRealistic\_v7**模型结合Pose ControlNet生成初步的人物姿态图像：

1. 首先通过`Load Checkpoint`加载majicmixRealistic\_v7模型
2. 通过`Load ControlNet Model`加载姿态控制模型
3. 输入的姿态图被送入`Apply ControlNet`节点与正向和负向提示词条件结合
4. 第一个`KSampler`节点（通常使用20-30步）生成基础的人物姿态图像
5. 通过`VAE Decode`解码得到第一阶段的像素空间图像

这个阶段主要关注正确的人物姿态、姿势和基本结构，确保生成的人物符合输入的骨骼姿态。

### 第二阶段：风格优化与细节增强

在第二阶段，将第一阶段的输出图像作为参考，使用**japaneseStyleRealistic\_v20**模型进行风格化和细节增强：

1. 第一阶段生成的图像通过`Upscale latent`节点创建的更大分辨率的潜在空间
2. 第二个`Load Checkpoint`加载japaneseStyleRealistic\_v20模型，这个模型专注于细节和风格
3. 第二个`KSampler`节点使用较低的`denoise`强度（通常0.4-0.6）进行细化，保留第一阶段的基础结构
4. 最终通过第二个`VAE Decode`和`Save Image`节点输出更高质量、更大分辨率的图像

这个阶段主要关注风格统一性、细节丰富度和提升整体画面质量。

## 二次图生图的优势

与单次生成相比，二次图生图方法具有以下优势：

1. **更高分辨率**：通过二次处理可以生成超出单次生成能力的高分辨率图像
2. **风格混合**：可以结合不同模型的优势，如第一阶段使用写实模型，第二阶段使用风格化模型
3. **更好的细节**：第二阶段可以专注于优化细节，而不必担心整体结构
4. **精确控制**：姿态控制在第一阶段完成后，第二阶段可以专注于风格和细节的完善
5. **降低GPU负担**：分两次生成可以在有限的GPU资源下生成高质量大图

<Tip>
  如需了解更多关于混合多个ControlNet的技巧，请参考[混合ControlNet模型](/zh-CN/tutorials/controlnet/mixing-controlnets.mdx)教程。
</Tip>


# ComfyUI Flux.1 ControlNet 示例
Source: https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-controlnet

本文将使用 Flux.1 ControlNet 来完成 ControlNet 的工作流示例。

![Flux.1 Canny Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-canny-controlnet.png)
![Flux.1 Depth Controlnet](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-1-depth-controlnet.png)

## FLUX.1 ControlNet 模型介绍

FLUX.1 Canny 和 Depth 是由 [Black Forest Labs](https://blackforestlabs.ai/) 推出的 ​[FLUX.1 Tools 套件](https://blackforestlabs.ai/flux-1-tools/) 中的两个强大模型。这套工具旨在为 FLUX.1 添加控制和引导能力，使用户能够修改和重新创建真实或生成的图像。

**FLUX.1-Depth-dev** 和 **FLUX.1-Canny-dev** 都是 12B 参数的 Rectified Flow Transformer 模型，能够基于文本描述生成图像，同时保持与输入图像的一致性。其中 Depth 版本通过深度图提取技术来维持源图像的空间结构，而 Canny 版本则利用边缘检测技术来保持源图像的结构特征，使得用户可以根据不同需求选择合适的控制方式。

这两个模型都具有以下特点：

* 顶级的输出质量和细节表现
* 出色的提示遵循能力，同时保持源图像的结构布局
* 使用引导蒸馏技术训练，提高效率
* 开放权重供社区研究使用
* 提供 API 接口（pro 版）和开源权重（dev 版）

此外，Black Forest Labs 还提供了从完整模型中提取的 **FLUX.1-Depth-dev-lora** 和 **FLUX.1-Canny-dev-lora** 适配器版本，它们可以应用于 FLUX.1 \[dev] 基础模型，以较小的文件体积提供类似的功能，特别适合资源受限的环境。

本文将以分别以完整版本的 **FLUX.1-Canny-dev** 和  **FLUX.1-Depth-dev-lora** 为例，完成ComfyUI 中 Flux  ControlNet 的工作流示例。

<Tip>
  Metadata 中包含工作流 json 的图片可直接拖入 ComfyUI 或使用菜单 `Workflows` -> `Open（ctrl+o）` 来加载对应的工作流。
  本篇示例中的图片包含对应模型的下载链接，直接拖入 ComfyUI 将会自动提示下载。

  对于图像预处理器，你可以使用以下自定义节点来完成图像的预处理，在本示例中，我们将提供处理过的图片作为输入。

  * [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
  * [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)
</Tip>

## FLUX.1-Canny-dev 完整版工作流

### 1. 工作流及相关素材

请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流

![ComfyUI 工作流 - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev.png)

请下载下面的图片，我们将使用它来作为输入图片

![ComfyUI Flux.1 Canny Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev-input.png)

### 2. 手动模型下载

<Note>
  如果你之前使用过[完整版本的 Flux 相关工作流](/zh-CN/tutorials/flux/flux-1-text-to-image)，那么你仅需要下载 **flux1-canny-dev.safetensors** 这个模型文件。
  由于你需要先同意 [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev) 的协议，所以请访问 [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev) 页面，确保你参照下图同意了对应的协议。
  ![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux1_canny_dev_agreement.jpg)
</Note>

完整模型列表：

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-canny-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/resolve/main/flux1-canny-dev.safetensors?download=true) （请确保你已经同意了对应 repo 的协议）

文件保存位置：

```
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-canny-dev.safetensors
```

### 3. 按步骤完成工作流的运行

![ComfyUI Flux.1 Canny Controlnet 步骤流程](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg)

1. 确保在`Load VAE`中加载了`ae.safetensors`
2. 确保在`Load Diffusion Model`加载了`flux1-canny-dev.safetensors`
3. 确保在`DualCLIPLoader`中下面的模型已加载：
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
4. 在`Load Image`节点中上传了文档中提供的输入图片
5. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流

### 4. 开始你的尝试

尝试使用[FLUX.1-Depth-dev](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev) 模型完成 Depth 版本的工作流

你可以使用下面的图片作为输入
![ComfyUI 室内深度图](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png)

或者借助下面自定义节点中完成图像预处理:

* [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
* [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## FLUX.1-Depth-dev-lora 工作流

LoRA 版本的工作流是在完整版本的基础上，添加了 LoRA 模型,相对于[完整版本的 Flux 工作流](/zh-CN/tutorials/flux/flux-1-text-to-image),增加了对应 LoRA 模型的加载使用节点。

### 1. 工作流及相关素材

请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流

![ComfyUI 工作流 - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora.png)

请下载下面的图片，我们将使用它来作为输入图片

![ComfyUI Flux.1 Depth Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora-input.png)

### 2. 手动模型下载

<Tip>
  如果你之前使用过[完整版本的 Flux 相关工作流](/zh-CN/tutorials/flux/flux-1-text-to-image)，那么你仅需要下载 **flux1-depth-dev-lora.safetensors** 这个模型文件。
</Tip>

完整模型列表：

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors?download=true)
* [flux1-depth-dev-lora.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora/resolve/main/flux1-depth-dev-lora.safetensors?download=true)

文件保存位置：

```
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   ├── diffusion_models/
│   │   └── flux1-dev.safetensors
│   └── loras/
│       └── flux1-depth-dev-lora.safetensors
```

### 3. 按步骤完成工作流的运行

![ComfyUI Flux.1 Depth Controlnet 步骤流程](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg)

1. 确保在`Load Diffusion Model`加载了`flux1-dev.safetensors`
2. 确保在`LoraLoaderModelOnly`中加载了`flux1-depth-dev-lora.safetensors`
3. 确保在`DualCLIPLoader`中下面的模型已加载：
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
4. 在`Load Image`节点中上传了文档中提供的输入图片
5. 确保在`Load VAE`中加载了`ae.safetensors`
6. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流

### 4. 开始你的尝试

尝试使用[FLUX.1-Canny-dev-lora](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora) 模型完成 Canny 版本的工作流

借助 [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet) 或者 [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux) 完成图像预处理

## 社区版本 Flux Controlnets

XLab 和 InstantX + Shakker Labs 已经为 Flux 发布了 Controlnet。

**InstantX:**

* [FLUX.1-dev-Controlnet-Canny](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Canny/blob/main/diffusion_pytorch_model.safetensors)
* [FLUX.1-dev-ControlNet-Depth](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Depth/blob/main/diffusion_pytorch_model.safetensors)
* [FLUX.1-dev-ControlNet-Union-Pro](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/blob/main/diffusion_pytorch_model.safetensors)

**XLab**: [flux-controlnet-collections](https://huggingface.co/XLabs-AI/flux-controlnet-collections)

将这些文件放在 `ComfyUI/models/controlnet` 目录下。

你可以访问[Flux Controlnet 示例](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/flux_controlnet_example.png)来获取对应工作流图片，并使用[这里](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/girl_in_field.png)的图片作为输入图片。


# ComfyUI Flux.1 fill dev 示例
Source: https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-fill-dev

本文将使用Flux.1 fill dev 来完成 Inpainting 和 Outpainting 的工作流示例。

![Flux.1 fill dev](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux-fill-dev-demo.jpeg)

## Flux.1 fill dev 模型介绍

Flux.1 fill dev 是 [Black Forest Labs](https://blackforestlabs.ai/) 推出的 ​[FLUX.1 Tools 套件](https://blackforestlabs.ai/flux-1-tools/) 中的核心工具之一，专为图像修复和扩展设计。

Flux.1 fill dev 的核心特点：

* 强大的图像重绘(Inpainting)和扩绘(Outpainting)能力，生成效果仅次于商业版的 FLUX.1 Fill \[pro]。
* 出色的提示词理解和跟随能力，能够精确捕捉用户意图并与原图保持高度一致性。
* 采用先进的引导蒸馏训练技术，使模型在保持高质量输出的同时更加高效。
* 友好的许可条款，生成的输出可用于个人、科学和商业目的，具体请参见 [FLUX.1 \[dev\] 非商业许可证](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md)。

模型开源地址：[FLUX.1 \[dev\]](https://huggingface.co/black-forest-labs/FLUX.1-dev)

本文将基于 Flux.1 fill dev 模型来完成 Inpainting 和 Outpainting 的工作流，
如果你不太了解 Inpainting 和 Outpainting 的工作流可以参考 [ComfyUI 布局重绘示例](/zh-CN/tutorials/basic/inpaint) 和 [ComfyUI 扩图示例](/zh-CN/tutorials/basic/outpaint)，部分的相关说明。

## Flux.1 Fill dev 工作流模型安装

在开始之前，让我们先完成 Flux.1 Fill dev 模型文件的安装， inpainting 和 outpainting 的工作流中会使用完全相同的模型文件，如果你之前使用过完整版本的 [Flux.1 文生图工作流](/zh-CN/tutorials/flux/flux-1-text-to-image)，那么在这个部分你仅需要下载 **flux1-fill-dev.safetensors** 这个模型文件。

不过由于下载对应模型需要同意对应的使用协议，所以请访问 [black-forest-labs/FLUX.1-Fill-dev](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev)页面，确保你参照下图同意了对应的协议。
![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux1_fill_dev_agreement.jpg)

完整模型列表：

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-fill-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/resolve/main/flux1-fill-dev.safetensors?download=true)

文件保存位置：

```
ComfyUI/
├── models/
│   ├── text_encoders/
│   │    ├── clip_l.safetensors
│   │    └── t5xxl_fp16.safetensors
│   ├── vae/
│   │    └── ae.safetensors
│   └── diffusion_models/
│        └── flux1-fill-dev.safetensors
```

## Flux.1 Fill dev inpainting 工作流

### 1. Inpainting 工作流及相关素材

请下载下面的图片，并拖入 ComfyUI 以加载对应的工作流
![ComfyUI Flux.1 inpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint.png)

请下载下面的图片，我们将使用它来作为输入图片
![ComfyUI Flux.1 inpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input.png)

<Note>
  对应的图片已经包含 alpha 通道，所以你不需要额外进行蒙版的绘制, 如果你想要自己进行蒙版的绘制请[点击这里](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input_original.png)获取不带蒙版的版本，并参考 [ComfyUI 布局重绘示例](/zh-CN/tutorials/basic/inpaint) 中的 MaskEditor 的使用部分来了解如何在`Load Image`节点中绘制蒙版。
</Note>

### 2. 参照图片序号检查完成工作流运行

![ComfyUI Flux.1 Fill dev Inpainting 工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_inpaint.jpg)

1. 确保在`Load Diffusion Model`节点加载了`flux1-fill-dev.safetensors`
2. 确保在`DualCLIPLoader`节点中下面的模型已加载：
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
3. 确保在`Load VAE`节点中加载了`ae.safetensors`
4. 在`Load Image`节点中上传了文档中提供的输入图片，如果你使用的是不带蒙版的版本，记得使用遮罩编辑器完成蒙版的绘制
5. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流

## Flux.1 Fill dev Outpainting 工作流

### 1. Outpainting 工作流

请下载下面的图片，并拖入 ComfyUI 以加载对应的工作流
![ComfyUI Flux.1 outpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint.png)

请下载下面的图片，我们将使用它来作为输入图片
![ComfyUI Flux.1 outpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint_input.png)

### 2. 参照图片序号检查完成工作流运行

![ComfyUI Flux.1 Fill dev Outpainting 工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_outpaint.jpg)

1. 确保在`Load Diffusion Model`节点加载了`flux1-fill-dev.safetensors`
2. 确保在`DualCLIPLoader`节点中下面的模型已加载：
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
3. 确保在`Load VAE`节点中加载了`ae.safetensors`
4. 在`Load Image`节点中上传了文档中提供的输入图片
5. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流


# ComfyUI Flux 文生图工作示例
Source: https://docs.comfy.org/zh-CN/tutorials/flux/flux-1-text-to-image

本文将简要介绍 Flux 绘图模型，并指导使用 Flux 模型进行文生图的示例包括原始完整版本和 FP8 Checkpoint 版本。

![Flux](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_example.png)
Flux 是目前最大的开源AI绘画模型之一，拥有 12B 参数，原始文件大小约为23GB。它由 [Black Forest Labs](https://blackforestlabs.ai/) 开发，该团队由前 Stable Diffusion 团队成员创立。
Flux 以其卓越的画面质量和灵活性而闻名，能够生成高质量、多样化的图像。

目前 Flux.1  模型主要有以下几个版本：

* **Flux.1 Pro：** 效果最佳模型，闭源模型，仅支持通过 API 调用。
* **[Flux.1 \[dev\]：](https://huggingface.co/black-forest-labs/FLUX.1-dev)** 开源但仅限非商业使用，从 Pro 版本蒸馏而来，效果接近Pro版。
* \*\*[Flux.1 \[schnell\]：](https://huggingface.co/black-forest-labs/FLUX.1-schnell)\*\*采用 Apache2.0 许可，仅需4步即可生成图像，适合低配置硬件。

**Flux.1 模型特点**

* **混合架构：** 结合了 Transformer 网络和扩散模型的优势，有效整合文本与图像信息，提升生成图像与提示词的对齐精度，对复杂的提示词依旧有非常好的还原能力。
* **参数规模：** Flux 拥有 12B 参数，可捕捉更复杂的模式关系，生成更逼真、多样化的图像。
* **支持多种风格：** 支持多样化的风格，对各种类型的图像都有非常好的表现能力。

在本篇示例中，我们将介绍使用 Flux.1 Dev 和 Flux.1 Schnell 两个版本进行文生图的示例，包括原始完整版模型和 FP8 Checkpoint 简化版本。

* **Flux 完整版本：** 效果最佳，但需要较大的显存资源（推荐16GB以上），需要安装多个模型文件。
* **Flux FP8 Checkpoint：** 仅需一个 fp8 版本的模型，但是质量相对完整版会有所降低。

<Tip>
  本篇示例中的所有工作流图片的 Metadata 中已包含对应模型下载信息，使用以下方式来加载工作流：

  * 直接拖入 ComfyUI
  * 或使用菜单 `Workflows` -> `Open（ctrl+o）`

  如果你使用的不是 Desktop 版本或者部分模型无法顺利下载，请参考手动安装部分保存模型文件到对应的文件夹。
  请在开始之前确保你的 ComfyUI 已更新到最新版本。
</Tip>

## Flux.1 原始版本模型文生图示例

<Note>
  请注意如果你无法下载 [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) 中的模型，请确保你已登录 Huggingface 并同意了对应 Repo 的协议。
  ![Flux Agreement](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flux_agreement.jpg)
</Note>

### Flux.1 Dev 完整版本工作流

#### 1. 工作流文件

请下载下面的图片，并拖入 ComfyUI 中加载工作流。
![Flux Dev 原始版本工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_t5fp16.png)

#### 2. 手动安装模型

<Note>
  * `flux1-dev.safetensors` 文件需要同意 [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) 的协议后才能使用浏览器进行下载。
  * 如果你的显存较低，可以尝试使用 [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true) 来替换 `t5xxl_fp16.safetensors` 文件。
</Note>

请下载下面的模型文件：

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true) 当你的显存大于 32GB 时推荐使用。
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors)

文件保存位置：

```
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-dev.safetensors
```

#### 3. 按步骤检查确保工作流可以正常运行

请参照下面的图片，确保各个模型文件都已经加载完成

![ComfyUI Flux Dev工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg)

1. 确保在`DualCLIPLoader`节点中下面的模型已加载：
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
2. 确保在`Load Diffusion Model`节点加载了`flux1-dev.safetensors`
3. 确保在`Load VAE`节点中加载了`ae.safetensors`
4. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流

<Tip>
  得益于 Flux 良好的提示词遵循能力，我们并不需要任何的负向提示词
</Tip>

### Flux.1 Schnell 完整版本工作流

#### 1. 工作流文件

请下载下面的图片，并拖入 ComfyUI 中加载工作流。

![Flux Schnell 版本工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_t5fp8.png)

#### 2. 手动安装模型

<Note>
  在这个工作流中，只有两个模型文件与 Flux1 Dev 版本的工作流不同,对于 t5xxl 你仍可使用 fp16 版本来获得更好的效果。

  * **t5xxl\_fp16.safetensors** -> **t5xxl\_fp8.safetensors**
  * **flux1-dev.safetensors** -> **flux1-schnell.safetensors**
</Note>

完整模型文件列表：

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-schnell.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors)

文件保存位置：

```
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp8_e4m3fn.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-schnell.safetensors
```

#### 3. 按步骤检查确保工作流可以正常运行

![Flux Schnell 版本工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg)

1. 确保在`DualCLIPLoader`节点中下面的模型已加载：
   * clip\_name1: t5xxl\_fp8\_e4m3fn.safetensors
   * clip\_name2: clip\_l.safetensors
2. 确保在`Load Diffusion Model`节点加载了`flux1-schnell.safetensors`
3. 确保在`Load VAE`节点中加载了`ae.safetensors`
4. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流

## Fp8 Checkpoint 版文生图示例

fp8 版本是对 flux1 原版 fp16 版本的量化版本，在一定程度上这个版本的质量会低于 fp16 版本，但同时它需要的显存也会更少，而且你仅需要安装一个模型文件即可尝试运行。

### Flux.1 Dev fp8 Checkpoint 版工作流

请下载下面的图片，并拖入 ComfyUI 中加载工作流。

![Flux Dev fp8 Checkpoint 版本工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_fp8.png)

请下载 [flux1-dev-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors?download=true)并保存至 `ComfyUI/models/Checkpoints/` 目录下。

确保对应的 `Load Checkpoint` 节点加载了 `flux1-dev-fp8.safetensors`，即可测试运行。

### Flux.1 Schnell fp8 Checkpoint 版工作流

请下载下面的图片，并拖入 ComfyUI 中加载工作流。

![Flux Schnell fp8 Checkpoint 版本工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_fp8.png)

请下载[flux1-schnell-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-schnell/resolve/main/flux1-schnell-fp8.safetensors?download=true)并保存至 `ComfyUI/models/Checkpoints/` 目录下。

确保对应的 `Load Checkpoint` 节点加载了 `flux1-schnell-fp8.safetensors`，即可测试运行。


# ComfyUI 混元视频示例
Source: https://docs.comfy.org/zh-CN/tutorials/video/hunyuan-video

本文介绍了如何在 ComfyUI 中完成混元文生视频及图生视频的工作流

<video controls className="w-full aspect-video" src="https://github.com/user-attachments/assets/442afb73-3092-454f-bc46-02361c285930" />

混元视频（Hunyuan Video）系列是是[腾讯](https://huggingface.co/tencent)研发并开源的，该模型以混合架构为核心，支持[文本生成视频](https://github.com/Tencent/HunyuanVideo)
和[图生成视频](https://github.com/Tencent/HunyuanVideo-I2V)，参数规模达 13B。

技术特点：

* **核心架构：** 采用类似Sora的DiT（Diffusion Transformer）架构，有效融合了文本、图像和动作信息，提高了生成视频帧之间的一致性、质量和对齐度，通过统一的全注意力机制实现多视角镜头切换，确保主体一致性。
* **3D VAE：** 定义的 3D VAE 将视频压缩到紧凑的潜空间，同时压缩视频，使得图生视频的生成更加高效。
* **卓越的图像-视频-文本对齐：** 使用 MLLM 文本编码器，在图像和视频生成中表现出色，能够更好地遵循文本指令，捕捉细节，并进行复杂推理。

你可以在[混元视频](https://github.com/Tencent/HunyuanVideo) 和[混元视频-I2V](https://github.com/Tencent/HunyuanVideo-I2V) 了解到更多开源信息。

本篇指南将引导你完成在 ComfyUI 中 **文生视频** 和 **图生视频** 的视频生成。

<Tip>
  本篇教程中的工作流图片的 Metadata 中已包含对应模型下载信息，直接拖入 ComfyUI 或使用菜单 `Workflows` -> `Open（ctrl+o）` 来加载对应的工作流，会提示完成对应的模型下载。

  另外在本篇指南中也提供了对应的模型地址 ，如果自动下载无法完成或者你使用的不是 Desktop 版本，请尝试手动完成模型的下载。

  所有模型保存在[这里](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/tree/main/split_files)可以下载
</Tip>

## 工作流共用模型

在文生视频和图生视频的工作流中下面的这些模型是共有的，请完成下载并保存到指定目录中

* [clip\_l.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors?download=true)
* [llava\_llama3\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors?download=true)
* [hunyuan\_video\_vae\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors?download=true)

保存位置：

```
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── llava_llama3_fp8_scaled.safetensors
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors
```

## 混元文生视频工作流

混元文生视频开源于 2024 年 12 月，支持通过自然语言描述生成 5 秒的短视频，支持中英文输入。

### 1. 文生视频相关工作流

请保存下面的图片，并拖入 ComfyUI 以加载工作流
![ComfyUI 工作流 - 混元文生视频](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/t2v/kitchen.webp)

### 2. 混元文生图模型

请下载 [hunyuan\_video\_t2v\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_t2v_720p_bf16.safetensors?download=true) 并保存至 `ComfyUI/models/diffusion_models` 文件夹中

确保包括共用模型文件夹有以下完整的模型文件：

```
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                       // 共用模型
│   │   └── llava_llama3_fp8_scaled.safetensors      // 共用模型
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors       // 共用模型
│   └── diffusion_models/
│       └── hunyuan_video_t2v_720p_bf16.safetensors  // T2V 模型
```

### 3. 按步骤完成工作流的运行

![ComfyUI 混元视频 T2V 工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg)

1. 确保在`DualCLIPLoader`中下面的模型已加载：
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. 确保在`Load Diffusion Model`加载了`hunyuan_video_t2v_720p_bf16.safetensors`
3. 确保在`Load VAE`中加载了`hunyuan_video_vae_bf16.safetensors`
4. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流

<Tip>
  `EmptyHunyuanLatentVideo` 节点的 `length` 设置为 1 时，该模型可以生成静态图像。
</Tip>

## 混元图生视频工作流

混元图生视频模型开源于2025年3月6日，基于 HunyuanVideo 框架，支持将静态图像转化为流畅的高质量视频，同时开放了 LoRA 训练代码，支持定制特殊视频效果如：头发生长、物体变形等等。

目前混元图生视频模型分为两个版本：

* v1 “concat” : 视频的运动流畅性较好，但比较少遵循图像引导
* v2 “replace”: 在v1 更新后的次日更新的版本，图像的引导性较好，但相对于 V1 版本似乎不那么有活力

<div class="flex justify-between">
  <div class="text-center">
    <p>v1 “concat”</p>

    <img src="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video.webp" alt="HunyuanVideo v1" />
  </div>

  <div class="text-center">
    <p>v2 “replace”</p>

    <img src="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video_v2.webp" alt="HunyuanVideo v2" />
  </div>
</div>

### v1 及 v2 版本共用的模型

请下载下面的文件，并保存到 `ComfyUI/models/clip_vision` 目录中

* [llava\_llama3\_vision.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors?download=true)

### v1 “concat” 图生视频工作流

#### 1. 工作流及相关素材

请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流
![ComfyUI 工作流 - 混元图生视频v1](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v1_robot.webp)

请下载下面的图片，我们将使用它作为图生视频的起始帧
![起始帧](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/hunyuan-video/i2v/robot-ballet.png)

#### 2. v1 版本模型

* [hunyuan\_video\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_image_to_video_720p_bf16.safetensors?download=true)

确保包括共用模型文件夹有以下完整的模型文件：

```
ComfyUI/
├── models/
│   ├── clip_vision/
│   │   └── llava_llama3_vision.safetensors                     // I2V 共用模型
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                                  //  共用模型
│   │   └── llava_llama3_fp8_scaled.safetensors                 //  共用模型
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors                  // 共用模型
│   └── diffusion_models/
│       └── hunyuan_video_image_to_video_720p_bf16.safetensors  // I2V v1 "concat" 版本模型
```

#### 3. 按步骤完成工作流

![ComfyUI 混元视频I2V v1 工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg)

1. 确保 `DualCLIPLoader` 中下面的模型已加载：
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. 确保  `Load CLIP Vision` 加载了 `llava_llama3_vision.safetensors`
3. 请在 `Load Image Model` 加载了 `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. 确保 `Load VAE` 中加载了 `hunyuan_video_vae_bf16.safetensors`
5. 确保 `Load Diffusion Model` 中加载了 `hunyuan_video_image_to_video_720p_bf16.safetensors`
6. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流

### v2 “replace” 图生视频工作流

v2 版本的工作流与 v1 版本的工作流基本相同，你只需要下载一个 **replace** 的模型，然后在 `Load Diffusion Model` 中使用即可。

#### 1. 工作流及相关素材

请下载下面的工作流图片,并拖入 ComfyUI 以加载工作流

![ComfyUI 工作流 - 混元图生视频v1](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v2_fennec_gril.webp)

请下载下面的图片，我们将使用它作为图生视频的起始帧
![起始帧](https://comfyanonymous.github.io/ComfyUI_examples/flux/flux_dev_example.png)

#### 2. v2 版本模型

* [hunyuan\_video\_v2\_replace\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors?download=true)

确保包括共用模型文件夹有以下完整的模型文件：

```
ComfyUI/
├── models/
│   ├── clip_vision/
│   │   └── llava_llama3_vision.safetensors                                // I2V 共用模型
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                                             //  共用模型
│   │   └── llava_llama3_fp8_scaled.safetensors                            //  共用模型
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors                             //  共用模型
│   └── diffusion_models/
│       └── hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors  // V2 "replace" 版本模型
```

#### 3. 按步骤完成工作流

![ComfyUI 混元视频I2V v2 工作流](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg)

1. 确保 `DualCLIPLoader` 中下面的模型已加载：
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. 确保 `Load CLIP Vision` 加载了 `llava_llama3_vision.safetensors`
3. 请在 `Load Image Model` 加载了 `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. 确保 `Load VAE` 中加载了 `hunyuan_video_vae_bf16.safetensors`
5. 确保 `Load Diffusion Model` 中加载了 `hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors`
6. 点击 `Queue` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来运行工作流

## 开始你的尝试

下面是我们提供了一些示例图片和对应的提示词，你可以基于这些内容，进行修改，创作出属于你自己的视频。

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png)

```
Futuristic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene
```

***

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/samurai.png)

```
Samurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic
```

***

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/a_flying_car.png)

```
flying car fastly moving and flying through the city
```

***

![example](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png)

```
cyberpunk car race in night city, dynamic, super fast, fast shot
```


# LTX-Video
Source: https://docs.comfy.org/zh-CN/tutorials/video/ltxv

快速生成可控视频

<Tip>
  将任意视频直接拖入 ComfyUI 即可开始使用
</Tip>

## 快速入门

[LTX-Video](https://huggingface.co/Lightricks/LTX-Video) 是 Lightricks 开发的高效视频生成模型。

使用该模型的关键是提供详细的长描述提示词。

请下载 [ltx-video-2b-v0.9.5.safetensors](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors?download=true) 文件并放入 `ComfyUI/models/checkpoints` 目录。

若尚未下载 [t5xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/mochi_preview_repackaged/resolve/main/split_files/text_encoders/t5xxl_fp16.safetensors?download=true) 文件，请将其放入 `ComfyUI/models/text_encoders` 目录。

## 多帧控制

通过系列图像控制视频生成。可下载输入图像：[起始帧](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house1.png) 和 [结束帧](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house2.png)。

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/workflow.webp" alt="LTX-Video 多帧控制工作流" />

## 图生视频

通过首帧图像控制视频生成：[示例首帧](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/girl1.png)。

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/workflow.webp" alt="LTX-Video 图生视频工作流" />

## 文生视频

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/t2v.webp" alt="LTX-Video 文生视频工作流" />


# ComfyUI Wan2.1 Fun Control 视频示例
Source: https://docs.comfy.org/zh-CN/tutorials/video/wan/fun-control

本文介绍了如何在 ComfyUI 中完成 Wan2.1 Fun Control 使用控制视频来完成视频生成的示例

## 关于 Wan2.1-Fun-Control

**Wan2.1-Fun-Control** 是阿里团队推出的开源视频生成与控制项目，通过引入创新性的控制代码（Control Codes）机制，结合深度学习和多模态条件输入，能够生成高质量且符合预设控制条件的视频。该项目专注于通过多模态控制条件实现对生成视频内容的精准引导。

目前 Fun Control 模型支持多种控制条件，包括 **Canny（线稿）**、**Depth（深度）**、**OpenPose（人体姿势）**、**MLSD（几何边缘）** 等，同时支持使用 **轨迹控制**。
模型还支持多分辨率视频预测，分辨率可选 512、768 和 1024，帧率为每秒 16 帧，最长可生成 81 帧（约 5 秒）的视频。

模型版本方面：

* **1.3B** 轻量版：适合本地部署和快速推理，**对显存要求较低**
* **14B** 高性能版：模型体积达 32GB+，效果更优但 **需高显存支持**

下面是相关代码仓库的示例

* [Wan2.1-Fun-1.3B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control)
* [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control)
* 代码仓库：[VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

**目前 ComfyUI 已原生支持了 Wan2.1 Fun Control 模型** ，在开始本篇教程前，请更新你的 ComfyUI 保证你的版本在[这个提交](https://github.com/comfyanonymous/ComfyUI/commit/3661c833bcc41b788a7c9f0e7bc48524f8ee5f82)版本之后

在本篇指南中我们将提供两个工作流：

* 仅使用原生的 Comfy Core 节点的工作流
* 使用自定义节点的工作流

<Tip>
  由于目前原生节点在视频支持方面有所欠缺，完全使用原生节点的工作流是为了能保证在使用过程中用户不需要安装自定义节点就可以完成对应的工作流, 但在视频相关的生成中，我们发现现阶段很难在不使用自定义节点的情况下同时提供良好的使用体验，所以在本篇指南中我们提供了两个版本的工作流。
</Tip>

## 相关模型安装

这些模型你仅需要安装一次，另外在对应的工作流图片中也包含了模型下载信息，你可以选择你喜欢的方式下载模型。

下面的模型你可以在 [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) 和 [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334) 找到

点击对应链接进行下载，如果你之前使用过 Wan 相关的工作流，那么你仅需要下载 **Diffusino models**

**Diffusion models** 选择 1.3B 或 14B, 14B 的文件体积更大（32GB）但是对于运行显存要求也较高，

* [wan2.1\_fun\_control\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_control_1.3B_bf16.safetensors?download=true)
* [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/blob/main/diffusion_pytorch_model.safetensors?download=true)： 建议下载后重命名为 `Wan2.1-Fun-14B-Control.safetensors`

**Text encoders** 选择下面两个模型中的一个，fp16 精度体积较大对性能要求高

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

文件保存位置

```
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 diffusion_models/
│   │   └── wan2.1_fun_control_1.3B_bf16.safetensors
│   ├── 📂 text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── 📂 vae/
│   │   └── wan_2.1_vae.safetensors
│   └── 📂 clip_vision/
│       └──  clip_vision_h.safetensors                 
```

## ComfyUI 原生工作流

在此工作流中，我们使用转换成 WebP 格式的视频，这是因为目前`Load Image` 节点还不支持 mp4 格式的视频，另外我们使用 Canny Edge 来对原始的视频进行图像的预处理, 由于经常有用户在安装自定义节点过程中遇到安装失败和环境的问题，所以这一版本的工作流完全使用原生节点来实现，来优先保证体验。

感谢我们强大的 ComfyUI 作者们，他们带来了功能丰富的相关节点，如果你需要直接查看相关版本直接查看[使用自定义节点的工作流](#使用自定义节点的工作流)

### 1. 工作流相关文件下载

#### 1.1 工作流文件

下载下面的图片，并拖入 ComfyUI 中以加载对应的工作流

![Wan2.1 Fun Control 原生工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_native.webp)

#### 1.2 输入图片及视频下载

请下载下面的图片及视频，我们将作为输入。

![输入参考图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_remix.png)

![输入参考视频](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_video.webp)

### 2. 按步骤完成工作流

![Wan2.1 Fun Control 工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_native_flow_diagram.png)

1. 确保 `Load Diffusion Model` 节点加载了 `wan2.1_fun_control_1.3B_bf16.safetensors`
2. 确保 `Load CLIP` 节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. 确保 `Load VAE` 节点加载了 `wan_2.1_vae.safetensors`
4. 确保 `Load CLIP Vision` 节点加载了 `clip_vision_h.safetensors `
5. 在 `Load Image` 节点（已被重命名为`Start_image`） 上传起始帧
6. 在第二个 `Load Image` 节点上传用于控制视频。注意： 目前这个节点还不支持 mp4 只能使用 Webp 视频
7. （可选）修改 Prompt 使用中英文都可以
8. （可选）在 `WanFunControlToVideo` 修改对应视频的尺寸，不要使用过大的尺寸
9. 点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成

### 3. 使用说明

* 由于我们需要和控制视频一致的帧数输入到 `WanFunControlToVideo`  节点，如果对应的帧数数值大于实际的控制视频帧数，将会导致多余的帧不符合控制条件的画面出现，这个问题我们将在[使用自定义节点的工作流](#使用自定义节点的工作流)中解决
* 使用类似 [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) 来实现更丰富的控制

## 使用自定义节点的工作流

我们将需要安装下面两个自定义节点：

* [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
* [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

你可以使用 [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) 安装缺失节点的功能或者参照对应自定义节点包的安装说明来完成对应节点的安装

### 1. 工作流相关文件下载

#### 1.1 工作流文件

下载下面的图片，并拖入 ComfyUI 中以加载对应的工作流

![工作流文件](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.webp)

<Note>
  由于视频文件体积较大，你也可以点击[这里](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.json)下载 Json 格式的工作流文件。
</Note>

#### 1.2 输入图片及视频下载

请下载下面的图片及视频，我们将会用于输入
![输入参考图片](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/02-robot's_eye.png)

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/02-man's_eye.mp4" />

### 2. 按步骤完成工作流

![Wan2.1 Fun Control 使用自定义节点的工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png)

> 模型部分基本是一致的，如果你已经体验过仅使用原生节点的工作流，你可以直接上传对应的图片然后运行即可

1. 确保 `Load Diffusion Model` 节点加载了 `wan2.1_fun_control_1.3B_bf16.safetensors`
2. 确保 `Load CLIP` 节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. 确保 `Load VAE` 节点加载了 `wan_2.1_vae.safetensors`
4. 确保 `Load CLIP Vision` 节点加载了 `clip_vision_h.safetensors `
5. 在 `Load Image` 节点上传起始帧
6. 在 `Load Video(Upload)` 自定义节点上传 mp4 格式视频，请注意对应工作流有对默认的 `frame_load_cap`进行了调整
7. `DWPose Estimator` 处针对当前图像仅使用了 `detect_face` 的选项
8. （可选）修改 Prompt 使用中英文都可以
9. （可选）在 `WanFunControlToVideo` 修改对应视频的尺寸，不要使用过大的尺寸
10. 点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成

### 3. 工作流说明

感谢 ComfyUI 社区作者带来的自定义节点包

* 在这个示例中使用了 `Load Video(Upload)` 来实现对 mp4 视频的支持
* `Load Video(Upload)` 中获取到的 `video_info` 我们得以对输出的视频保持同样的 `fps`
* 你可以替换 `DWPose Estimator` 为 `ComfyUI-comfyui_controlnet_aux` 节点包中的其它预处理器

## 使用技巧

![Apply Multi Control Videos](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/apply_multi_control_videos.jpg)

* 一个有用的技巧是，你可以结合多种图像预处理技术，然后使用 `Image Blend` 节点来实现同时应用多种控制方法的目的。
* 你可以使用 `ComfyUI-VideoHelperSuite` 的 `Video Combine` 节点来实现将对应视频存储为 mp4 格式
* 我们使用 `SaveAnimatedWEBP` 是因为我们目前并不支持在 **mp4** 中嵌入工作流信息, 而且有些自定义节点可能没有考虑工作流嵌入，为了在视频中保存工作流，所以我们选择 `SaveAnimatedWEBP` 节点。
* 不要设置过大的画面尺寸，这可能导致采样过程非常耗时，可以试着先生成小尺寸的图片然后再进行采样放大
* 发挥你的想象力，在这个工作流基础上加上一些文生图或者其它类型的工作流，实现直接从文本到视频生成风格转换
* 在 `WanFunControlToVideo` 节点中，`control_video` 不是必须的，所以有时候你可以不使用控制视频，先生成特别小尺寸的视频比如 320x320，然后使用再把它们作为控制视频输入来获得确定的结果

## 其它 Wan2.1 Fun Control 或者视频相关自定义节点

* [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)
* [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)


# ComfyUI Wan2.1 Fun InP 视频示例
Source: https://docs.comfy.org/zh-CN/tutorials/video/wan/fun-inp

本文介绍了如何在 ComfyUI 中完成 Wan2.1 Fun InP 视频首尾帧视频生成示例

## 关于 Wan2.1-Fun-InP

Wan-Fun InP 是阿里巴巴推出的开源视频生成模型，属于 ​​Wan2.1-Fun​​ 系列的一部分，专注于通过图像生成视频并实现首尾帧控制。

**核心功能**：

* 首尾帧控制：支持输入首帧和尾帧图像，生成中间过渡视频，提升视频连贯性与创意自由度。相比早期社区版本，阿里官方模型的生成效果更稳定且质量显著提升。
* 多分辨率支持：支持生成512×512、768×768、1024×1024等分辨率的视频，适配不同场景需求。

**模型版本方面**：

* 1.3B 轻量版：适合本地部署和快速推理，对显存要求较低
* 14B 高性能版：模型体积达 32GB+，效果更优但需高显存支持

下面是相关模型权重和代码仓库：

* [Wan2.1-Fun-1.3B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Input)
* [Wan2.1-Fun-14B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Input)
* 代码仓库：[VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

<Tip>
  目前 ComfyUI 已原生支持了 Wan2.1 Fun InP 模型，在开始本篇教程前，请更新你的 ComfyUI 保证你的版本在[这个提交](https://github.com/comfyanonymous/ComfyUI/commit/0a1f8869c9998bbfcfeb2e97aa96a6d3e0a2b5df)版本之后
</Tip>

## Wan2.1 Fun Control 工作流

下载下面的图片，并拖入 ComfyUI 中以加载对应的工作流

![工作流文件](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_inp/wan2.1_fun_inp.webp)

### 1. 工作流文件下载

### 2. 手动模型安装

如果对应的自动模型下载无效，请手动进行模型下载，并保存到对应的文件夹

下面的模型你可以在 [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) 和 [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334) 找到

**Diffusion models** 选择 1.3B 或 14B, 14B 的文件体积更大（32GB）但是对于运行显存要求也较高，

* [wan2.1\_fun\_inp\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_inp_1.3B_bf16.safetensors?download=true)
* [Wan2.1-Fun-14B-InP](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP/resolve/main/diffusion_pytorch_model.safetensors?download=true)： 建议下载后重命名为 `Wan2.1-Fun-14B-InP.safetensors`

**Text encoders** 选择下面两个模型中的一个，fp16 精度体积较大对性能要求高

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

文件保存位置

```
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 diffusion_models/
│   │   └── wan2.1_fun_inp_1.3B_bf16.safetensors
│   ├── 📂 text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── 📂 vae/
│   │   └── wan_2.1_vae.safetensors
│   └── 📂 clip_vision/
│       └──  clip_vision_h.safetensors                 
```

### 3. 按步骤完成工作流

![ComfyUI Wan2.1 Fun Control 视频生成工作流步骤图](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/fun_inp_flow_diagram.png)

1. 确保 `Load Diffusion Model` 节点加载了 `wan2.1_fun_inp_1.3B_bf16.safetensors`
2. 确保 `Load CLIP` 节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. 确保 `Load VAE` 节点加载了 `wan_2.1_vae.safetensors`
4. 确保 `Load CLIP Vision` 节点加载了 `clip_vision_h.safetensors `
5. 在 `Load Image` 节点（已被重命名为`Start_image`） 上传起始帧
6. 在第二个 `Load Image` 节点上传用于控制视频。注意： 目前这个节点还不支持 mp4 只能使用 Webp 视频
7. （可选）修改 Prompt 使用中英文都可以
8. （可选）在 `WanFunInpaintToVideo` 修改对应视频的尺寸，不要使用过大的尺寸
9. 点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成

### 4. 工作流说明

<Tip>
  请注意要使用正确的模型，因为 `wan2.1_fun_inp_1.3B_bf16.safetensors` 和  `wan2.1_fun_control_1.3B_bf16.safetensors` 都保存在同一文件夹，同时名称又极为相似，请确保使用了正确的模型。
</Tip>

* 在体验 Wan Fun InP 时，你可能需要频繁修改提示词，从而来确保对应画面的过渡的准确性

## 其它 Wan2.1 Fun Inp 或者视频相关自定义节点

* [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
* [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)
* [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)


# ComfyUI Wan2.1 Video 示例
Source: https://docs.comfy.org/zh-CN/tutorials/video/wan/wan-video

本文介绍了如何在 ComfyUI 中完成 Wan2.1 Video 视频首尾帧视频生成示例

Wan2.1 Video 系列为阿里巴巴于 2025年2月开源的视频生成模型，其开源协议为 [Apache 2.0](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file)，提供 14B（140亿参数）和 1.3B（13亿参数）两个版本，覆盖文生视频（T2V）、图生视频（I2V）等多项任务。
该模型不仅在性能上超越现有开源模型，更重要的是其轻量级版本仅需 8GB 显存即可运行，大大降低了使用门槛。

<video controls>
  <source src="https://github.com/user-attachments/assets/4aca6063-60bf-4953-bfb7-e265053f49ef" type="video/mp4" />
</video>

* [Wan2.1 代码仓库](https://github.com/Wan-Video/Wan2.1)
* [Wan2.1 相关模型仓库](https://huggingface.co/Wan-AI)

## Wan2.1 ComfyUI 原生（native）工作流示例

## 模型安装

本篇指南涉及的所有模型你都可以在[这里](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files)找到, 下面是本篇示例中将会使用到的共用的模型，你可以提前进行下载：

从**Text encoders** 选择一个版本进行下载，

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

文件保存位置

```
ComfyUI/
├── models/
│   ├── diffusion_models/
│   ├── ...                  # 我们在对应的工作流中进行补充说明
│   ├── text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── vae/
│   │   └──  wan_2.1_vae.safetensors
│   └── clip_vision/
│       └──  clip_vision_h.safetensors   
```

<Note>
  对于 diffusion 模型，我们在本篇示例中将使用 fp16 精度的模型，因为我们发现相对于 bf16 的版本 fp16 版本的效果更好，如果你需要其它精度的版本，请访问[这里](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models)进行下载
</Note>

## Wan2.1 文生视频工作流

在开始工作流前请下载 [wan2.1\_t2v\_1.3B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors?download=true)，并保存到 `ComfyUI/models/diffusion_models/` 目录下。

> 如果你需要其它的 t2v 精度版本，请访问[这里](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models)进行下载

### 1. 工作流文件下载

下载下面的文件，并拖入 ComfyUI 以加载对应的工作流

![Wan2.1 文生视频工作流](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_t2v_1.3b.webp)

### 2. 按流程完成工作流运行

![ComfyUI Wan2.1 工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg)

1. 确保`Load Diffusion Model`节点加载了 `wan2.1_t2v_1.3B_fp16.safetensors` 模型
2. 确保`Load CLIP`节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors` 模型
3. 确保`Load VAE`节点加载了 `wan_2.1_vae.safetensors` 模型
4. （可选）可以在`EmptyHunyuanLatentVideo` 节点设置了视频的尺寸，如果有需要你可以修改
5. （可选）如果你需要修改提示词（正向及负向）请在序号`5` 的 `CLIP Text Encoder` 节点中进行修改
6. 点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成

## Wan2.1 图生视频工作流

**由于 Wan Video 将 480P 和 720P 的模型分开** ，所以在本篇中我们将需要分别对两中清晰度的视频做出示例，除了对应模型不同之外，他们还有些许的参数差异

### 480P 版本

#### 1. 工作流及输入图片

下载下面的图片，并拖入 ComfyUI 中来加载对应的工作流
![Wan2.1 图生视频工作流 14B 480P Workflow 输入图片示例](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_480P.webp)

我们将使用下面的图片作为输入：

![Wan2.1 图生视频工作流 14B 480P Workflow 输入图片示例](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/flux_dev_example.png)

#### 2. 模型下载

请下载[wan2.1\_i2v\_480p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp16.safetensors?download=true)，并保存到 `ComfyUI/models/diffusion_models/` 目录下

#### 3. 按步骤完成工作流的运行

![ComfyUI Wan2.1 工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg)

1. 确保`Load Diffusion Model`节点加载了 `wan2.1_i2v_480p_14B_fp16.safetensors` 模型
2. 确保`Load CLIP`节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors` 模型
3. 确保`Load VAE`节点加载了 `wan_2.1_vae.safetensors` 模型
4. 确保`Load CLIP Vision`节点加载了 `clip_vision_h.safetensors` 模型
5. 在`Load Image`节点中上传我们提供的输入图片
6. （可选）在`CLIP Text Encoder`节点中输入你想要生成的视频描述内容，
7. （可选）在`WanImageToVideo` 节点中设置了视频的尺寸，如果有需要你可以修改
8. 点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成

### 720P 版本

#### 1. 工作流及输入图片

下载下面的图片，并拖入 ComfyUI 中来加载对应的工作流
![Wan2.1 图生视频工作流 14B 720P Workflow 输入图片示例](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_720P.webp)

我们将使用下面的图片作为输入：

![Wan2.1 图生视频工作流 14B 720P Workflow 输入图片示例](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/magician.png)

#### 2. 模型下载

请下载[wan2.1\_i2v\_720p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_720p_14B_fp16.safetensors?download=true)，并保存到 `ComfyUI/models/diffusion_models/` 目录下

#### 3. 按步骤完成工作流的运行

![ComfyUI Wan2.1 工作流步骤](https://mintlify.s3.us-west-1.amazonaws.com/dripart/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg)

1. 确保`Load Diffusion Model`节点加载了 `wan2.1_i2v_720p_14B_fp16.safetensors` 模型
2. 确保`Load CLIP`节点加载了 `umt5_xxl_fp8_e4m3fn_scaled.safetensors` 模型
3. 确保`Load VAE`节点加载了 `wan_2.1_vae.safetensors` 模型
4. 确保`Load CLIP Vision`节点加载了 `clip_vision_h.safetensors` 模型
5. 在`Load Image`节点中上传我们提供的输入图片
6. （可选）在`CLIP Text Encoder`节点中输入你想要生成的视频描述内容，
7. （可选）在`WanImageToVideo` 节点中设置了视频的尺寸，如果有需要你可以修改
8. 点击 `Run` 按钮，或者使用快捷键 `Ctrl(cmd) + Enter(回车)` 来执行视频生成


